{"config":{"lang":["en"],"separator":"[\\s\\-\\_,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#neuron-analysis-and-visualization","title":"Neuron Analysis and Visualization","text":"<p>NAVis is a Python library for analysis and visualization of neuron morphology. It stands on the shoulders of the excellent <code>natverse</code> for R.</p> <p>Features - Quickstart - Installation</p> \u276e \u276f"},{"location":"#features","title":"Features","text":"<ul> <li> <p> Polgyglot</p> <p>Support for all kinds of neuron types: skeletons, meshes, dotprops and images.</p> </li> <li> <p> Exploration</p> <p>Designed to let you explore your data interactively from Jupyter notebooks, terminal or via scripts.</p> </li> <li> <p> Analysis</p> <p>Calculate Strahler indices, cable length, volume, tortuosity, NBLAST and many other morphometrics.</p> </li> <li> <p> Visualization</p> <p>Generate beautiful, publication-ready 2D (matplotlib) and 3D (octarine, vispy or plotly) figures.</p> </li> <li> <p> Processing</p> <p>Smoothing, resampling, skeletonization, meshing and more!</p> </li> <li> <p> Fast</p> <p>Uses compiled Rust code under-the-hood and out-of-the-box support for multiprocessing.</p> </li> <li> <p> Clustering</p> <p>Cluster your neurons by e.g. morphology using NBLAST.</p> </li> <li> <p> Transforms</p> <p>Fully featured transform system to move neurons between brain spaces. We support CMTK, Elastix, landmark-based transforms and more!</p> </li> <li> <p> Import/Export</p> <p>Read and write from/to SWC, NRRD, Neuroglancer's precomputed format, OBJ, STL and more!</p> </li> <li> <p> Connected</p> <p>Load neurons straight from Allen's MICrONS datasets, neuromorpho, neuPrint, the H01 dataset or any NeuroGlancer source.</p> </li> <li> <p> Interfaces</p> <p>Load neurons into Blender 3D, simulate neurons and networks using NEURON, or use the R natverse library via <code>rpy2</code>.</p> </li> <li> <p> Extensible</p> <p>Write your own library built on top of NAVis functions. See our ecosystem for examples.</p> </li> </ul> <p>Check out the Tutorials and API reference to see what you can do with NAVis.</p> <p>Need help? Use discussions on Github to ask questions!</p> <p>NAVis is licensed under the GNU GPL v3+ license. The source code is hosted at Github. Feedback, feature requests and bug reports are very welcome and best placed in a Github issue</p>"},{"location":"api/","title":"API Documentation","text":""},{"location":"api/#api-overview","title":"API Overview","text":"<p>NAVis has grown a lot! Last I looked, there were over 120 functions and classes exposed at top level (e.g. <code>navis.plot3d</code>) and easily another 100 available via submodules (e.g. <code>navis.transforms.AffineTransform</code>). This can be a bit daunting at first - especially if you don't exactly know what you are looking for!</p> <p>This document provides a curated, high-level summary. I recommend you either just have a browse, use the search field (upper right) or simply search in page (CONTROL/CMD-F). Failing that, please feel free to open a thread on Github Discussions with your question.</p> <p>This API reference is a more or less complete account of the primary functions:</p> <ol> <li>Neuron- and NeuronList functions and methods</li> <li>Functions for visualization</li> <li>Manipulate or analyze neuron morphology</li> <li>Transforming and mirroring data</li> <li>Analyze connectivity</li> <li>Import/Export</li> <li>Utility functions</li> <li>Which functions work with which neuron types?</li> </ol> <p>In addition NAVis has interfaces to various external datasets and tools:</p> <ul> <li>NEURON simulator</li> <li>Neuromorpho</li> <li>neuPrint</li> <li>InsectBrain DB</li> <li>Blender 3D</li> <li>Cytoscape</li> <li>Allen MICrONS datasets</li> <li>R and the natverse libraries</li> </ul> <p>Most of these functions include examples of how to use them. Click on them to learn more!</p>"},{"location":"api/#neurons-neuronlists","title":"Neurons &amp; NeuronLists","text":"<p><code>TreeNeurons</code>, <code>MeshNeurons</code>, <code>VoxelNeurons</code> and <code>Dotprops</code> are neuron classes. <code>NeuronLists</code> are containers thereof.</p> Class Description <code>navis.TreeNeuron</code> Skeleton representation of a neuron. <code>navis.MeshNeuron</code> Meshes with vertices and faces. <code>navis.VoxelNeuron</code> 3D images (e.g. from confocal stacks). <code>navis.Dotprops</code> Point cloud + vector representations, used for NBLAST. <code>navis.NeuronList</code> Containers for neurons."},{"location":"api/#general-neuron-methods","title":"General Neuron methods","text":"<p>Despite being fundamentally different data types, all neurons share some common methods (i.e. functions) which they inherit from their abstract parent class <code>BaseNeurons</code>.</p> Method Description <code>Neuron.copy()</code> Return a copy of the neuron. <code>Neuron.plot3d()</code> Plot neuron using <code>navis.plot3d</code>. <code>Neuron.plot2d()</code> Plot neuron using <code>navis.plot2d</code>. <code>Neuron.summary()</code> Get a summary of this neuron. <code>Neuron.convert_units()</code> Convert coordinates to different unit. <code>Neuron.map_units()</code> Convert units to match neuron space. <code>Neuron.memory_usage()</code> Return estimated memory usage of this neuron. <p>In addition to methods, neurons also have properties. These properties common to all neurons:</p>"},{"location":"api/#general-neuron-properties","title":"General Neuron properties","text":"Property Description <code>Neuron.connectors</code> Connector table. If none, will return <code>None</code>. <code>Neuron.postsynapses</code> Table with postsynapses (filtered from connectors table). <code>Neuron.presynapses</code> Table with presynapses (filtered from connectors table). <code>Neuron.datatables</code> Names of all DataFrames attached to this neuron. <code>Neuron.id</code> ID of the neuron. <code>Neuron.name</code> Neuron name. <code>Neuron.units</code> Units for coordinate space. <code>Neuron.type</code> Neuron type. <code>Neuron.soma</code> The soma of the neuron (if any). <code>Neuron.bbox</code> Bounding box of neuron. <p>Note</p> <p>Neurons can have the above properties but that is not guaranteed. For example, a neuron might not have any associated synapses in which case <code>Neuron.connectors</code> will be <code>None</code>.</p>"},{"location":"api/#skeletons","title":"Skeletons","text":"<p>A <code>navis.TreeNeuron</code> represents a skeleton. These are class methods available specific for this neuron type. Note that most of them are simply short-hands for the other NAVis functions:</p> Method Description [<code>TreeNeuron.convert_units()</code>][navis.TreeNeuron.convert_units] Convert coordinates to different unit. <code>TreeNeuron.cell_body_fiber()</code> Prune neuron to its cell body fiber. <code>TreeNeuron.downsample()</code> Downsample the neuron by given factor. <code>TreeNeuron.get_graph_nx()</code> Calculate and return networkX representation of neuron. <code>TreeNeuron.get_igraph()</code> Calculate and return iGraph representation of neuron. <code>TreeNeuron.prune_by_longest_neurite()</code> Prune neuron down to the longest neurite. <code>TreeNeuron.prune_by_strahler()</code> Prune neuron based on Strahler order. <code>TreeNeuron.prune_by_volume()</code> Prune neuron by intersection with given volume(s). <code>TreeNeuron.prune_distal_to()</code> Cut off nodes distal to given nodes. <code>TreeNeuron.prune_proximal_to()</code> Remove nodes proximal to given node. Reroots neuron to cut node. <code>TreeNeuron.prune_twigs()</code> Prune terminal twigs under a given size. <code>TreeNeuron.reload()</code> Reload neuron. Must have filepath as <code>.origin</code> as attribute. <code>TreeNeuron.reroot()</code> Reroot neuron to given node ID or node tag. <code>TreeNeuron.resample()</code> Resample neuron to given resolution. <code>TreeNeuron.snap()</code> Snap xyz location(s) to closest node or synapse. <p>In addition, a <code>navis.TreeNeuron</code> has a range of different properties:</p> Method Description <code>TreeNeuron.adjacency_matrix</code> Adjacency matrix of the skeleton. <code>TreeNeuron.cable_length</code> Cable length. <code>TreeNeuron.cycles</code> Cycles in neuron (if any). <code>TreeNeuron.downsample</code> Downsample the neuron by given factor. <code>TreeNeuron.edges</code> Edges between nodes. <code>TreeNeuron.edge_coords</code> Coordinates of edges between nodes. <code>TreeNeuron.igraph</code> iGraph representation of this neuron. <code>TreeNeuron.is_tree</code> Whether neuron is a tree. <code>TreeNeuron.n_branches</code> Number of branch points. <code>TreeNeuron.n_leafs</code> Number of leaf nodes. <code>TreeNeuron.n_skeletons</code> Number of seperate skeletons in this neuron. <code>TreeNeuron.n_trees</code> Count number of connected trees in this neuron. <code>TreeNeuron.nodes</code> Node table. <code>TreeNeuron.root</code> Root node(s). <code>TreeNeuron.sampling_resolution</code> Average cable length between child -&gt; parent nodes. <code>TreeNeuron.segments</code> Neuron broken down into linear segments (see also <code>.small_segments</code>). <code>TreeNeuron.simple</code> Simplified representation consisting only of root, branch points and leafs. <code>TreeNeuron.soma_pos</code> Search for soma and return its position. <code>TreeNeuron.subtrees</code> List of subtrees. Sorted by size as sets of node IDs. <code>TreeNeuron.vertices</code> Vertices of the skeleton. <code>TreeNeuron.volume</code> Radius-based volume."},{"location":"api/#skeleton-utility-functions","title":"Skeleton utility functions","text":"Function Description <code>navis.rewire_skeleton()</code> Rewire neuron from graph. <code>navis.insert_nodes()</code> Insert new nodes between existing nodes. <code>navis.remove_nodes()</code> Drop nodes from neuron without disconnecting it. <code>navis.graph.simplify_graph()</code> Simplify skeleton graph (networkX or igraph). <code>navis.graph.skeleton_adjacency_matrix()</code> Generate adjacency matrix for a skeleton."},{"location":"api/#mesh-neurons","title":"Mesh neurons","text":"<p>Properties specific to <code>navis.MeshNeuron</code>:</p> Property Description <code>MeshNeuron.faces</code> Faces making up the neuron. <code>MeshNeuron.vertices</code> Vertices making up the neuron. <code>MeshNeuron.trimesh</code> Trimesh representation of the neuron. <code>MeshNeuron.volume</code> Volume of the neuron. <code>MeshNeuron.sampling_resolution</code> Average distance between vertices. <p>Methods specific to <code>navis.MeshNeuron</code>:</p> Method Description <code>MeshNeuron.skeletonize()</code> Skeletonize mesh. <code>MeshNeuron.snap()</code> Snap xyz location(s) to closest vertex or synapse. <code>MeshNeuron.validate()</code> Use trimesh to try and fix some common mesh issues."},{"location":"api/#voxel-neurons","title":"Voxel neurons","text":"<p>VoxelNeurons (e.g. from confocal image stacks) are a relatively recet addition to NAVis and the interface might still change.</p> <p>These are methods and properties specific to VoxelNeurons:</p> Property Description <code>VoxelNeuron.density</code> Fraction of filled voxels. <code>VoxelNeuron.grid</code> Voxel grid representation. <code>VoxelNeuron.max</code> Maximum value (excludes zeros). <code>VoxelNeuron.min</code> Minimum value (excludes zeros). <code>VoxelNeuron.nnz</code> Number of non-zero voxels. <code>VoxelNeuron.offset</code> Offset (in voxels). <code>VoxelNeuron.shape</code> Shape of voxel grid. <code>VoxelNeuron.strip()</code> Strip empty voxels (leading/trailing planes of zeros). <code>VoxelNeuron.threshold()</code> Drop below-threshold voxels. <code>VoxelNeuron.voxels</code> Voxels making up the neuron."},{"location":"api/#dotprops","title":"Dotprops","text":"<p><code>navis.Dotprops</code> are typically indirectly generated from e.g. skeletons or point clouds using <code>navis.make_dotprops()</code>.</p> <p>These are methods and properties specific to Dotprops:</p> Function Description <code>Dotprops.points</code> Center of tangent vectors. <code>Dotprops.vect</code> Tangent vectors. <code>Dotprops.alpha</code> Alpha values for tangent vectors. <code>Dotprops.to_skeleton()</code> Turn Dotprop into a TreeNeuron. <code>Dotprops.snap()</code> Snap xyz location(s) to closest point or synapse."},{"location":"api/#converting-between-types","title":"Converting between types","text":"<p>These functions will let you convert between neuron types:</p> Function Description <code>navis.make_dotprops()</code> Produce dotprops from neurons or point clouds. <code>navis.skeletonize()</code> Turn neuron into skeleton. <code>navis.mesh()</code> Generate mesh from object(s). <code>navis.voxelize()</code> Turn neuron into voxels. <code>navis.conversion.voxels2mesh()</code> Generate mesh from voxels using marching cubes. <code>navis.conversion.tree2meshneuron()</code> Convert TreeNeuron to MeshNeuron. <p>See also Utility for functions to convert to/from basic data types.</p>"},{"location":"api/#neuronlist-methods","title":"NeuronList methods","text":"<p><code>NeuronLists</code> let you access all the properties and methods of the neurons they contain. In addition there are a few <code>NeuronList</code>-specific methods and properties.</p> <p>Methods:</p> Method Description <code>NeuronList.apply()</code> Apply function across all neurons in this NeuronList. <code>NeuronList.head()</code> Return summary for top N neurons. <code>NeuronList.itertuples()</code> Helper to mimic <code>pandas.DataFrame.itertuples()</code>. <code>NeuronList.mean()</code> Return mean numeric and boolean values over all neurons. <code>NeuronList.remove_duplicates()</code> Remove duplicate neurons from list. <code>NeuronList.sum()</code> Return sum numeric and boolean values over all neurons. <code>NeuronList.summary()</code> Get summary over all neurons in this NeuronList. <code>NeuronList.tail()</code> Return summary for bottom N neurons. <code>NeuronList.unmix()</code> Split into NeuronLists of the same neuron type. <p>Properties:</p> Property Description <code>NeuronList.bbox</code> Bounding box across all neurons in the list. <code>NeuronList.empty</code> Return True if NeuronList is empty. [<code>NeuronList.id</code>][navis.NeuronList.id] An array with the IDs of the neurons contained in the list. [<code>NeuronList.idx</code>][navis.NeuronList.idx] An indexer similar to pandas' <code>iloc</code> that accepts neuron IDs. <code>NeuronList.is_degenerated</code> Return True if contains neurons with non-unique IDs. <code>NeuronList.is_mixed</code> Return True if contains more than one type of neuron. <code>NeuronList.shape</code> Shape of NeuronList (N, ). <code>NeuronList.types</code> Return neuron types present in this list. <p>Please see the tutorial on <code>NeuronList</code> for more information, including how to index them.</p>"},{"location":"api/#visualization","title":"Visualization","text":"<p>Various functions for plotting neurons and volumes.</p> Function Description <code>navis.plot3d()</code> Generate interactive 3D plot. <code>navis.plot2d()</code> Generate 2D plots of neurons and neuropils. <code>navis.plot1d()</code> Plot neuron topology in 1D according to Cuntz et al. (2010). <code>navis.plot_flat()</code> Plot neuron as flat diagrams. <code>navis.clear3d()</code> Clear viewer 3D canvas. <code>navis.close3d()</code> Close existing 3D viewer (wipes memory). <code>navis.pop3d()</code> Remove the last item added to the 3D canvas. <code>navis.get_viewer()</code> Grab active 3D viewer."},{"location":"api/#plotting-volumesmeshes","title":"Plotting Volumes/Meshes","text":"<p>To plot meshes, you can pass <code>trimesh.Trimesh</code> objects directly to <code>plot3d()</code> or <code>plot2d()</code>. However, NAVis has a custom class to represent meshes that has some useful perks: <code>navis.Volume</code>.</p> Class Description <code>navis.Volume</code> Mesh consisting of vertices and faces. <code>navis.Volume.combine</code> Merge multiple volumes into a single object. <code>navis.Volume.plot3d</code> Plot volume using <code>navis.plot3d</code>. <code>navis.Volume.validate</code> Use trimesh to try and fix issues (holes/normals). <code>navis.Volume.resize</code> Resize volume."},{"location":"api/#vispy-3d-viewer","title":"Vispy 3D viewer","text":"<p>Using <code>navis.plot3d()</code> with <code>backend=\"vispy\"</code> from a terminal will spawn a Vispy 3D viewer object which has a bunch of useful methods. Note that this requires one of navis' <code>vispy-*</code> extras to be installed, so that vispy has a backend.</p> Function Description <code>navis.Viewer</code> Vispy 3D viewer. <code>navis.Viewer.add()</code> Add objects to canvas. <code>navis.Viewer.clear()</code> Clear canvas. <code>navis.Viewer.close()</code> Close viewer. `navis.Viewer.colorize() Colorize neurons using a seaborn color palette. <code>navis.Viewer.set_colors()</code> Set neuron color. <code>navis.Viewer.hide_neurons()</code> Hide given neuron(s). <code>navis.Viewer.unhide_neurons()</code> Unhide given neuron(s). <code>navis.Viewer.screenshot()</code> Save a screenshot of this viewer. <code>navis.Viewer.show()</code> Show viewer. <code>navis.Viewer.toggle_bounds()</code> Toggle bounding box."},{"location":"api/#octarine-3d-viewer","title":"Octarine 3D viewer","text":"<p>Using <code>navis.plot3d()</code> with <code>backend=\"octarine\"</code> from a terminal will return an <code>octarine.Viewer</code> 3D viewer. Please see the <code>Octarine</code> documentation for details about the viewer.</p>"},{"location":"api/#neuron-morphology","title":"Neuron Morphology","text":"<p>Collection of functions to analyze and manipulate neuronal morphology.</p>"},{"location":"api/#analyse","title":"Analyse","text":"<p>Functions to analyze morphology.</p> Function Description <code>navis.find_main_branchpoint()</code> Find main branch point of unipolar (e.g. insect) neurons. <code>navis.form_factor()</code> Calculate form factor for given neuron. <code>navis.persistence_points()</code> Calculate points for a persistence diagram. <code>navis.persistence_vectors()</code> Produce vectors from persistence points. <code>navis.strahler_index()</code> Calculate Strahler Index (SI). <code>navis.segment_analysis()</code> Calculate morphometric properties a neuron's segments. <code>navis.ivscc_features()</code> Calculate IVSCC features for neuron(s). <code>navis.sholl_analysis()</code> Run Sholl analysis for given neuron(s). <code>navis.tortuosity()</code> Calculate tortuosity of a neuron. <code>navis.betweeness_centrality()</code> Calculate vertex/node betweenness."},{"location":"api/#manipulate","title":"Manipulate","text":"<p>Functions to edit morphology:</p> Function Description <code>navis.average_skeletons()</code> Compute an average from a list of skeletons. <code>navis.break_fragments()</code> Break neuron into its connected components. <code>navis.despike_skeleton()</code> Remove spikes in skeleton (e.g. from jumps in image data). <code>navis.drop_fluff()</code> Remove small disconnected pieces of \"fluff\". <code>navis.cell_body_fiber()</code> Prune neuron to its cell body fiber. <code>navis.combine_neurons()</code> Combine multiple neurons into one. <code>navis.cut_skeleton()</code> Split skeleton at given point and returns two new neurons. <code>navis.guess_radius()</code> Guess radii for skeleton nodes. <code>navis.heal_skeleton()</code> Heal fragmented skeleton(s). <code>navis.longest_neurite()</code> Return a neuron consisting of only the longest neurite(s). <code>navis.prune_by_strahler()</code> Prune neuron based on Strahler order. <code>navis.prune_twigs()</code> Prune terminal twigs under a given size. <code>navis.prune_at_depth()</code> Prune all neurites past a given distance from a source. <code>navis.reroot_skeleton()</code> Reroot neuron to new root. <code>navis.split_axon_dendrite()</code> Split a neuron into axon and dendrite. <code>navis.split_into_fragments()</code> Split neuron into fragments. <code>navis.stitch_skeletons()</code> Stitch multiple skeletons together. <code>navis.subset_neuron()</code> Subset a neuron to a given set of nodes/vertices. <code>navis.smooth_skeleton()</code> Smooth skeleton(s) using rolling windows. <code>navis.smooth_mesh()</code> Smooth meshes (TriMesh, MeshNeuron, Volume). <code>navis.smooth_voxels()</code> Smooth voxel(s) using a Gaussian filter. <code>navis.thin_voxels()</code> Skeletonize image data to single voxel width."},{"location":"api/#resampling","title":"Resampling","text":"<p>Functions to down- or resample neurons.</p> Function Description <code>navis.resample_skeleton()</code> Resample skeleton(s) to given resolution. <code>navis.resample_along_axis()</code> Resample neuron such that nodes lie exactly on given 1d grid. <code>navis.downsample_neuron()</code> Downsample neuron(s) by a given factor. <code>navis.simplify_mesh()</code> Simplify meshes (TriMesh, MeshNeuron, Volume)."},{"location":"api/#comparing-morphology","title":"Comparing morphology","text":"<p>NBLAST and related functions:</p> Module Description <code>navis.nblast</code> NBLAST query against target neurons. <code>navis.nblast_smart</code> Smart(er) NBLAST query against target neurons. <code>navis.nblast_allbyall</code> All-by-all NBLAST of inputs neurons. <code>navis.nblast_align</code> Run NBLAST on pairwise-aligned neurons. <code>navis.synblast</code> Synapsed-based variant of NBLAST. <code>navis.persistence_distances</code> Calculate morphological similarity using persistence diagrams."},{"location":"api/#utilities-for-creating-your-own-score-matrices-for-nblast","title":"Utilities for creating your own score matrices for NBLAST:","text":"Function Description <code>navis.nbl.smat.Lookup2d</code> Convenience class inheriting from LookupNd for the common 2D float case. <code>navis.nbl.smat.Digitizer</code> Class converting continuous values into discrete indices. <code>navis.nbl.smat.LookupDistDotBuilder</code> Class for building a 2-dimensional score lookup for NBLAST."},{"location":"api/#utilities-for-nblast","title":"Utilities for NBLAST","text":"Function Description <code>navis.nbl.make_clusters()</code> Form flat clusters. <code>navis.nbl.update_scores()</code> Update score matrix by running only new query-&gt;target pairs. <code>navis.nbl.compress_scores()</code> Compress scores. <code>navis.nbl.extract_matches()</code> Extract top matches from score matrix. [<code>navis.nbl.nblast_prime()</code>][navis.nbl.nblast_prime] Error finding docstring for navis.nbl.nblast_prime: module 'navis.nbl' has no attribute 'nblast_prime'"},{"location":"api/#polarity-metrics","title":"Polarity metrics","text":"Function Description <code>navis.bending_flow()</code> Calculate synapse \"bending\" flow. <code>navis.flow_centrality()</code> Calculate flow between leaf nodes. <code>navis.synapse_flow_centrality()</code> Calculate synapse flow centrality (SFC). <code>navis.arbor_segregation_index()</code> Per arbor seggregation index (SI). <code>navis.segregation_index()</code> Calculate segregation index (SI)."},{"location":"api/#distances","title":"Distances","text":"<p>Functions to calculate Euclidean and geodesic (\"along-the-arbor\") distances.</p> Function Description <code>navis.cable_overlap()</code> Calculate the amount of cable of neuron A within distance of neuron B. <code>navis.distal_to()</code> Check if nodes A are distal to nodes B. <code>navis.dist_between()</code> Get the geodesic distance between nodes in nanometers. <code>navis.dist_to_root()</code> Calculate distance to root for each node. <code>navis.geodesic_matrix()</code> Generate geodesic (\"along-the-arbor\") distance matrix between nodes/vertices. <code>navis.segment_length()</code> Get length of a linear segment."},{"location":"api/#intersection","title":"Intersection","text":"<p>Functions to intersect points and neurons with volumes. For example, if you'd like to know which part of a neuron is inside a certain brain region.</p> Function Description <code>navis.in_volume()</code> Test if points/neurons are within a given volume. <code>navis.intersection_matrix()</code> Compute intersection matrix between a set of neurons and volumes."},{"location":"api/#transforming-and-mirroring","title":"Transforming and Mirroring","text":"<p>Functions to transform spatial data, e.g. move neurons from one brain space to another. Check out the tutorials for examples on how to use them.</p> <p>High-level functions:</p> Function Description <code>navis.xform()</code> Apply transform(s) to data. <code>navis.xform_brain()</code> Transform 3D data between template brains. <code>navis.symmetrize_brain()</code> Symmetrize 3D object (neuron, coordinates). <code>navis.mirror_brain()</code> Mirror 3D object (neuron, coordinates) about given axis. <code>navis.transforms.mirror()</code> [<code>navis.align.align_rigid()</code>][navis.align.align_rigid] Align neurons using a rigid registration. [<code>navis.align.align_deform()</code>][navis.align.align_deform] Align neurons using a deformable registration. [<code>navis.align.align_pca()</code>][navis.align.align_pca] Align neurons along their first principal components. [<code>navis.align.align_pairwise()</code>][navis.align.align_pairwise] Run a pairwise alignment between given neurons. <p>NAVis supports several types of transforms:</p> Class Description <code>navis.transforms.AffineTransform</code> Affine transformation of 3D spatial data. <code>navis.transforms.ElastixTransform</code> Elastix transforms of 3D spatial data. <code>navis.transforms.CMTKtransform</code> CMTK transforms of 3D spatial data. <code>navis.transforms.H5transform</code> Hdf5 transform of 3D spatial data. <code>navis.transforms.GridTransform</code> Deformation or coordinate field transform of 3D spatial data. <code>navis.transforms.TPStransform</code> Thin Plate Spline transforms of 3D spatial data. <code>navis.transforms.AliasTransform</code> Helper transform that simply passes points through. [<code>navis.transforms.MovingLeastSquaresTransform</code>][] Error finding docstring for navis.transforms.MovingLeastSquaresTransform: 'NoneType' object has no attribute 'split' <p>The <code>TemplateRegistry</code> keeps track of template brains, transforms and such:</p> Class Description <code>navis.transforms.templates.TemplateRegistry</code> Tracks template brains, available transforms and produces bridging sequences. <p>The relevant instance of this class is <code>navis.transforms.registry</code>. So to register and use a new transform you would look something like this:</p> <pre><code>&gt;&gt;&gt; transform = navis.transforms.AffineTransform(...)\n&gt;&gt;&gt; navis.transforms.registry.register_transform(transform,\n...                                              source='brainA',\n...                                              target='brainB')\n&gt;&gt;&gt; xf = navis.xform_brain(data, 'brainA', 'brainB')\n</code></pre> <p>You can check which transforms are registered like so:</p> <pre><code>&gt;&gt;&gt; navis.transforms.registry.summary()  # this outputs a dataframe\n</code></pre> <p>These are the methods and properties of <code>registry</code>:</p> Method Description <code>TemplateRegistry.register_transform()</code> Register a transform. <code>TemplateRegistry.register_transformfile()</code> Parse and register a transform file. <code>TemplateRegistry.register_templatebrain()</code> Register a template brain. <code>TemplateRegistry.register_path()</code> Register path(s) to scan for transforms. <code>TemplateRegistry.scan_paths()</code> Scan registered paths for transforms and add to registry. <code>TemplateRegistry.plot_bridging_graph()</code> Draw bridging graph using networkX. <code>TemplateRegistry.find_mirror_reg()</code> Search for a mirror transformation for given template. <code>TemplateRegistry.find_bridging_path()</code> Find bridging path from source to target. <code>TemplateRegistry.shortest_bridging_seq()</code> Find shortest bridging sequence to get from source to target. <code>TemplateRegistry.clear_caches()</code> Clear caches of all cached functions. <code>TemplateRegistry.summary()</code> Generate summary of available transforms. <code>TemplateRegistry.transforms()</code> Registered transforms (bridging + mirror). <code>TemplateRegistry.mirrors()</code> Registered mirror transforms. <code>TemplateRegistry.bridges()</code> Registered bridging transforms."},{"location":"api/#connectivity","title":"Connectivity","text":"<p>Collection of functions to work with graphs and adjacency matrices.</p> Function Description <code>navis.NeuronConnector</code> Class which creates a connectivity graph from a set of neurons."},{"location":"api/#connectivity-metrics","title":"Connectivity metrics","text":"<p>Functions to analyse/cluster neurons based on connectivity.</p> Function Description <code>navis.connectivity_similarity()</code> Calculate connectivity similarity. <code>navis.connectivity_sparseness()</code> Calculate sparseness. <code>navis.cable_overlap()</code> Calculate the amount of cable of neuron A within distance of neuron B. <code>navis.synapse_similarity()</code> Cluster neurons based on their synapse placement."},{"location":"api/#importexport","title":"Import/Export","text":"<p>Functions to import neurons.</p> Function Description <code>navis.read_swc()</code> Create Neuron/List from SWC file. <code>navis.read_nrrd()</code> Create Neuron/List from NRRD file. <code>navis.read_mesh()</code> Load mesh file into Neuron/List. <code>navis.read_tiff()</code> Create Neuron/List from TIFF file. <code>navis.read_nmx()</code> Read NMX files into Neuron/Lists. <code>navis.read_nml()</code> Read xml-based NML files into Neuron/Lists. <code>navis.read_rda()</code> Read objects from nat R data (.rda) file. <code>navis.read_json()</code> Load neuron from JSON (file or string). <code>navis.read_precomputed()</code> Read skeletons and meshes from neuroglancer's precomputed format. <code>navis.read_parquet()</code> Read parquet file into Neuron/List. <code>navis.scan_parquet()</code> Scan parquet file. <p>Functions to export neurons.</p> Function Description <code>navis.write_swc()</code> Write TreeNeuron(s) to SWC. <code>navis.write_nrrd()</code> Write VoxelNeurons or Dotprops to NRRD file(s). <code>navis.write_mesh()</code> Export meshes (MeshNeurons, Volumes, Trimeshes) to disk. <code>navis.write_json()</code> Save neuron(s) to json-formatted file. <code>navis.write_precomputed()</code> Export skeletons or meshes to neuroglancer's (legacy) precomputed format. <code>navis.write_parquet()</code> Write TreeNeuron(s) or Dotprops to parquet file."},{"location":"api/#utility","title":"Utility","text":"<p>Various utility functions.</p> Function Description <code>navis.health_check()</code> Run a health check on TreeNeurons and flag potential issues. <code>navis.set_pbars()</code> Set global progress bar behaviors. <code>navis.set_loggers()</code> Set levels for all associated module loggers. <code>navis.set_default_connector_colors()</code> Set/update default connector colors. <code>navis.config.remove_log_handlers()</code> Remove all handlers from the <code>navis</code> logger. <code>navis.patch_cloudvolume()</code> Monkey patch cloud-volume to return navis neurons. <code>navis.example_neurons()</code> Load example neuron(s). <code>navis.example_volume()</code> Load an example volume."},{"location":"api/#conversion","title":"Conversion","text":"<p>Functions to convert between data types.</p> Function Description <code>navis.neuron2nx()</code> Turn Tree-, Mesh- or VoxelNeuron into an NetworkX graph. <code>navis.neuron2igraph()</code> Turn Tree-, Mesh- or VoxelNeuron(s) into an iGraph graph. <code>navis.neuron2KDTree()</code> Turn neuron into scipy KDTree. <code>navis.neuron2tangents()</code> Turn skeleton(s) into points + tangent vectors. <code>navis.network2nx()</code> Generate NetworkX graph from edge list or adjacency. <code>navis.network2igraph()</code> Generate iGraph graph from edge list or adjacency. <code>navis.nx2neuron()</code> Create TreeNeuron from NetworkX Graph. <code>navis.edges2neuron()</code> Create TreeNeuron from edges and (optional) vertex coordinates."},{"location":"api/#network-models","title":"Network Models","text":"<p>NAVis comes with a simple network traversal model (used in Schlegel, Bates et al., 2021).</p> <p>Not imported at top level! Must be imported explicitly:</p> <pre><code>from navis import models\n</code></pre> Class Description <code>navis.models.TraversalModel</code> Model for traversing a network starting with given seed nodes. <code>navis.models.BayesianTraversalModel</code> Model for traversing a network starting with given seed nodes."},{"location":"api/#interfaces","title":"Interfaces","text":"<p>Interfaces with various external tools/websites. These modules have to be imported explicitly as they are not imported at top level.</p>"},{"location":"api/#neuron-simulator","title":"NEURON simulator","text":"<p>Functions to facilitate creating models of neurons/networks. Please see the tutorials for examples.</p> <p>Not imported at top level! Must be imported explicitly:</p> <pre><code>import navis.interfaces.neuron as nrn\n</code></pre>"},{"location":"api/#compartment-models","title":"Compartment models","text":"<p>A single-neuron compartment model is represented by <code>CompartmentModel</code>:</p> Class Description <code>CompartmentModel</code> Compartment model representing a single neuron in NEURON. <code>DrosophilaPN</code> Compartment model of an olfactory projection neuron in Drosophila. <p>The <code>DrosophilaPN</code>  class is a subclass of <code>CompartmentModel</code> with pre-defined properties from Tobin et al. (2017).</p> Method Description <code>CompartmentModel.add_current_record()</code> Add current recording to model. <code>CompartmentModel.add_spike_detector()</code> Add a spike detector at given node(s). <code>CompartmentModel.add_synaptic_current()</code> Add synaptic current(s) (AlphaSynapse) to model. <code>CompartmentModel.add_synaptic_input()</code> Add synaptic input to model. <code>CompartmentModel.add_voltage_record()</code> Add voltage recording to model. <code>CompartmentModel.clear_records()</code> Clear records. <code>CompartmentModel.clear_stimuli()</code> Clear stimuli. <code>CompartmentModel.connect()</code> Connect object to model. <code>CompartmentModel.get_node_section()</code> Return section(s) for given node(s). <code>CompartmentModel.get_node_segment()</code> Return segment(s) for given node(s). <code>CompartmentModel.inject_current_pulse()</code> Add current injection (IClamp) stimulation to model. <code>CompartmentModel.plot_results()</code> Plot results. <code>CompartmentModel.insert()</code> Insert biophysical mechanism for model. <code>CompartmentModel.uninsert()</code> Remove biophysical mechanism from model. Attribute Description <code>CompartmentModel.Ra</code> Axial resistance [Ohm * cm] of all sections. <code>CompartmentModel.cm</code> Membran capacity [micro Farads / cm^2] of all sections. <code>CompartmentModel.label</code> Name/label of the neuron. <code>CompartmentModel.n_records</code> Number of records (across all types) active on this model. <code>CompartmentModel.n_sections</code> Number of sections in this model. <code>CompartmentModel.n_stimuli</code> Number of stimuli active on this model. <code>CompartmentModel.nodes</code> Node table of the skeleton. <code>CompartmentModel.records</code> Return mapping of node ID(s) to recordings. <code>CompartmentModel.sections</code> List of sections making up this model. <code>CompartmentModel.stimuli</code> Return mapping of node ID(s) to stimuli. <code>CompartmentModel.synapses</code> Return mapping of node ID(s) to synapses. <code>CompartmentModel.t</code> The global time. Should be the same for all neurons."},{"location":"api/#network-models_1","title":"Network models","text":"<p>A network of point-processes is represented by <code>PointNetwork</code>:</p> Class Description <code>PointNetwork</code> A Network of Leaky-Integrate-and-Fire (LIF) point processes. Methods Description <code>PointNetwork.add_background_noise()</code> Add background noise to given neurons. <code>PointNetwork.add_neurons()</code> Add neurons to network. <code>PointNetwork.add_stimulus()</code> Add stimulus to given neurons. <code>PointNetwork.connect()</code> Connect two neurons. <code>PointNetwork.from_edge_list()</code> Generate network from edge list. <code>PointNetwork.get_spike_counts()</code> Get matrix of spike counts. <code>PointNetwork.plot_raster()</code> Raster plot of spike timings. <code>PointNetwork.plot_traces()</code> Plot mean firing rate. <code>PointNetwork.run_simulation()</code> Run the simulation. <code>PointNetwork.set_labels()</code> Set labels for neurons. Attributes Description <code>PointNetwork.edges</code> Edges between nodes of the network. <code>PointNetwork.ids</code> IDs of neurons in the network. <code>PointNetwork.labels</code> Labels of neurons in the network. <code>PointNetwork.neurons</code> Neurons in the network."},{"location":"api/#neuromorpho-api","title":"NeuroMorpho API","text":"<p>Set of functions to grab data from NeuroMorpho which hosts thousands of neurons (see tutorials).</p> <p>Not imported at top level! Must be imported explicitly:</p> <pre><code>from navis.interfaces import neuromorpho\n</code></pre> Function Description <code>neuromorpho.get_neuron_info()</code> Fetch neuron info by ID or by name. <code>neuromorpho.get_neuron()</code> Fetch neuron by ID or by name. <code>neuromorpho.get_neuron_fields()</code> List all available neuron fields. <code>neuromorpho.get_available_field_values()</code> List all possible values for given neuron field."},{"location":"api/#neuprint-api","title":"neuPrint API","text":"<p>NAVis wraps <code>neuprint-python</code> and adds a few navis-specific functions. You must have <code>neuprint-python</code> installed for this to work:</p> <pre><code>pip install neuprint-python -U\n</code></pre> <p>You can then import neuprint from NAVis like so:</p> <pre><code>from navis.interfaces import neuprint\n</code></pre> <p>These are the additional functions added by NAVis:</p> Function Description <code>neuprint.fetch_roi()</code> Fetch given ROI. <code>neuprint.fetch_skeletons()</code> Fetch neuron skeletons as navis.TreeNeurons. <code>neuprint.fetch_mesh_neuron()</code> Fetch neuron meshes as navis.MeshNeuron. <p>Please also check out the tutorials for examples of how to fetch and work with data from neuPrint.</p>"},{"location":"api/#insectbrain-db-api","title":"InsectBrain DB API","text":"<p>Set of functions to grab data from InsectBrain which hosts some neurons and standard brains (see tutorials).</p> <p>Not imported at top level! Must be imported explicitly:</p> <pre><code>from navis.interfaces import insectbrain_db\n</code></pre> Function Description <code>insectbrain_db.authenticate()</code> Authenticate against Insect Brain DB. <code>insectbrain_db.get_brain_meshes()</code> Fetch brain meshes for given species. [<code>insectbrain_db.get_species_info()</code>][navis.interfaces.insectbrain_db.get_species_info] Get all info for given species. [<code>insectbrain_db.get_available_species()</code>][navis.interfaces.insectbrain_db.get_available_species] Get all info for given species. <code>insectbrain_db.get_skeletons()</code> Fetch skeletons for given neuron(s). <code>insectbrain_db.get_skeletons_species()</code> Fetch all skeletons for given species. <code>insectbrain_db.search_neurons()</code> Search for neurons matching given parameters."},{"location":"api/#blender-api","title":"Blender API","text":"<p>Functions to be run inside Blender 3D and import CATMAID data (see Examples). Please note that this requires Blender &gt;2.8 as earlier versions are shipped with older Python versions not supported by NAVis. See the tutorials for an introduction of how to use NAVis in Blender.</p> <p>Not imported at top level! Must be imported explicitly:</p> <pre><code>from navis.interfaces import blender\n</code></pre> <p>The interface is realised through a <code>navis.interfaces.blender.Handler</code> object. It is used to import objects and facilitate working with them programmatically once they are imported.</p> Class Description <code>blender.Handler</code> Class that interfaces with scene in Blender."},{"location":"api/#objects","title":"Objects","text":"Method Description <code>blender.Handler.add()</code> Add neuron(s) to scene. <code>blender.Handler.clear()</code> Clear all neurons <code>blender.Handler.select()</code> Select given neurons. <code>blender.Handler.hide()</code> Hide all neuron-related objects. <code>blender.Handler.unhide()</code> Unide all neuron-related objects."},{"location":"api/#materials","title":"Materials","text":"Properties Description <code>blender.Handler.color()</code> Assign color to all neurons. <code>blender.Handler.colorize()</code> Randomly colorize ALL neurons. <code>blender.Handler.emit()</code> Change emit value. <code>blender.Handler.use_transparency()</code> Change transparency (True/False). <code>blender.Handler.alpha()</code> Change alpha (0-1). <code>blender.Handler.bevel()</code> Change bevel of ALL neurons."},{"location":"api/#selections","title":"Selections","text":"<p>You can use <code>Handler.select()</code> to select a group of neurons e.g. by type. That method then returns <code>ObjectList</code> which can be used to modify just the selected objects:</p> Methods Description <code>blender.Handler.select()</code> Select given neurons. <code>blender.ObjectList.select()</code> Select objects in 3D viewer <code>blender.ObjectList.color()</code> Assign color to all objects in the list. <code>blender.ObjectList.colorize()</code> Assign colors across the color spectrum. <code>blender.ObjectList.emit()</code> Change emit value. <code>blender.ObjectList.use_transparency()</code> Change transparency (True/False). <code>blender.ObjectList.alpha()</code> Change alpha (0-1). <code>blender.ObjectList.bevel()</code> Change bevel radius of objects. <code>blender.ObjectList.hide()</code> Hide objects. <code>blender.ObjectList.unhide()</code> Unhide objects. <code>blender.ObjectList.hide_others()</code> Hide everything BUT these objects. <code>blender.ObjectList.delete()</code> Delete neurons in the selection."},{"location":"api/#cytoscape-api","title":"Cytoscape API","text":"<p>Warning</p> <p>The Cytoscape API is depcrecated and will be removed in a future version of NAVis.</p> <p>Functions to use Cytoscape via the cyREST API.</p> <p>Not imported at top level! Must be imported explicitly:</p> <pre><code>from navis.interfaces import cytoscape\n</code></pre> Function Description [<code>cytoscape.generate_network()</code>][navis.interfaces.cytoscape.generate_network] Error finding docstring for navis.interfaces.cytoscapecytoscape.generate_network: module 'navis.interfaces' has no attribute 'cytoscapecytoscape' [<code>cytoscape.get_client()</code>][navis.interfaces.cytoscape.get_client] Error finding docstring for navis.interfaces.cytoscapecytoscape.get_client: module 'navis.interfaces' has no attribute 'cytoscapecytoscape'"},{"location":"api/#allen-microns-datasets","title":"Allen MICrONS datasets","text":"<p>Functions to fetch neurons (including synapses) from the Allen Institute's MICrONS EM datasets.</p> <p>Requires <code>caveclient</code> and <code>cloud-volume</code> as additional dependencies:</p> <pre><code>pip3 install caveclient cloud-volume -U\n</code></pre> <p>Please see caveclient's docs for details on how to retrieve and set credentials.</p> <p>Not imported at top level! Must be imported explicitly:</p> <pre><code>from navis.interfaces import microns\n</code></pre> Function Description <code>microns.get_cave_client()</code> Get caveclient for given datastack. <code>microns.fetch_neurons()</code> Fetch neuron meshes. [<code>microns.get_somas()</code>][navis.interfaces.microns.get_somas] Error finding docstring for navis.interfaces.microns.get_somas: module 'navis.interfaces.microns' has no attribute 'get_somas' <p>Please also see the MICrONS tutorial.</p>"},{"location":"api/#h01-dataset","title":"H01 dataset","text":"<p>Functions to fetch neurons (including synapses) from the H01 connectome dataset.</p> <p>Requires <code>caveclient</code> and <code>cloud-volume</code> as additional dependencies:</p> <pre><code>pip3 install caveclient cloud-volume -U\n</code></pre> <p>Not imported at top level! Must be imported explicitly:</p> <pre><code>from navis.interfaces import h01\n</code></pre> Function Description <code>h01.get_cave_client()</code> Get caveclient for H01 dataset. <code>h01.fetch_neurons()</code> Fetch neuron meshes. [<code>h01.get_somas()</code>][navis.interfaces.h01.get_somas] Error finding docstring for navis.interfaces.h01.get_somas: module 'navis.interfaces.h01' has no attribute 'get_somas' <p>Please also see the H01 tutorial.</p>"},{"location":"api/#r-interface","title":"R interface","text":"<p>Bundle of functions to use R natverse libraries.</p> <p>Not imported at top level! Must be imported explicitly:</p> <pre><code>from navis.interfaces import r\n</code></pre> Function Description [<code>r.data2py()</code>][navis.interfaces.r.data2py] Error finding docstring for navis.interfaces.r.data2py: module 'navis.interfaces' has no attribute 'r' [<code>r.get_neuropil()</code>][navis.interfaces.r.data2py] Error finding docstring for navis.interfaces.r.get_neuropil: module 'navis.interfaces' has no attribute 'r' [<code>r.init_rcatmaid()</code>][navis.interfaces.r.data2py] Error finding docstring for navis.interfaces.r.init_rcatmaid: module 'navis.interfaces' has no attribute 'r' [<code>r.load_rda()</code>][navis.interfaces.r.data2py] Error finding docstring for navis.interfaces.r.load_rda: module 'navis.interfaces' has no attribute 'r' [<code>r.nblast()</code>][navis.interfaces.r.data2py] Error finding docstring for navis.interfaces.r.nblast: module 'navis.interfaces' has no attribute 'r' [<code>r.nblast_allbyall()</code>][navis.interfaces.r.data2py] Error finding docstring for navis.interfaces.r.nblast_allbyall: module 'navis.interfaces' has no attribute 'r' [<code>r.NBLASTresults()</code>][navis.interfaces.r.data2py] Error finding docstring for navis.interfaces.r.NBLASTresults: module 'navis.interfaces' has no attribute 'r' [<code>r.neuron2py()</code>][navis.interfaces.r.data2py] Error finding docstring for navis.interfaces.r.neuron2py: module 'navis.interfaces' has no attribute 'r' [<code>r.neuron2r()</code>][navis.interfaces.r.data2py] Error finding docstring for navis.interfaces.r.neuron2r: module 'navis.interfaces' has no attribute 'r' [<code>r.xform_brain()</code>][navis.interfaces.r.data2py] Error finding docstring for navis.interfaces.r.xform_brain: module 'navis.interfaces' has no attribute 'r' [<code>r.mirror_brain()</code>][navis.interfaces.r.data2py] Error finding docstring for navis.interfaces.r.mirror_brain: module 'navis.interfaces' has no attribute 'r'"},{"location":"api/#neuron-types-and-functions","title":"Neuron types and functions","text":"<p>As you can imagine not all functions will work on all neuron types. For example it is currently not possible to find the longest neurite via <code>longest_neurite()</code> for a <code>VoxelNeuron</code>. Conversely, some functionality like \"smoothing\" makes sense for multiple neuron types but the application is so vastly different between e.g. meshes and skeletons that there are specicialized functions for every neuron type.</p> <p>Below table has an overview for which functions work with which neuron types:</p> TreeNeuron MeshNeuron VoxelNeuron Dotprops <code>navis.plot2d</code> yes yes limited yes <code>navis.plot3d</code> yes yes see backend yes <code>navis.plot1d</code> yes no no no <code>navis.plot_flat</code> yes no no no <code>navis.subset_neuron</code> yes yes yes yes <code>navis.in_volume</code> yes yes yes yes smoothing <code>navis.smooth_skeleton</code> <code>navis.smooth_mesh</code> <code>navis.smooth_voxels</code> no <code>navis.downsample_neuron</code> yes yes yes yes resampling (e.g. <code>navis.resample_skeleton</code>) yes no no no <code>navis.make_dotprops</code> yes yes yes yes NBLAST (<code>navis.nblast</code>, etc.) no<sup>1</sup> no<sup>1</sup> no<sup>1</sup> yes <code>navis.xform_brain</code> yes yes yes (slow!) yes <code>navis.mirror_brain</code> yes yes no yes <code>navis.skeletonize</code> no yes no no <code>navis.mesh</code> yes no yes no <code>navis.voxelize</code> yes yes no yes <code>navis.drop_fluff</code> yes yes no no <code>navis.break_fragments</code> yes yes no no <ol> <li> <p>Use <code>navis.make_dotprops</code> to turn these neurons into <code>navis.Dotprops</code>.\u00a0\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#changelog","title":"Changelog","text":"<p>This is a selection of features added, changes made and bugs fixed with each version. For a full list of changes please see the commits history on NAVis' Github repository.</p>"},{"location":"changelog/#dev","title":"dev","text":"<p>Date: ongoing</p> <p>To install the current <code>dev</code> version of NAVis:</p> <pre><code>pip uninstall navis -y\npip install git+https://github.com/navis-org/navis@master\n</code></pre>"},{"location":"changelog/#version-1110","title":"Version <code>1.11.0</code>","text":"<p>Date: 27/02/26</p>"},{"location":"changelog/#breaking","title":"Breaking","text":"<ul> <li>dropped support for Python 3.9</li> </ul>"},{"location":"changelog/#improvements","title":"Improvements","text":"<ul> <li><code>split_axon_dendrite</code> now allows setting the in-/output ratio for the split (see <code>split</code> parameter)</li> <li>major speed-up for <code>heal_skeleton</code></li> <li>minor speed-up for <code>resample_skeleton</code></li> <li>add <code>progress</code> parameter to <code>mirror_brain</code>, <code>symmetrize_brain</code> and [<code>combine_meshes</code>][navis.combine_meshes]</li> <li><code>persistence_vectors</code> now accepts list of distances to be sampled as <code>samples</code></li> <li><code>make_dotprops</code> has a new <code>on_issue</code> parameter that determines what to do when issues with the inputs are encountered (e.g. NaNs)</li> <li>two new <code>VoxelNeuron</code> methods:<ul> <li><code>flip()</code> flips the neuron along specified axes</li> <li><code>normalize()</code> scales values to a 0-1 range</li> </ul> </li> <li><code>neuprint</code> interface:<ul> <li><code>fetch_skeletons</code> and <code>fetch_mesh_neuron</code> will now also look for <code>tosomaLocation</code> to set the root/soma if there is no <code>somaLocation</code></li> <li>avoid fetching unused ROI info in <code>fetch_skeletons</code> and <code>fetch_mesh_neuron</code> (minor speed-up)</li> </ul> </li> <li><code>pointlabels_to_meshes</code> can now also output voxels instead of meshes</li> <li>transforms:<ul> <li>new transform type: <code>GridTransform</code> is a class for generic deformation-field transforms</li> <li><code>TPStransform</code> now has a <code>.matrix_rigid</code> property that extracts the rigid part of the TPS affine as a 4x4 matrix</li> <li><code>TPStransforms</code> and [<code>MovingLeastSquaresTransforms</code>][navis.transforms.MovingLeastSquaresTransform] transforms now transform in batches to avoid memory issues when transforming large numbers of points</li> <li>new methods for <code>CMTKtransform</code>: <code>to_dfield</code> and <code>to_grid_transform</code> can be used to sample the CMTK transform into a deformation field (this is experimental)</li> <li>new <code>H5transform</code> method: <code>xform_image</code> can be used to apply the transform to images (this is experimental)</li> <li><code>TransformRegistry.register_transform</code> now accepts an optional <code>weight_inv</code> parameter; can be used to penalize expensive inverse transforms (e.g. CMTK)</li> </ul> </li> <li>input/output:<ul> <li><code>read_xxx</code> functions will now use threads instead of processes for parallelization when reading from URLs (much faster)</li> <li><code>read_precomputed</code> will now also look for <code>.ngmesh</code> files when given a folder to search</li> <li><code>read_xxx</code> function can now load data straight from Google buckets (<code>gs://...</code>, requires the optional <code>gcsfs</code>)</li> </ul> </li> </ul>"},{"location":"changelog/#fixes","title":"Fixes","text":"<ul> <li>using <code>connectors=\"pre/postsynapses\"</code> now actually works in <code>plot2d</code> and <code>plot3d</code></li> <li>fixed an issue in <code>resample_skeleton</code> where adding new nodes could cause an overflow error for node IDs</li> <li>subsetting neuron meshes with connectors will now correctly carry over vertex IDs</li> <li>plotting skeleton where the soma has no radius will not break anymore</li> <li><code>write_xxx</code> functions do not break anymore when a neuron has id <code>0</code></li> <li>plotting of connectors:</li> <li>parameters <code>cn_alpha</code>, <code>cn_colors</code> and <code>cn_mesh_colors</code> now work across all 3d backends</li> <li><code>plot2d</code> now respects <code>cn_alpha</code></li> <li>fixed an issue where checking for available mesh backends (pyfqmr, pymeshlab, etc) could cause a crash</li> <li>Blender interface: fixed an issue adding skeletons caused by pandas &gt;= 3.0</li> </ul> <p>Full Changelog: v1.10.0...v1.11.0</p>"},{"location":"changelog/#version-1100","title":"Version <code>1.10.0</code>","text":"<p>Date: 06/02/25</p>"},{"location":"changelog/#improvements_1","title":"Improvements","text":"<ul> <li>made reading neurons from <code>.tar</code> archives much faster</li> <li><code>read_swc</code> now works if additional columns are present</li> </ul>"},{"location":"changelog/#fixes_1","title":"Fixes","text":"<ul> <li><code>opacity</code> parameter <code>plot3d</code> now works correctly when using the plotly backend</li> <li>fixed an issue with Elastix transforms on Windows machines</li> <li>fixes for <code>navis.longest_neurite</code> when <code>from_root=False</code></li> <li>fixed issues with neuPrint interface when using multiple clients</li> <li>fixed an issue with the MICrONS interface</li> <li>fixed an issue with mesh simplification and the pymeshlab and Blender backends (@floesche)</li> <li>fixed two (potential) issues in navis.longest_neurite when <code>from_root=False</code></li> <li>fixed various issues related to numpy 2.0 (@floesche)</li> </ul> <p>Full Changelog: v1.9.1...v1.10.0</p>"},{"location":"changelog/#version-191","title":"Version <code>1.9.1</code>","text":"<p>Date: 24/10/24</p>"},{"location":"changelog/#improvements_2","title":"Improvements","text":"<ul> <li>MICrONS &amp; H01 interfaces:<ul> <li><code>fetch_neurons</code> now accepts a <code>materialization</code> parameter that determines which materialization version is used for synapse and nucleus queries; defaults to \"auto\" which means <code>navis</code> will try to find a materialization version matching the queried root IDs</li> <li><code>fetch_neurons</code> will now also assign soma positions for H01 neurons (see the <code>.soma_pos</code> neuron property)</li> </ul> </li> <li><code>CloudVolume.mesh.get_navis</code> (see <code>navis.patch_cloudvolume</code>) now accepts a <code>process</code> (default is <code>False</code>) parameter that determines whether the NeuroGlancer mesh is processed (deduplication of vertices, etc.); contribute by Forrest Collman</li> </ul>"},{"location":"changelog/#fixes_2","title":"Fixes","text":"<ul> <li>fixed a bug in <code>navis.subset_neuron</code> that caused branch points to not be re-calculated</li> </ul> <p>Full Changelog: v1.9.0...v1.9.1</p>"},{"location":"changelog/#version-190","title":"Version <code>1.9.0</code>","text":"<p>Date: 17/10/24</p> <p>This version brings a shiny new interface to the H01 human cortex dataset and various other quality of life improvements.</p>"},{"location":"changelog/#breaking_1","title":"Breaking","text":"<ul> <li>The default for <code>radius</code> (<code>navis.plot2d</code> and <code>navis.plot3d</code>) was changed to <code>False</code> (from <code>\"auto\"</code>); this is to make sure that the defaults allow visualizing large numbers of skeletons, i.e. prioritizing performance over beauty</li> </ul>"},{"location":"changelog/#additions","title":"Additions","text":"<ul> <li>New interface to the H01 dataset (by Jinhan Choi and Jakob Troidl; see the new tutorial for details)</li> </ul>"},{"location":"changelog/#improvements_3","title":"Improvements","text":"<ul> <li>I/O:<ul> <li><code>read_nrrd</code>, <code>read_tiff</code> and <code>read_mesh</code> now use the same backend as e.g. <code>read_swc</code> which enables some niceties such as reading directly from URLs and archives, parallel processing, etc</li> <li>all <code>read_*</code> functions now have an <code>error</code> parameter that can be used to skip errors</li> </ul> </li> <li>Image data:<ul> <li>new function: <code>navis.thin_voxels</code> can be used to thin images and <code>VoxelNeurons</code> to single-pixel width (see also below)</li> <li>new <code>thin</code> parameter for <code>read_nrrd</code>, <code>read_tiff</code></li> </ul> </li> <li><code>TreeNeurons</code>:<ul> <li>skeletons can now be initialized from a <code>(vertices, edges)</code> tuple - see also <code>navis.edges2neuron</code></li> <li>new property: <code>TreeNeuron.vertices</code> gives read-only to node (vertex) coordinates</li> </ul> </li> <li><code>VoxelNeurons</code>:<ul> <li>new properties: <code>VoxelNeuron.nnz</code> and <code>VoxelNeuron.density</code></li> </ul> </li> <li><code>navis.drop_fluff</code> and <code>navis.neuron2nx</code> now also works with <code>Dotprops</code></li> </ul>"},{"location":"changelog/#experimental","title":"Experimental","text":"<ul> <li>setting <code>navis.config.add_units=True</code> (default is <code>False</code> for the time being) will add units to certain neuron properties such as <code>.cable_length</code> to make them human-readable</li> </ul> <p>Full Changelog: v1.8.0...v1.9.0</p>"},{"location":"changelog/#version-180","title":"Version <code>1.8.0</code>","text":"<p>Date: 22/09/24</p> <p>This version contains a major internal rework of both <code>navis.plot2d</code> and <code>navis.plot3d</code> to make them more consistent and easier to use.</p>"},{"location":"changelog/#breaking_2","title":"Breaking","text":"<ul> <li>Plotting: the <code>synapse_layout</code> parameter was renamed to <code>cn_layout</code> (matching e.g. other parameters such as <code>cn_colors</code>)</li> <li>Negative views in <code>navis.plot2d</code> (e.g. <code>view=(\"x\", \"-z\")</code>) will now invert axes rather than changing the underlying data</li> <li>Minimum version of <code>matplotlib</code> is now <code>3.9</code> (was <code>3.6</code>)</li> <li>The <code>plotly</code> backend is not part of a minimal install anymore (still installed using <code>navis[all]</code>)</li> <li>The Vispy backend is now deprecated and will be removed in a future release</li> <li>Removed <code>navis.screenshot</code> - please use the Octarine/Vispy viewer's <code>.screenshot()</code> method instead</li> <li><code>navis.tortuosity</code> now calculates tortuosity as-is (i.e. without resampling) by default</li> </ul>"},{"location":"changelog/#additions_1","title":"Additions","text":"<ul> <li>Added Octarine as the default backend for plotting from terminal</li> <li>New Function: <code>navis.ivscc_features</code> computes some basic IVSCC features</li> <li>New function: <code>navis.graph.skeleton_adjacency_matrix</code> computes the node adjacency for skeletons</li> <li>New function: <code>navis.graph.simplify_graph</code> simplifies skeleton graphs to only root, branch and leaf nodes while preserving branch length (i.e. weights)</li> <li>New <code>NeuronList</code> method: <code>get_neuron_attributes</code> is analagous to <code>dict.get</code></li> <li><code>NeuronLists</code> now implement the <code>|</code> (<code>__or__</code>) operator which can be used to get the union of two <code>NeuronLists</code></li> <li><code>navis.Volume</code> now have an (optional) <code>.units</code> property similar to neurons</li> <li><code>Tree/MeshNeurons</code> and <code>Dotprops</code> now support addition/subtraction (similar to the already existing multiplication and division) to allow offsetting neurons</li> </ul>"},{"location":"changelog/#improvements_4","title":"Improvements","text":"<ul> <li>Plotting:<ul> <li><code>navis.plot3d</code>:</li> <li><code>legendgroup</code> parameter (plotly backend) now also sets the legend group's title</li> <li>new parameters for the plotly backend:<ul> <li><code>legend</code> (default <code>True</code>): determines whether legends is shown</li> <li><code>legend_orientation</code> (default <code>v</code>): determines whether legend is aranged vertically (<code>v</code>) or horizontally (<code>h</code>)</li> <li><code>linestyle</code> (default <code>-</code>): determines line style for skeletons</li> </ul> </li> <li>default for <code>radius</code> is now <code>\"auto\"</code></li> <li><code>navis.plot2d</code>:</li> <li>the <code>view</code> parameter now also works with <code>methods</code> <code>3d</code> and <code>3d_complex</code></li> <li>the <code>color_by</code> and <code>shade_by</code> parameters now also work when plotting skeletons with <code>radius=True</code></li> <li>new defaults: <code>radius=\"auto\"</code>, <code>alpha=1</code>, <code>figsize=None</code> (use matplotlib defaults)</li> <li>new parameters for methods <code>3d</code> and <code>3d_complex</code>: <code>mesh_shade=False</code> and <code>non_view_axes3d</code></li> <li>the <code>scalebar</code> and <code>soma</code> parameters can now also be dictionaries to style (color, width, etc) the scalebar/soma</li> <li>the <code>connectors</code> parameter can now be used to show specific connector types (e.g. <code>connectors=\"pre\"</code>)</li> </ul> </li> <li>I/O:<ul> <li><code>read_*</code> functions are now able to read from FTP servers (<code>ftp://...</code>)</li> <li>the <code>limit</code> parameter used in many <code>read_*</code> functions can now also be a regex pattern or a <code>slice</code></li> </ul> </li> <li>New parameter in <code>navis.resample_skeleton</code>: use <code>map_column</code> to include arbitrary columns in the resampling</li> <li><code>navis.prune_twigs</code> and <code>navis.morpho.cable_length</code> now accept a <code>mask</code> parameter</li> <li>General improvements to docs and tutorials</li> </ul>"},{"location":"changelog/#fixes_3","title":"Fixes","text":"<ul> <li>Memory usage of <code>Neuron/Lists</code> is now correctly re-calculated when the neuron is modified</li> <li>Various fixes and improvements for the MICrONS interface (<code>navis.interfaces.microns</code>)</li> <li><code>navis.graph.node_label_sorting</code> now correctly prioritizes total branch length</li> <li><code>navis.TreeNeuron.simple</code> now correctly drops soma nodes if they aren't root, branch or leaf points themselves</li> </ul> <p>Full Changelog: v1.7.0...v1.8.0</p>"},{"location":"changelog/#version-170","title":"Version <code>1.7.0</code>","text":"<p>Date: 25/07/24</p>"},{"location":"changelog/#breaking_3","title":"Breaking","text":"<ul> <li>Plotting: dropped the <code>cluster</code> parameter in favor of an improved <code>color_by</code> logic (see below)</li> </ul>"},{"location":"changelog/#additions_2","title":"Additions","text":"<ul> <li>NAVis now uses <code>navis-fastcore</code> if present to dramatically speed up core functions (see updated install instructions)</li> <li>New method <code>navis.NeuronList.add_metadata</code> to quickly add metadata to neurons</li> </ul>"},{"location":"changelog/#improvements_5","title":"Improvements","text":"<ul> <li><code>navis.find_soma</code> and <code>navis.graph.neuron2nx</code> (used under the hood) are now much faster</li> <li>All I/O functions such as <code>navis.read_swc</code> now show which file caused an error (if any); original filenames are tracked as <code>file</code> property</li> <li><code>navis.NeuronList</code> will only search the first 100 neurons for autocompletion to avoid freezing with large lists</li> <li>Plotting functions: <code>color_by</code> now accepts either a list of labels (one per neuron) or the name of a neuron property</li> <li><code>navis.subset_neuron</code> is now faster and more memory efficient when subsetting meshes</li> <li><code>navis.TreeNeuron.cable_length</code> is now faster</li> <li>Fixed a bug in plotting when using vertex colors</li> <li>Fixed the progress bar in <code>navis.interfaces.neuprint.fetch_mesh_neuron</code></li> <li>Fixed a bug in <code>navis.synblast</code> that caused multiprocessing to fail (pickling issue with <code>pykdtree</code>)</li> <li><code>navis.interfaces.neuprint.fetch_mesh_neuron</code> will now ignore the <code>lod</code> parameter if the data source does not support it instead of breaking</li> <li>Fixed a number of deprecation warnings in the codebase</li> </ul> <p>Full Changelog: v1.6.0...v1.7.0</p>"},{"location":"changelog/#version-160","title":"Version <code>1.6.0</code>","text":"<p>Date: 07/04/24</p>"},{"location":"changelog/#breaking_4","title":"Breaking","text":"<ul> <li>Dropped support for Python 3.8, per NEP 29</li> <li><code>navis.write_swc</code> no longer supports writing Dotprops to SWC files</li> </ul>"},{"location":"changelog/#additions_3","title":"Additions","text":"<ul> <li>New property <code>TreeNeuron.surface_area</code></li> <li>New (experimental) functions <code>navis.read_parquet</code> and <code>navis.write_parquet</code> store skeletons and dotprops in parquet files (see here for format specs)</li> <li>New <code>navis.read_nml</code> function to read single NML file</li> <li>New <code>navis.NeuronConnector</code> class for creating connectivity graphs from groups neurons with consistent connector IDs</li> <li>New method for CMTKtransforms: <code>navis.transforms.CMTKTransform.xform_image</code></li> </ul>"},{"location":"changelog/#improvements_6","title":"Improvements","text":"<ul> <li>Improved performance for adding recordings to <code>CompartmentModel</code></li> <li><code>navis.heal_skeleton</code> and <code>navis.resample_skeleton</code> are now faster</li> <li>Improved logic for splitting NBLASTs across multiple cores</li> <li><code>navis.xform_brain</code> now allows specifying multiple intermediate template spaces through the <code>via</code> parameter and to ignore spaces through the <code>avoid</code> parameter</li> <li>I/O functions can now read directly from <code>.tar</code> or <code>.tar.gz</code> files</li> <li><code>navis.read_precomputed</code> now accepts a <code>limit</code> parameter similar to <code>navis.read_swc</code></li> </ul>"},{"location":"changelog/#fixes_4","title":"Fixes","text":"<ul> <li>Fixed interface to InsectBrainDB</li> <li><code>navis.read_precomputed</code>:<ul> <li>now correctly parses the <code>info</code> file depending on the source</li> <li>reading large files (i.e. meshes) directly from a URL should not break anymore</li> </ul> </li> <li>Fixed writing vertex properties in <code>navis.write_precomputed</code></li> <li>Fixed a bug in <code>navis.resample_skeleton</code></li> <li>Fixed an occasional issue when plotting skeletons with radii</li> <li>Fixed bug in <code>navis.subset_neuron</code> that caused connectors to be dropped when using mask</li> <li>Fixed a bug in <code>navis.despike_skeleton</code> that caused the <code>reverse</code> argument to be ignored</li> <li>Fixed two small bugs in <code>navis.interfaces.neuprint.fetch_mesh_neuron</code></li> </ul> <p>Full Changelog: v1.5.0...v1.6.0</p>"},{"location":"changelog/#version-150","title":"Version <code>1.5.0</code>","text":"<p>Date: 27/07/23</p>"},{"location":"changelog/#breaking_5","title":"Breaking","text":"<ul> <li>Dropped support for Python 3.7</li> </ul>"},{"location":"changelog/#additions_4","title":"Additions","text":"<ul> <li>New function: <code>navis.pop3d</code> removes the most recently added object from the vispy 3d viewer</li> <li>New experimental functions for (pairwise) alignment of neurons using the <code>pycpd</code> package: <code>navis.nblast_align</code>, <code>navis.align.align_deform</code>, <code>navis.align.align_rigid</code>, <code>navis.align.align_pca</code>, <code>navis.align.align_pairwise</code></li> <li>New <code>NeuronList</code> method: <code>navis.NeuronList.set_neuron_attributes</code></li> <li>New utility functions: <code>navis.nbl.compress_scores</code>, <code>navis.nbl.nblast_prime</code></li> </ul>"},{"location":"changelog/#improvements_7","title":"Improvements","text":"<ul> <li><code>navis.xform_brain</code> now recognizes the target template's units if available</li> <li>Improved persistence functions: <code>navis.persistence_distances</code>, <code>navis.persistence_vector</code>, <code>navis.persistence_diagram</code></li> <li><code>navis.longest_neurite</code> and <code>navis.cell_body_fiber</code> now also allow removing the longest neurite and CBF, respectively</li> <li><code>navis.heal_skeleton</code> now accepts a <code>mask</code> parameter that allows restricting where fragments are stitched</li> </ul>"},{"location":"changelog/#fixes_5","title":"Fixes","text":"<ul> <li>Various other bugfixes</li> </ul> <p>Full Changelog: v1.4.0...v1.5.0</p>"},{"location":"changelog/#version-140","title":"Version <code>1.4.0</code>","text":"<p>Date: 21/12/22</p>"},{"location":"changelog/#breaking_6","title":"Breaking","text":"<ul> <li><code>navis.flow_centrality</code> was renamed to <code>navis.synapse_flow_centrality</code> and a new non-synaptic <code>navis.flow_centrality</code> function was added. This also impacts the <code>method</code> parameter in <code>navis.split_axon_dendrite</code>!</li> <li><code>vispy</code> is now a soft dependency</li> </ul>"},{"location":"changelog/#additions_5","title":"Additions","text":"<ul> <li>New function: <code>navis.read_tiff</code> to read image stacks from TIFF files</li> <li>New utility function: <code>navis.nbl.extract_matches</code></li> </ul>"},{"location":"changelog/#improvements_8","title":"Improvements","text":"<ul> <li>NBLASTs: single progress bar instead of one for each process</li> <li>New <code>via</code> parameter for <code>navis.xform_brain</code></li> <li><code>navis.write_swc</code> can now save Dotprops to SWC files</li> <li><code>navis.make_dotprops</code> can now downsample point cloud inputs</li> <li>Various improvements to <code>navis.split_axon_dendrite</code>, <code>navis.nblast_allbyall</code>, <code>navis.interfaces.neuprint.fetch_mesh_neuron</code>, <code>navis.interfaces.neuprint.fetch_skeletons</code></li> </ul>"},{"location":"changelog/#fixes_6","title":"Fixes","text":"<ul> <li>Tons of bug fixes</li> </ul> <p>Full Changelog: v1.3.1...v1.4.0</p>"},{"location":"changelog/#version-131","title":"Version <code>1.3.1</code>","text":"<p>Date: 10/06/22</p>"},{"location":"changelog/#fixes_7","title":"Fixes","text":"<ul> <li>Various bugs fixed</li> </ul> <p>Full Changelog: v1.3.0...v1.3.1</p>"},{"location":"changelog/#version-130","title":"Version <code>1.3.0</code>","text":"<p>Date: 10/05/22</p>"},{"location":"changelog/#breaking_7","title":"Breaking","text":"<ul> <li>As of this version <code>pip install navis</code> won't install a vispy backend</li> </ul>"},{"location":"changelog/#additions_6","title":"Additions","text":"<ul> <li>New interface to fetch data from Virtual Fly Brain: <code>navis.interfaces.vfb</code></li> <li>Tools to build custom NBLAST score matrices</li> <li>Bayesian implementation of the network traversal model: <code>navis.models.network_models.BayesianTraversalModel</code></li> <li>New morphometrics functions: <code>navis.segment_analysis</code> &amp; <code>navis.form_factor</code></li> <li>New function to write meshes: <code>navis.write_mesh</code></li> </ul>"},{"location":"changelog/#improvements_9","title":"Improvements","text":"<ul> <li>NBLASTs: new <code>approx_nn</code> parameter</li> <li>Example neurons now come with some meta data</li> </ul>"},{"location":"changelog/#fixes_8","title":"Fixes","text":"<ul> <li>Lots of fixes and improvements in particular for I/O-related functions</li> </ul> <p>Full Changelog: v1.2.1...v1.3.0</p>"},{"location":"changelog/#version-121","title":"Version <code>1.2.1</code>","text":"<p>Date: 25/02/22</p>"},{"location":"changelog/#fixes_9","title":"Fixes","text":"<ul> <li>Hot fix for <code>navis.split_axon_dendrite</code></li> </ul> <p>Full Changelog: v1.2.0...v1.2.1</p>"},{"location":"changelog/#version-120","title":"Version <code>1.2.0</code>","text":"<p>Date: 24/02/22</p>"},{"location":"changelog/#additions_7","title":"Additions","text":"<ul> <li>New function: <code>navis.betweeness_centrality</code></li> <li>New function: <code>navis.combine_neurons</code> to simply concatenate neurons</li> <li>New set of persistence functions: <code>navis.persistence_vectors</code>, <code>navis.persistence_points</code> and <code>navis.persistence_distances</code></li> <li>Added a new interface with the Allen Cell Types Atlas</li> </ul>"},{"location":"changelog/#improvements_10","title":"Improvements","text":"<ul> <li>Improvements to various functions: e.g. <code>navis.bending_flow</code>, <code>navis.synapse_flow_centrality</code>, <code>navis.split_axon_dendrite</code>, <code>navis.longest_neurite</code></li> <li><code>navis.write_nrrd</code> and <code>navis.read_nrrd</code> can now be used to write/read Dotprops to/from NRRD files</li> <li><code>navis.read_swc</code> now accepts a <code>limit</code> parameter that enables reading on the first N neurons</li> <li><code>navis.nblast</code> (and variants) now accept a <code>precision</code> parameter</li> <li><code>navis.simplify_mesh</code> (and therefore <code>navis.downsample_neuron</code> with skeletons) now uses the <code>pyfqmr</code> if present</li> <li>Improved the interface to Neuromorpho</li> </ul>"},{"location":"changelog/#fixes_10","title":"Fixes","text":"<ul> <li>Myriads of small and big bugfixes</li> </ul> <p>Full Changelog: v1.1.0...v1.2.0</p>"},{"location":"changelog/#version-110","title":"Version <code>1.1.0</code>","text":"<p>Date: 18/11/21</p>"},{"location":"changelog/#additions_8","title":"Additions","text":"<ul> <li>New function: <code>navis.sholl_analysis</code></li> <li>Plotly is now correctly chosen as default backend in Google colab</li> </ul>"},{"location":"changelog/#fixes_11","title":"Fixes","text":"<ul> <li>Fixed a critical bug with plotting skeletons with plotly <code>5.4.0</code></li> </ul> <p>Full Changelog: v1.0.0...v1.1.0</p>"},{"location":"changelog/#version-100","title":"Version <code>1.0.0</code>","text":"<p>Date: 11/11/21</p>"},{"location":"changelog/#breaking_8","title":"Breaking","text":"<ul> <li><code>navis.MeshNeuron</code>: <code>__getattr__</code> does not search <code>trimesh</code> representation anymore</li> <li>NBLASTs: queries/targets now MUST be <code>navis.Dotprops</code> (no more automatic conversion, use <code>navis.make_dotprops</code>)</li> <li>Renamed functions to make it clear they work only on <code>TreeNeurons</code>:</li> <li><code>smooth_neuron</code> <code>navis.smooth_skeleton</code></li> <li><code>reroot_neuron</code> <code>navis.reroot_skeleton</code></li> <li><code>rewire_neuron</code> <code>navis.rewire_skeleton</code></li> <li><code>despike_neuron</code> <code>navis.despike_skeleton</code></li> <li><code>average_neurons</code> <code>navis.average_skeletons</code></li> <li><code>heal_fragmented_neuron</code> <code>navis.heal_skeleton</code></li> <li><code>stitch_neurons</code> <code>navis.stitch_skeletons</code></li> <li><code>cut_neuron</code> <code>navis.cut_skeleton</code></li> <li>Removals and other renamings:<ul> <li><code>navis.clustering</code> module was removed and with it <code>navis.cluster_xyz</code> and <code>ClustResult</code> class</li> <li>renamed <code>cluster_by_synapse_placement</code> <code>navis.synapse_similarity</code></li> <li>renamed <code>cluster_by_connectivity</code> <code>navis.connectivity_similarity</code></li> <li>renamed <code>sparseness</code> <code>navis.connectivity_sparseness</code></li> <li>renamed <code>navis.write_google_binary</code> <code>navis.write_precomputed</code></li> </ul> </li> <li><code>navis.geodesic_matrix</code> renamed parameter <code>tn_ids</code> <code>from_</code></li> </ul>"},{"location":"changelog/#additions-improvements","title":"Additions &amp; Improvements","text":"<ul> <li><code>navis.NeuronList.apply()</code> now allows omitting failures</li> <li><code>navis.VoxelNeuron</code>:<ul> <li>new class representing neurons as voxels</li> <li>new (experimental) class representing neurons as voxels</li> <li><code>navis.read_nrrd</code> now returns <code>VoxelNeuron</code> instead of <code>Dotprops</code> by default</li> <li>currently works with only a selection of functions</li> </ul> </li> <li><code>navis.TreeNeuron</code>:<ul> <li>can now be initialized directly with <code>skeletor.Skeleton</code></li> <li>new method: <code>navis.TreeNeuron.snap</code></li> </ul> </li> <li><code>navis.MeshNeuron</code>:<ul> <li><code>navis.in_volume</code>, <code>navis.subset_neuron</code> and <code>navis.break_fragments</code> now work on <code>MeshNeurons</code></li> <li>new properties: <code>.skeleton</code>, <code>.graph</code> and <code>.igraph</code></li> <li>new methods: <code>navis.MeshNeuron.skeletonize</code> and <code>navis.MeshNeuron.snap</code></li> <li>can now be initialized with <code>skeletor.Skeleton</code> and <code>(vertices, faces)</code> tuple</li> <li>plotting: <code>color_by</code> parameter now works with <code>MeshNeurons</code></li> </ul> </li> <li><code>navis.Dotprops</code>:<ul> <li>new property: <code>.sampling_resolution</code> (used e.g. for scaling vectors for plotting)</li> <li>new method: <code>navis.Dotprops.snap</code></li> </ul> </li> <li>Experimental support for non-isometric <code>.units</code> for neurons</li> <li>NBLASTs:<ul> <li>new parameter <code>limit_dist</code> allows speeding up NBLASTs with minor precision loss</li> <li>new experimental parameter <code>batch_size</code> to NBLAST neurons in batches</li> <li>overall faster initialization with large lists of neurons</li> </ul> </li> <li>SWC I/O (<code>navis.read_swc</code> &amp; <code>navis.write_swc</code>):<ul> <li>by default we will now deposit neuron meta data (name, id, units) in the SWC header (see <code>write_meta</code> parameter)</li> <li>meta data in SWC header can also be read back (see <code>read_meta</code> parameter)</li> <li>filenames can now be parsed into specific neuron properties (see <code>fmt</code> parameter)</li> <li>node IDs now start with 0 instead of 1 when writing SWC files</li> </ul> </li> <li>I/O to/from Google neuroglancer's precomputed format:<ul> <li>total rework of this module</li> <li>renamed <code>navis.write_google_binary</code> <code>navis.write_precomputed</code></li> <li>new function: <code>navis.read_precomputed</code></li> </ul> </li> <li>Plotting:<ul> <li>new function <code>navis.plot_flat</code> plots neurons as dendrograms</li> <li><code>navis.plot3d</code> with plotly backend now returns a plotly <code>Figure</code> instead of a figure dictionary</li> <li>new k3d backend for plotting in Jupyter environments: try <code>navis.plot3d(x, backend='k3d')</code></li> <li>new parameter for <code>navis.plot2d</code> and <code>navis.plot3d</code>: use <code>clusters=[0, 0, 0, 1, 1, ...]</code> to assigns clusters and have them automatically coloured accordingly</li> <li><code>navis.plot2d</code> now allows <code>radius=True</code> parameter</li> </ul> </li> <li>Transforms:<ul> <li>support for elastix (<code>navis.transforms.ElastixTransform</code>)</li> <li>whether transforms are invertible is now determined by existence of <code>__neg__</code> method</li> </ul> </li> <li>Most functions that work with <code>TreeNeurons</code> now also work with <code>MeshNeurons</code></li> <li>New high-level wrappers to convert neurons: <code>navis.voxelize</code>, <code>navis.mesh</code> and <code>navis.skeletonize</code></li> <li><code>navis.make_dotprops</code> now accepts <code>parallel=True</code> parameter for parallel processing</li> <li><code>navis.smooth_skeleton</code> can now be used to smooth arbitrary numeric columns in the node table</li> <li>New function <code>navis.drop_fluff</code> removes small disconnected bits and pieces from neurons</li> <li>New function <code>navis.patch_cloudvolume</code> monkey-patches <code>cloudvolume</code> (see the new tutorial)</li> <li>New function <code>navis.write_nrrd</code> writes <code>VoxelNeurons</code> to NRRD files</li> <li>New functions to read/write <code>MeshNeurons</code>: <code>navis.read_mesh</code> and <code>navis.write_mesh</code></li> <li>New function <code>navis.read_nmx</code> reads pyKNOSSOS files</li> <li>New function <code>navis.smooth_mesh</code> smoothes meshes and <code>MeshNeurons</code></li> <li>Improved/updated the InsectBrain DB interface (see the tutorial)</li> <li>Under-the-hood fixes and improvements to: <code>navis.plot2d</code>, <code>navis.split_axon_dendrite</code>, <code>navis.tortuosity</code>, <code>navis.resample_skeleton</code>, <code>navis.mirror_brain</code></li> <li>First pass at a <code>NEURON</code> interface (see the new tutorial)</li> <li>First pass at interface with the Allen's MICrONS datasets (see the new tutorial)</li> <li><code>NAVIS_SKIP_LOG_SETUP</code> environment variable prevents default log setup for library use</li> <li>Improved <code>navis.cable_overlap</code></li> </ul>"},{"location":"changelog/#fixes_12","title":"Fixes","text":"<ul> <li>Under-the-hood fixes and improvements</li> </ul> <p>Full Changelog: v0.6.0...v1.0.0</p>"},{"location":"changelog/#version-060","title":"Version <code>0.6.0</code>","text":"<p>Date: 12/05/21</p>"},{"location":"changelog/#additions_9","title":"Additions","text":"<ul> <li>new functions: <code>navis.prune_at_depth</code>, <code>navis.read_rda</code>, <code>navis.cell_body_fiber</code></li> <li>new functions to map units into neuron space: <code>BaseNeuron.map_units</code> and <code>navis.to_neuron_space</code></li> </ul>"},{"location":"changelog/#improvements_11","title":"Improvements","text":"<ul> <li>many spatial parameters (e.g. in <code>navis.resample_skeleton</code>) can now be passed as unit string, e.g. <code>\"5 microns\"</code></li> <li>many functions now accept a <code>parallel=True</code> parameter to use multiple cores (depends on <code>pathos</code>)</li> <li><code>navis.read_swc</code> and <code>navis.write_swc</code> can now read/write directly from/to zip files</li> <li>reworked <code>navis.read_json</code>, and <code>navis.write_json</code></li> <li><code>nblast</code> functions now let you use your own scoring function (thanks to Ben Pedigo!)</li> <li>added <code>threshold</code> parameter to <code>navis.read_nrrd</code></li> <li><code>navis.nblast_smart</code>: drop <code>quantile</code> and add <code>score</code> criterion</li> <li>functions that manipulate neurons will now always return something (even if <code>inplace=True</code>)</li> <li><code>navis.cut_skeleton</code> now always returns a single <code>NeuronList</code></li> <li><code>navis.mirror_brain</code> now works with <code>k=0/None</code> Dotprops</li> <li>all <code>reroot_to_soma</code> parameters have been renamed to <code>reroot_soma</code></li> <li><code>navis.TreeNeuron</code> now has a <code>soma_pos</code> property that can also be used to set the soma by position</li> <li>made transforms more robust against points outside deformation fields</li> <li>better deal if node ID of soma is <code>0</code> (e.g. during plotting)</li> <li><code>navis.neuron2tangents</code> now drops zero-length vectors</li> </ul>"},{"location":"changelog/#fixes_13","title":"Fixes","text":"<ul> <li>fixed <code>navis.guess_radius</code></li> <li>fixed NBLAST progress bars in notebook environments</li> <li>fixed a couple bugs with <code>CMTK</code> transforms</li> </ul> <p>Full Changelog: v0.5.3...v0.6.0</p>"},{"location":"changelog/#version-053","title":"Version <code>0.5.3</code>","text":"<p>Date: 10/04/21</p>"},{"location":"changelog/#additions_10","title":"Additions","text":"<ul> <li>new functions: <code>navis.nblast_smart</code>, <code>navis.synblast</code>, <code>navis.symmetrize_brain</code></li> <li><code>navis.plot2d</code>: <code>rasterize=True</code> will rasterize neurons (but not axes or labels) to help keep file sizes low</li> <li><code>navis.plot3d</code> (plotly): <code>hover_name=True</code> will show neuron names on hover</li> </ul>"},{"location":"changelog/#improvements_12","title":"Improvements","text":"<ul> <li><code>navis.simplify_mesh</code> now supports 3 backends: Blender3D, <code>open3d</code> or <code>pymeshlab</code></li> <li><code>navis.make_dotprops</code> can now produce <code>Dotprops</code> purely from skeleton edges (set <code>k=None</code>)</li> <li>reworked <code>navis.write_swc</code> (faster, easier to work with)</li> <li>a new type of landmark-based transform: moving least square transforms (thanks to Chris Barnes)</li> <li>vispy <code>navis.Viewer</code>: press B to show a bounding box</li> <li>moved tests from Travis to Github Actions (this now also includes testing tutorial notebooks)</li> </ul>"},{"location":"changelog/#fixes_14","title":"Fixes","text":"<ul> <li>a great many small and big bug fixes</li> </ul> <p>Full Changelog: v0.5.2...v0.5.3</p>"},{"location":"changelog/#version-052","title":"Version <code>0.5.2</code>","text":"<p>Date: 02/02/21</p>"},{"location":"changelog/#additions_11","title":"Additions","text":"<ul> <li>new functions: <code>navis.xform</code>, <code>navis.write_precomputed</code></li> </ul>"},{"location":"changelog/#improvements_13","title":"Improvements","text":"<ul> <li><code>navis.downsample_neuron</code> now also works on <code>Dotprops</code></li> <li>Neurons: connectors are now included in bounding box calculations</li> <li>NeuronLists: added progress bar for division / multiplication</li> </ul> <p>Full Changelog: v0.5.1...v0.5.2</p>"},{"location":"changelog/#version-051","title":"Version <code>0.5.1</code>","text":"<p>Date: 10/01/21</p>"},{"location":"changelog/#fixes_15","title":"Fixes","text":"<ul> <li>Various under-the-hood improvements and bugfixes</li> </ul> <p>Full Changelog: v0.5.0...v0.5.1</p>"},{"location":"changelog/#version-050","title":"Version <code>0.5.0</code>","text":"<p>Date: 05/01/21</p>"},{"location":"changelog/#additions_12","title":"Additions","text":"<ul> <li>new functions for transforming spatial data (locations, neurons, etc) between brain spaces:<ul> <li><code>navis.xform_brain</code> transforms data from one space to another</li> <li><code>navis.mirror_brain</code> mirrors data about given axis</li> <li>see the new tutorials for explanations</li> </ul> </li> <li>low-level interfaces to work with affine, H5-, CMTK- and thin plate spline transforms</li> </ul>"},{"location":"changelog/#improvements_14","title":"Improvements","text":"<ul> <li>de-cluttered top level namespace: some more obscure functions are now only available through modules</li> </ul> <p>Full Changelog: v0.4.3...v0.5.0</p>"},{"location":"changelog/#version-043","title":"Version <code>0.4.3</code>","text":"<p>Date: 22/12/20</p>"},{"location":"changelog/#fixes_16","title":"Fixes","text":"<ul> <li>Small bugfixes</li> </ul> <p>Full Changelog: v0.4.2...v0.4.3</p>"},{"location":"changelog/#version-042","title":"Version <code>0.4.2</code>","text":"<p>Date: 22/12/20</p>"},{"location":"changelog/#fixes_17","title":"Fixes","text":"<ul> <li>Small bugfixes</li> </ul> <p>Full Changelog: v0.4.1...v0.4.2</p>"},{"location":"changelog/#version-041","title":"Version <code>0.4.1</code>","text":"<p>Date: 06/12/20</p>"},{"location":"changelog/#fixes_18","title":"Fixes","text":"<ul> <li>Critical bugfix in NBLAST</li> </ul> <p>Full Changelog: v0.4.0...v0.4.1</p>"},{"location":"changelog/#version-040","title":"Version <code>0.4.0</code>","text":"<p>Date: 06/12/20</p>"},{"location":"changelog/#additions_13","title":"Additions","text":"<ul> <li>native implementation of NBLAST: <code>navis.nblast</code> and <code>navis.nblast_allbyall</code>!</li> <li>new parameter <code>navis.plot3d</code> (plotly backend) with <code>hover_id=True</code> will show node IDs on hover</li> <li><code>navis.Volume.resize</code> has now <code>inplace=False</code> as default</li> </ul> <p>Full Changelog: v0.3.4...v0.4.0</p>"},{"location":"changelog/#version-034","title":"Version <code>0.3.4</code>","text":"<p>Date: 24/11/20</p>"},{"location":"changelog/#improvements_15","title":"Improvements","text":"<ul> <li>improved <code>navis.Dotprops</code>:</li> <li>more control over generation in <code>navis.make_dotprops</code></li> <li><code>navis.Dotprops</code> now play nicely with R interface</li> </ul> <p>Full Changelog: v0.3.3...v0.3.4</p>"},{"location":"changelog/#version-033","title":"Version <code>0.3.3</code>","text":"<p>Date: 23/11/20</p>"},{"location":"changelog/#additions_14","title":"Additions","text":"<ul> <li>new module: <code>models</code> for modelling networks and neurons</li> <li>new functions <code>navis.resample_along_axis</code>, <code>navis.insert_nodes</code>, <code>navis.remove_nodes</code></li> <li>full rework of <code>navis.Dotprops</code>:</li> <li>make them a subclass of BaseNeuron</li> <li>implement <code>nat:dotprops</code> in <code>navis.make_dotprops</code></li> <li>added <code>navis.read_nrrd</code> and <code>navis.write_nrrd</code></li> <li>side-effect: renamed <code>navis.from_swc</code> <code>read_swc</code> and <code>navis.to_swc</code> <code>write_swc</code></li> <li>improved conversion between nat and NAVis <code>Dotprops</code></li> <li>full rework of topology-related functions:</li> <li><code>navis.strahler_index</code>, <code>navis.segregation_index</code>, <code>navis.bending_flow</code>, <code>navis.synapse_flow_centrality</code> and <code>navis.split_axon_dendrite</code> now work better, faster and more accurately. See their docs for details.</li> <li>new function: <code>navis.arbor_segregation_index</code></li> <li>new <code>color_by</code> and <code>shade_by</code> parameters for <code>plot3d</code> and <code>plot2d</code> that lets you color/shade a neuron by custom properties (e.g. by Strahler index or compartment)</li> </ul>"},{"location":"changelog/#improvements_16","title":"Improvements","text":"<ul> <li>neurons are now more memory efficient:<ul> <li>pandas \"categoricals\" are used for connector and node \"type\" and \"label\" columns</li> <li>add a <code>.memory_usage</code> method analogous to that of <code>pandas.DataFrames</code></li> </ul> </li> <li><code>navis.NeuronList</code> can now be pickled!</li> <li>made <code>navis.Viewer</code> faster</li> <li><code>navis.prune_twigs</code> can now (optionally) prune by <code>exactly</code> the desired length</li> <li>improved <code>navis.NeuronList.apply</code></li> </ul>"},{"location":"changelog/#fixes_19","title":"Fixes","text":"<ul> <li>small bugfixes and improvements</li> </ul> <p>Full Changelog: v0.3.2...v0.3.3</p>"},{"location":"changelog/#version-032","title":"Version <code>0.3.2</code>","text":"<p>Date: 18/10/20</p>"},{"location":"changelog/#improvements_17","title":"Improvements","text":"<ul> <li><code>navis.plot2d</code> and <code>navis.plot3d</code> now accept <code>trimesh.Trimesh</code> directly</li> <li><code>navis.in_volume</code> now works with any mesh-like object, not just <code>navis.Volumes</code></li> </ul>"},{"location":"changelog/#fixes_20","title":"Fixes","text":"<ul> <li>lots of small bugfixes and improvements</li> </ul> <p>Full Changelog: v0.3.1...v0.3.2</p>"},{"location":"changelog/#version-031","title":"Version <code>0.3.1</code>","text":"<p>Date: 07/10/20</p>"},{"location":"changelog/#additions_15","title":"Additions","text":"<ul> <li>new function <code>navis.rewire_skeleton</code></li> </ul>"},{"location":"changelog/#improvements_18","title":"Improvements","text":"<ul> <li><code>navis.heal_skeleton</code> and <code>navis.stitch_skeletons</code> are now much much faster</li> <li><code>navis.reroot_skeleton</code> can now reroot to multiple roots in one go</li> <li><code>navis.plot3d</code> now accepts a <code>soma</code> argument</li> <li>improved caching for neurons</li> <li>improved multiplication/division of neurons</li> <li>faster <code>r.nblast</code> and <code>r.nblast_allbyall</code></li> <li><code>r.xform_brain</code> now also adjusts the soma radius</li> <li><code>neuprint.fetch_skeletons</code> now returns correct soma radius</li> </ul>"},{"location":"changelog/#fixes_21","title":"Fixes","text":"<ul> <li>lots of small bugfixes</li> </ul> <p>Full Changelog: v0.3.0...v0.3.1</p>"},{"location":"changelog/#version-030","title":"Version <code>0.3.0</code>","text":"<p>Date: 06/10/20</p>"},{"location":"changelog/#additions_16","title":"Additions","text":"<ul> <li>Started module to manipulate mesh data (see e.g. <code>navis.simplify_mesh</code>)</li> </ul>"},{"location":"changelog/#improvements_19","title":"Improvements","text":"<ul> <li>Improved interfaces with R NBLAST and <code>xform_brain</code></li> <li>Improved attribute caching for neurons</li> </ul> <p>Full Changelog: v0.2.3...v0.3.0</p>"},{"location":"changelog/#version-023","title":"Version <code>0.2.3</code>","text":"<p>Date: 06/09/20</p>"},{"location":"changelog/#additions_17","title":"Additions","text":"<ul> <li>New Neuron property <code>.label</code> that if present will be used for plot legends</li> <li>New function for R interface: <code>navis.interfaces.r.load_rda</code></li> </ul>"},{"location":"changelog/#improvements_20","title":"Improvements","text":"<ul> <li>Blender interface: improved scatter plot generation</li> </ul>"},{"location":"changelog/#version-022","title":"Version <code>0.2.2</code>","text":"<p>Date: 15/08/20</p>"},{"location":"changelog/#additions_18","title":"Additions","text":"<ul> <li>New <code>plot3d</code> parameter: with plotly backend, use <code>fig</code> to add data to existing plotly figure</li> <li>New <code>plot3d</code> parameter: with vispy backend, use <code>center=False</code> to not re-center camera on adding new data</li> <li>New <code>r.mirror_brain</code> parameter: use e.g. <code>via='FCWB'</code> if source space does not have mirror transform</li> <li>New <code>NeuronList</code> method: <code>append()</code> works like <code>list.append()</code></li> <li>First implementation of smarter (re-)calculation of temporary Neuron properties using <code>.is_stale</code> property</li> <li>Neurons can now be multiplied/divided by array/list of x/y/z coordinates for non-isometric transforms</li> </ul>"},{"location":"changelog/#fixes_22","title":"Fixes","text":"<ul> <li>Fix issues with newer rpy2 versions</li> <li>Various improvements and bug fixes</li> </ul>"},{"location":"changelog/#version-021","title":"Version <code>0.2.1</code>","text":"<p>Date: 20/04/20</p>"},{"location":"changelog/#additions_19","title":"Additions","text":"<ul> <li>New <code>plot3d</code> parameter: with plotly backend, use <code>radius=True</code> plots TreeNeurons with radius</li> <li>New <code>plot2d</code> parameter: <code>orthogonal=False</code> sets view to perspective</li> </ul>"},{"location":"changelog/#improvements_21","title":"Improvements","text":"<ul> <li>Various improvements to e.g. <code>nx2neuron</code></li> </ul>"},{"location":"changelog/#version-020","title":"Version <code>0.2.0</code>","text":"<p>Date: 29/06/20</p>"},{"location":"changelog/#breaking_9","title":"Breaking","text":"<ul> <li><code>navis.nx2neuron</code> now returns a <code>navis.TreeNeuron</code> instead of a <code>DataFrame</code></li> </ul>"},{"location":"changelog/#additions_20","title":"Additions","text":"<ul> <li>New neuron class <code>navis.MeshNeuron</code></li> <li>New <code>navis.TreeNeuron</code> property <code>.volume</code></li> <li>New example data from the Janelia hemibrain data set</li> </ul>"},{"location":"changelog/#improvements_22","title":"Improvements","text":"<ul> <li>Clean-up in neuromorpho interface</li> <li>We now use ncollpyde for ray casting (intersections)</li> </ul>"},{"location":"changelog/#fixes_23","title":"Fixes","text":"<ul> <li>Fix bugs in <code>navis.Volume</code> pickling</li> </ul>"},{"location":"changelog/#version-0116","title":"Version <code>0.1.16</code>","text":"<p>Date: 26/05/20</p>"},{"location":"changelog/#fixes_24","title":"Fixes","text":"<ul> <li>Many small bugfixes</li> </ul>"},{"location":"changelog/#version-0115","title":"Version <code>0.1.15</code>","text":"<p>Date: 15/05/20</p>"},{"location":"changelog/#improvements_23","title":"Improvements","text":"<ul> <li>Improvements to R and Blender interface</li> <li>Improved loading from SWCs (up to 2x faster)</li> <li><code>TreeNeurons</code>: allow rerooting by setting the <code>.root</code> attribute</li> </ul>"},{"location":"changelog/#version-0114","title":"Version <code>0.1.14</code>","text":"<p>Date: 05/05/20</p>"},{"location":"changelog/#fixes_25","title":"Fixes","text":"<ul> <li>Emergency fixes for critical bugs</li> </ul>"},{"location":"changelog/#version-0113","title":"Version <code>0.1.13</code>","text":"<p>Date: 05/05/20</p>"},{"location":"changelog/#additions_21","title":"Additions","text":"<ul> <li>new function: <code>navis.vary_color</code></li> </ul>"},{"location":"changelog/#improvements_24","title":"Improvements","text":"<ul> <li>improvements to Blender interface and various other functions</li> </ul>"},{"location":"changelog/#version-0112","title":"Version <code>0.1.12</code>","text":"<p>Date: 02/04/20</p>"},{"location":"changelog/#imnprovements","title":"Imnprovements","text":"<ul> <li><code>navis.Volume</code> is now sublcass of <code>trimesh.Trimesh</code></li> </ul>"},{"location":"changelog/#version-0111","title":"Version <code>0.1.11</code>","text":"<p>Date: 28/02/20</p>"},{"location":"changelog/#improvements_25","title":"Improvements","text":"<ul> <li>improved <code>navis.stitch_neurons</code>: much faster now if you have iGraph</li> </ul>"},{"location":"changelog/#fixes_26","title":"Fixes","text":"<ul> <li>fixed errors when using multiprocessing (e.g. in <code>NeuronList.apply</code>)</li> <li>fixed bugs in <code>navis.downsample_neuron</code></li> </ul>"},{"location":"changelog/#version-0110","title":"Version <code>0.1.10</code>","text":"<p>Date: 24/02/20</p>"},{"location":"changelog/#fixes_27","title":"Fixes","text":"<ul> <li>Fixed bugs in Blender interface introduced in 0.1.9</li> </ul>"},{"location":"changelog/#version-019","title":"Version <code>0.1.9</code>","text":"<p>Date: 24/02/20</p>"},{"location":"changelog/#fixes_28","title":"Fixes","text":"<ul> <li>Removed hard-coded swapping and translation of axes in the Blender interface</li> <li>Fixed bugs in <code>navis.stitch_neurons</code></li> </ul>"},{"location":"changelog/#version-018","title":"Version <code>0.1.8</code>","text":"<p>Date: 21/02/20</p>"},{"location":"changelog/#fixes_29","title":"Fixes","text":"<ul> <li>Again lots of fixed bugs</li> </ul>"},{"location":"changelog/#version-010","title":"Version <code>0.1.0</code>","text":"<p>Date: 23/05/19</p>"},{"location":"changelog/#fixes_30","title":"Fixes","text":"<ul> <li>Many small bugfixes</li> </ul>"},{"location":"changelog/#version-001","title":"Version <code>0.0.1</code>","text":"<p>Date: 29/01/19</p>"},{"location":"changelog/#fixes_31","title":"Fixes","text":"<ul> <li>First commit, lots to fix.</li> </ul>"},{"location":"ecosystem/","title":"<span style=\"color:rgb(250,175,3)\">NAVis</span> & Friends","text":""},{"location":"ecosystem/#navis-friends","title":"NAVis &amp; Friends","text":"<p>NAVis comes with batteries included but is also highly extensible. Here are some libraries that are built directly on top of NAVis.</p> <p></p>"},{"location":"ecosystem/#flybrains","title":"flybrains","text":"<p>flybrains is a package that bundles fly template brains and transforms that NAVis can use to map spatial data (e.g. neurons) from one brain space to another. If you installed NAVis via <code>pip</code> with the <code>[flybrains]</code> option, you should already have this package.</p> <pre><code>import navis\nimport flybrains  # importing registers the transforms with NAVis\n\n# Plot one of the template brains\nnavis.plot2d(flybrains.JRC2018U)\n\n# Transform neurons to another brain space\nn = navis.example_neurons(3, kind='skeleton')\nxf = navis.xform_brain(n, source='JRCFIB2018F', target='JRC2018F')\n</code></pre>"},{"location":"ecosystem/#pymaid","title":"pymaid","text":"<p>pymaid provides an interface with CATMAID servers. It allows you to pull data (neurons, connectivity) that can be directly plugged into NAVis. Conversely, you can also take NAVis neurons and push them to a CATMAID server. <code>pymaid</code> is a great example of how to extend NAVis.</p> <pre><code>import navis\nimport pymaid\n\n# Connect to a public CATMAID server\nrm = pymaid.CatmaidInstance(server=\"https://fafb.catmaid.virtualflybrain.org/\", api_token=None)\n\n# Fetch some neurons\nnl = pymaid.get_neurons('annotation:Paper: Engert et al 2022')\n\n# CATMAID neurons can be directly used in all NAVis functions\nnavis.plot2d(nl, radius=False)\n</code></pre>"},{"location":"ecosystem/#fafbseg","title":"fafbseg","text":"<p>fafbseg contains tools to work with autosegmented data for the FAFB (full adult fly brain) EM dataset. It brings together data from FlyWire, Google's segmentation of FAFB and synapse predictions by Buhmann et al. (2019).</p> <pre><code>from fafbseg import flywire\n\n# Grab a neuron mesh by its ID\nn = flywire.get_mesh_neuron(720575940613091290)\n\n# Skeletonize using NAVis\ns navis.skeletonize(n)\n</code></pre>"},{"location":"ecosystem/#natverse","title":"natverse","text":"<p>The natverse is NAVis's equivalent in R. While we are aiming for feature parity, it can be useful to access <code>natverse</code> functions from Python. For this, NAVis offers some convenience functions using the R-Python interface <code>rpy2</code>. Check out the tutorial.</p>"},{"location":"gallery/","title":"Tutorials","text":""},{"location":"gallery/#tutorials","title":"Tutorials","text":"<p>These tutorials are meant to illustrate certain NAVis functions. As per usual: have a look at the API reference to find out details about a given function. There you will also find more examples.</p> <ul> <li> <p> The Basics</p> <p>These tutorials will teach you the basics of working with NAVis neurons:</p> <ul> <li>Basics</li> </ul> </li> </ul> <p>Not seeing what you're looking for? Need additional pointers? Found a broken example? Open an Issue or use Discussions on Github to ask questions!</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installing-navis","title":"Installing NAVis","text":"<p>NAVis requires Python 3.10 or later. The instructions below assume that you have already installed Python and its package manager <code>pip</code>.</p> <p>By the way</p> <p>You can use NAVis without having to install anything on your local machine! Follow this link to open an example notebook in Google's Colaboratory.</p> <p>NAVis is published as a Python package and can be installed with <code>pip</code>, ideally by using a virtual environment. Open up a terminal and install NAVis with:</p> Full InstallMinimalDev <p>This is the \"batteries included\" install that will install NAVis plus a number of extra dependencies that are just nice to have.</p> <pre><code>pip install navis[all] -U\n</code></pre> <p>If you run into issues, try the minimal install instead.</p> <p>If you're running into issues with the full install, you can try this minimal install instead:</p> <pre><code>pip install navis -U\n</code></pre> <p>If you go down this route some functions in NAVis might complain about missing dependencies. No worries though: they should also tell you how to install them. See also the section on Optional Dependencies below.</p> <p>To install the latest version from Github:</p> <pre><code>pip install git+https://github.com/navis-org/navis@master\n</code></pre> <p>To install the latest dev with extras:</p> <pre><code>pip install \"navis[all] @ git+https://github.com/navis-org/navis@master\"\n</code></pre> <p>Note</p> <p>MacOS (both Intel and the new ARM chips) and Linux should work off the bat without any problems. On Windows, you might run into issues with some of the dependencies. If that happens, we recommend you check out the Windows Subsystem for Linux (WSL).</p>"},{"location":"installation/#optional-dependencies","title":"Optional dependencies","text":"<p>If you installed NAVis using the \"batteries-included\" <code>[all]</code> option, you can ignore this section.</p> <p>If you opted for the minimal install, you might want to consider adding some of these optional dependencies as they provide e.g. speed-boosts in certain situations or are for certain functions.</p> <p>These extras can be installed directly, or along with NAVis with</p> <pre><code>pip install navis[extra1,extra2]\n</code></pre> <p>The user-facing extras, the dependencies they install, and how to install those dependencies directly, are listed below:</p> Performance <p>These dependencies aren't strictly necessary but will speed up certain operations:</p> Visualization <p>NAVis supports various different backends for 2D and 3D visualization. For 2D visualizations we use <code>matplotlib</code> by default which is installed automatically. For 3D visualizations, you can use <code>octarine3d</code>, <code>vispy</code>, <code>plotly</code> or <code>k3d</code> backends.</p> Miscellaneous"},{"location":"installation/#fastcore-navis-fastcore","title":"<code>fastcore</code>: navis-fastcore","text":"<p><code>navis-fastcore</code> re-implements a bunch of low-level functions in Rust and wraps them in Python. NAVis will use <code>fastcore</code> under the hood if it is available. This is a highly recommended extra as it can speed up operations such as geodesic distances, Strahler Index, pruning and other downstream functions by several orders of magnitude.</p> <pre><code>pip install navis-fastcore\n</code></pre>"},{"location":"installation/#kdtree-pykdtree","title":"<code>kdtree</code>: pykdtree","text":"<p>Faster than scipy's cKDTree implementation. If available, will be used to speed up e.g. NBLAST.</p> <pre><code>pip install pykdtree\n</code></pre>"},{"location":"installation/#pathos-pathos","title":"<code>pathos</code>: pathos","text":"<p>Pathos is a multiprocessing library. NAVis uses it to parallelize functions across lists of neurons.</p> <pre><code>pip install pathos\n</code></pre>"},{"location":"installation/#hash-xxhash","title":"<code>hash</code>: xxhash","text":"<p>For speeding up some lookup tables.</p> <pre><code>pip install xxhash\n</code></pre>"},{"location":"installation/#meshes-open3d-pyfqmr","title":"<code>meshes</code>: open3d, pyfqmr","text":"<p>Assorted functionality associated with meshes. <code>pyfqmr</code> in particular is highly recommended if you want to downsample meshes.</p> <pre><code>pip install open3d pyfqmr\n</code></pre>"},{"location":"installation/#octarine3d-octarine3d","title":"<code>octarine3d</code>: octarine3d","text":"<p>For 3D visualisation in terminal and Jupyter notebooks.</p> <p>Octarine3d is a modern, high-performance, WGPU-based viewer for interactive 3D visualisation of neurons and is the default 3D viewer for NAVis. By default, <code>navis[all]</code> will install <code>octarine3d</code> with standard windows manager <code>pyside6</code> and Jupyter notebook manager <code>jupyter_rfb</code>. It will also install the <code>navis-octarine-plugin</code> which is required to use <code>octarine3d</code> as a viewer for NAVis. This is equivalent to the following command:</p> <pre><code>pip install octarine3d[all] octarine-navis-plugin\n</code></pre> <p>Please see <code>octarine3d</code> installation instructions for information on how to choose a different backend.</p> <p>Note</p> <p>Older systems (pre ~2018) might not support WGPU. If you are running into issues try updating your operating system and/or your graphics drivers. Failing that, you can fall back to the <code>vispy</code> backend.</p>"},{"location":"installation/#vispy-backends-vispy","title":"<code>vispy-*</code> backends: vispy","text":"<p>For 3D visualisation in terminal and Jupyter notebooks.</p> <p>Vispy provides a high-performance, OpenGL-based viewer for 3D data visualisation. Vispy itself has a choice of window managers: the one which works for you will depend on your operating system, hardware, other installed packages, and how you're using navis. The default, supplied with NAVis' <code>vispy-default</code> extra, is <code>pyside6</code> (for use from the console) and <code>jupyter_rfb</code> (for use in Jupyter notebooks). Each of vispy's backends, listed here can be installed through vispy and its extras, or NAVis' <code>vispy-*</code> extras.</p> <pre><code>pip install navis[vispy-pyqt5]\n# or\npip install vispy[pyqt5]\n</code></pre> <p>Note</p> <p>The Vispy backend is deprecated in favor of Octarine. We might still decide to keep it if people end up having problems with Octarine. Please get in touch on Github if that's the case.</p>"},{"location":"installation/#plotly-plotly","title":"<code>plotly</code>: plotly","text":"<p>For 3D visualisation in Jupyter notebooks.</p> <pre><code>pip install plotly\n</code></pre>"},{"location":"installation/#k3d-k3d","title":"<code>k3d</code>: k3d","text":"<p>For 3D visualisation in Jupyter notebooks.</p> <pre><code>pip install k3d\n</code></pre>"},{"location":"installation/#r-rpy2","title":"<code>r</code>: Rpy2","text":"<p>Provides interface with R. This allows you to use e.g. the natverse R packages. Note that this package is not installed automatically as it would fail if R is not already installed on the system. You have to install Rpy2 manually!</p> <pre><code>pip install rpy2\n</code></pre>"},{"location":"installation/#shapely-shapely","title":"<code>shapely</code>: Shapely","text":"<p>This is used to get 2D outlines of <code>navis.Volumes</code> when plotting in 2D with <code>volume_outlines=True</code>.</p> <pre><code>pip install shapely\n</code></pre>"},{"location":"installation/#flybrains-flybrains","title":"<code>flybrains</code>: flybrains","text":"<p>Transforming data between some template Drosophila brains.</p> <pre><code>pip install flybrains\n</code></pre>"},{"location":"installation/#cloudvolume-cloud-volume","title":"<code>cloudvolume</code>: cloud-volume","text":"<p>Reading and writing images, meshes, and skeletons in Neuroglancer precomputed format. This is required required for e.g. the MICrONs interface.</p> <pre><code>pip install cloud-volume\n</code></pre>"},{"location":"installation/#what-next","title":"What next?","text":"<ul> <li> <p> Quickstart     ---</p> <p>Check out the quickstart tutorial for an overview of basic concepts in NAVis.</p> <p> Quickstart</p> </li> <li> <p> Tutorials     ---</p> <p>Check out the tutorials!</p> <p> Tutorials</p> </li> </ul>"},{"location":"publications/","title":"Publications","text":""},{"location":"publications/#publications-using-navis","title":"Publications using NAVis","text":"<p>NAVis has been used in a range of high-profile publications<sup>1</sup>. Below we have collected a partial<sup>2</sup> list.</p> <p>Not seeing your work listed here?</p> <p>We'd love to know if you found NAVis useful for your research! You can help us spread the word by citing the DOI provided by Zenodo: <code>10.5281/zenodo.4699382</code></p>"},{"location":"publications/#2026","title":"2026","text":"<ol> <li> Drosophila DNp03 descending neurons serve as a hub within a flight saccade network. Croke et al., Current Biology (2026). 10.1016/j.cub.2025.11.035 </li> </ol> <ol> <li> Inhibitory circuits control leg movements during Drosophila grooming. Syed et al., eLife (2026). 10.7554/eLife.106446.4 </li> </ol> <ol> <li> Neuronal calcium spikes enable vector inversion in the Drosophila brain. Ishida et al., Cell (2026). 10.1016/j.cell.2025.11.040 </li> </ol> <ol> <li> A dedicated brain circuit controls forward walking in Drosophila. Dallmann et al., Cell (2026). 10.64898/2026.01.04.697356 </li> </ol>"},{"location":"publications/#2025","title":"2025","text":"<ol> <li> Sexual dimorphism in the complete connectome of the Drosophila male central nervous system. Berg et al., bioRxiv (2025). 10.1101/2025.10.09.680999 </li> </ol> <ol> <li> The Drosophila escape motor circuit shows differential vulnerability to aging linked to functional decay. Gaitanidis et al., PLOS Biology (2025). 10.1371/journal.pbio.3003553 </li> </ol> <ol> <li> MoMo - Combining Neuron Morphology and Connectivity for Interactive Motif Analysis in Connectomes. Shewarega et al., EEE Transactions on Visualization and Computer Graphics (2025). 10.1109/TVCG.2025.3634808 </li> </ol> <ol> <li> Spatial and morphological organization of mitochondria in neurons across a connectome. Sager et al., Science (2025). 10.1126/science.ads6674 </li> </ol> <ol> <li> Visual Function Profiles via Multi-Path Aggregation Reveal Neuron-Level Responses in the Drosophila Brain. Xie et al., arXiv (2025). 10.48550/arXiv.2512.06934 </li> </ol> <ol> <li> Distinct circuit motifs evaluate opposing innate values of odors. Someya et al., Cell (2025). 10.1016/j.cell.2025.08.032 </li> </ol> <ol> <li> SynAnno: Interactive Guided Proofreading of Synaptic Annotations. Lauenburg et al., EEE Transactions on Visualization and Computer Graphics (2025). 10.1109/TVCG.2025.3634824 </li> </ol> <ol> <li> Population Morphology Implies a Common Developmental Blueprint for Drosophila Motion Detectors. Drummond et al., bioRxiv (2025). 10.1101/2025.11.15.688637 </li> </ol> <ol> <li> Connectivity biases generate a learning hierarchy in the Drosophila mushroom body. MacKenzie et al., bioRxiv (2025). 10.1101/2025.10.29.684686 </li> </ol> <ol> <li> The Connectome Interpreter Toolkit. Yin et al., bioRxiv (2025). 10.1101/2025.09.29.679410 </li> </ol> <ol> <li> Network synchrony creates neural filters promoting quiescence in Drosophila. Raccuglia et al., Nature (2025). 10.1038/s41586-025-09376-2 </li> </ol> <ol> <li> Distributed control circuits across a brain-and-cord connectome. Bates et al., bioRxiv (2025). 10.1101/2025.07.31.667571 </li> </ol> <ol> <li> The exponential distance rule-based network model predicts topology and reveals functionally relevant properties of the Drosophila projectome. P\u00e9ntek and Ercsey-Ravasz, Network Neuroscience (2025). 10.1162/netn_a_00455 </li> </ol> <ol> <li> A cell type in the visual system that receives feedback about limb movement. Hartman et al., Current Biology (2025). 10.1016/j.cub.2025.06.055 </li> </ol> <ol> <li> A Developmental Atlas of the Drosophila Nerve Cord Uncovers a Global Temporal Code for Neuronal Identity. Cachero et al., bioRxiv (2025). 10.1101/2025.07.16.664682 </li> </ol> <ol> <li> The Drosophila connectome reveals Axo-Axonic Synapses on Descending Neurons. Ceballos et al., bioRxiv (2025). 10.1101/2025.09.04.674108 </li> </ol> <ol> <li> Fishexplorer: A multimodal cellular atlas platform for neuronal circuit dissection in larval zebrafish. Vohra et al., bioRxiv (2025). 10.1101/2025.07.14.664689 </li> </ol> <ol> <li> Synaptic density and relative connectivity conservation maintain circuit stability across development. Fritz et al., bioRxiv (2025). 10.1101/2025.07.26.666968 </li> </ol> <ol> <li> An applicable and efficient retrograde monosynaptic circuit mapping tool for larval zebrafish. Chen et al., bioRxiv (2025). 10.1101/2024.06.27.601104 </li> </ol> <ol> <li> Selective life-long suppression of an odor processing channel in response to critical period experience. Leier et al., bioRxiv (2025). 10.1101/2025.07.18.665601  </li> </ol> <ol> <li> Recurrent connectivity supports carbon dioxide sensitivity in Aedes aegypti mosquitoes. Bao et al., bioRxiv (2025). 10.1101/2025.07.29.667487 </li> </ol> <ol> <li> A comprehensive mechanosensory connectome reveals a somatotopically organized neural circuit architecture controlling stimulus-aimed grooming of the Drosophila head. Calle-Schuler et al., bioRxiv (2025). 10.1101/2025.05.19.654894 </li> </ol> <ol> <li> Sexually-dimorphic neurons in the Drosophila whole-brain connectome. Deutsch et al., bioRxiv (2025). 10.1101/2025.06.10.658788 </li> </ol> <ol> <li> Hierarchical diversification of neurons regulating motivated behaviors. Elkahlah et al., bioRxiv (2025). 10.1101/2025.06.03.657692 </li> </ol> <ol> <li> Neural connectivity of a computational map for fly flight control. Dhawan et al., bioRxiv (2025). 10.1101/2025.05.29.656834 </li> </ol> <ol> <li> Functional imaging and connectome analyses reveal organizing principles of taste circuits in Drosophila. Li et al., Current Biology (2025). 10.1016/j.cub.2025.04.035 </li> </ol> <ol> <li> Correlative light and electron microscopy reveals the fine circuit structure underlying evidence accumulation in larval zebrafish. Boulanger-Weill et al., bioRxiv (2025). 10.1101/2025.03.14.643363 </li> </ol> <ol> <li> Odour representations supporting ethology-relevant categorisation and discrimination in the Drosophila mushroom body. Chan et al., bioRxiv (2025). 10.1101/2025.01.25.634657 </li> </ol>"},{"location":"publications/#2024","title":"2024","text":"<ol> <li> A neural circuit for context-dependent multimodal signaling in Drosophila. Steinfath et al., bioRxiv (2024). 10.1101/2024.12.04.625245 </li> </ol> <ol> <li> Synaptic connectome of a neurosecretory network in the Drosophila brain. McKim et al., bioRxiv (2024). 10.1101/2024.08.28.609616 </li> </ol> <ol> <li> Invariant synaptic density across species. Castro and Cardona, bioRxiv (2024). 10.1101/2024.07.18.604056 </li> </ol> <ol> <li> An applicable and efficient retrograde monosynaptic circuit mapping tool for larval zebrafish. Chen et al., bioRxiv (2024). 10.1101/2024.06.27.601104 </li> </ol> <ol> <li> Comparative connectomics of the descending and ascending neurons of the Drosophila nervous system: stereotypy and sexual dimorphism.  St\u00fcrner et al., bioRxiv (2024). 10.1101/2024.06.04.596633 </li> </ol> <ol> <li> Whole-brain annotation and multi-connectome cell typing of Drosophila. Schlegel et al., Nature 634, 139\u2013152 (2024). 10.1038/s41586-024-07686-5 </li> </ol> <ol> <li> Neuronal wiring diagram of an adult brain. Dorkenwald et al., Nature 634, 124\u2013138 (2024). 10.1038/s41586-024-07558-y </li> </ol> <ol> <li> Heterogeneity of synaptic connectivity in the fly visual system.Cornean et al., Nat Commun 15, 1570 (2024). 10.1038/s41467-024-45971-z </li> </ol> <ol> <li> A Drosophila computational brain model reveals sensorimotor processing. Shiu et al., Nature 634, 210\u2013219 (2024). 10.1038/s41586-024-07763-9 </li> </ol> <ol> <li> Diversity of visual inputs to Kenyon cells of the Drosophila mushroom body. Ganguly et al., Nat Commun 15, 5698 (2024). 10.1038/s41467-024-49616-z </li> </ol> <ol> <li> Connectome-driven neural inventory of a complete visual system. Nern et al., bioRxiv (2024). 10.1101/2024.04.16.589741 </li> </ol>"},{"location":"publications/#2023","title":"2023","text":"<ol> <li> Synaptic connectome of the Drosophila circadian clock. Reinhard et al., bioRxiv (2023). 10.1101/2023.09.11.557222 </li> </ol> <ol> <li> Systematic annotation of a complete adult male Drosophila nerve cord connectome reveals principles of functional organisation. Marin et al., bioRxiv (2023); doi: 10.1101/2023.06.05.543407 </li> </ol> <ol> <li> Interactions between specialized gain control mechanisms in olfactory processing. Barth-Maron, D\u2019Alessandro and Wilson, Current Biology (2023). 10.1016/j.cub.2023.10.041 </li> </ol> <ol> <li> Bisected graph matching improves automated pairing of bilaterally homologous neurons from connectomes. Pedigo et al., Network Neuroscience (2023) 10.1162/netn_a_00287 </li> </ol> <ol> <li> Vimo - Visual Analysis of Neuronal Connectivity Motifs. Troidl et al., IEEE transactions on visualization and computer graphics (2023). 10.1109/TVCG.2023.3327388 </li> </ol> <ol> <li> BIFROST: a method for registering diverse imaging datasets. Brezovec et al., bioRxiv (2023). 10.1101/2023.06.09.544408 </li> </ol> <ol> <li> Lineages to circuits: the developmental and evolutionary architecture of information channels into the central complex. Kandimalla et al., J Comp Physiol (2023). 10.1007/s00359-023-01616-y </li> </ol> <ol> <li> A network-based method for extracting the organization of brain-wide circuits from reconstructed connectome datasets. Manjunatha et al., bioRxiv (2023). 10.1101/2023.05.21.541471 </li> </ol> <ol> <li> Multisensory learning binds modality-specific neurons into a cross-modal memory engram. Okray et al., bioRxiv (2022). 10.1101/2022.07.08.499174 </li> </ol> <ol> <li> Diversity of visual inputs to Kenyon cells of the Drosophila mushroom body. Ganguly et al., bioRxiv  (2023). 10.1101/2023.10.12.561793 </li> </ol> <ol> <li> Synaptic and peptidergic connectomes of the Drosophila circadian clock. Reinhard et al., bioRxiv (2023). 10.1101/2023.09.11.557222 </li> </ol> <ol> <li> Heterogeneity of synaptic connectivity in the fly visual system. Cornean et al., bioRxiv (2023). 10.1101/2023.08.29.555204 </li> </ol> <ol> <li> Hue selectivity from recurrent circuitry in Drosophila. Christenson et al., bioRxiv (2023). 10.1101/2023.07.12.548573 </li> </ol>"},{"location":"publications/#2022","title":"2022","text":"<ol> <li> Gliotransmission of D-serine promotes thirst-directed behaviors in Drosophila. Park et al., Current Biology (2022). 10.1016/j.cub.2022.07.038 </li> </ol> <ol> <li> FlyWire: online community for whole-brain connectomics. Dorkenwald et al., Nat Methods (2022). 10.1038/s41592-021-01330-0 </li> </ol> <ol> <li> Synaptic wiring motifs in posterior parietal cortex support decision-making. Kuan et al., bioRxiv (2022). 10.1101/2022.04.13.488176 </li> </ol> <ol> <li> A Survey of Visualization and Analysis in High\u2010Resolution Connectomics. Beyer et al., Computer Graphics Forum. (2022). 10.1111/cgf.14574 </li> </ol> <ol> <li> Eye structure shapes neuron function in Drosophila motion vision. Zhao et al., bioRxiv (2022). 10.1101/2022.12.14.520178 </li> </ol> <ol> <li> Dendrite architecture determines mitochondrial distribution patterns in vivo. Donovan et al., bioRxiv (2022). 10.1101/2022.07.01.497972 </li> </ol> <ol> <li> Structured sampling of olfactory input by the fly mushroom body. Zheng et al., Current Biology (2022). 10.1016/j.cub.2022.06.031 </li> </ol> <ol> <li> The connectome of an insect brain. Winding et al., Science (2023 ). 10.1126/science.add9330 </li> </ol>"},{"location":"publications/#2021","title":"2021","text":"<ol> <li> Information flow, cell types and stereotypy in a full olfactory connectome. Schlegel, Bates et al., eLife (2021). 10.7554/eLife.66018 </li> </ol> <ol> <li> A sex-specific switch between visual and olfactory inputs underlies adaptive sex differences in behavior. Nojima et al., Current Biology (2021). 10.1016/j.cub.2020.12.047 </li> </ol> <ol> <li> A developmental framework linking neurogenesis and circuit formation in the Drosophila CNS. Mark et al., eLife (2021). 10.7554/eLife.67510 </li> </ol> <ol> <li> Reconstruction of motor control circuits in adult Drosophila using automated transmission electron microscopy. Phelps et al., Cell (2021). 10.1016/j.cell.2020.12.013 </li> </ol>"},{"location":"publications/#2020","title":"2020","text":"<ol> <li> Complete connectomic reconstruction of olfactory projection neurons in the fly brain. Bates et al., Current Biology (2020). 10.1016/j.cub.2020.06.042 </li> </ol> <ol> <li> Convergence of distinct subpopulations of mechanosensory neurons onto a neural circuit that elicits grooming. Hampel et al., bioRxiv (2020). 10.1101/2020.06.08.141341 </li> </ol> <ol> <li> The Wiring Logic of an Identified Serotonergic Neuron That Spans Sensory Networks. Coates et al., Journal of Neuroscience (2020). 10.1523/JNEUROSCI.0552-20.2020 </li> </ol> <ol> <li> Input Connectivity Reveals Additional Heterogeneity of Dopaminergic Reinforcement in Drosophila. Otto et al., Current Biology (2020). 10.1016/j.cub.2020.05.077 </li> </ol> <ol> <li> The Neuroanatomical Ultrastructure and Function of a Biological Ring Attractor. Turner-Evans et al., Neuron (2020). 10.1016/j.neuron.2020.08.006 </li> </ol> <ol> <li> Connectomics Analysis Reveals First-, Second-, and Third-Order Thermosensory and Hygrosensory Neurons in the Adult Drosophila Brain. Marin et al., Curr Biol. (2020). 10.1016/j.cub.2020.06.028 </li> </ol> <ol> <li> <p>Some of these papers will have used NAVis indirectly - e.g. through pymaid or fafbseg.\u00a0\u21a9</p> </li> <li> <p>Searching for occurrences of \"navis\" in the literature is surprisingly difficult.\u00a0\u21a9</p> </li> </ol>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#quickstart","title":"Quickstart","text":"<p>This short introduction will show you the basics of how to use NAVis. This is not supposed to be comprehensive but rather to give you an idea of the basic concepts. For inspiration, explore the example gallery and for detailed explanations have a look at the API documentation.</p>"},{"location":"quickstart/#single-neurons","title":"Single Neurons","text":"<p>For demonstration purposes NAVis comes with a bunch of fruit fly neurons from the Janelia hemibrain project:</p> <pre><code>import navis\n\n# Load a single example neuron\nn = navis.example_neurons(n=1, kind='skeleton')\nn\n</code></pre> <pre><code>type                                             navis.TreeNeuron\nname                                                    DA1_lPN_R\nid                                                     1734350788\nn_nodes                                                      4465\nn_connectors                                                 2705\nn_branches                                                    599\nn_leafs                                                       618\ncable_length                                           266476.875\nsoma                                                         4177\nunits                                                 8 nanometer\ncreated_at                             2026-03-01 17:35:50.378839\norigin          /home/runner/work/navis/navis/navis/data/swc/1...\nfile                                               1734350788.swc\ndtype: object\n</code></pre> <p>Loading your own neurons</p> <p>Almost all tutorials will use the example neurons shipped with NAVis (see <code>navis.example_neurons</code> for details).</p> <p>You will most likely want to load your own neuron data. NAVis has dedicated functions such as <code>navis.read_swc</code> for that . Check out the I/O Tutorials to learn more!</p> <p>NAVis represents neurons as <code>navis.TreeNeuron</code>, <code>navis.MeshNeuron</code>, <code>navis.VoxelNeuron</code> or <code>navis.Dotprops</code> - see the tutorial on Neuron Types for details.</p> <p>In above code we asked for a skeleton, so the neuron returned is a <code>TreeNeuron</code>. Like all neuron types, this class is essentially a wrapper around the actual neuron data (the node table in case of skeletons) and has some convenient features.</p> <p>The skeleton's node data is stored as <code>pandas.DataFrame</code>:</p> <pre><code>n.nodes.head()\n</code></pre> <pre><code>   node_id label        x        y        z     radius  parent_id  type\n0        1     0  15784.0  37250.0  28062.0  10.000000         -1  root\n1        2     0  15764.0  37230.0  28082.0  18.284300          1  slab\n2        3     0  15744.0  37190.0  28122.0  34.721401          2  slab\n3        4     0  15744.0  37150.0  28202.0  34.721401          3  slab\n4        5     0  15704.0  37130.0  28242.0  34.721401          4  slab\n</code></pre> <p>Pandas</p> <p>pandas is the data science library for Python and will help you analyze and visualize your data. We highly recommend familiarizing yourself with pandas! There are plenty of good tutorials out there but pandas' own 10 Minutes to pandas is a good place to start.</p> <p>Once you have your neuron loaded in NAVis things are as simple as passing it to the function that does what you need:</p> <pre><code>fig, ax = navis.plot2d(n, view=('x', '-z'), color=\"coral\", method='2d')\n</code></pre> 2026-03-01T17:35:50.468397 image/svg+xml Matplotlib v3.10.8, https://matplotlib.org/"},{"location":"quickstart/#lists-of-neurons","title":"Lists of Neurons","text":"<p>When working with multiple neurons, NAVis uses <code>navis.NeuronLists</code>:</p> <pre><code># Ask for 3 example neurons\nnl = navis.example_neurons(n=3, kind='skeleton')\nnl\n</code></pre> <pre><code>&lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 3 neurons (875.1KiB)\n               type  ...            file\n0  navis.TreeNeuron  ...  1734350788.swc\n1  navis.TreeNeuron  ...  1734350908.swc\n2  navis.TreeNeuron  ...   722817260.swc\n</code></pre> <p><code>navis.NeuronLists</code> are containers and behave like a <code>list</code> (with some extras, of course):</p> <pre><code># Access the first neuron in the list\nnl[0]\n</code></pre> <pre><code>type                                             navis.TreeNeuron\nname                                                    DA1_lPN_R\nid                                                     1734350788\nn_nodes                                                      4465\nn_connectors                                                 2705\nn_branches                                                    599\nn_leafs                                                       618\ncable_length                                           266476.875\nsoma                                                         4177\nunits                                                 8 nanometer\ncreated_at                             2026-03-01 17:35:50.529961\norigin          /home/runner/work/navis/navis/navis/data/swc/1...\nfile                                               1734350788.swc\ndtype: object\n</code></pre> <p><code>navis.NeuronLists</code> provide quick and convenient access to all functions (methods) and properties of the neurons it contains:</p> <pre><code># Get the cable length for all neurons in the list\nnl.cable_length\n</code></pre> <pre><code>[266476.88 304332.66 274703.38]\n</code></pre> <p>Most functions that accept single neurons, also happily work with <code>NeuronLists</code>:</p> <pre><code># Generate a plot of our neurons\nfig, ax = navis.plot2d(nl, view=('x', '-z'), method='2d')\n</code></pre> 2026-03-01T17:35:50.670056 image/svg+xml Matplotlib v3.10.8, https://matplotlib.org/ <p>See the Lists of Neurons tutorial for more information.</p>"},{"location":"quickstart/#methods-vs-functions","title":"Methods vs Functions","text":"<p>NAVis neurons and neuron lists have methods that serve as shortcuts to main functions.</p> <p>These code snippets are equivalent:</p> Full functionShortcut <pre><code>import navis\n\ns = navis.example_neuron(n=1, type='skeleton')\nds = navis.downsample_neuron(s, 5)\n</code></pre> <pre><code>import navis\n\ns = navis.example_neuron(n=1, type='skeleton')\nds = s.downsample(5)\n# Under the hood, `s.downsample()` calls `navis.downsample_neuron(s)`\n</code></pre>"},{"location":"quickstart/#the-inplace-parameter","title":"The <code>inplace</code> Parameter","text":"<p>You may notice that many NAVis functions that modify neurons (resampling, pruning, etc.) have an <code>inplace</code> parameter. This is analogous to <code>pandas</code> where <code>inplace</code> defines whether we modify the original (<code>inplace=True</code>) or operate on a copy (<code>inplace=False</code>, default).</p> <p>Downsample a copy of our skeleton and leaving the original unchanged (this is the default for almost all functions):</p> <pre><code>n_ds = navis.downsample_neuron(neuron, 10, inplace=False)\n</code></pre> <p>Downsample the original neuron:</p> <pre><code>navis.downsample_neuron(neuron, 10, inplace=True)\n</code></pre> <p>Using <code>inplace=True</code> can be useful if you work with lots of neurons and want avoid making unnecessary copies to keep the memory footprint low!</p>"},{"location":"quickstart/#getting-help","title":"Getting Help","text":"<p>Feeling a bit lost? No worries! Check out the Tutorials or browse the API documentation.</p> <p>NAVis also supports autocompletion: typing <code>navis.</code> and pressing the TAB key should bring up a list of available functions. This also works with neuron properties and methods!</p> <p>If you don't know what a function does, try e.g. <code>help(navis.plot3d)</code> or find it in the API documentation to get a nicely rendered docstring:</p> <pre><code>help(navis.prune_twigs)\n</code></pre> <pre><code>Prune terminal twigs under a given size.\n\n    By default this function will simply drop all terminal twigs shorter than\n    `size`. This is very fast but rather stupid: for example, if a twig is\n    just 1 nanometer longer than `size` it will not be touched at all. If you\n    require precision, set `exact=True` which will prune *exactly* `size`\n    off the terminals but is about an order of magnitude slower.\n\n    Parameters\n    ----------\n    x :             TreeNeuron | MeshNeuron | NeuronList\n    size :          int | float | str\n                    Twigs shorter than this will be pruned. If the neuron has\n                    its `.units` set, you can also pass a string including the\n                    units, e.g. '5 microns'.\n    exact:          bool\n                    See notes above.\n    mask :          iterable | callable, optional\n                    Either a boolean mask, a list of node IDs or a callable taking\n                    a neuron as input and returning one of the former. If provided,\n                    only nodes that are in the mask will be considered for pruning.\n    inplace :       bool, optional\n                    If False, pruning is performed on copy of original neuron\n                    which is then returned.\n    recursive :     int | bool, optional\n                    If `int` will undergo that many rounds of recursive\n                    pruning. If True will prune iteratively until no more\n                    terminal twigs under the given size are left. Only\n                    relevant if `exact=False`.\n    parallel :      bool\n                    If True and input is NeuronList, use parallel\n                    processing. Requires `pathos`.\n    n_cores :       int, optional\n                    Numbers of cores to use if `parallel=True`.\n                    Defaults to half the available cores.\n    progress :      bool\n                    Whether to show a progress bar. Overruled by\n                    `navis.set_pbars`.\n    omit_failures : bool\n                    If True will omit failures instead of raising\n                    an exception. Ignored if input is single neuron.\n\n\n    Returns\n    -------\n    TreeNeuron/List\n                    Pruned neuron(s).\n\n    Examples\n    --------\n    Simple pruning\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(2)\n    &gt;&gt;&gt; # Prune twigs smaller than 5 microns\n    &gt;&gt;&gt; # (example neuron are in 8x8x8nm units)\n    &gt;&gt;&gt; n_pr = navis.prune_twigs(n,\n    ...                          size=5000 / 8,\n    ...                          recursive=float('inf'),\n    ...                          inplace=False)\n    &gt;&gt;&gt; all(n.n_nodes &gt; n_pr.n_nodes)\n    True\n\n    Exact pruning\n\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; # Prune twigs by exactly 5 microns\n    &gt;&gt;&gt; # (example neuron are in 8x8x8nm units)\n    &gt;&gt;&gt; n_pr = navis.prune_twigs(n,\n    ...                          size=5000 / 8,\n    ...                          exact=True,\n    ...                          inplace=False)\n    &gt;&gt;&gt; n.n_nodes &gt; n_pr.n_nodes\n    True\n\n    Prune using units\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; # Example neurons are in 8x8x8nm units...\n    &gt;&gt;&gt; n.units\n    &lt;Quantity(8, 'nanometer')&gt;\n    &gt;&gt;&gt; # ... therefore we can use units for `size`\n    &gt;&gt;&gt; n_pr = navis.prune_twigs(n,\n    ...                          size='5 microns',\n    ...                          inplace=False)\n    &gt;&gt;&gt; n.n_nodes &gt; n_pr.n_nodes\n    True\n</code></pre> <p>Note that most functions have helpful <code>Examples</code>!</p>"},{"location":"quickstart/#what-next","title":"What next?","text":"<ul> <li> <p> Neuron types     ---</p> <p>Find out more about the different neuron types in NAVis.</p> <p> Neuron types tutorial</p> </li> <li> <p> Lists of Neurons     ---</p> <p>Check out the guide on lists of neurons.</p> <p> NeuronLists tutorial</p> </li> <li> <p> Neuron I/O</p> <p>Learn about how to load your own neurons into NAVis.</p> <p> I/O Tutorials</p> </li> </ul>"},{"location":"generated/gallery/","title":"Tutorials","text":""},{"location":"generated/gallery/#general-tutorials","title":"General Tutorials","text":"<p>These are some general introductory tutorials:</p> <p> The Basics </p> <p> Neuron Types </p> <p> Lists of Neurons </p>"},{"location":"generated/gallery/#import-export","title":"Import / Export","text":"<p>These tutorials will illustrate how to load and save your data:</p> <p> Skeletons </p> <p> Meshes </p> <p> Dotprops </p> <p> Pickling </p> <p> Skeletons from light-level data </p>"},{"location":"generated/gallery/#plotting","title":"Plotting","text":"<p>These tutorials will show you how to visualize your neurons:</p> <p> Plotting Overview </p> <p> Coloring </p> <p> Neuron \"Barcodes\" </p> <p> Neuron Topology </p> <p> Fine-tuning Skeletons </p> <p> Depth-coloring </p> <p> Cortical Neurons </p> <p> XKCD Style </p>"},{"location":"generated/gallery/#morphology","title":"Morphology","text":"<p>These tutorials will show you how to analyse and manipulate your neurons' morphology:</p> <p> Manipulate Morphology </p> <p> Analyzing Neuron Morphology </p>"},{"location":"generated/gallery/#interfaces","title":"Interfaces","text":"<p>These tutorials cover interfaces between NAVis and external tools:</p> <p> NEURON simulator </p> <p> Visualize NEURON model </p> <p> Blender 3D </p>"},{"location":"generated/gallery/#remote-data-sources","title":"Remote Data Sources","text":"<p>These tutorials will show you how to load data from remote data sources:</p> <p> neuPrint </p> <p> Neuroglancer &amp; CloudVolume </p> <p> The MICrONS Datasets </p> <p> Insect Brain DB </p> <p> H01 Dataset </p>"},{"location":"generated/gallery/#nblast","title":"NBLAST","text":"<p>These tutorials will teach you how to run NBLASTs to compare neuron morphology.</p> <p> NBLAST </p> <p> Custom score matrices </p> <p> NBLAST against FlyCircuit </p> <p> NBLAST using light-level data </p>"},{"location":"generated/gallery/#misc","title":"Misc","text":"<p> Multiprocessing </p> <p> Transformations </p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/mg_execution_times/","title":"Computation times","text":""},{"location":"generated/gallery/mg_execution_times/#computation-times","title":"Computation times","text":"<p>00:02.933 total execution time for generated_gallery files:</p> <p>+----------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_basic_01_neurons (docs/examples/tutorial_basic_01_neurons.py)             | 00:01.946 | 0.0 MB | +----------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_basic_02_neuronlists (docs/examples/tutorial_basic_02_neuronlists.py) | 00:00.603 | 0.0 MB | +----------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_basic_00_basics (docs/examples/tutorial_basic_00_basics.py)                | 00:00.383 | 0.0 MB | +----------------------------------------------------------------------------------------------------------------------+-----------+--------+</p>"},{"location":"generated/gallery/tutorial_basic_00_basics/","title":"The Basics","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/tutorial_basic_00_basics/#the-basics","title":"The Basics","text":"<p>This tutorial will introduce some of the basics concepts in NAVis.</p> <p>This is not supposed to be comprehensive but rather to give you a flavor of how things work. For inspiriation, explore the example gallery and for detailed explanations have a look at the API documentation.</p>"},{"location":"generated/gallery/tutorial_basic_00_basics/#single-neurons","title":"Single Neurons","text":"<p>NAVis lets you import neurons from a variety of local and remote sources. For demonstration purposes NAVis comes with a bunch of fruit fly neurons from the Janelia hemibrain project:</p> <p>Note</p> <p>We will cover loading neurons from different sources in a different tutorial.</p> <pre><code>import navis\n\n# Load a single neuron\nn = navis.example_neurons(n=1, kind='skeleton')\nn\n</code></pre> type navis.TreeNeuron name DA1_lPN_R id 1734350788 n_nodes 4465 n_connectors 2705 n_branches 599 n_leafs 618 cable_length 266476.875 soma 4177 units 8 nanometer created_at 2026-03-01 17:18:31.170628 origin /home/runner/work/navis/navis/navis/data/swc/1... file 1734350788.swc <p>In above code we loaded one of the example neurons. NAVis represents neurons as <code>navis.TreeNeuron</code>, <code>navis.MeshNeuron</code>, <code>navis.VoxelNeuron</code> or <code>navis.Dotprops</code>.</p> <p>In the above example we asked for a skeleton, so the neuron returned is a <code>TreeNeuron</code>. This class is essentially a wrapper around the actual neuron data (the SWC table in this case) and has some convenient features.</p> <p>Node data is stored as <code>pandas.DataFrame</code>:</p> <pre><code>n.nodes.head()\n</code></pre> node_id label x y z radius parent_id type 0 1 0 15784.0 37250.0 28062.0 10.000000 -1 root 1 2 0 15764.0 37230.0 28082.0 18.284300 1 slab 2 3 0 15744.0 37190.0 28122.0 34.721401 2 slab 3 4 0 15744.0 37150.0 28202.0 34.721401 3 slab 4 5 0 15704.0 37130.0 28242.0 34.721401 4 slab <p>Pandas</p> <p>pandas is the data science library for Python and will help you analyze and visualize your data. We highly recommend familiarizing yourself with pandas! There are plenty of good tutorials out there but pandas' own 10 Minutes to pandas is a good place to start.</p> <p>Try typing in \"<code>n.</code>\" and hitting tab: most attributes and functions are accessible via autocompletion. If you don't know what a function does, check out the documentation using <code>help()</code> or via the API documentation: </p><pre><code>help(navis.TreeNeuron.root)\n</code></pre><p></p> <pre><code>help(navis.TreeNeuron.downsample)\n</code></pre> <p>You will notice that many NAVis functions that accept neurons have an <code>inplace</code> parameter. This is analogous to pandas:</p> <p>Downsample a copy, leaving the original unchanged (this is the default for almost all functions)</p> <pre><code>n_ds = n.downsample(10, inplace=False)\n\n# Downsample original neuron\nn.downsample(10, inplace=True)\n\nn\n</code></pre> type navis.TreeNeuron name DA1_lPN_R id 1734350788 n_nodes 1304 n_connectors 2705 n_branches 599 n_leafs 618 cable_length 247213.921875 soma 4177 units 8 nanometer created_at 2026-03-01 17:18:31.170628 origin /home/runner/work/navis/navis/navis/data/swc/1... file 1734350788.swc <p><code>navis.TreeNeuron</code> functions such as <code>.downsample()</code> are shorthands for calling the actual NAVis functions. So above code is equivalent to:</p> <pre><code>n = navis.example_neurons(n=1, kind='skeleton')\nn_ds = navis.downsample_neuron(n, downsampling_factor=10, inplace=False)\nn_ds\n</code></pre> type navis.TreeNeuron name DA1_lPN_R id 1734350788 n_nodes 1304 n_connectors 2705 n_branches 599 n_leafs 618 cable_length 247213.921875 soma 4177 units 8 nanometer created_at 2026-03-01 17:18:31.234144 origin /home/runner/work/navis/navis/navis/data/swc/1... file 1734350788.swc"},{"location":"generated/gallery/tutorial_basic_00_basics/#lists-of-neurons","title":"Lists of Neurons","text":"<p>For multiple neurons, NAVis uses <code>navis.NeuronList</code>:</p> <pre><code>nl = navis.example_neurons(n=3, kind='skeleton')\nnl\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 3 neurons (875.1KiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron DA1_lPN_R 1734350788 4465 2705 599 618 266476.87500 4177.0 8 nanometer 2026-03-01 17:18:31.267965 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 1 navis.TreeNeuron DA1_lPN_R 1734350908 4847 3042 735 761 304332.65625 6.0 8 nanometer 2026-03-01 17:18:31.277072 /home/runner/work/navis/navis/navis/data/swc/1... 1734350908.swc 2 navis.TreeNeuron DA1_lPN_R 722817260 4332 3136 633 656 274703.37500 NaN 8 nanometer 2026-03-01 17:18:31.283721 /home/runner/work/navis/navis/navis/data/swc/7... 722817260.swc <p><code>navis.NeuronList</code> is a container and behaves like a glorified <code>list</code>:</p> <pre><code>nl[0]\n</code></pre> type navis.TreeNeuron name DA1_lPN_R id 1734350788 n_nodes 4465 n_connectors 2705 n_branches 599 n_leafs 618 cable_length 266476.875 soma 4177 units 8 nanometer created_at 2026-03-01 17:18:31.267965 origin /home/runner/work/navis/navis/navis/data/swc/1... file 1734350788.swc <p><code>navis.NeuronList</code> lets you run/access all functions (methods) and properties of the neurons it contrains:</p> <pre><code>nl.cable_length\n</code></pre> <p>Out:</p> <pre><code>array([266476.88, 304332.66, 274703.38], dtype=float32)\n</code></pre> <pre><code>nl_ds = nl.downsample(10, inplace=False)\n\nnl_ds\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 3 neurons (670.4KiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron DA1_lPN_R 1734350788 1304 2705 599 618 247213.921875 4177.0 8 nanometer 2026-03-01 17:18:31.267965 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 1 navis.TreeNeuron DA1_lPN_R 1734350908 1558 3042 735 761 282457.125000 6.0 8 nanometer 2026-03-01 17:18:31.277072 /home/runner/work/navis/navis/navis/data/swc/1... 1734350908.swc 2 navis.TreeNeuron DA1_lPN_R 722817260 1346 3136 633 656 254727.546875 NaN 8 nanometer 2026-03-01 17:18:31.283721 /home/runner/work/navis/navis/navis/data/swc/7... 722817260.swc <p>Let's finish this primer with some eye candy</p> <pre><code>nl.plot3d(backend='plotly')\n</code></pre>"},{"location":"generated/gallery/tutorial_basic_00_basics/#what-next","title":"What next?","text":"<ul> <li> <p> Neuron types     ---</p> <p>Find out more about the different neuron types in NAVis.</p> <p> Neuron types tutorial</p> </li> <li> <p> Lists of Neurons     ---</p> <p>Check out the guide on lists of neurons.</p> <p> NeuronLists tutorial</p> </li> <li> <p> Neuron I/O     ---</p> <p>Learn about how to load your own neurons into NAVis.</p> <p> I/O Tutorials</p> </li> </ul> <p>Total running time of the script: ( 0 minutes  0.383 seconds)</p> <p> Download Python source code: tutorial_basic_00_basics.py</p> <p> Download Jupyter notebook: tutorial_basic_00_basics.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/tutorial_basic_01_neurons/","title":"Neuron Types","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/tutorial_basic_01_neurons/#neuron-types","title":"Neuron Types","text":"<p>This tutorial will show you the different neuron types and how to work with them.</p> <p>Depending your data/workflows, you will use different representations of neurons. If, for example, you work with light-level data you might end up extracting point clouds or neuron skeletons from image stacks. If, on the other hand, you work with segmented EM data, you will typically work with meshes.</p> <p>To cater for these different representations, neurons in NAVis come in four flavours:</p> Neuron type Description Core data <code>navis.TreeNeuron</code> A hierarchical skeleton consisting of nodes and edges. - <code>.nodes</code>: the SWC node table <code>navis.MeshNeuron</code> A mesh with faces and vertices. - <code>.vertices</code>: <code>(N, 3)</code> array of x/y/z vertex coordinates- <code>.faces</code>: <code>(M, 3)</code> array of faces <code>navis.VoxelNeuron</code> An image represented by either a 2d array of voxels or a 3d voxel grid. - <code>.voxels</code>: <code>(N, 3)</code> array of voxels- <code>.values</code>: <code>(N, )</code> array of values (i.e. intensity)- <code>.grid</code>: <code>(N, M, K)</code> 3D voxelgrid <code>navis.Dotprops</code> A cloud of points, each with an associated local vector. - <code>.points</code>: <code>(N, 3)</code> array of point coordinates- <code>.vect</code>: <code>(N, 3)</code> array of normalized vectors <p>Note that functions in NAVis may only work on a subset of neuron types: check out this table in the API reference for details. If necessary, NAVis can help you convert between the different neuron types (see further below)!</p> <p>Important</p> <p>In this guide we introduce the different neuron types using data bundled with NAVis. To learn how to load your own neurons into NAVis please see the tutorials on Import/Export.</p>"},{"location":"generated/gallery/tutorial_basic_01_neurons/#treeneurons","title":"TreeNeurons","text":"<p><code>TreeNeurons</code> represent a neuron as a tree-like \"skeleton\" - effectively a directed acyclic graph, i.e. they consist of nodes and each node connects to at most one parent. This format is commonly used to describe a neuron's topology and often shared using SWC files.</p> <p></p> <p>A <code>navis.TreeNeuron</code> is typically loaded from an SWC file via <code>navis.read_swc</code> but you can also constructed one yourself from e.g. <code>pandas.DataFrame</code> or a <code>networkx.DiGraph</code>. See the skeleton I/O tutorial for details.</p> <p>NAVis ships with a couple example Drosophila neurons from the Janelia hemibrain project published in Scheffer et al. (2020) and available at https://neuprint.janelia.org (see also the neuPrint tutorial):</p> <pre><code>import navis\n\n# Load one of the example neurons\nsk = navis.example_neurons(n=1, kind=\"skeleton\")\n\n# Inspect the neuron\nsk\n</code></pre> type navis.TreeNeuron name DA1_lPN_R id 1734350788 n_nodes 4465 n_connectors 2705 n_branches 599 n_leafs 618 cable_length 266476.875 soma 4177 units 8 nanometer created_at 2026-03-01 17:18:31.737243 origin /home/runner/work/navis/navis/navis/data/swc/1... file 1734350788.swc <p><code>navis.TreeNeuron</code> stores nodes and other data as attached <code>pandas.DataFrames</code>:</p> <pre><code>sk.nodes.head()\n</code></pre> node_id label x y z radius parent_id type 0 1 0 15784.0 37250.0 28062.0 10.000000 -1 root 1 2 0 15764.0 37230.0 28082.0 18.284300 1 slab 2 3 0 15744.0 37190.0 28122.0 34.721401 2 slab 3 4 0 15744.0 37150.0 28202.0 34.721401 3 slab 4 5 0 15704.0 37130.0 28242.0 34.721401 4 slab"},{"location":"generated/gallery/tutorial_basic_01_neurons/#meshneurons","title":"MeshNeurons","text":"<p><code>MeshNeurons</code> consist of vertices and faces, and are a typical output of e.g. image segmentation.</p> <p></p> <p>A <code>navis.MeshNeuron</code> can be constructed from any object that has <code>.vertices</code> and <code>.faces</code> properties, a dictionary of <code>vertices</code> and <code>faces</code> or a file that can be parsed by <code>trimesh.load</code>. See the mesh I/O tutorial for details.</p> <p>Each of the example neurons in NAVis also comes as mesh representation:</p> <pre><code>m = navis.example_neurons(n=1, kind=\"mesh\")\nm\n</code></pre> type navis.MeshNeuron name DA1_lPN_R id 1734350788 units 8 nanometer n_vertices 6309 n_faces 13054 <p><code>navis.MeshNeuron</code> stores vertices and faces as attached numpy arrays:</p> <pre><code>m.vertices, m.faces\n</code></pre> <p>Out:</p> <pre><code>(TrackedArray([[16384.        , 34792.03125   , 24951.88085938],\n              [16384.        , 36872.0625    , 25847.89453125],\n              [16384.        , 36872.0625    , 25863.89453125],\n              ...,\n              [ 5328.08105469, 21400.07617188, 16039.99414062],\n              [ 6872.10498047, 19560.04882812, 13903.96191406],\n              [ 6872.10498047, 19488.046875  , 13927.96191406]],\n             shape=(6309, 3)), TrackedArray([[3888, 3890, 3887],\n              [3890, 1508, 3887],\n              [1106, 1104, 1105],\n              ...,\n              [5394, 5426, 5548],\n              [5852, 5926, 6017],\n              [ 207,  217,  211]], shape=(13054, 3)))\n</code></pre>"},{"location":"generated/gallery/tutorial_basic_01_neurons/#dotprops","title":"Dotprops","text":"<p><code>Dotprops</code> represent neurons as point clouds where each point is associated with a vector describing the local orientation. This simple representation often comes from e.g. light-level data or as direvative of skeletons/meshes (see <code>navis.make_dotprops</code>).</p> <p></p> <p>Dotprops are used e.g. for NBLAST. See the dotprops I/O tutorial for details.</p> <p><code>navis.Dotprops</code> consist of <code>.points</code> and associated <code>.vect</code> (vectors). They are typically created from other types of neurons using <code>navis.make_dotprops</code>:</p> <p>Turn our above skeleton into dotprops</p> <pre><code>dp = navis.make_dotprops(sk, k=5)\ndp\n</code></pre> type navis.Dotprops name DA1_lPN_R id 1734350788 k 5 units 8 nanometer n_points 4465 <pre><code>dp.points, dp.vect\n</code></pre> <p>Out:</p> <pre><code>(array([[15784., 37250., 28062.],\n       [15764., 37230., 28082.],\n       [15744., 37190., 28122.],\n       ...,\n       [14544., 36430., 28422.],\n       [14944., 36510., 28282.],\n       [15264., 36870., 28282.]], shape=(4465, 3), dtype=float32), array([[-0.3002053 , -0.39364937,  0.8688596 ],\n       [-0.10845336, -0.2113751 ,  0.9713694 ],\n       [-0.0435693 , -0.45593134,  0.8889479 ],\n       ...,\n       [-0.38446087,  0.44485292, -0.80888546],\n       [-0.9457323 , -0.1827982 , -0.26865458],\n       [-0.79947734, -0.5164282 , -0.30681902]],\n      shape=(4465, 3), dtype=float32))\n</code></pre> <p>Check out the NBLAST tutorial for further details on dotprops!</p>"},{"location":"generated/gallery/tutorial_basic_01_neurons/#voxelneurons","title":"VoxelNeurons","text":"<p><code>VoxelNeurons</code> represent neurons as either 3d image or x/y/z voxel coordinates typically obtained from e.g. light-level microscopy.</p> <p></p> <p><code>navis.VoxelNeuron</code> consist of either a dense 3d <code>(N, M, K)</code> array (a \"grid\") or a sparse 2d <code>(N, 3)</code> array of voxel coordinates (COO format). You will probably find yourself loading these data from image files (e.g. <code>.nrrd</code> via <code>navis.read_nrrd()</code>). That said we can also \"voxelize\" other neuron types to produce <code>VoxelNeurons</code>:</p> <pre><code># Load an example mesh\nm = navis.example_neurons(n=1, kind=\"mesh\")\n\n# Voxelize:\n# - with a 0.5 micron voxel size\n# - some Gaussian smoothing\n# - use number of vertices (counts) for voxel values\nvx = navis.voxelize(m, pitch=\"0.5 microns\", smooth=2, counts=True)\nvx\n</code></pre> type navis.VoxelNeuron name DA1_lPN_R id 1734350788 units 500.0 nanometer shape (298, 392, 286) dtype float32 <p>This is the grid representation of the neuron:</p> <pre><code>vx.grid.shape\n</code></pre> <p>Out:</p> <pre><code>(298, 392, 286)\n</code></pre> <p>And this is the <code>(N, 3)</code> voxel coordinates + <code>(N, )</code> values sparse representation of the neuron:</p> <pre><code>vx.voxels.shape, vx.values.shape\n</code></pre> <p>Out:</p> <pre><code>((643611, 3), (643611,))\n</code></pre> <p>Note</p> <p>You may have noticed that all neurons share some properties irrespective of their type, for example <code>.id</code>, <code>.name</code> or <code>.units</code>. These properties are optional and can be set when you first create the neuron, or at a later point.</p> <p>In particular the <code>.id</code> property is important because many functions in NAVis will return results that are indexed by the neurons' IDs. If <code>.id</code> is not set explicitly, it will default to some rather cryptic random UUID - you have been warned! </p>"},{"location":"generated/gallery/tutorial_basic_01_neurons/#neuron-meta-data","title":"Neuron meta data","text":""},{"location":"generated/gallery/tutorial_basic_01_neurons/#connectors","title":"Connectors","text":"<p>NAVis was designed with connectivity data in mind! Therefore, each neuron - regardless of type - can have a <code>.connectors</code> table. Connectors are meant to bundle all kinds of connections: pre- &amp; postsynapses, electrical synapses, gap junctions and so on.</p> <p>A connector table must minimally contain an <code>x/y/z</code> coordinate and a <code>type</code> for each connector. Here is an example of a connector table:</p> <pre><code>n = navis.example_neurons(1)\nn.connectors.head()\n</code></pre> connector_id node_id type x y z roi confidence 0 0 1436 pre 6444 21608 14516 LH(R) 0.959 1 1 1436 pre 6457 21634 14474 LH(R) 0.997 2 2 2638 pre 4728 23538 14179 LH(R) 0.886 3 3 1441 pre 5296 22059 16048 LH(R) 0.967 4 4 1872 pre 4838 23114 15146 LH(R) 0.990 <p>Connector tables aren't just passive meta data: certain functions in NAVis use or even require them. The most obvious example is probably for plotting:</p> <pre><code># Plot neuron including its connectors\nfig, ax = navis.plot2d(\n    n,  # the neuron\n    connectors=True,  # plot the neurons' connectors\n    color=\"k\",  # make the neuron black\n    cn_size=3,  # slightly increase connector size\n    view=(\"x\", \"-z\"),  # set frontal view\n    method=\"2d\"  # connectors are better visible in 2d\n)\n</code></pre> <p></p> <p>In above plot, red dots are presynapses (outputs) and cyan dots are postsynapses (inputs).</p>"},{"location":"generated/gallery/tutorial_basic_01_neurons/#somas","title":"Somas","text":"<p>Unless a neuron is truncated, it should have a soma somewhere. Knowing where the soma is can be very useful, e.g. as point of reference for distance calculations or for plotting. Therefore, {{ soma }} neurons have a <code>.soma</code> property:</p> <pre><code>n = navis.example_neurons(1)\nn.soma\n</code></pre> <p>Out:</p> <pre><code>np.int32(4177)\n</code></pre> <p>In case of this exemplary <code>navis.TreeNeuron</code>, the <code>.soma</code> points to an ID in the node table. We can also get the position:</p> <pre><code>n.soma_pos\n</code></pre> <p>Out:</p> <pre><code>array([[14957.1, 36540.7, 28432.4]], dtype=float32)\n</code></pre> <p>Other neuron types also support soma annotations but they may look slightly different. For a <code>navis.MeshNeuron</code>, annotating a node position makes little sense. Instead, we track the x/y/z position directly:</p> <pre><code>m = navis.example_neurons(1, kind=\"mesh\")\nm.soma_pos\n</code></pre> <p>Out:</p> <pre><code>array([14957.1, 36540.7, 28432.4])\n</code></pre> <p>For the record: <code>.soma</code> / <code>.soma_pos</code> can be set manually like any other property (there are some checks and balances to avoid issues) and can also be <code>None</code>:</p> <pre><code># Set the skeleton's soma on node with ID 1\nn.soma = 1\nn.soma\n</code></pre> <p>Out:</p> <pre><code>1\n</code></pre> <p>Drop soma from MeshNeuron</p> <pre><code>m.soma_pos = None\n</code></pre>"},{"location":"generated/gallery/tutorial_basic_01_neurons/#units","title":"Units","text":"<p>NAVis supports assigning units to neurons. The neurons shipping with NAVis, for example, are in 8x8x8nm voxel space<sup>1</sup>:</p> <pre><code>m = navis.example_neurons(1, kind=\"mesh\")\nprint(m.units)\n</code></pre> <p>Out:</p> <pre><code>8 nanometer\n</code></pre> <p>To set the neuron's units simply use a descriptive string:</p> <pre><code>m.units = \"10 micrometers\"\nprint(m.units)\n</code></pre> <p>Out:</p> <pre><code>10 micrometer\n</code></pre> <p>Note</p> <p>Setting the units as we did above does not actually change the neuron's coordinates. It merely sets a property that can be used by other functions to interpret the neuron's coordinate space. See below on how to convert the units of a neuron.</p> <p>Tracking units is good practice in general but is also very useful in a variety of scenarios:</p> <p>First, certain NAVis functions let you pass quantities as unit strings:</p> <pre><code># Load example neuron which is in 8x8x8nm space\nn = navis.example_neurons(1, kind=\"skeleton\")\n\n# Resample to 1 micrometer\nrs = navis.resample_skeleton(n, resample_to=\"1 um\")\n</code></pre> <p>Second, NAVis optionally uses the neuron's units to make certain properties more interpretable. By default, properties like cable length or volume are returned in the neuron's units, i.e. in 8x8x8nm voxel space in our case:</p> <pre><code>print(n.cable_length)\n</code></pre> <p>Out:</p> <pre><code>266476.88\n</code></pre> <p>You can tell NAVis to use the neuron's <code>.units</code> to make these properties more readable:</p> <pre><code>navis.config.add_units = True\nprint(n.cable_length)\nnavis.config.add_units = False  # reset to default\n</code></pre> <p>Out:</p> <pre><code>2.131814956665039 millimeter\n</code></pre> <p>Note</p> <p>Note that <code>n.cable_length</code> is now a <code>pint.Quantity</code> object. This may make certain operations a bit more cumbersome which is why this feature is optional. You can to a float by calling <code>.magnitude</code>:</p> <pre><code>n.cable_length.magnitude\n</code></pre> <p>Check out Pint's documentation to learn more.</p> <p>To actually convert the neuron's coordinate space, you have two options:</p> Multiply/DivideConvert units <p>You can multiply or divide any neuron or <code>NeuronList</code> by a number to change the units:</p> <pre><code># Example neuron are in 8x8x8nm voxel space\nn = navis.example_neurons(1)\n# Multiply by 8 to get to nanometer space\nn_nm = n * 8\n# Divide by 1000 to get micrometers\nn_um = n_nm / 1000\n</code></pre> <p>For non-isometric conversions you can pass a vector of scaling factors: </p><pre><code>neuron * [4, 4, 40]\n</code></pre> Note that for <code>TreeNeurons</code>, this is expected to be scaling factors for <code>(x, y, z, radius)</code>.<p></p> <p>If your neuron has known units, you can let NAVis do the conversion for you:</p> <pre><code>n = navis.example_neurons(1)\n# Convert to micrometers\nn_um = n.convert_units(\"micrometers\")\n</code></pre> <p>Addition &amp; Subtraction</p> <p>Multiplication and division will scale the neuro as you've seen above. Similarly, adding or subtracting to/from neurons will offset the neuron's coordinates: </p><pre><code>n = navis.example_neurons(1)\n\n# Convert to microns\nn_um = n.convert_units(\"micrometers\")\n\n# Add 100 micrometers along all axes to the neuron\nn_offset = n + 100\n\n# Subtract 100 micrometers along just one axis\nn_offset = n - [0, 0, 100]#\n</code></pre><p></p>"},{"location":"generated/gallery/tutorial_basic_01_neurons/#operating-on-neurons","title":"Operating on neurons","text":"<p>Above we've already seen examples of passing neurons to functions - for example <code>navis.plot2d(n)</code>.</p> <p>For some NAVis functions, neurons offer have shortcut \"methods\":</p> Using shorthand methodsUsing NAVis functions <pre><code>import navis\nsk = navis.example_neurons(1, kind='skeleton')\n\nsk.reroot(sk.soma, inplace=True)  # reroot the neuron to its soma\n\nlh = navis.example_volume('LH')\nsk.prune_by_volume(lh, inplace=True)  # prune the neuron to a volume#\n\nsk.plot3d(color='red')  # plot the neuron in 3d\n</code></pre> <pre><code>import navis\nsk = navis.example_neurons(1, kind='skeleton')\n\nnavis.reroot_skeleton(sk, sk.soma, inplace=True)  # reroot the neuron to its soma\n\nlh = navis.example_volume('LH')\nnavis.in_volume(sk, lh, inplace=True)  # prune the neuron to a volume\n\nnavis.plot3d(sk, color='red')  # plot the neuron in 3d\n</code></pre> <p>Note</p> <p>In some cases the shorthand methods might offer only a subset of the full function's functionality.</p>"},{"location":"generated/gallery/tutorial_basic_01_neurons/#the-inplace-parameter","title":"The <code>inplace</code> parameter","text":"<p>The <code>inplace</code> parameter is part of many NAVis functions and works like e.g. in the <code>pandas</code> library:</p> <ul> <li>if <code>inplace=True</code> operations are performed directly on the input neuron(s)</li> <li>if <code>inplace=False</code> (default) a modified copy of the input is returned and the input is left unchanged</li> </ul> <p>If you know you don't need the original, you can use <code>inplace=True</code> to save memory (and a bit of time):</p> <pre><code># Load a neuron\nn = navis.example_neurons(1)\n# Load an example neuropil\nlh = navis.example_volume(\"LH\")\n\n# Prune neuron to neuropil but leave original intact\nn_lh = n.prune_by_volume(lh, inplace=False)\n\nprint(f\"{n.n_nodes} nodes before and {n_lh.n_nodes} nodes after pruning\")\n</code></pre> <p>Out:</p> <pre><code>4465 nodes before and 344 nodes after pruning\n</code></pre>"},{"location":"generated/gallery/tutorial_basic_01_neurons/#all-neurons-are-equal","title":"All neurons are equal...","text":"<p>... but some are more equal than others.</p> <p>In Python the <code>==</code> operator compares two objects:</p> <pre><code>1 == 1\n</code></pre> <p>Out:</p> <pre><code>True\n</code></pre> <pre><code>2 == 1\n</code></pre> <p>Out:</p> <pre><code>False\n</code></pre> <p>For NAVis neurons this is comparison done by looking at the neurons' attribues: morphologies (soma &amp; root nodes, cable length, etc) and meta data (name).</p> <pre><code>n1, n2 = navis.example_neurons(n=2)\nn1 == n1\n</code></pre> <p>Out:</p> <pre><code>True\n</code></pre> <pre><code>n1 == n2\n</code></pre> <p>Out:</p> <pre><code>False\n</code></pre> <p>To find out which attributes are compared, check out the neuron's <code>.EQ_ATTRIBUTES</code> property:</p> <pre><code>navis.TreeNeuron.EQ_ATTRIBUTES\n</code></pre> <p>Out:</p> <pre><code>['n_nodes', 'n_connectors', 'soma', 'root', 'n_branches', 'n_leafs', 'cable_length', 'name']\n</code></pre> <p>Edit this list to establish your own criteria for equality.</p>"},{"location":"generated/gallery/tutorial_basic_01_neurons/#making-custom-changes","title":"Making custom changes","text":"<p>Under the hood NAVis calculates certain properties when you load a neuron: e.g. it produces a graph representation (<code>.graph</code> or <code>.igraph</code>) and a list of linear segments (<code>.segments</code>) for <code>TreeNeurons</code>. These data are attached to a neuron and are crucial for many functions. Therefore NAVis makes sure that any changes to a neuron automatically propagate into these derived properties. See this example:</p> <pre><code>n = navis.example_neurons(1, kind=\"skeleton\")\n\nprint(f\"Nodes in node table: {n.nodes.shape[0]}\")\nprint(f\"Nodes in graph: {len(n.graph.nodes)}\")\n</code></pre> <p>Out:</p> <pre><code>Nodes in node table: 4465\nNodes in graph: 4465\n</code></pre> <p>Making changes will cause the graph representation to be regenerated:</p> <pre><code>n.prune_by_strahler(1, inplace=True)\n\nprint(f\"Nodes in node table: {n.nodes.shape[0]}\")\nprint(f\"Nodes in graph: {len(n.graph.nodes)}\")\n</code></pre> <p>Out:</p> <pre><code>Nodes in node table: 1761\nNodes in graph: 1761\n</code></pre> <p>If, however, you make changes to the neurons that do not use built-in functions there is a chance that NAVis won't realize that things have changed and properties need to be regenerated!</p> <pre><code>n = navis.example_neurons(1)\n\nprint(f\"Nodes in node table before: {n.nodes.shape[0]}\")\nprint(f\"Nodes in graph before: {len(n.graph.nodes)}\")\n\n# Truncate the node table by 55 nodes\nn.nodes = n.nodes.iloc[:-55].copy()\n\nprint(f\"\\nNodes in node table after: {n.nodes.shape[0]}\")\nprint(f\"Nodes in graph after: {len(n.graph.nodes)}\")\n</code></pre> <p>Out:</p> <pre><code>Nodes in node table before: 4465\nNodes in graph before: 4465\n\nNodes in node table after: 4410\nNodes in graph after: 4410\n</code></pre> <p>Here, the changes to the node table automatically triggered a regeneration of the graph. This works because NAVis checks hash values of neurons and in this instance it detected that the node node table - which represents the core data for <code>TreeNeurons</code> - had changed. It would not work the other way around: changing the graph does not trigger changes in the node table.</p> <p>Again: as long as you are using built-in functions, you don't have to worry about this. If you do run some custom manipulation of neurons be aware that you might want to make sure that the data structure remains intact. If you ever need to manually trigger a regeneration you can do so like this:</p> <p>Clear temporary attributes of the neuron </p><pre><code>n._clear_temp_attr()\n</code></pre><p></p>"},{"location":"generated/gallery/tutorial_basic_01_neurons/#converting-neuron-types","title":"Converting neuron types","text":"<p>NAVis provides a couple functions to move between neuron types:</p> <ul> <li><code>navis.make_dotprops</code>: Convert any neuron to dotprops</li> <li><code>navis.skeletonize</code>: Convert any neuron to a skeleton</li> <li><code>navis.mesh</code>: Convert any neuron to a mesh</li> <li><code>navis.voxelize</code>: Convert any neuron to a voxel grid</li> </ul> <p>In particular meshing and skeletonizing are non-trivial and you might have to play around with the parameters to optimize results with your data! Let's demonstrate on some example:</p> <pre><code># Start with a mesh neuron\nm = navis.example_neurons(1, kind=\"mesh\")\n\n# Skeletonize the mesh\ns = navis.skeletonize(m)\n\n# Make dotprops (this works from any other neuron type\ndp = navis.make_dotprops(s, k=5)\n\n# Voxelize the mesh\nvx = navis.voxelize(m, pitch=\"2 microns\", smooth=1, counts=True)\n\n# Mesh the voxels\nmm = navis.mesh(vx.threshold(0.5))\n</code></pre> <p>Inspect the results:</p> <pre><code># Co-visualize the mesh and the skeleton\nnavis.plot3d(\n    [m, s],\n    color=[(0.7, 0.7, 0.7, 0.2), \"r\"],  # transparent mesh, skeleton in red\n    radius=False,  # False so that skeleton is drawn as a line\n)\n</code></pre> <p>Co-visualize the mesh and the dotprops</p> <pre><code>navis.plot3d(\n    [m, dp],\n    color=[(0.7, 0.7, 0.7, 0.2), \"r\"],  # transparent mesh, dotprops in red\n)\n</code></pre> <p>Co-visualize the mesh and the dotprops (note that plotly is not great at visualizing voxels)</p> <pre><code>navis.plot3d([m * 8, vx])\n</code></pre> <p>Co-visualize the original mesh and the meshed voxels</p> <pre><code>navis.plot3d([vx, mm], fig_autosize=True)\n</code></pre>"},{"location":"generated/gallery/tutorial_basic_01_neurons/#neuron-attributes","title":"Neuron attributes","text":"<p>This is a selection of neuron class (i.e. <code>navis.TreeNeuron</code>, <code>navis.MeshNeuron</code>, etc.) attributes.</p> <p>All neurons have this:</p> <ul> <li><code>name</code>: a name</li> <li><code>id</code>: a (hopefully unique) identifier - defaults to random UUID if not set explicitly</li> <li><code>bbox</code>: Bounding box of neuron</li> <li><code>units</code> (optional): spatial units (e.g. \"1 micrometer\" or \"8 nanometer\" voxels)</li> <li><code>connectors</code> (optional): connector table</li> </ul> <p>Only for <code>TreeNeurons</code>:</p> <ul> <li><code>nodes</code>: node table</li> <li><code>cable_length</code>: cable length(s)</li> <li><code>soma</code>: node ID(s) of soma (if applicable)</li> <li><code>root</code>: root node ID(s)</li> <li><code>segments</code>: list of linear segments</li> <li><code>graph</code>: NetworkX graph representation of the neuron</li> <li><code>igraph</code>: iGraph representation of the neuron (if library available)</li> </ul> <p>Only for <code>MeshNeurons</code>:</p> <ul> <li><code>vertices</code>/<code>faces</code>: vertices and faces</li> <li><code>volume</code>: volume of mesh</li> <li><code>soma_pos</code>: x/y/z position of soma</li> </ul> <p>Only for <code>VoxelNeurons</code>:</p> <ul> <li><code>voxels</code>: <code>(N, 3)</code> sparse representation</li> <li><code>grid</code>: <code>(N, M, K)</code> voxel grid representation</li> </ul> <p>Only for <code>Dotprops</code>:</p> <ul> <li><code>points</code> <code>(N, 3</code>) x/y/z points</li> <li><code>vect</code>: <code>(N, 3)</code> array of the vector associated with each point</li> </ul> <p>All above attributes can be accessed via <code>NeuronLists</code> containing the neurons. In addition you can also get:</p> <ul> <li><code>is_mixed</code>: returns <code>True</code> if list contains more than one neuron type</li> <li><code>is_degenerated</code>: returns <code>True</code> if list contains neurons with non-unique IDs</li> <li><code>types</code>: tuple with all types of neurons in the list</li> <li><code>shape</code>: size of neuronlist <code>(N, )</code></li> </ul> <p>All attributes and methods are accessible through auto-completion.</p>"},{"location":"generated/gallery/tutorial_basic_01_neurons/#what-next","title":"What next?","text":"<ul> <li> <p> Lists of Neurons</p> <p>Check out the guide on lists of neurons.</p> <p> NeuronLists tutorial</p> </li> <li> <p> Neuron I/O</p> <p>Learn about how to load your own neurons into NAVis.</p> <p> I/O Tutorials</p> </li> </ul> <p>Total running time of the script: ( 0 minutes  1.946 seconds)</p> <p> Download Python source code: tutorial_basic_01_neurons.py</p> <p> Download Jupyter notebook: tutorial_basic_01_neurons.ipynb</p> <p>Gallery generated by mkdocs-gallery</p> <ol> <li> <p>The example neurons are from the Janelia hemibrain connectome project which as imaged at 8x8x8nm resolution.\u00a0\u21a9</p> </li> </ol>"},{"location":"generated/gallery/tutorial_basic_02_neuronlists/","title":"Lists of Neurons","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/tutorial_basic_02_neuronlists/#lists-of-neurons","title":"Lists of Neurons","text":"<p>This tutorial will show you how to use NeuronLists to efficiently work with many neurons at a time.</p> <p>Note</p> <p>If you haven't please check out the neuron types tutorial first.</p> <p>NAVis will typically collect multiple neurons into a <code>navis.NeuronList</code> as container. This container behaves like a mix of lists, numpy arrays and pandas dataframes, and allows you to quickly sort, filter and manipulate neurons.</p>"},{"location":"generated/gallery/tutorial_basic_02_neuronlists/#overview","title":"Overview","text":"<pre><code>import navis\n\n# Grab three example skeletons (TreeNeurons) as a NeuronList\nnl = navis.example_neurons(n=3)\nnl\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 3 neurons (875.1KiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron DA1_lPN_R 1734350788 4465 2705 599 618 266476.87500 4177.0 8 nanometer 2026-03-01 17:18:33.869310 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 1 navis.TreeNeuron DA1_lPN_R 1734350908 4847 3042 735 761 304332.65625 6.0 8 nanometer 2026-03-01 17:18:33.876625 /home/runner/work/navis/navis/navis/data/swc/1... 1734350908.swc 2 navis.TreeNeuron DA1_lPN_R 722817260 4332 3136 633 656 274703.37500 NaN 8 nanometer 2026-03-01 17:18:33.882841 /home/runner/work/navis/navis/navis/data/swc/7... 722817260.swc <p>Note</p> <p>Note how just printing <code>nl</code> at the end of the cell will produce a nice summary table? If you want to get this table as pandas DataFrame, use the <code>summary()</code> method: </p><pre><code>df = nl.summary()\n</code></pre><p></p> <pre><code>import matplotlib.pyplot as plt\n\nnavis.plot2d(nl, view=('x', '-z'), method='2d')\nplt.tight_layout()\n</code></pre> <p></p>"},{"location":"generated/gallery/tutorial_basic_02_neuronlists/#creating-neuronlists","title":"Creating NeuronLists","text":"<p>To create a <code>NeuronList</code> from scratch simply pass a list of neurons to the constructor:</p> <pre><code>n = navis.example_neurons(n=1)\nnl = navis.NeuronList([n, n, n])  # a list with 3x the same neuron\nnl\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 3 neurons (821.3KiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron DA1_lPN_R 1734350788 4465 2705 599 618 266476.875 4177 8 nanometer 2026-03-01 17:18:34.122231 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 1 navis.TreeNeuron DA1_lPN_R 1734350788 4465 2705 599 618 266476.875 4177 8 nanometer 2026-03-01 17:18:34.122231 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 2 navis.TreeNeuron DA1_lPN_R 1734350788 4465 2705 599 618 266476.875 4177 8 nanometer 2026-03-01 17:18:34.122231 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc"},{"location":"generated/gallery/tutorial_basic_02_neuronlists/#accessing-neuron-attributes","title":"Accessing Neuron Attributes","text":"<p><code>NeuronLists</code> give you quick and easy access to data and across all neurons:</p> <pre><code># Get the number of nodes in the first skeleton\nnl = navis.example_neurons(n=3)\nnl[0].n_nodes\n</code></pre> <p>Out:</p> <pre><code>4465\n</code></pre> <p>Use the <code>NeuronList</code> to collect number of nodes across all neurons:</p> <pre><code>nl.n_nodes\n</code></pre> <p>Out:</p> <pre><code>array([4465, 4847, 4332])\n</code></pre> <p>This works on any neuron attribute:</p> <pre><code>nl.cable_length\n</code></pre> <p>Out:</p> <pre><code>array([266476.88, 304332.66, 274703.38], dtype=float32)\n</code></pre> <p>Note</p> <p>The <code>n_{attribute}</code> pattern works with any \"countable\" neuron attributes like nodes, connectors, etc.</p> <p>If the neuron attribute is a dataframe, the <code>NeuronList</code> will concatenate them and add a new column with the neuron ID:</p> <pre><code>nl.nodes  # note the `neuron` column\n</code></pre> label node_id parent_id radius type x y z neuron 0 0 1 -1 10.000000 root 15784.0 37250.0 28062.0 1734350788 1 0 2 1 18.284300 slab 15764.0 37230.0 28082.0 1734350788 2 0 3 2 34.721401 slab 15744.0 37190.0 28122.0 1734350788 3 0 4 3 34.721401 slab 15744.0 37150.0 28202.0 1734350788 4 0 5 4 34.721401 slab 15704.0 37130.0 28242.0 1734350788 ... ... ... ... ... ... ... ... ... ... 13639 6 4328 4242 33.000000 end 5112.0 21598.0 15786.0 722817260 13640 6 4329 1461 33.000000 end 4958.0 23622.0 15170.0 722817260 13641 6 4330 4035 51.225399 end 4716.0 23490.0 15676.0 722817260 13642 6 4331 2603 33.000000 end 4980.0 23226.0 14994.0 722817260 13643 6 4332 1971 33.000000 end 5156.0 23204.0 15148.0 722817260 <p>13644 rows \u00d7 9 columns</p> <p><code>NeuronLists</code> can also contain a mix of different neuron types:</p> <pre><code>nl_mix = navis.example_neurons(n=2, kind='mix')\nnl_mix\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 2 neurons (875.5KiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file n_vertices n_faces 0 navis.TreeNeuron DA1_lPN_R 1734350788 4465.0 2705 599.0 618.0 266476.875 4177 8 nanometer 2026-03-01 17:18:34.175892 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 4465 NaN 1 navis.MeshNeuron DA1_lPN_R 1734350788 NaN 2705 NaN NaN NA NA 8 nanometer NA NA NA 6309 13054.0 <p>Note how <code>nl_mix</code> contains a <code>TreeNeuron</code> and a <code>MeshNeuron</code>?</p> <p>In such cases you have to be a bit more careful about asking for attributes that are not shared across all neurons:</p> <pre><code># This will throw an error because MeshNeurons\n# don't have a `cable_length` attribute\nnl_mix.cable_length\n</code></pre> <pre><code># Instead use the `get_neuron_attributes()` method with a default value:\nnl_mix.get_neuron_attributes('cable_length', None)\n</code></pre> <p>Out:</p> <pre><code>array([np.float32(266476.88), None], dtype=object)\n</code></pre>"},{"location":"generated/gallery/tutorial_basic_02_neuronlists/#indexing-neuronlists","title":"Indexing NeuronLists","text":"<p>A <code>NeuronList</code> works similar to normal lists with a bunch of additional perks:</p> <pre><code># Get the first neuron in the list\nnl = navis.example_neurons(n=3)\nnl[0]\n</code></pre> type navis.TreeNeuron name DA1_lPN_R id 1734350788 n_nodes 4465 n_connectors 2705 n_branches 599 n_leafs 618 cable_length 266476.875 soma 4177 units 8 nanometer created_at 2026-03-01 17:18:34.212430 origin /home/runner/work/navis/navis/navis/data/swc/1... file 1734350788.swc"},{"location":"generated/gallery/tutorial_basic_02_neuronlists/#index-by-position","title":"Index by position","text":"<p><code>NeuronLists</code> are designed to behave similar to numpy arrays in that they allow some fancing indexing.</p> <p>You've already seen how to extract a single neuron from a <code>NeuronList</code> using a single integer index. Like with numpy arrays, this also works for lists of indices...</p> <pre><code>nl = navis.example_neurons(n=3)\nnl[[0, 2]]\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 2 neurons (571.4KiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron DA1_lPN_R 1734350788 4465 2705 599 618 266476.875 4177.0 8 nanometer 2026-03-01 17:18:34.242544 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 1 navis.TreeNeuron DA1_lPN_R 722817260 4332 3136 633 656 274703.375 NaN 8 nanometer 2026-03-01 17:18:34.255767 /home/runner/work/navis/navis/navis/data/swc/7... 722817260.swc <p>... or slices</p> <pre><code>nl[:2]\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 2 neurons (577.6KiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron DA1_lPN_R 1734350788 4465 2705 599 618 266476.87500 4177 8 nanometer 2026-03-01 17:18:34.242544 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 1 navis.TreeNeuron DA1_lPN_R 1734350908 4847 3042 735 761 304332.65625 6 8 nanometer 2026-03-01 17:18:34.249301 /home/runner/work/navis/navis/navis/data/swc/1... 1734350908.swc"},{"location":"generated/gallery/tutorial_basic_02_neuronlists/#index-by-attributes","title":"Index by attributes","text":"<p>You can index <code>NeuronLists</code> by boolean <code>numpy.arrays</code> - that includes neuron attributes, e.g. <code>n_nodes</code>, <code>cable_length</code>, <code>soma</code>, etc.</p> <p>Index using node count:</p> <pre><code>subset = nl[nl.n_branches &gt; 700]\nsubset\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 1 neurons (303.8KiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron DA1_lPN_R 1734350908 4847 3042 735 761 304332.65625 6 8 nanometer 2026-03-01 17:18:34.249301 /home/runner/work/navis/navis/navis/data/swc/1... 1734350908.swc <p>Here is an example where we subset to neurons that have a soma:</p> <pre><code>subset = nl[nl.soma != None]  # Index by boolean array\nsubset\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 2 neurons (577.6KiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron DA1_lPN_R 1734350788 4465 2705 599 618 266476.87500 4177 8 nanometer 2026-03-01 17:18:34.242544 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 1 navis.TreeNeuron DA1_lPN_R 1734350908 4847 3042 735 761 304332.65625 6 8 nanometer 2026-03-01 17:18:34.249301 /home/runner/work/navis/navis/navis/data/swc/1... 1734350908.swc"},{"location":"generated/gallery/tutorial_basic_02_neuronlists/#index-by-name","title":"Index by name","text":"<p><code>navis.TreeNeuron</code> can (but don't have to) have names (<code>.name</code>). If you, for example, import neurons from SWC files they will inherit their name from the file by default.</p> <p>Our example neurons all have the same name, so to demo this feature we will need to make those names unique:</p> <pre><code>for i, n in enumerate(nl):\n    n.name = n.name + str(i + 1)\nnl\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 3 neurons (875.1KiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron DA1_lPN_R1 1734350788 4465 2705 599 618 266476.87500 4177.0 8 nanometer 2026-03-01 17:18:34.242544 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 1 navis.TreeNeuron DA1_lPN_R2 1734350908 4847 3042 735 761 304332.65625 6.0 8 nanometer 2026-03-01 17:18:34.249301 /home/runner/work/navis/navis/navis/data/swc/1... 1734350908.swc 2 navis.TreeNeuron DA1_lPN_R3 722817260 4332 3136 633 656 274703.37500 NaN 8 nanometer 2026-03-01 17:18:34.255767 /home/runner/work/navis/navis/navis/data/swc/7... 722817260.swc <p>You can index by single...</p> <pre><code>nl[\"DA1_lPN_R1\"]\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 1 neurons (273.8KiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron DA1_lPN_R1 1734350788 4465 2705 599 618 266476.875 4177 8 nanometer 2026-03-01 17:18:34.242544 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc <p>... or multiple names:</p> <pre><code>nl[[\"DA1_lPN_R1\", \"DA1_lPN_R2\"]]\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 2 neurons (577.6KiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron DA1_lPN_R1 1734350788 4465 2705 599 618 266476.87500 4177 8 nanometer 2026-03-01 17:18:34.242544 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 1 navis.TreeNeuron DA1_lPN_R2 1734350908 4847 3042 735 761 304332.65625 6 8 nanometer 2026-03-01 17:18:34.249301 /home/runner/work/navis/navis/navis/data/swc/1... 1734350908.swc"},{"location":"generated/gallery/tutorial_basic_02_neuronlists/#using-regex","title":"Using regex","text":"<p>Under the hood NAVis uses <code>re.fullmatch</code> to match neuron names - so you can use regex!</p> <pre><code>nl[\".*DA1.*\"]\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 3 neurons (875.1KiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron DA1_lPN_R1 1734350788 4465 2705 599 618 266476.87500 4177.0 8 nanometer 2026-03-01 17:18:34.242544 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 1 navis.TreeNeuron DA1_lPN_R2 1734350908 4847 3042 735 761 304332.65625 6.0 8 nanometer 2026-03-01 17:18:34.249301 /home/runner/work/navis/navis/navis/data/swc/1... 1734350908.swc 2 navis.TreeNeuron DA1_lPN_R3 722817260 4332 3136 633 656 274703.37500 NaN 8 nanometer 2026-03-01 17:18:34.255767 /home/runner/work/navis/navis/navis/data/swc/7... 722817260.swc"},{"location":"generated/gallery/tutorial_basic_02_neuronlists/#index-by-id","title":"Index by ID","text":"<p>All neurons have an ID - even if you don't explicitly assign one, a UUID will assigned under the hood.</p> <pre><code>nl[0].id\n</code></pre> <p>Out:</p> <pre><code>1734350788\n</code></pre> <p>Neuron lists can be indexed by their ID (similar to <code>.loc[]</code> in pandas DataFrames) by using the <code>.idx</code> indexer:</p> <pre><code>nl.idx[1734350908]\n</code></pre> type navis.TreeNeuron name DA1_lPN_R2 id 1734350908 n_nodes 4847 n_connectors 3042 n_branches 735 n_leafs 761 cable_length 304332.65625 soma 6 units 8 nanometer created_at 2026-03-01 17:18:34.249301 origin /home/runner/work/navis/navis/navis/data/swc/1... file 1734350908.swc"},{"location":"generated/gallery/tutorial_basic_02_neuronlists/#neuron-math","title":"Neuron Math","text":"<p>NAVis implements a very simple and intuitive syntax to add and remove items from a <code>navis.NeuronList</code>:</p>"},{"location":"generated/gallery/tutorial_basic_02_neuronlists/#addition","title":"Addition","text":"<p>To merge two lists in Python, you can simply add them:</p> <pre><code>[1] + [3]\n</code></pre> <p>Out:</p> <pre><code>[1, 3]\n</code></pre> <p><code>navis.NeuronList</code> works exactly the same:</p> <pre><code>nl[:2] + nl[2:]\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 3 neurons (875.1KiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron DA1_lPN_R1 1734350788 4465 2705 599 618 266476.87500 4177.0 8 nanometer 2026-03-01 17:18:34.242544 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 1 navis.TreeNeuron DA1_lPN_R2 1734350908 4847 3042 735 761 304332.65625 6.0 8 nanometer 2026-03-01 17:18:34.249301 /home/runner/work/navis/navis/navis/data/swc/1... 1734350908.swc 2 navis.TreeNeuron DA1_lPN_R3 722817260 4332 3136 633 656 274703.37500 NaN 8 nanometer 2026-03-01 17:18:34.255767 /home/runner/work/navis/navis/navis/data/swc/7... 722817260.swc <p>This also works on with two single <code>navis.TreeNeurons</code>! You can use that to combine them into a list:</p> <pre><code>nl[0] + nl[1]\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 2 neurons (577.6KiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron DA1_lPN_R1 1734350788 4465 2705 599 618 266476.87500 4177 8 nanometer 2026-03-01 17:18:34.242544 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 1 navis.TreeNeuron DA1_lPN_R2 1734350908 4847 3042 735 761 304332.65625 6 8 nanometer 2026-03-01 17:18:34.249301 /home/runner/work/navis/navis/navis/data/swc/1... 1734350908.swc"},{"location":"generated/gallery/tutorial_basic_02_neuronlists/#substraction","title":"Substraction","text":"<p>To remove an item from a Python list, you would call the <code>.pop()</code> method:</p> <pre><code>l = [1, 2, 3]\nl.pop(2)\nl\n</code></pre> <p>Out:</p> <pre><code>[1, 2]\n</code></pre> <p>For <code>navis.NeuronList</code> you can use substraction:</p> <pre><code>nl - nl[2]\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 2 neurons (577.6KiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron DA1_lPN_R1 1734350788 4465 2705 599 618 266476.87500 4177 8 nanometer 2026-03-01 17:18:34.242544 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 1 navis.TreeNeuron DA1_lPN_R2 1734350908 4847 3042 735 761 304332.65625 6 8 nanometer 2026-03-01 17:18:34.249301 /home/runner/work/navis/navis/navis/data/swc/1... 1734350908.swc"},{"location":"generated/gallery/tutorial_basic_02_neuronlists/#bitwise-and","title":"Bitwise AND","text":"<p>To find the intersection between two lists, you would use <code>sets</code> and the <code>&amp;</code> operator:</p> <pre><code>set([0, 1, 2]) &amp; set([2, 3, 4])\n</code></pre> <p>Out:</p> <pre><code>{2}\n</code></pre> <p><code>navis.NeuronList</code> work similarly:</p> <pre><code>nl[[0, 1]] &amp; nl[[1, 2]]\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 1 neurons (303.8KiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron DA1_lPN_R2 1734350908 4847 3042 735 761 304332.65625 6 8 nanometer 2026-03-01 17:18:34.249301 /home/runner/work/navis/navis/navis/data/swc/1... 1734350908.swc"},{"location":"generated/gallery/tutorial_basic_02_neuronlists/#bitwise-or","title":"Bitwise OR","text":"<p>To generate the union between two lists, you would use <code>sets</code> and the <code>|</code> operator:</p> <pre><code>set([0, 1, 2]) | set([2, 3, 4])\n</code></pre> <p>Out:</p> <pre><code>{0, 1, 2, 3, 4}\n</code></pre> <p><code>navis.NeuronLists</code> work similarly:</p> <pre><code>nl[[0, 1]] | nl[[1, 2]]\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 3 neurons (875.1KiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron DA1_lPN_R1 1734350788 4465 2705 599 618 266476.87500 4177.0 8 nanometer 2026-03-01 17:18:34.242544 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 1 navis.TreeNeuron DA1_lPN_R2 1734350908 4847 3042 735 761 304332.65625 6.0 8 nanometer 2026-03-01 17:18:34.249301 /home/runner/work/navis/navis/navis/data/swc/1... 1734350908.swc 2 navis.TreeNeuron DA1_lPN_R3 722817260 4332 3136 633 656 274703.37500 NaN 8 nanometer 2026-03-01 17:18:34.255767 /home/runner/work/navis/navis/navis/data/swc/7... 722817260.swc <p>Important</p> <p>Be aware that bitwise AND and OR will likely change the order of the neurons in the list.</p>"},{"location":"generated/gallery/tutorial_basic_02_neuronlists/#multiplication-and-division","title":"Multiplication and Division","text":"<p>So far, all operations have led to changes in the structure of the <code>navis.NeuronList</code>. Multiplication and division are different! Just like multiplying/dividing individual neurons by a number, multiplying/dividing a <code>navis.NeuronList</code> will change the coordinates of nodes, vertices, etc. (including associated data such as radii or connector positions) of the neurons in the list:</p> <pre><code>nl.units  # our neurons are originally in 8x8x8 nm voxels\n</code></pre> Magnitude<pre>[8 8 8]</pre>Unitsnanometer <pre><code>nl_um = nl * 8 / 1000  # convert neurons: voxels -&gt; nm -&gt; um\nnl_um.units\n</code></pre> Magnitude<pre>[1.0 1.0 1.0]</pre>Unitsmicrometer <p>The above will have changed the coordinates for all neurons in the list.</p>"},{"location":"generated/gallery/tutorial_basic_02_neuronlists/#comparing-neuronlists","title":"Comparing NeuronLists","text":"<p><code>navis.NeuronList</code> implements some of the basic arithmetic and comparison operators that you might know from standard <code>lists</code> or <code>numpy.arrays</code>. Most this should be fairly intuitive (I hope) but there are a few things you should be aware of. The following examples will illustrate that.</p> <p>In Python the <code>==</code> operator compares two elements:</p> <pre><code>1 == 1\n</code></pre> <p>Out:</p> <pre><code>True\n</code></pre> <pre><code>2 == 1\n</code></pre> <p>Out:</p> <pre><code>False\n</code></pre> <p>For <code>navis.TreeNeuron</code> this is comparison done by looking at the neurons' attribues: morphologies (soma &amp; root nodes, cable length, etc) and meta data (name).</p> <pre><code>nl[0] == nl[0]\n</code></pre> <p>Out:</p> <pre><code>True\n</code></pre> <pre><code>nl[0] == nl[1]\n</code></pre> <p>Out:</p> <pre><code>False\n</code></pre> <p>To find out which attributes are compared, check out:</p> <pre><code>navis.TreeNeuron.EQ_ATTRIBUTES\n</code></pre> <p>Out:</p> <pre><code>['n_nodes', 'n_connectors', 'soma', 'root', 'n_branches', 'n_leafs', 'cable_length', 'name']\n</code></pre> <p>Edit this list to establish your own criteria for equality.</p> <p>For <code>NeuronList</code>, we do the same comparison pairwise between the neurons in both lists:</p> <pre><code>nl == nl\n</code></pre> <p>Out:</p> <pre><code>True\n</code></pre> <pre><code>nl == nl[:2]\n</code></pre> <p>Out:</p> <pre><code>False\n</code></pre> <p>Because the comparison is done pairwise and in order, shuffling a <code>NeuronList</code> will result in a failed comparison:</p> <pre><code>nl == nl[[2, 1, 0]]\n</code></pre> <p>Out:</p> <pre><code>False\n</code></pre> <p>Comparisons are safe against copying but making any changes to the neurons will cause inequality:</p> <pre><code>nl[0] == nl[0].copy()\n</code></pre> <p>Out:</p> <pre><code>True\n</code></pre> <pre><code>nl[0] == nl[0].downsample(2, inplace=False)\n</code></pre> <p>Out:</p> <pre><code>False\n</code></pre> <p>You can also ask if a neuron is in a given <code>NeuronList</code>:</p> <pre><code>nl[0] in nl\n</code></pre> <p>Out:</p> <pre><code>True\n</code></pre> <pre><code>nl[0] in nl[1:]\n</code></pre> <p>Out:</p> <pre><code>False\n</code></pre>"},{"location":"generated/gallery/tutorial_basic_02_neuronlists/#operating-on-neuronlists","title":"Operating on NeuronLists","text":"<p>With very few exceptions, all NAVis functions that work on individual neurons also work on <code>navis.NeuronList</code>.</p> <p>Note</p> <p>In general, NAVis functions expect multiple neurons to be passed as a <code>NeuronList</code> - not as a list of neurons: </p><pre><code>n1, n2 = navis.example_neurons(2)  # grab two individual neurons\n\n# This will raise an error\nnavis.downsample_neuron([n1, n2], 2)\n\n# This will work\nnavis.downsample_neuron(navis.NeuronList([n1, n2]), 2)\n</code></pre><p></p>"},{"location":"generated/gallery/tutorial_basic_02_neuronlists/#neuronlist-methods","title":"NeuronList methods","text":"<p>Similar to individual neurons, <code>navis.NeuronLists</code> have a number of methods that allow you to manipulate the neurons in the list. In fact, (almost) all shorthand methods on individual neurons also work on neuron lists:</p> Operating on individual neuronsUsing the neuronlist <pre><code>nl = navis.example_neurons(2)\nfor n in nl:\n   n.reroot(n.soma, inplace=True)  # reroot the neuron to its soma\n</code></pre> <pre><code>nl = navis.example_neurons(2)\nnl.reroot(nl.soma, inplace=True)  # reroot the neuron to its soma\n</code></pre> <p>In addition <code>navis.NeuronLists</code> have a number of specialised methods:</p> <pre><code>nl = navis.example_neurons(3)  # load a neuron list\ndf = nl.summary()  # get a summary table with all neurons\ndf.head()\n</code></pre> type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron DA1_lPN_R 1734350788 4465 2705 599 618 266476.87500 4177.0 8 nanometer 2026-03-01 17:18:34.419352 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 1 navis.TreeNeuron DA1_lPN_R 1734350908 4847 3042 735 761 304332.65625 6.0 8 nanometer 2026-03-01 17:18:34.425825 /home/runner/work/navis/navis/navis/data/swc/1... 1734350908.swc 2 navis.TreeNeuron DA1_lPN_R 722817260 4332 3136 633 656 274703.37500 NaN 8 nanometer 2026-03-01 17:18:34.431921 /home/runner/work/navis/navis/navis/data/swc/7... 722817260.swc <pre><code># Quickly map new attributes onto the neurons\nnl.set_neuron_attributes(['Huey', 'Dewey', 'Louie'], name='name')\nnl.set_neuron_attributes(['Nephew1', 'Nephew2', 'Nephew3'], name='id')\nnl\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 3 neurons (875.1KiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron Huey Nephew1 4465 2705 599 618 266476.87500 4177.0 8 nanometer 2026-03-01 17:18:34.419352 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 1 navis.TreeNeuron Dewey Nephew2 4847 3042 735 761 304332.65625 6.0 8 nanometer 2026-03-01 17:18:34.425825 /home/runner/work/navis/navis/navis/data/swc/1... 1734350908.swc 2 navis.TreeNeuron Louie Nephew3 4332 3136 633 656 274703.37500 NaN 8 nanometer 2026-03-01 17:18:34.431921 /home/runner/work/navis/navis/navis/data/swc/7... 722817260.swc <pre><code># Sort the neurons by their name\nnl.sort_values('name')  # this is always done inplace\nnl\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 3 neurons (875.1KiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron Louie Nephew3 4332 3136 633 656 274703.37500 NaN 8 nanometer 2026-03-01 17:18:34.431921 /home/runner/work/navis/navis/navis/data/swc/7... 722817260.swc 1 navis.TreeNeuron Huey Nephew1 4465 2705 599 618 266476.87500 4177.0 8 nanometer 2026-03-01 17:18:34.419352 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 2 navis.TreeNeuron Dewey Nephew2 4847 3042 735 761 304332.65625 6.0 8 nanometer 2026-03-01 17:18:34.425825 /home/runner/work/navis/navis/navis/data/swc/1... 1734350908.swc <p>Of course there are also a number of <code>NeuronList</code>-specific properties:</p> <ul> <li><code>is_mixed</code>: returns <code>True</code> if list contains more than one neuron type</li> <li><code>is_degenerated</code>: returns <code>True</code> if list contains neurons with non-unique IDs</li> <li><code>types</code>: tuple with all types of neurons in the list</li> <li><code>shape</code>: size of neuronlist <code>(N, )</code></li> </ul> <p>All attributes and methods are accessible through auto-completion.</p>"},{"location":"generated/gallery/tutorial_basic_02_neuronlists/#what-next","title":"What next?","text":"<ul> <li> <p> Neuron I/O     ---</p> <p>Learn about how to load your own neurons into NAVis.</p> <p> I/O Tutorials</p> </li> <li> <p> Visualizations     ---</p> <p>Check out the guides on visualizations.</p> <p> I/O Tutorials</p> </li> </ul> <p>Total running time of the script: ( 0 minutes  0.603 seconds)</p> <p> Download Python source code: tutorial_basic_02_neuronlists.py</p> <p> Download Jupyter notebook: tutorial_basic_02_neuronlists.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/0_io/mg_execution_times/","title":"Computation times","text":""},{"location":"generated/gallery/0_io/mg_execution_times/#computation-times","title":"Computation times","text":"<p>01:14.276 total execution time for generated_gallery_0_io files:</p> <p>+------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_io_00_skeletons (docs/examples/0_io/tutorial_io_00_skeletons.py)                   | 01:00.872 | 0.0 MB | +------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_io_02_dotprops (docs/examples/0_io/tutorial_io_02_dotprops.py)                      | 00:13.385 | 0.0 MB | +------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_io_01_meshes (docs/examples/0_io/tutorial_io_01_meshes.py)                            | 00:00.018 | 0.0 MB | +------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_io_04_pickle (docs/examples/0_io/tutorial_io_04_pickle.py)                            | 00:00.000 | 0.0 MB | +------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | zzz_tutorial_io_05_skeletonize (docs/examples/0_io/zzz_tutorial_io_05_skeletonize.py) | 00:00.000 | 0.0 MB | +------------------------------------------------------------------------------------------------------------------------------+-----------+--------+</p>"},{"location":"generated/gallery/0_io/tutorial_io_00_skeletons/","title":"Skeletons","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/0_io/tutorial_io_00_skeletons/#skeletons","title":"Skeletons","text":"<p>This tutorial will show you how to load and save skeletons.</p> <p>Skeletons are probably the most common representation of neurons and are stored as a series of connected nodes (the \"skeleton\"). In NAVis, skeletons are represented by the <code>navis.TreeNeuron</code> class.</p> <p>You can either construct these manually (see bottom of this page) or use one of the built-in functions to them from one of the various file formats:</p> <p>Note</p> <p>NAVis has dedicated interfaces for loading skeletons from remote data sources (e.g. the MICrONS, neuromorpho, Virtual Fly Brain or Janelia hemibrain datasets). These are covered in separate tutorials.</p> <p>If you have light-level microscopy data, you might also be interested in the tutorial on skeletons from light-level data.</p>"},{"location":"generated/gallery/0_io/tutorial_io_00_skeletons/#from-swc-files","title":"From SWC files","text":"<p>SWC is a common format for storing neuron skeletons. Thus NAVis provides functions to both read and write SWC files. To demo these, we will be using supplemental data from Bates, Schlegel et al. (Current Biology, 2020). If you want to follow along, please download Supplemental Data S1 (link). If you do, make sure to adjust the filepaths in the examples according to where you saved it to.</p> <p>I extracted the archive with the supplemental data inside my downloads folder.</p> <p>It contains a bunch of CSV files with meta data but the important file for us is the <code>\"skeletons_swc.zip\"</code>. Now you could extract that zip archive too but NAVis can actually read directly from (and write to) zip files!</p> <pre><code>import navis\nskeletons = navis.read_swc(\n    'mmc2/skeletons_swc.zip',\n    include_subdirs=True\n)\nskeletons\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 480 neurons (3.7MiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron af245389-3c04-4a9c-983e-a0cc95546942 af245389-3c04-4a9c-983e-a0cc95546942 11634 None 600 611 2.899338e+06 None 1 dimensionless 2026-03-01 17:18:34.739814 mmc2/skeletons_swc.zip af245389-3c04-4a9c-983e-a0cc95546942.swc 1 navis.TreeNeuron a6f13d47-997e-4934-8f1c-e6fd54ab7241 a6f13d47-997e-4934-8f1c-e6fd54ab7241 1670 None 24 25 6.282917e+05 None 1 dimensionless 2026-03-01 17:18:34.727820 mmc2/skeletons_swc.zip a6f13d47-997e-4934-8f1c-e6fd54ab7241.swc ... ... ... ... ... ... ... ... ... ... ... ... ... ... 478 navis.TreeNeuron a76da20e-43ff-4e0f-a455-a1cca325ffb7 a76da20e-43ff-4e0f-a455-a1cca325ffb7 14782 None 663 673 2.714858e+06 None 1 dimensionless 2026-03-01 17:18:36.067849 mmc2/skeletons_swc.zip a76da20e-43ff-4e0f-a455-a1cca325ffb7.swc 479 navis.TreeNeuron 90db0156-7aa0-42e1-a1ad-162a3c2168e9 90db0156-7aa0-42e1-a1ad-162a3c2168e9 6256 None 367 371 1.229948e+06 None 1 dimensionless 2026-03-01 17:18:36.062412 mmc2/skeletons_swc.zip 90db0156-7aa0-42e1-a1ad-162a3c2168e9.swc <p>Let's say you are looking at a huge collection of SWC files and you only want to sample a few of them:</p> <p>Load only the first 10 skeletons</p> <pre><code>sample = navis.read_swc(\n    './mmc2/skeletons_swc.zip',\n    include_subdirs=True,\n    limit=10\n)\nsample\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 11 neurons (1.1MiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron af245389-3c04-4a9c-983e-a0cc95546942 af245389-3c04-4a9c-983e-a0cc95546942 11634 None 600 611 2.899338e+06 None 1 dimensionless 2026-03-01 17:18:36.299553 mmc2/skeletons_swc.zip af245389-3c04-4a9c-983e-a0cc95546942.swc 1 navis.TreeNeuron a6f13d47-997e-4934-8f1c-e6fd54ab7241 a6f13d47-997e-4934-8f1c-e6fd54ab7241 1670 None 24 25 6.282917e+05 None 1 dimensionless 2026-03-01 17:18:36.306363 mmc2/skeletons_swc.zip a6f13d47-997e-4934-8f1c-e6fd54ab7241.swc ... ... ... ... ... ... ... ... ... ... ... ... ... ... 9 navis.TreeNeuron f2c2d64e-531b-4df8-94a9-52bdefa365b5 f2c2d64e-531b-4df8-94a9-52bdefa365b5 2694 None 112 115 8.098384e+05 None 1 dimensionless 2026-03-01 17:18:36.375418 mmc2/skeletons_swc.zip f2c2d64e-531b-4df8-94a9-52bdefa365b5.swc 10 navis.TreeNeuron 9f0f8e35-dac0-46d5-9947-de46871fe4ea 9f0f8e35-dac0-46d5-9947-de46871fe4ea 0 None 0 0 0.000000e+00 None 1 dimensionless 2026-03-01 17:18:36.380050 mmc2/skeletons_swc.zip 9f0f8e35-dac0-46d5-9947-de46871fe4ea.swc <p>We can also point <code>navis.read_swc()</code> at single files instead of folders or zip archives:</p> <p>For this I extraced the skeletons_swc.zip archive</p> <pre><code>s = navis.read_swc('./mmc2/swc/CENT/11519759.swc')\ns\n</code></pre> type navis.TreeNeuron name 11519759 n_nodes 14782 n_connectors None n_branches 663 n_leafs 673 cable_length 2714858.5 soma None units 1 dimensionless created_at 2026-03-01 17:18:36.410018 origin mmc2/swc/CENT/11519759.swc file 11519759.swc <p>You can even use URLs or FTP servers directly:</p> <pre><code># From URL:\ns = navis.read_swc('https://v2.virtualflybrain.org/data/VFB/i/jrch/jup2/VFB_00101567/volume.swc')\n</code></pre> <pre><code># From an FTP folder:\nnl = navis.read_swc('ftp://download.brainimagelibrary.org/biccn/zeng/pseq/morph/200526/', limit=3)\n\n\n# !!! tip\n#     [`read_swc`][navis.read_swc] is super flexible and can handle a variety of inputs (file names, folders, archives, URLs, etc.).\n#     Importantly, it also let you customize which/how neurons are loaded. For example:\n#      - the `limit` parameter can also be used to load only files matching a given pattern\n#      - the `fmt` parameter lets you specify how to parse filenames into neuron names and ids\n#     Many of the other `navis.read_*` functions share these features!\n</code></pre>"},{"location":"generated/gallery/0_io/tutorial_io_00_skeletons/#to-swc-files","title":"To SWC files","text":"<p>Now let's say you have skeletons and you want to save them to disk. Easy!</p> <pre><code># Write a single neuron:\nnavis.write_swc(s, './mmc2/my_neuron.swc')\n</code></pre> <pre><code># Write a whole list of skeletons to a folder and use the neurons' `name` property as filename:\nnavis.write_swc(sample, './mmc2/{neuron.name}.swc')\n</code></pre> <pre><code># Write directly to a zip file:\nnavis.write_swc(sample, './mmc2/skeletons.zip')\n</code></pre> <pre><code># Write directly to a zip file and use the neuron name as filename:\nnavis.write_swc(sample, './mmc2/{neuron.name}.swc@skeletons.zip')\n</code></pre> <p>See <code>navis.write_swc</code> for further details!</p>"},{"location":"generated/gallery/0_io/tutorial_io_00_skeletons/#from-nmx-files","title":"From NMX files","text":"<p>NMX is a xml-based format used e.g. by pyKNOSSOS to store skeletons plus meta data. NAVis supports reading (but not writing) this format. If you want to follow along download this dataset by Wanner et al. (2016). Just like the SWCs, I extracted the archive to my downloads folder:</p> <p>Read a single file</p> <pre><code>s = navis.read_nmx('./WannerAA201605_SkeletonsGlomeruli/Neuron_id0001.nmx')\ns\n</code></pre> type navis.TreeNeuron name NML id Neuron_id0001 n_nodes 3369 n_connectors None n_branches 64 n_leafs 69 cable_length 151639.15625 soma None units 1 dimensionless <p>Read all files in folder</p> <pre><code>nl = navis.read_nmx('./WannerAA201605_SkeletonsGlomeruli/')\nnl\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 1022 neurons (142.1MiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units 0 navis.TreeNeuron NML Neuron_id0506 5590 None 193 209 253986.734375 None 1 dimensionless 1 navis.TreeNeuron NML Neuron_id0004 3299 None 133 148 308490.062500 None 1 dimensionless ... ... ... ... ... ... ... ... ... ... ... 1020 navis.TreeNeuron NML Neuron_id1244 4310 None 114 119 403854.562500 None 1 dimensionless 1021 navis.TreeNeuron NML Neuron_id0587 3968 None 112 119 263185.093750 None 1 dimensionless <pre><code>navis.plot2d(nl[:10], method='2d', radius=False)\n</code></pre> <p></p> <p>Out:</p> <pre><code>Plot neurons:   0%|          | 0/10 [00:00&lt;?, ?it/s]\n\nPlot neurons: 100%|##########| 10/10 [00:00&lt;00:00, 83.56it/s]\n\n\n(&lt;Figure size 640x480 with 1 Axes&gt;, &lt;Axes: xlabel='x', ylabel='y'&gt;)\n</code></pre> <p>Note</p> <p>If you encounter an error message while reading: NMX files don't always contain skeletons. If NAVis comes across one that can't be turned into a <code>navis.TreeNeuron</code>, it will skip the file and produce a warning.</p>"},{"location":"generated/gallery/0_io/tutorial_io_00_skeletons/#from-neuroglancer-precomputed","title":"From Neuroglancer Precomputed","text":"<p>Among other formats, neuroglancer supports a \"precomputed\" format for skeletons (see specs here. This binary format is more compact than uncompressed SWC files but is not used outside of neuroglancer as far as I know. That said: NAVis lets you read and write skeletons from/to precomputed format using <code>navis.read_precomputed</code> and <code>navis.write_precomputed</code>. Note that these functions work on both precomputed skeletons and meshes.</p> <p>Also check out the tutorial on reading skeletons straight from a neuroglancer source using <code>cloud-volume</code>.</p>"},{"location":"generated/gallery/0_io/tutorial_io_00_skeletons/#manual-construction","title":"Manual construction","text":"<p>What if you have some obscure data format for which NAVis does not have a read function? The data underlying a <code>navis.TreeNeuron</code> is a simple SWC table - so as long as you can produce that from your data, you can create your own skeletons.</p> <p>Here's a quick &amp; dirty example:</p> <pre><code>import pandas as pd\n\n# Create a mock SWC table for a 2-node skeleton\nswc = pd.DataFrame()\nswc['node_id'] = [0, 1]\nswc['parent_id'] = [-1, 0]   # negative indices indicate roots\nswc['x'] = [0, 1]\nswc['y'] = [0, 1]\nswc['z'] = [0, 1]\nswc['radius'] = 0\n\nswc\n</code></pre> node_id parent_id x y z radius 0 0 -1 0 0 0 0 1 1 0 1 1 1 0 <p>This SWC can now be used to construct a <code>TreeNeuron</code>:</p> <pre><code>s = navis.TreeNeuron(swc, name='my_neuron', units='microns')\ns\n</code></pre> type navis.TreeNeuron name my_neuron n_nodes 2 n_connectors None n_branches 0 n_leafs 1 cable_length 1.732051 soma None units 1 micrometer <p>There are a few other ways to construct a <code>navis.TreeNeuron</code> (e.g. using a graph) - see the docstring for details.</p> <p>Also note that all NAVis neurons can be stored to disk using <code>pickle</code> - see the pickling tutorial.</p> <p>Hopefully the above has given you some entry points on how to load your data. See also the I/O API reference.</p> <p>Please also keep in mind that you can also convert one neuron type into another - for example by skeletonizing <code>MeshNeurons</code> (see also the API reference on neuron conversion).</p> <p>Total running time of the script: ( 1 minutes  0.872 seconds)</p> <p> Download Python source code: tutorial_io_00_skeletons.py</p> <p> Download Jupyter notebook: tutorial_io_00_skeletons.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/0_io/tutorial_io_01_meshes/","title":"Meshes","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/0_io/tutorial_io_01_meshes/#meshes","title":"Meshes","text":"<p>This tutorial will teach you how to load and save meshes.</p> <p>NAVis knows two types of meshes:</p> <ol> <li><code>navis.MeshNeuron</code> for neurons</li> <li><code>navis.Volume</code> for meshes that are not neurons, e.g. neuropil or brain meshes</li> </ol> <p>Both of these are subclasses of <code>trimesh.Trimesh</code> and can be used as such.</p> <p>Note</p> <p>NAVis has dedicated interfaces for loading meshes from remote data sources (e.g. the MICrONS, neuromorpho, Virtual Fly Brain or Janelia hemibrain datasets). These are covered in separate tutorials.</p>"},{"location":"generated/gallery/0_io/tutorial_io_01_meshes/#from-files","title":"From files","text":"<p>For reading run-of-the-mill files containing meshes, NAVis provides a single function: <code>navis.read_mesh</code>. Under the hood, that function uses <code>trimesh.load_mesh</code> which supports most of the common formats (<code>.obj</code>, <code>.ply</code>, <code>.stl</code>, etc.).</p> <pre><code>import navis\n</code></pre> <pre><code># Load an example file (here a FlyWire neuron I downloaded and saved locally)\nmesh = navis.read_mesh('test_neuron.stl')\n</code></pre> <p>The interface is similar to <code>navis.read_swc</code> in that you can point <code>navis.read_mesh</code> at single file or at folders with multiple files:</p> <pre><code> # When reading all files in folder you have to specificy the file extension (e.g. *.stl)\n meshes = navis.read_mesh('neurons/*.stl')\n</code></pre> <p>By default, <code>navis.read_mesh</code> will return neurons. Use the <code>output</code> parameter to get a <code>navis.Volume</code> or a <code>trimesh.Trimesh</code> instead:</p> <pre><code># Load a mesh file into a Volume\nvol = navis.read_mesh('test_mesh.stl', output='volume')\n</code></pre>"},{"location":"generated/gallery/0_io/tutorial_io_01_meshes/#manual-construction","title":"Manual construction","text":"<p>It's super easy to construct <code>navis.MeshNeuron</code> or <code>navis.Volume</code> from scratch - they are just vertices and faces after all.</p> <p>So if e.g. your mesh file format is not covered by <code>navis.read_mesh</code> or you created the mesh yourself (e.g. using a marching cube algorithm), just create the objects yourself:</p> <pre><code>import numpy as np\n\n# Create some mock vertices\nvertices = np.array([[1, 0, 0],\n                     [0, 1, 0],\n                     [0, 0, 1]])\n# Make a single triangular face using the vertex indices\nfaces = np.array([[0, 1, 2]])\n</code></pre> <p>Turn into MeshNeuron</p> <pre><code>m = navis.MeshNeuron((vertices, faces), name='my_mesh', id=1, units='microns')\nm\n</code></pre> type navis.MeshNeuron name my_mesh id 1 units 1 micrometer n_vertices 3 n_faces 1 <pre><code>navis.plot3d(m)\n</code></pre> <p>Turn into Volume</p> <pre><code>vol = navis.Volume(vertices, faces, name='my_volume')\nvol\n</code></pre> <p>Out:</p> <pre><code>&lt;navis.Volume(name=my_volume, units=1 dimensionless, color=(0.85, 0.85, 0.85, 0.2), vertices.shape=(3, 3), faces.shape=(1, 3))&gt;\n</code></pre>"},{"location":"generated/gallery/0_io/tutorial_io_01_meshes/#to-files","title":"To files","text":"<p>For saving <code>navis.MeshNeurons</code> or <code>navis.Volumes</code> to disk, use <code>navis.write_mesh</code>.</p> <p>Save single neuron to file: </p><pre><code>m = navis.example_neurons(1, kind='mesh')\nnavis.write_mesh(m, '~/Downloads/neuron.obj')\n</code></pre><p></p> <p>Save a bunch of neurons to mesh: </p><pre><code>nl = navis.example_neurons(3, kind='mesh')\nnavis.write_mesh(nl, '~/Downloads/', filetype='obj')\n</code></pre><p></p> <p>By default, <code>navis.write_mesh</code> will write multiple neurons to files named <code>{neuron.id}.obj</code>. You can change this behavior by specifying the format in the filename: </p><pre><code># Use the neuron name instead of the id\nnavis.write_mesh(nl, '~/Downloads/{neuron.name}.obj')\n</code></pre><p></p> <p>Important</p> <p>One thing to keep in mind here is that NAVis only works with triangular faces, i.e. no quads or polygons! Please see the documentation of <code>navis.MeshNeuron</code> and <code>navis.Volume</code> for details.</p> <p>This tutorial has hopefully given you some entry points on how to load your data. See also the I/O API reference. Also note that all NAVis neurons can be stored to disk using <code>pickle</code> - see the pickling tutorial.</p> <p>Please also keep in mind that you can also convert one neuron type into another - for example by skeletonizing <code>MeshNeurons</code> (see also the API reference on neuron conversion).</p> <p>Total running time of the script: ( 0 minutes  0.018 seconds)</p> <p> Download Python source code: tutorial_io_01_meshes.py</p> <p> Download Jupyter notebook: tutorial_io_01_meshes.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/0_io/tutorial_io_02_dotprops/","title":"Dotprops","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/0_io/tutorial_io_02_dotprops/#dotprops","title":"Dotprops","text":"<p>This tutorial will show you how to load/create Dotprops.</p> <p><code>navis.Dotprops</code> are point clouds with associated principal vectors which are mostly used for NBLASTing. They are typically derivatives of skeletons or meshes but you can load them straight from e.g. confocal image stacks using <code>navis.read_nrrd</code> or <code>navis.read_tiff</code>.</p> <p></p> <pre><code>import navis\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"generated/gallery/0_io/tutorial_io_02_dotprops/#from-image-data","title":"From image data","text":"<p>For this example we will use a stack from Janelia's split Gal4 collection. This <code>LH2094</code> line is also available from Virtual Fly Brain where, conveniently, they can be downloaded in NRRD format which we can directly read into NAVis.</p> <p>Let's do this step-by-step first:</p> <pre><code># Load raw NRRD image\nim, header = navis.read_nrrd(\n    \"https://v2.virtualflybrain.org/data/VFB/i/0010/2926/VFB_00101567/volume.nrrd\",\n    output=\"raw\"\n)\n\n# Plot a maximum projection\nmax_proj = im.max(axis=2)\nplt.imshow(\n    max_proj.T,\n    extent=(0, int(0.5189 * 1210), (0.5189 * 566), 0),  # extent is calculated from the spacing (see `header`) times the no of x/y pixels\n    cmap='Greys_r',\n    vmax=10  # make it really bright so we can see neurons + outline of the brain\n    )\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;matplotlib.image.AxesImage object at 0x7f41a1420c50&gt;\n</code></pre> <p>At this point we could threshold the image, extract above-threshold voxels and convert them to a Dotprops object. However, the easier option is to use <code>navis.read_nrrd</code> with the <code>output=\"dotprops\"</code> parameter:</p> <pre><code>dp = navis.read_nrrd(\n    \"https://v2.virtualflybrain.org/data/VFB/i/0010/2926/VFB_00101567/volume.nrrd\",\n    output=\"dotprops\",\n    threshold=5,  # threshold to determine which voxels are used for the dotprops\n    thin=True,   # see note below on this parameter!\n    k=10  # number of neighbours to consider when calculating the tangent vector\n)\n</code></pre> <p>Thinning</p> <p>In the above <code>read_nrrd</code> call we used <code>thin=True</code>. This is a post-processing step that thins the image to a single pixel width. This will produce \"cleaner\" dotprops but can also remove denser neurites thus emphasizing the backbone of the neuron. This option requires the <code>scikit-image</code> package:</p> <pre><code>pip install scikit-image\n</code></pre> <p>Let's overlay the dotprops on the maximum projection:</p> <pre><code>fig, ax = plt.subplots()\nax.imshow(\n    max_proj.T,\n    extent=(0, int(0.5189 * 1210), (0.5189 * 566), 0),\n    cmap='Greys_r',\n    vmax=10\n    )\nnavis.plot2d(dp, ax=ax, view=(\"x\", \"-y\"), method=\"2d\", color=\"r\", linewidth=1.5)\n</code></pre> <p></p> <p>Out:</p> <pre><code>(&lt;Figure size 640x480 with 1 Axes&gt;, &lt;Axes: xlabel='x', ylabel='y'&gt;)\n</code></pre> <p>This looks pretty good but we have a bit of little fluff around the brain which we may want to get rid off:</p> <pre><code># Drop everything but the two largest connected components\ndp = navis.drop_fluff(dp, n_largest=2)\n\n# Plot again\nfig, ax = plt.subplots()\nax.imshow(\n    max_proj.T,\n    extent=(0, int(0.5189 * 1210), (0.5189 * 566), 0),\n    cmap='Greys_r',\n    vmax=10\n    )\nnavis.plot2d(dp, ax=ax, view=(\"x\", \"-y\"), method=\"2d\", color=\"r\", linewidth=1.5)\n</code></pre> <p></p> <p>Out:</p> <pre><code>(&lt;Figure size 640x480 with 1 Axes&gt;, &lt;Axes: xlabel='x', ylabel='y'&gt;)\n</code></pre> <p>Note</p> <p>To extract the connected components, <code>navis.drop_fluff</code> treats all pairs of points within a certain distance as connected. The distance is determined by the <code>dp_dist</code> parameter which defaults to 5 x the average distance between points. That is a good value for this example but you may need adjust it for your data.</p>"},{"location":"generated/gallery/0_io/tutorial_io_02_dotprops/#from-other-neurons","title":"From other neurons","text":"<p>Let's say you have a bunch of skeletons and you need to convert them to dotprops for NBLAST. For that you <code>navis.make_dotprops</code>:</p> <pre><code>sk = navis.example_neurons(3, kind=\"skeleton\")\ndp = navis.make_dotprops(sk, k=5)\n\n# Plot one of the dotprops\nfig, ax = navis.plot2d(dp[0], view=(\"x\", \"-z\"), method=\"2d\", color=\"red\")\n\n# Add a zoom-in\naxins = ax.inset_axes([0.03, 0.03, 0.47, 0.47], xticklabels=[], yticklabels=[])\n_ = navis.plot2d(dp[0], view=(\"x\", \"-z\"), method=\"2d\", color=\"red\", ax=axins)\naxins.set_xlim(17e3, 19e3)\naxins.set_ylim(15e3, 13e3)\nax.indicate_inset_zoom(axins, edgecolor=\"black\")\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;matplotlib.inset.InsetIndicator object at 0x7f41bed392d0&gt;\n</code></pre> <p>Note</p> <p>The <code>k</code> parameter in <code>make_dotprops</code> determines how many neighbours are considered to generated the tangent vector for a given point. Higher <code>k</code> = smoother. Lower <code>k</code> = more detailed but also more noisy. If you have clean data such as these connectome-derived skeletons, you can go with a low <code>k</code>. For confocal data, you might want to go with a higher <code>k</code> (e.g. 20) to smooth out the noise. You can pass <code>k</code> to <code>navis.read_nrrd</code> as well.</p>"},{"location":"generated/gallery/0_io/tutorial_io_02_dotprops/#manual-construction","title":"Manual construction","text":"<p>If not loaded from file, you would typically create <code>Dotprops</code> via <code>navis.make_dotprops</code> but just like all other neuron types, <code>Dotprops</code> can be constructed manually:</p> <pre><code>import numpy as np\n\n# Create some x/y/z coordinates\npoints = np.array([[0, 0, 0], [1, 1, 1], [2, 2, 2]])\n\n# Create vectors for each point\n# You can skip this point and just provide the `k` parameter\nvect = np.array([[1, 0, 0], [0, 1, 0], [0, 1, 0]])\n\ndp = navis.Dotprops(points, k=None, vect=vect)\ndp\n</code></pre> type navis.Dotprops name None k None units 1 dimensionless n_points 3 <p>There is no established format to store dotprops. But like all other neuron types in navis, you can pickle data for later (re)use - see the pickling tutorial. See also the I/O API reference.</p> <p>Total running time of the script: ( 0 minutes  13.385 seconds)</p> <p> Download Python source code: tutorial_io_02_dotprops.py</p> <p> Download Jupyter notebook: tutorial_io_02_dotprops.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/0_io/tutorial_io_04_pickle/","title":"Pickling","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/0_io/tutorial_io_04_pickle/#pickling","title":"Pickling","text":"<p>This tutorial shows how to use Python's <code>pickle</code> module to quickly store and load neurons.</p> <p>All neuron types including whole <code>NeuronLists</code> can be \"pickled\" . If you don't know what that is: pickling is storing the actual Python object as a bytes stream. This is incredibly fast and works very well for short-term storage but has a few downsides:</p> <ol> <li>Pickle files can only be re-opened in Python</li> <li>The pickled object is (sort of) specific to your current Python environment. If    you e.g. update Python, NAVis or even just <code>numpy</code> or <code>pandas</code> you may not    be able to open the file again.</li> <li>Pickle files can contain arbitrary Python code. Never open a pickle file from    an untrusted source!</li> </ol> <p>With that out of the way, pickling is incredibly easy:</p> <pre><code>import navis\nimport pickle\n\n# Load some example neurons\nnl = navis.example_neurons(3, kind='mesh')\n\n# Pickle neurons to file\nwith open('/Users/philipps/Downloads//meshes.pkl', 'wb') as f:\n    pickle.dump(nl, f)\n</code></pre> <p>To \"unpickle\", i.e. read the file back into Python:</p> <pre><code># Read neurons back from pickle file\nwith open('/Users/philipps/Downloads//meshes.pkl', 'rb') as f:\n    nl = pickle.load(f)\n</code></pre> <p>This tutorial has hopefully given you some entry points on how to load your data. See also the I/O API reference.</p> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: tutorial_io_04_pickle.py</p> <p> Download Jupyter notebook: tutorial_io_04_pickle.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/0_io/zzz_tutorial_io_05_skeletonize/","title":"Skeletons from light-level data","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/0_io/zzz_tutorial_io_05_skeletonize/#skeletons-from-light-level-data","title":"Skeletons from light-level data","text":"<p>This tutorial will show you how to extract skeletons from confocal microscopy stacks.</p> <p>This example is not executed</p> <p>In contrast to almost all other tutorials, this one is not executed when the documentation is built. Consequently, it also does not display any actual code output or plots - images shown are statically embedded. The main reason for this is that the example requires downloading a large-ish file which is a pain in the neck to get to work in the CI environment.</p> <p>Extracting neuron skeletons from microscopy data is a common but non-trivial task. There are about as many ways to do this as there are people doing it - from fully manual to fully automated tracing.</p> <p>In this tutorial, we will show you a fully automated way using a number of easy-to-install Python packages. If this isn't for you, check out the Alternatives section at the end of this tutorial.</p>"},{"location":"generated/gallery/0_io/zzz_tutorial_io_05_skeletonize/#requirements","title":"Requirements:","text":"<p>Please make sure you have the following packages installed:</p> <ul> <li><code>pynrrd</code> to load image stacks   <pre><code>pip install pynrrd -U\n</code></pre></li> <li><code>connected-components-3d</code> (cc3d) to label connected components   <pre><code>pip install connected-components-3d -U\n</code></pre></li> <li><code>kimimaro</code> to extract the skeletons   <pre><code>pip install kimimaro -U\n</code></pre></li> </ul>"},{"location":"generated/gallery/0_io/zzz_tutorial_io_05_skeletonize/#preparing-the-data","title":"Preparing the data","text":"<p>The pipeline we're using here was designed for pre-segmented data, so there is little in the way of dealing with noisy data. Fortunately, the image stack we will use is exceptionally clean which makes the skeletonization process very straightforward.</p> <p>In practice, you may have to do some pre-processing to clean up your data before running the skeletonization. If your run-of-the-mill thresholding, denoising, etc. doesn't cut it, you can also try more advanced segmentation techniques.</p> <p>There are various fairly easy-to-use tools available for this, e.g. Ilastik (see the pixel classification and voxel segmentation tutorials) or DeepImageJ.</p>"},{"location":"generated/gallery/0_io/zzz_tutorial_io_05_skeletonize/#download-image-stack","title":"Download Image Stack","text":"<p>As example data, we will use a confocal stack from the Janelia Split-Gal4 collection. We picked the SS00731 line because it's already fairly clean as is and there are high-resolution stacks with stochastic multi-color labeling of individual neurons available for download.</p> <p>Scroll all the way to the bottom of the page and in the dropdown for the left-most image, select \"Download H5J stack: Unaligned\".</p> <p></p>"},{"location":"generated/gallery/0_io/zzz_tutorial_io_05_skeletonize/#convert-to-nrrd","title":"Convert to NRRD","text":"<p>Next, we need to open this file in Fiji/ImageJ to convert it to a format we can work with in Python:</p> <ol> <li>Fire up Fiji/ImageJ</li> <li>Drag &amp; drop the <code>SS00731-20140620_20_C5-f-63x-ventral-Split_GAL4-unaligned_stack.h5j</code> file into Fiji</li> <li>Go to \"Image\" -&gt; \"Colors\" -&gt; \"Split Channels\" to split the image into the channels</li> <li>Discard all but the red \"C1\" channel with our neurons</li> <li>Go to \"Image\" -&gt; \"Type\" -&gt; \"8-bit\" to convert the image to 8-bit (optional but recommended)</li> <li>Save via \"File\" -&gt; \"Save As\" -&gt; \"NRRD\" and save the file as <code>neuron.nrrd</code></li> </ol> <p></p>"},{"location":"generated/gallery/0_io/zzz_tutorial_io_05_skeletonize/#extracting-the-skeleton","title":"Extracting the Skeleton","text":"<p>Now that we have that file in a format we can load it into Python, we can get started:</p> <pre><code>import kimimaro\nimport nrrd\nimport navis\nimport cc3d\nimport numpy as np\n</code></pre> <p>First load the image stack:</p> <pre><code># `im` is numpy array, `header` is a dictionary\nim, header = nrrd.read(\n    \"neuron.nrrd\"\n)\n</code></pre> <p>Next, we need to find some sensible threshold to binarize the image. This is not strictly necessary (see the further note down) but at least for starters this more intuitive.</p> <pre><code># Threshold the image\nmask = (im &gt;= 20).astype(np.uint8)\n</code></pre> <p>You can inspect the mask to see if the thresholding worked as expected: </p><pre><code>import matplotlib.pyplot as plt\nplt.imshow(mask.max(axis=2))\n</code></pre><p></p> <p>With the <code>octarine</code> backend, you can also visualize the volume in 3D: </p><pre><code># spacing can be found in the `header` dictionary\nimport octarine as oc\nv = oc.Viewer()\nv.add_volume(mask, spacing=(.19, .19, .38))\n</code></pre><p></p> <p></p> <p>A couple notes on the thresholding:</p> <ul> <li>feel free to test the thresholding in e.g. ImageJ/Fiji</li> <li>remove as much background as possible without disconnecting neurites</li> <li>perfection is the enemy of progress: we can denoise/reconnect during postprocessing</li> </ul> <p>Next, we we need to label the connected components in the image:</p> <p>Extract the labels</p> <pre><code>labels, N = cc3d.connected_components(mask, return_N=True)\n</code></pre> <p>Visualize the labels: </p><pre><code>import cmap\nimport octarine as oc\nv = oc.Viewer()\nv.add_volume(labels, spacing=(.19, .19, .38), color=cmap.Colormap('prism'))\n</code></pre><p></p> <p></p> <p>Experiment</p> <p><code>cc3d.connected_component</code> also works with non-thresholded image - see the <code>delta</code> parameter.</p> <pre><code># Collect some statistics\nstats = cc3d.statistics(labels)\n\nprint(\"Total no. of labeled componenents:\", N)\nprint(\"Per-label voxel counts:\", np.sort(stats[\"voxel_counts\"])[::-1])\nprint(\"Label IDs:\", np.argsort(stats[\"voxel_counts\"])[::-1])\n</code></pre> <pre><code>Total no. of labeled componenents: 37836\nPer-label voxel counts: [491996140    527374    207632 ...         1         1         1]\nLabel IDs: [    0  6423  6091 ... 22350 22351 18918]\n</code></pre> <p>Note how label <code>0</code> has suspiciously many voxels? That's because this is the background label. We need to make sure to exlude it from the skeletonization process:</p> <pre><code>to_skeletonize = np.arange(1, N)\n</code></pre> <p>Now we can run the actual skeletonization!</p> <p>Skeletonization paramters</p> <p>There are a number of parameters that are worth explaining first because you might want to tweak them for your data:</p> <ul> <li><code>scale</code> &amp; <code>const</code>: control how detailed your skeleton will be: lower = more detailed but more noise</li> <li><code>anisotropy</code>: controls the voxel size - see the <code>header</code> dictionary for the voxel size of our image</li> <li><code>dust_threshold</code>: controls how small connected components are skipped</li> <li><code>object_ids</code>:  a list of labels to process (remember that we skipped the background label)</li> <li><code>max_path</code>: if this is set, the algorithm will only process N paths in each skeleton - you can use   this to finish early (e.g. for testing)</li> </ul> <p>See the <code>kimimaro</code> repository for a detailed explanation of the parameters!</p> <pre><code>skels = kimimaro.skeletonize(\n    labels,\n    teasar_params={\n        \"scale\": 1.5,\n        \"const\": 1,  # physical units (1 micron in our case)\n        \"pdrf_scale\": 100000,\n        \"pdrf_exponent\": 4,\n        \"soma_acceptance_threshold\": 3.5,  # physical units\n        \"soma_detection_threshold\": 1,  # physical units\n        \"soma_invalidation_const\": 0.5,  # physical units\n        \"soma_invalidation_scale\": 2,\n        \"max_paths\": None,  # default None\n    },\n    object_ids=list(to_skeletonize), # process only the specified labels\n    dust_threshold=500,  # skip connected components with fewer than this many voxels\n    anisotropy=(0.19, .19, 0.38),  # voxel size in physical units\n    progress=True,  # show progress bar\n    parallel=6,  # &lt;= 0 all cpu, 1 single process, 2+ multiprocess\n    parallel_chunk_size=1,  # how many skeletons to process before updating progress bar\n)\n</code></pre> <p><code>skels</code> is a dictionary of <code>{label: cloudvolume.Skeleton}</code>. Let's convert these to NAVis neurons:</p> <pre><code># Convert skeletons to NAVis neurons\nnl = navis.NeuronList([navis.read_swc(s.to_swc(), id=i) for i, s in skels.items()])\n</code></pre> <p>Based on the voxel sizes in <code>stats</code>, we can make an educated guess that label <code>6423</code> is one of our neurons. Let's visualize it in 3D:</p> <pre><code>import octarine as oc\nv = oc.Viewer()\nv.add_neurons(nl.idx[6423], color='r', linewidth=2, radius=False))\nv.add_volume(im, spacing=(.19, .19, .38), opacity=.5)\n</code></pre> <p></p> <p>This looks pretty good off the bat! Now obviously we will have the other large neuron (label <code>6091</code>) plus bunch of smaller skeletons in our NeuronList. Let's have a look at those as well:</p> <p></p> <p>Zooming in on <code>6091</code> you will see that it wasn't fully skeletonized: some of the branches are missing and others are disconnected. That's either because our threshold for the mask was too high (this neuron had a weaker signal than the other) and/or we dropped too many fragments during the skeletonization process (see the <code>dust_threshold</code> parameter).</p> <p></p>"},{"location":"generated/gallery/0_io/zzz_tutorial_io_05_skeletonize/#alternatives","title":"Alternatives","text":"<p>If the pipeline described in this tutorial does not work for you, there are a number of alternatives:</p> <ol> <li>Simple Neurite Tracer is a popular ImageJ plugin for semi-automated tracing</li> <li>Folks at the Allen Institute for Brain Science have published a protocol for reconstructing neurons</li> <li>NeuTube is an open-source software for reconstructing neurongs from fluorescence microscopy images</li> </ol>"},{"location":"generated/gallery/0_io/zzz_tutorial_io_05_skeletonize/#acknowledgements","title":"Acknowledgements","text":"<p>The packages we used here were written by the excellent Will Silversmith from the Seung lab in Princeton. The image stack we processed is from the Janelia Split-Gal4 collection and was published as part of the Cheong, Eichler, Stuerner, et al. (2024) paper.</p> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: zzz_tutorial_io_05_skeletonize.py</p> <p> Download Jupyter notebook: zzz_tutorial_io_05_skeletonize.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/1_plotting/mg_execution_times/","title":"Computation times","text":""},{"location":"generated/gallery/1_plotting/mg_execution_times/#computation-times","title":"Computation times","text":"<p>02:47.066 total execution time for generated_gallery_1_plotting files:</p> <p>+------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_plotting_00_intro (docs/examples/1_plotting/tutorial_plotting_00_intro.py)             | 01:35.716 | 0.0 MB | +------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_plotting_03_dend (docs/examples/1_plotting/tutorial_plotting_03_dend.py)                | 00:36.230 | 0.0 MB | +------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_plotting_06_cortex (docs/examples/1_plotting/tutorial_plotting_06_cortex.py)          | 00:15.965 | 0.0 MB | +------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_plotting_01_colors (docs/examples/1_plotting/tutorial_plotting_01_colors.py)          | 00:10.963 | 0.0 MB | +------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_plotting_07_xkcd (docs/examples/1_plotting/tutorial_plotting_07_xkcd.py)                | 00:02.608 | 0.0 MB | +------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_plotting_04_skeletons (docs/examples/1_plotting/tutorial_plotting_04_skeletons.py) | 00:02.595 | 0.0 MB | +------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_plotting_02_1d (docs/examples/1_plotting/tutorial_plotting_02_1d.py)                      | 00:02.485 | 0.0 MB | +------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_plotting_05_depth (docs/examples/1_plotting/tutorial_plotting_05_depth.py)             | 00:00.505 | 0.0 MB | +------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_00_intro/","title":"Plotting Overview","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_00_intro/#plotting-overview","title":"Plotting Overview","text":"<p>This tutorial gives an overview of the plotting capabilities of NAVis. We will cover 2D and 3D plotting with various backends and their pro's and con's.</p> <p>NAVis contains functions for (static) 2D and (interactive) 3D plotting. These functions can use various different backends for plotting. For 2D plots we use <code>matplotlib</code> and for 3D plots we use either <code>octarine</code>, <code>vispy</code>, <code>plotly</code> or <code>k3d</code>.</p> <p>Which plotting method (2D/3D) and which backend (octarine, plotly, etc.) to use depends on what you are after (e.g. static, publication quality figures vs interactive data exploration) and your environment (e.g. Jupyter/VS code or terminal). Here's a quick summary:</p> Backend Used in Pros Cons matplotlib <code>navis.plot2d</code><code>navis.plot1d</code><code>navis.plot_flat</code> - high quality (vector graphics!)- works in Jupyter and terminal- exports to vector graphics- myriads of ways to adjust plots - struggles with correct depth layering in complex scenes- not very interactive (although you can adjust perspective) - slow for large scenes- not good for voxel data (e.g. image volumes) octarine <code>navis.plot3d</code> - blazingly fast thanks to WGPU backend- works in terminal and Jupyter- very interactive - may not work on older systems (use <code>vispy</code> instead)- not persistent (i.e. dies with notebook kernel)- can't share interactive plot (screenshots only) vispy <code>navis.plot3d</code> - very interactive- good render quality and performance - can't share interactive plot (screenshots only)- not persistent (i.e. dies with notebook kernel)- deprecated in favor of octarine plotly <code>navis.plot3d</code> - works \"inline\" for Jupyter environments- persistent (i.e. plots get saved alongside notebook)- can produce offline HTML plots for sharing - not very fast for large scenes- large file sizes (i.e. makes for large <code>.ipynb</code> notebook files)- horrendous for voxel data (i.e. images) k3d <code>navis.plot3d</code> - works \"inline\" for Jupyter environments- super fast and performant- in memory (i.e. does not increase notebook file size) - does not work in terminal sessions- not persistent (i.e. dies with notebook kernel)- can't share interactive plot (screenshots only) <p>In theory there is feature parity across backends but due to built-in limitations there are minor differences.</p> <p>If you installed NAVis via <code>pip install navis[all]</code> all of the above backends should be available to you. If you ran a minimal install via <code>pip install navis</code> you may need to install the backends separately - NAVis will complain if you try to use a backend that is not installed!</p> <p>Note</p> <p>The plots in this tutorial are optimized for light-mode. If you are using dark-mode, you may have trouble seeing e.g. axis or labels.</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_00_intro/#2d-plots","title":"2D plots","text":"<p>This uses <code>matplotlib</code> to generate static 2D plots. The big advantage is that you can save these plots as vector graphics. Unfortunately, matplotlib's capabilities regarding 3D data are limited. The main problem is that depth (z) is only simulated by trying to layer objects (lines, vertices, etc.) according to their z-order rather than doing proper rendering which is why you might see some neurons being plotted in front of others even though they are actually behind them. It's still great for plotting individual neurons or small groups thereof!</p> <p>Let's demonstrate with a simple example using the default settings:</p> <pre><code>import navis\nimport matplotlib.pyplot as plt\n\nnl = navis.example_neurons(kind=\"skeleton\")\n\n# Plot using default settings\nfig, ax = navis.plot2d(nl, view=(\"x\", \"-z\"), method=\"2d\")\nplt.tight_layout()\n</code></pre> <p></p> <p>Note</p> <p>We set <code>view(\"x\", \"-z\")</code> above to get a frontal view of the example neurons. You may need to adjust this depending on the orientation of your neurons.</p> <p>Above plot used the default <code>matplotlib</code> 2D plot. You might notice that the plot looks rather \"flat\" - i.e. neurons seem to be layered on top of each other without intertwining. That is one of the limitations of <code>matplotlib</code>'s 3d backend. We can try to ameliorate this by adjust the <code>method</code> parameter:</p> <pre><code># Plot settings for more complex scenes - comes at a small performance cut though\nfig, ax = navis.plot2d(nl, method=\"3d_complex\", view=(\"x\", \"-z\"))\nplt.tight_layout()\n</code></pre> <p></p> <p>Looks better now, doesn't it? Now what if we wanted to adjust the perspective? For 3d axes, <code>matplotlib</code> lets us adjust the viewing angle by setting the <code>elev</code>, <code>azim</code> and <code>roll</code> attributes. See also this official explanation.</p> <p>Let's give that a shot:</p> <p>Plot again</p> <pre><code>fig, ax = nl.plot2d(\n    method=\"3d_complex\", view=(\"x\", \"-z\"), non_view_axes3d=\"show\", radius=True\n)\n\n# Change view to see the neurons from a different angle\nax.elev = -20\nax.azim = 45\nax.roll = 180\n\nplt.tight_layout()\n</code></pre> <p></p> <p>Note</p> <p>Did you note that we set <code>non_view_axes3d='show'</code> in above example? By default, NAVis hides the axis that is parallel to the viewing direction is hidden to not clutter the image. Because we were going to change the perspective, we set it to <code>show</code>. FYI: if the plot is rendered in a separate window (e.g. if you run Python from terminal), you can change the perspective by dragging the image.</p> <p>We can use this to generate small animations:</p> <pre><code># Render 3D rotation\nfor i in range(0, 360, 10):\n   # Change rotation\n   ax.azim = i\n   # Save each incremental rotation as frame\n   plt.savefig('frame_{0}.png'.format(i), dpi=200)\n</code></pre> <p></p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_00_intro/#3d-plots","title":"3D plots","text":"<p>By \"3D plots\" we typically mean interactive 3D plots as opposed to the (mostly) static 2D or semi-3D plots above. 3D plots are great for great for exploring your data interactively but you can also use them to generate high-quality static images.</p> <p>As laid out at the top of this page: for 3D plots, we are using either octarine, vispy, plotly or k3d. In brief:</p> backend Jupyter Terminal octarine yes yes plotly yes yes but only via export to html vispy (depcrecated) yes yes k3d yes no <p>By default, the choice is automatic and depends on (1) what backends are installed and (2) the context:</p> <ul> <li>from IPython/Terminal/scripts: octarine  vispy  plotly</li> <li>from Jupyter Lab/Notebook: plotly  octarine  k3d</li> </ul> <p>You can always force a specific backend using the <code>backend</code> parameter in <code>navis.plot3d</code>:</p> Automatic (default)OctarinePlotlyVispyk3d <pre><code>n = navis.example_neurons()\nnavis.plot3d(n)\n</code></pre> <pre><code>n = navis.example_neurons()\nnavis.plot3d(n, backend='octarine')\n</code></pre> <pre><code>n = navis.example_neurons()\nnavis.plot3d(n, backend='plotly')\n</code></pre> <pre><code>n = navis.example_neurons()\nnavis.plot3d(n, backend='vispy')\n</code></pre> <pre><code>n = navis.example_neurons()\nnavis.plot3d(n, backend='k3d')\n</code></pre> <p>Alternatively, you can also set the default backend using an environment variable. For example: </p><pre><code>export NAVIS_PLOT3D_BACKEND=\"octarine\"\n</code></pre><p></p> <p>Google Collaboratory</p> <p>The <code>jupyter_rfb</code> used by Octarine and Vispy to render 3D plots in Jupyter does not work in Google Collaboratory. If you are using Google Collaboratory, we recommend you use the plotly backend.</p> <p>With that out of the way, let's have a look at some 3D plots! You will notice that for the <code>octarine</code>, <code>vispy</code> and <code>k3d</code> backends we're just showing screenshots - that's because their interactive plots can't be embedded into this documentation.</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_00_intro/#octarinevispy","title":"Octarine/Vispy","text":"<p>Octarine and Vispy are pretty similar in that they both work via a <code>Viewer</code> object which allows you to interactively add/remove objects, change colors, etc. The main difference is that Octarine uses modern WGPU instead of OpenGL which makes it much faster than Vispy:</p> OctarineVispy <p></p><pre><code>nl = navis.example_neurons()\nviewer = navis.plot3d(nl, backend='octarine')\n</code></pre> <p></p> <p></p><pre><code>nl = navis.example_neurons()\nviewer = navis.plot3d(nl, backend='vispy')\n</code></pre> <p></p> <p>Important</p> <p>If you are using Octarine/Vispy from Jupyter, we may have to explicitly call the <code>viewer.show()</code> method in the last line of the cell for the widget to show up.</p> <p>A few important notes regarding the Octarine/Vispy backends:</p> <ul> <li>The <code>viewer</code> is dynamic: you can keep adding/removing items in other cells. However, it will die with the kernel (unlike <code>plotly</code>)!</li> <li> <p>By default, NAVis will track the \"primary\" viewer and subsequent calls of <code>navis.plot3d</code> will add object to that primary viewer</p> <ul> <li>if you want to force a new viewer: <code>navis.plot3d(nl, viewer='new')</code></li> <li>if you want to add to a specific viewer: <code>navis.plot3d(nl, viewer=viewer)</code></li> </ul> </li> <li> <p>You can dynamically resize the canvas (in Jupyter by dragging the lower right corner)</p> </li> <li>For Jupyter: the rendering runs in your Jupyter Kernel and the frames are sent to Jupyter via a remote frame buffer (<code>jupyter_rbf</code>). If your   Jupyter kernel runs on a remote machine you might experiences some lag depending on the connection speed and quality.</li> </ul> <p>Some important methods for the <code>viewer</code> object:</p> <pre><code># Close the viewer\nviewer.close()\n\n# Close the current primary viewer\nnavis.close3d()\n\n# Add neurons to the primary viewer\nnavis.plot3d(nl)\n\n# Add neurons to a specific\nnavis.plot3d(nl, viewer=viewer)\n\n# Clear viewer\nviewer.clear3d()\n\n# Clear the primary viewer\nnavis.clear3d()\n</code></pre> <p>The Octarine viewer itsel has a bunch of neat features - check out the documentation to learn more.</p> <p>Important</p> <p>The Vispy backend is deprecated and will be removed in future versions of NAVis. If you can please switch to Octarine. If you have any issues with Octarine and want us to keep the Vispy backend, please let us know!</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_00_intro/#k3d","title":"K3d","text":"<p><code>k3d</code> plots work in Jupyter (and only there) but unlike <code>plotly</code> don't persist across sessions. Hence we will only briefly demo them using static screenshots and then move on to plotly. Almost everything you can do with the <code>plotly</code> backend can also be done with <code>k3d</code> (or <code>octarine/vispy</code> for that matter)!</p> <pre><code>p = navis.plot3d(nl, backend=\"k3d\")\n</code></pre> <p></p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_00_intro/#plotly","title":"Plotly","text":"<p>Last but not least, we have the <code>plotly</code> backend. This is the only backend which allows us to embed interactive 3D plots into this documentation. The main advantage of <code>plotly</code> is that it works \"inline\" in Jupyter notebooks and that you can export the plots as standalone HTML files. The main disadvantage is that it can be quite slow for large scenes and that the resulting <code>.ipynb</code> notebook files can get quite large.</p> <p>Using plotly as backend generates \"inline\" plots by default (i.e. they are rendered right away):</p> <pre><code>navis.plot3d(\n    nl,\n    backend=\"plotly\",\n    connectors=False,\n    radius=True,  # use node radii for skeletons\n    legend_orientation=\"h\",  # horizontal legend (more space for plot)\n)\n</code></pre> <p>Instead of inline plotting, you can also export your plotly figure as 3D html file that can be opened in any browser:</p> <pre><code>import plotly\n\n# Prevent inline plotting\nfig = nl.plot3d(backend='plotly', connectors=False, width=1400, height=1000, inline=False)\n\n# Save figure to html file\nplotly.offline.plot(fig, filename='~/Documents/3d_plot.html')\n</code></pre>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_00_intro/#navigating-the-3d-viewers","title":"Navigating the 3D viewers","text":"Action Octarine/Vispy Plotly K3d Rotate Left Mouse + Drag Left Mouse + Drag Left Mouse + Drag Zoom Mousewheel Mousewheel Mousewheel Pan Right Mouse + Drag Right Mouse + Drag Right Mouse + Drag Hide/Unhideobjects <code>viewer.hide()</code><code>viewer.show()</code> Click on legend Click on legend <p>Camera rotation</p> <p>If the camera rotation using plotly causes problems, try clicking on the \"Orbital rotation\" in the upper right tool bar. </p> <p>%%</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_00_intro/#high-quality-renderings","title":"High-quality renderings","text":"<p>Above we demo'ed making a little GIF using matplotlib. While that's certainly fun, it's not very high production value. For high quality videos and renderings I recommend you check out the tutorial on navis' Blender interface. Here's a little taster:</p> <p>Total running time of the script: ( 1 minutes  35.716 seconds)</p> <p> Download Python source code: tutorial_plotting_00_intro.py</p> <p> Download Jupyter notebook: tutorial_plotting_00_intro.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_01_colors/","title":"Coloring","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_01_colors/#coloring","title":"Coloring","text":"<p>This tutorial demonstrates how to adjust colors in NAVis plots.</p> <p>By now, you should already have a basic understanding on how to plot neurons in NAVis (2d vs 3d plots, the various backends and plotting methods, etc.) - if not, check out the plotting tutorial.</p> <p>In this tutorial we will focus on how to finetune these plots by changing colors . We will demonstrate this using <code>matplotlib</code> (<code>plot2d</code>) and <code>plotly</code> (<code>plot3d</code>) but everything shown here also works for the other backends (Octarine, Vispy and K3d)!</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_01_colors/#neuron-colors","title":"Neuron Colors","text":"<p>Undoubtedly one of the most important aspects of a plot is the color scheme. In NAVis you can control the color of individual neurons, their compartments, synapses, etc. in a variety of ways. Here are a few examples that should get you started:</p> <pre><code>import navis\nimport matplotlib.pyplot as plt\n\nnl = navis.example_neurons(3, kind=\"mesh\")\n</code></pre> <p>A single color for all neurons:</p> <pre><code>navis.plot2d(nl, color=\"r\", view=(\"x\", \"-z\"), method=\"2d\")\nplt.tight_layout()\n</code></pre> <p></p> <p>A list of colors for each neuron:</p> <pre><code>navis.plot2d(nl, color=[\"r\", \"g\", \"b\"], view=(\"x\", \"-z\"), method=\"2d\")\nplt.tight_layout()\n</code></pre> <p></p> <p>A palette to choose colors from:</p> <pre><code>navis.plot2d(nl, palette=\"Greens\", view=(\"x\", \"-z\"), method=\"2d\")\nplt.tight_layout()\n</code></pre> <p></p> <p>A mapping of neuron ID -&gt; color:</p> <pre><code>colors = dict(zip(nl.id, [\"r\", \"g\", \"b\"]))\nnavis.plot2d(nl, color=colors, view=(\"x\", \"-z\"), method=\"2d\")\nplt.tight_layout()\n</code></pre> <p></p> <p>Individual colors can be provided as:</p> <ul> <li>names (e.g. \"red\", \"green\", \"blue\") like we did above</li> <li>hex codes (e.g. \"#FF0000\", \"#00FF00\", \"#0000FF\")</li> <li>RGB or RGBA tuples (e.g. <code>(1, 0, 0)</code> for red)</li> </ul> <pre><code># Provide a list of 3 colors - one for each neuron - in various formats:\nnavis.plot2d(nl, color=[\"red\", \"#FF0000\", (0, 0, 0)], view=(\"x\", \"-z\"), method=\"2d\")\nplt.tight_layout()\n</code></pre> <p></p> <p>What if you want to color neurons by some categorical property - for example their type or brain region? Easy peasy: just use the <code>color_by</code> parameter!</p> <pre><code># A list with labels, one for each neuron\ntypes = [\"typeA\", \"typeB\", \"typeA\"]\n\nnavis.plot2d(nl, color_by=types, palette=\"tab10\", view=(\"x\", \"-z\"), method=\"2d\")\nplt.tight_layout()\n</code></pre> <p></p> <p>NAVis automatically assigns a color to each unique label using the provided palette. You can also provide a dictionary to manually set the colors:</p> <pre><code>palette = {\"typeA\": \"red\", \"typeB\": \"blue\"}\n\nnavis.plot2d(nl, color_by=types, palette=palette, view=(\"x\", \"-z\"), method=\"2d\")\nplt.tight_layout()\n</code></pre> <p></p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_01_colors/#coloring-neurites","title":"Coloring Neurites","text":"<p>So far so good but what if you want to color the neurites of an individual neuron? For example make its axon red and its dendrites blue? Also easy peasy: <code>color_by</code> can also be used to color nodes/vertices!</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_01_colors/#by-labels","title":"By Labels","text":"<pre><code>n = navis.example_neurons(1, kind=\"skeleton\")\n\n# This will add a \"compartment\" for each node in the neuron\nnavis.split_axon_dendrite(n, label_only=True)\n\nn.nodes.head()\n</code></pre> node_id label x y z radius parent_id type compartment 0 1 0 15784.0 37250.0 28062.0 10.000000 -1 root linker 1 2 0 15764.0 37230.0 28082.0 18.284300 1 slab linker 2 3 0 15744.0 37190.0 28122.0 34.721401 2 slab linker 3 4 0 15744.0 37150.0 28202.0 34.721401 3 slab linker 4 5 0 15704.0 37130.0 28242.0 34.721401 4 slab linker <p>Now we can color the neuron based on the \"compartment\" label:</p> <pre><code>navis.plot2d(n, color_by=\"compartment\", palette=\"tab10\", view=(\"x\", \"-z\"), method=\"2d\")\nplt.tight_layout()\n</code></pre> <p></p> <p>We can also set the colors manually:</p> <pre><code>colors = {\"axon\": \"coral\", \"dendrite\": \"cyan\", \"linker\": \"limegreen\"}\nnavis.plot2d(n, color_by=\"compartment\", palette=colors, view=(\"x\", \"-z\"), method=\"2d\")\nplt.tight_layout()\n</code></pre> <p></p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_01_colors/#by-values","title":"By Values","text":"<p>You can also color neurites based on some numerical value. This is especially useful for things like Strahler index, branch order, etc.</p> <p>Coloring by e.g. Strahler index:</p> <pre><code>n = navis.example_neurons(1, kind=\"skeleton\")\n\n# This adds an `strahler_index` column to the node table\nnavis.strahler_index(n)\n\nn.nodes.head()\n</code></pre> node_id label x y z radius parent_id type strahler_index 0 1 0 15784.0 37250.0 28062.0 10.000000 -1 root 6 1 2 0 15764.0 37230.0 28082.0 18.284300 1 slab 6 2 3 0 15744.0 37190.0 28122.0 34.721401 2 slab 6 3 4 0 15744.0 37150.0 28202.0 34.721401 3 slab 6 4 5 0 15704.0 37130.0 28242.0 34.721401 4 slab 6 <p>Plot with color based on Strahler index:</p> <pre><code>navis.plot2d(\n    n, color_by=\"strahler_index\", palette=\"viridis\", view=(\"x\", \"-z\"), method=\"2d\"\n)\nplt.tight_layout()\n</code></pre> <p></p> <p>Note</p> <p>You can use the <code>vmin</code> and <code>vmax</code> parameters to control the normalization of the color scale.</p> <p>All of this also works with <code>MeshNeurons</code>. Here, we have to provide a label for each vertex in the mesh:</p> <pre><code>m = navis.example_neurons(1, kind=\"mesh\")\nnavis.strahler_index(m)\nm.strahler_index  # this is an array with one value per vertex\n</code></pre> <p>Out:</p> <pre><code>array([2, 3, 3, ..., 1, 4, 4], shape=(6309,))\n</code></pre> <pre><code># Let's use plot3d this time\nnavis.plot3d(m, color_by=\"strahler_index\", palette=\"viridis\", legend=False)\n</code></pre> <p>Note</p> <p>In the examples above we have provided <code>color_by</code> as the name of a property or a column in the node table. We could have also provided an array of values directly: </p><pre><code>navis.plot2d(n, color_by=n.nodes.strahler_index, palette=\"viridis\")\n</code></pre><p></p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_01_colors/#neuron-opacity","title":"Neuron Opacity","text":"<p>You can also control the opacity of the neurons. This is especially useful when plotting multiple neurons on top of each other:</p> <pre><code># A single opacity for all neurons\nnavis.plot2d(nl, alpha=0.25, view=(\"x\", \"-z\"), method='2d')\nplt.tight_layout()\n</code></pre> <p></p> <pre><code># A list of alpha values, one for each neuron\nnavis.plot2d(nl, color=\"k\", alpha=[1, 0.2, 0.2], view=(\"x\", \"-z\"), method='2d')\nplt.tight_layout()\n</code></pre> <p></p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_01_colors/#shading","title":"Shading","text":"<p>Analogous to the <code>color_by</code> parameter, you can also shade neurons based on some property. Let's demonstrate this by shading a neuron based on its distance from the soma:</p> <pre><code>n = navis.example_neurons(1, kind=\"skeleton\")\nn.reroot(n.soma, inplace=True)\nn.nodes[\"root_dist\"] = n.nodes.node_id.map(navis.dist_to_root(n, weight=\"weight\")) * -1\nn.nodes.head()\n</code></pre> node_id label x y z radius parent_id type root_dist 0 1 0 15784.0 37250.0 28062.0 10.000000 2 end -1302.983639 1 2 0 15764.0 37230.0 28082.0 18.284300 3 slab -1268.342622 2 3 0 15744.0 37190.0 28122.0 34.721401 4 slab -1208.342622 3 4 0 15744.0 37150.0 28202.0 34.721401 5 slab -1118.899903 4 5 0 15704.0 37130.0 28242.0 34.721401 6 slab -1058.899903 <pre><code># Plot with shading based on distance from the soma:\nnavis.plot2d(n, shade_by=\"root_dist\", view=(\"x\", \"-z\"), radius=True, method=\"2d\")\nplt.tight_layout()\n</code></pre> <p></p> <p>We can combine <code>color_by</code> and <code>shade_by</code> to color and shade the neuron at the same time:</p> <pre><code>navis.plot2d(\n    n,\n    color_by=\"root_dist\",\n    shade_by=\"root_dist\",\n    palette=\"viridis\",\n    view=(\"x\", \"-z\"),\n    method=\"2d\",\n    radius=True\n)\nplt.tight_layout()\n</code></pre> <p></p> <p>Total running time of the script: ( 0 minutes  10.963 seconds)</p> <p> Download Python source code: tutorial_plotting_01_colors.py</p> <p> Download Jupyter notebook: tutorial_plotting_01_colors.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_02_1d/","title":"Neuron \"Barcodes\"","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_02_1d/#neuron-barcodes","title":"Neuron \"Barcodes\"","text":"<p>In this tutorial we'll explore a unique way to visualize the branching pattern of neurons using \"barcodes\".</p> <p>This visualization technique is based on Cuntz et al (2010) and turns a neuron's branching pattern into a unique \"barcode\" using toplogical sorting.</p> <pre><code>import navis\nimport matplotlib.pyplot as plt\n\nn = navis.example_neurons(n=1, kind=\"skeleton\")  # we need skeletons for this\n\nn.reroot(n.soma, inplace=True)  # set the somas as the root\n\nax = navis.plot1d(n)\nplt.tight_layout()\n</code></pre> <p></p> <p>Important</p> <p><code>navis.plot1d</code> only works with <code>navis.TreeNeurons</code>. These neurons must be have only a single root (i.e. a single connected component) which is used as the point of reference for the sorting.</p> <p>So what's actually happening here? The barcode is a way to represent the branching pattern of a neuron in a unique way. The barcode is created by breaking the neuron into linear segments between branch points and sorting them by walking from the root to the leafs in a depth-first manner. At each branch point we prioritize the branch that maximizes the distance to the root.</p> <p>In the plot each segment is represented by a rectangle where the width correspond to the segment's length. Segments that terminate in a leaf node are plotted with a darker color.</p> <p>Here's a simple example to illustrate this:</p> <p></p> <p>The example skeleton we used above is pretty complex with lots of little side branches which makes the barcode quite complicated. Let's simplify the neurons a bit to make the barcode easier to read:</p> <pre><code># Prune each neuron to its 20 longest branches\nn_pruned = navis.longest_neurite(n, n=20)\n\nax = navis.plot1d(n_pruned, lw=1)\nplt.tight_layout()\n</code></pre> <p></p> <p>Note</p> <p>Instead of <code>navis.longest_neurite</code> you could also use e.g. <code>navis.prune_twigs</code> to remove small branches below a certain size.</p> <p>That's easier to interpet! Let's pull up the neuron to compare:</p> <pre><code># Draw the full neuron in black\nfig, ax = navis.plot2d(n, view=('x', '-z'), method='2d', color='k')\n# Add the pruned neuron in red on top\nfig, ax = navis.plot2d(n_pruned, view=('x', '-z'), method='2d', color='r', ax=ax, lw=1.1)\n</code></pre> <p></p> <p>With that side-by-side comparison you can hopefully see how the barcode captures the branching pattern of the neuron.</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_02_1d/#multiple-neurons","title":"Multiple Neurons","text":"<p>We can also plot barcodes for multiple neurons at a time:</p> <pre><code>nl = navis.example_neurons(n=5, kind=\"skeleton\")\nnl = nl[nl.soma != None]  # remove neurons without soma\nnavis.heal_skeleton(nl, inplace=True)  # heal any potential breaks in the skeletons\nnl.reroot(nl.soma)  # reroot all neurons to their soma\nnavis.longest_neurite(nl, 10, inplace=True)\n\nfig, ax = plt.subplots(figsize=(18, 6))\n\n# Plot the barcodes\nax = navis.plot1d(nl, ax=ax)\nplt.tight_layout()\n</code></pre> <p></p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_02_1d/#customizing-the-plot","title":"Customizing the Plot","text":"<p>Similar to other plotting functions in NAVis, you can customize the appearance of the barcode plot:</p> <pre><code>ax = navis.plot1d(n_pruned, color=\"red\")\nplt.tight_layout()\n</code></pre> <p></p> <p>We can also color segments based on some property of the neuron:</p> <pre><code># Add a \"root_dist\" column to the neuron's node table\nn_pruned.nodes['root_dist'] = n_pruned.nodes.node_id.map(\n    navis.dist_to_root(n_pruned)\n)\n\n# Plot the barcode with segments colored by their distance to the root\nax = navis.plot1d(n_pruned, color_by=\"root_dist\", palette=\"jet\")\n</code></pre> <p></p> <p>Total running time of the script: ( 0 minutes  2.485 seconds)</p> <p> Download Python source code: tutorial_plotting_02_1d.py</p> <p> Download Jupyter notebook: tutorial_plotting_02_1d.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_03_dend/","title":"Neuron Topology","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_03_dend/#neuron-topology","title":"Neuron Topology","text":"<p>This example demonstrates how to plot neurons' topology using various different layouts.</p> <p>Skeletons in NAVis are hierarchical trees (hence the name <code>TreeNeuron</code>). As such they can be visualized as dendrograms or flat, graph-like plots using <code>navis.plot_flat</code>. This is useful to unravel otherwise complicated, overlapping branching patterns - e.g. when you want to show compartments or synapse positions within the neuron. Let's demo some layouts!</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_03_dend/#subway-maps","title":"Subway Maps","text":"<p>First up: the <code>subway</code> layout. This one is nice for side-by-side comparisons of neurons.</p> <pre><code>import navis\nimport matplotlib.pyplot as plt\n\n# Load example neurons\nnl = navis.example_neurons(n=4, kind=\"skeleton\")\n\n# Convert example neurons from voxels to nanometers\nnl_nm = nl.convert_units(\"nm\")\n\n# Reroot to soma\nnl_nm[nl_nm.soma != None].reroot(nl_nm[nl_nm.soma != None].soma, inplace=True)\n\n# Generate one axis for each neuron\nfig, axes = plt.subplots(3, 1, figsize=(15, 10), sharex=True)\n\nnavis.plot_flat(nl_nm[0], layout=\"subway\", connectors=True, ax=axes[0])\nnavis.plot_flat(nl_nm[1], layout=\"subway\", connectors=True, ax=axes[1])\nnavis.plot_flat(nl_nm[3], layout=\"subway\", connectors=True, ax=axes[2])\n\n# Clean up the axes\nfor ax in axes[:-1]:\n    ax.set_axis_off()\n\nfor sp in [\"left\", \"right\", \"top\"]:\n    axes[-1].spines[sp].set_visible(False)\naxes[-1].set_yticks([])\n\n_ = axes[-1].set_xlabel(\"distance [nm]\")\nplt.tight_layout()\n</code></pre> <p></p> <p>Important</p> <p>For the other layouts you will need to have pygraphviz and the underlying <code>graphviz</code> library installed. For details on the layout, check out the graphviz docs.</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_03_dend/#dendrograms","title":"Dendrograms","text":"<p><code>dot</code> and <code>twopi</code> are dendrogram layouts. They (should) preserve branch lengths similar to <code>subway</code>:</p> <pre><code>ax = navis.plot_flat(nl_nm[0], layout=\"dot\", connectors=True, color=\"k\")\nplt.tight_layout()\n</code></pre> <p></p> <pre><code>n = nl_nm[0]\nax = navis.plot_flat(n, layout=\"twopi\", connectors=True, color=\"k\")\nplt.tight_layout()\n</code></pre> <p></p> <p>You can also highlight specific connectors by their ID (here we just use the first 100):</p> <pre><code>highlight = n.connectors.connector_id[:100]\n\nax = navis.plot_flat(\n    nl_nm[0],\n    layout=\"dot\",\n    highlight_connectors=highlight,\n    color=\"k\",\n    syn_marker_size=2,\n)\nplt.tight_layout()\n</code></pre> <p></p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_03_dend/#force-directed-layouts","title":"Force-Directed Layouts","text":"<p><code>neato</code>, <code>fdp</code> and <code>sfdp</code> are force-directed layouts.</p> <p>Some layouts (like <code>neato</code> &amp; <code>fdp</code>) can be quite expensive to calculate in which case it's worth downsampling your neuron before plotting</p> <pre><code>ds = navis.downsample_neuron(nl_nm[0], 10, preserve_nodes=\"connectors\")\n\nax = navis.plot_flat(ds, layout=\"neato\", connectors=True, color=\"k\")\nplt.tight_layout()\n</code></pre> <p></p> <pre><code>ax = navis.plot_flat(nl[0], layout=\"sfdp\", connectors=True, color=\"k\")\nplt.tight_layout()\n</code></pre> <p></p> <p>Total running time of the script: ( 0 minutes  36.230 seconds)</p> <p> Download Python source code: tutorial_plotting_03_dend.py</p> <p> Download Jupyter notebook: tutorial_plotting_03_dend.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_04_skeletons/","title":"Fine-tuning Skeletons","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_04_skeletons/#fine-tuning-skeletons","title":"Fine-tuning Skeletons","text":"<p>In this example we will demonstrate various ways to fine-tune plots with skeletons.</p> <p>By now, you should already have a basic understanding on how to plot neurons in NAVis (2d vs 3d plots, the various backends and plotting methods, etc.) - if not, check out the plotting tutorial.</p> <p>We will focus on how to finetune <code>plot2d</code> plots because <code>matplotlib</code> is much more flexible than the <code>plot3d</code> backends when it comes to rendering lines. That said: some of the things we show here will also work for the other backends (Octarine, Vispy and K3d) - just not all.</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_04_skeletons/#radii","title":"Radii","text":"<p>If your skeletons have radii (i.e. there is a non-empty <code>radius</code> column in their <code>.nodes</code> SWC table), you can plot them as tubes instead of lines using the <code>radius</code> parameter. By default, <code>radius</code> is set to <code>False</code> and skeletons are plotted as lines<sup>1</sup>.</p> <pre><code>import navis\nimport matplotlib.pyplot as plt\n\n# Load a neuron\nn = navis.example_neurons(1, kind=\"skeleton\")\n\n# Plot as lines\nfig, ax = navis.plot2d(n, view=(\"x\", \"-z\"), method=\"2d\")\nplt.tight_layout()\n</code></pre> <p></p> <p>Setting <code>radius=True</code> will plot skeletons as tubes using its nodes' radii information:</p> <pre><code>fig, ax = navis.plot2d(n, view=(\"x\", \"-z\"), method=\"2d\", radius=True)\nplt.tight_layout()\n</code></pre> <p></p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_04_skeletons/#line-width","title":"Line width","text":"<p>You can change the line width of the skeletons using the <code>linewidth</code> parameter. The default is <code>1</code>.</p> <pre><code>fig, ax = navis.plot2d(\n    n,\n    view=(\"x\", \"-z\"),\n    method=\"2d\",\n    radius=False,\n    linewidth=2,  # default linewidth is 1\n)\n</code></pre> <p></p> <p><code>linewidth</code> can also be used to scale the size of the tubes when <code>radius=True</code>:</p> <pre><code>fig, ax = navis.plot2d(\n    n,\n    view=(\"x\", \"-z\"),\n    method=\"2d\",\n    radius=True,\n    linewidth=2,  # double the tube radii\n)\n</code></pre> <p></p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_04_skeletons/#line-style","title":"Line style","text":"<p>When <code>radius=False</code>, you can change the line style of the skeletons using the <code>linestyle</code> parameter. The default is <code>\"-\"</code> (i.e. a solid line).</p> <pre><code>fig, ax = navis.plot2d(\n    n,\n    view=(\"x\", \"-z\"),\n    method=\"2d\",\n    radius=False,\n    linewidth=2,\n    linestyle=\"--\"  # dashed line\n)\n</code></pre> <p></p> <p>The <code>radius</code> and <code>linewidth</code> parameters will also work with <code>plot3d</code> but the <code>linestyle</code> parameter will not.</p> <p>Total running time of the script: ( 0 minutes  2.595 seconds)</p> <p> Download Python source code: tutorial_plotting_04_skeletons.py</p> <p> Download Jupyter notebook: tutorial_plotting_04_skeletons.ipynb</p> <p>Gallery generated by mkdocs-gallery</p> <ol> <li> <p>This is because plotting tubes can be slow for large number of skeletons.\u00a0\u21a9</p> </li> </ol>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_05_depth/","title":"Depth-coloring","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_05_depth/#depth-coloring","title":"Depth-coloring","text":"<p>This example shows how to color neurons by depth.</p> <p>The obvious issue with 2d plots is that they are... well, 2d. This means that you can't easily convey depth information. What we can do, however, is color the neuron by depth - that is by the distance to the camera. This is a simple way to give a sense of the neuron's 3d structure.</p> <p>Note that this currently works only for <code>navis.plot2d</code> (i.e. <code>matplotlib</code>):</p> <pre><code>import navis\nimport matplotlib.pyplot as plt\n\nn = navis.example_neurons(1, kind=\"skeleton\")\n\nfig, ax = navis.plot2d(\n    n,\n    depth_coloring=True,\n    method='2d',\n    view=(\"x\", \"-z\"),\n)\nplt.tight_layout()\n</code></pre> <p></p> <p>The <code>depth_coloring</code> parameter will color the neuron by distance from the camera. For this neuron, the ventral dendrites are closest to the camera whereas the dorsal axon is further away.</p> <p>By default, this will use the <code>jet</code> colormap. You can change this to any of <code>matplotlib</code>'s colormaps using the <code>palette</code> parameter:</p> <pre><code>fig, ax = navis.plot2d(\n    n,\n    depth_coloring=True,\n    palette=\"hsv\",\n    method='2d',\n    view=(\"x\", \"-z\"),\n)\n</code></pre> <p></p> <p>This should work with both <code>TreeNeurons</code> and <code>MeshNeurons</code> and methods <code>2d</code> and <code>3d</code>.</p> <p>Total running time of the script: ( 0 minutes  0.505 seconds)</p> <p> Download Python source code: tutorial_plotting_05_depth.py</p> <p> Download Jupyter notebook: tutorial_plotting_05_depth.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_06_cortex/","title":"Cortical Neurons","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_06_cortex/#cortical-neurons","title":"Cortical Neurons","text":"<p>This tutorial demonstrates how to plot cortical neurons.</p> <p>In this exercise we will visualize morphological data from \"Integrated Morphoelectric and Transcriptomic Classification of Cortical GABAergic Cells\" by Gouwens, Sorensen et al., Cell (2020). Specifically, we will re-create a plot similar to their Figure 4A.</p> <p>For brevity, we will use some fixed cell IDs and properties from the dataset. These were taken from the <code>20200711_patchseq_metadata_mouse.csv</code> file provided alongside the supplementary material of the paper:</p> <pre><code># The cell IDs we will use (it's the first 5 in the meta data file)\nids = [601506507, 601790961, 601803754, 601808698, 601810307]\n\n# The normalized soma depths for these cells (also from the meta data file)\nsoma_depths = [0.36101451, 0.62182935, 0.16423996, 0.48303029, 0.2956563]\n</code></pre>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_06_cortex/#part-i-loading-and-aligning-neurons","title":"Part I: Loading and Aligning Neurons","text":"<p>First we need to load the neurons. Here, we will take them straight from their FTP server but you can of course download them first and then load from disk!</p> <pre><code>import navis\n\nnl = navis.read_swc(\n    \"ftp://download.brainimagelibrary.org/biccn/zeng/pseq/morph/200526/\",\n    limit=[f\"{i}_transformed.swc\" for i in ids],  #  Load only the files we need\n    fmt=\"{name,id:int}_transformed.swc\",  # Parse the name and id from the file name\n)\n</code></pre> <p>To make our lives a bit easier, we will attach the soma depth to the neurons as metadata:</p> <pre><code>nl.set_neuron_attributes(\n    soma_depths,\n    name=\"cell_soma_normalized_depth\",\n    register=True\n    )\n\nnl\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 5 neurons (1.5MiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file cell_soma_normalized_depth 0 navis.TreeNeuron 601506507 601506507 3680 None 30 32 6012.066895 1 1 dimensionless 2026-03-01 17:22:24.088855 download.brainimagelibrary.org:21/bil/data/bic... 601506507_transformed.swc 0.361015 1 navis.TreeNeuron 601790961 601790961 12333 None 163 169 19280.503906 1 1 dimensionless 2026-03-01 17:22:24.928894 download.brainimagelibrary.org:21/bil/data/bic... 601790961_transformed.swc 0.621829 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 3 navis.TreeNeuron 601808698 601808698 13565 None 252 265 17969.757812 1 1 dimensionless 2026-03-01 17:22:28.335318 download.brainimagelibrary.org:21/bil/data/bic... 601808698_transformed.swc 0.483030 4 navis.TreeNeuron 601810307 601810307 13583 None 277 285 23013.343750 1 1 dimensionless 2026-03-01 17:22:29.173471 download.brainimagelibrary.org:21/bil/data/bic... 601810307_transformed.swc 0.295656 <p>Next, we need to align the neurons according to their soma depth! The normalized <code>cell_soma_normalized_depth</code> should map to a physical range of <code>0</code> to <code>922.5861720311</code> microns.</p> <p>Let's demo with one neuron before we run this for all neurons:</p> <pre><code># Grab one of the neurons\nn = nl[0]\n\n# This is the normalized soma depth:\nprint(f\"Normalized soma depth: {n.cell_soma_normalized_depth}\")\n</code></pre> <p>Out:</p> <pre><code>Normalized soma depth: 0.36101451\n</code></pre> <p>The physical soma depth is simply the normalized depth multiplied by the total depth of the cortex. Note that we're positioning from the bottom - i.e. 922.586 will be at the surface and 0 at the bottom! This is to make our lifes easier when it comes to plotting since the origin in <code>matplotlib</code> figures is in the bottom left corner.</p> <pre><code>phys_y = (1 - n.cell_soma_normalized_depth) * 922.5861720311\nprint(f\"Physical soma depth: {phys_y}\")\n\n# Current soma\nprint(f\"Current soma coordinates: {n.soma_pos[0]}\")\n</code></pre> <p>Out:</p> <pre><code>Physical soma depth: 589.5191772025167\nCurrent soma coordinates: [382.803   433.69104 118.07384]\n</code></pre> <p>We will now offset the neuron such that the soma is at <code>(0, 589.519, 0)</code>:</p> <pre><code>offset = [0, phys_y, 0] - n.soma_pos[0]\noffset\n</code></pre> <p>Out:</p> <pre><code>array([-382.80300903,  155.82813716, -118.07383728])\n</code></pre> <p>Moving or scaling neurons in NAVis is super straight forward: adding, subtracting, dividing or multiplying neurons by a number or an <code>[x, y, z]</code> vector will change their coordinates:</p> <pre><code># Move the neuron to the new centered position\nn += offset\n\n# Check the that the soma is now in the correct position\nn.soma_pos[0]\n</code></pre> <p>Out:</p> <pre><code>array([  0.       , 589.5191772,   0.       ])\n</code></pre> <p>That looks good! Let's do it for all neurons:</p> <pre><code>for n in nl:\n    phys_y = (1 - n.cell_soma_normalized_depth) * 922.5861720311\n    offset = [0, phys_y, 0] - n.soma_pos[0]\n    n += offset\n</code></pre> <p>Check that all soma positions are correct:</p> <pre><code>nl.soma_pos.reshape(-1, 3)\n</code></pre> <p>Out:</p> <pre><code>array([[  0.        , 589.5191772 ,   0.        ],\n       [  0.        , 348.89501236,   0.        ],\n       [  0.        , 771.06065604,   0.        ],\n       [  0.        , 476.9491058 ,   0.        ],\n       [  0.        , 649.81775798,   0.        ]])\n</code></pre>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_06_cortex/#part-ii-plotting","title":"Part II: Plotting","text":"<p>Now that we have loaded and aligned the neurons, let's recreate a plot similar to those in Figure 4A:</p> <pre><code>def plot_neurons(to_plot, color=\"purple\", axon_color=\"magenta\", offset=500):\n    \"\"\"Plot all neurons of a given transcriptomic type.\n\n    Parameters\n    ----------\n    neurons : NeuronList\n        The aligned neurons to plot.\n    color : str\n        The color of the dendrites.\n    axon_color : str\n        The color of the axon.\n    offset : int\n        The offset between neurons along the x-axis.\n\n    Returns\n    -------\n    fig, ax\n        The matplotlib figure and axis.\n\n    \"\"\"\n    # Offset the neurons along the x-axis so that they don't overlap\n    to_plot = [n + [offset * i, 0, 0] for i, n in enumerate(to_plot)]\n\n    # The SWC files for this dataset include a `label` column which\n    # indicates the compartment type:\n    # 1 = soma\n    # 2 = axon\n    # 3 = dendrites\n    # We will use this `label` to color the neurons' compartments.\n\n    # Here we define a color palette for the compartments:\n    compartment_palette = {1: color, 2: axon_color, 3: color}\n\n    # Plot the neuron\n    fig, ax = navis.plot2d(\n        to_plot,\n        radius=False,\n        lw=1.5,\n        soma=dict(\n            fc=\"black\",  # soma fill color\n            ec=\"white\",  # highlight the soma with a white outline\n            radius=10,   # override the default soma radius\n        ),\n        color_by=\"label\",  # color by `label` column in node table\n        palette=compartment_palette,\n        figsize=(\n            len(to_plot) * 2,\n            10,\n        ),  # scale the figure size with the number of neurons\n        method=\"2d\",\n    )\n\n    # Add the layer boundaries (top bound for each layer in microns)\n    layer_bounds = {\n        \"L1\": 0,\n        \"L2/3\": 115.1112491335,\n        \"L4\": 333.4658190171,\n        \"L5\": 453.6227158132,\n        \"L6\": 687.6482650269,\n        \"L6b\": 883.1308910545,\n    }\n\n    for layer, y in layer_bounds.items():\n        y = 922.5861720311 - y  # flip the y-axis\n        # Add a dashed line\n        ax.axhline(y, color=\"gray\", ls=\"--\", lw=1)\n        # Add the layer name\n        ax.text(-300, y - 25, layer, color=\"gray\", va=\"center\", size=10)\n    # Add the bottom bound\n    ax.axhline(0, color=\"gray\", ls=\"--\", lw=1)\n\n    # Set the axis y limits according to the layers\n    ax.set_ylim(-10, 930)\n\n    # Hide axes\n    ax.axis(\"off\")\n\n    return fig, ax\n\n\nfig, ax = plot_neurons(nl)\n</code></pre> <p></p> <p>That looks close enough. The last bit is to add the little KDE plots for the depth-distribution of cable length!</p> <p>We're going to be cheap here and simply generate a histogram over the node positions. To make this representative, we should make sure that the number of nodes per unit of cable is homogeneous across neurons. For that we will resample the neurons:</p> <pre><code>print(\n    f\"Sampling rate (nodes per micron of cable) before resampling: {nl.sampling_resolution.mean():.2f}\"\n)\n\n# Resample to 2 nodes per micron\nresampled = navis.resample_skeleton(\n    nl,\n    resample_to=0.5,\n    map_columns=\"label\",  # make sure label column is carried over\n)\n\nprint(\n    f\"Sampling rate (nodes per micron of cable) after resampling: {resampled.sampling_resolution.mean():.2f}\"\n)\n</code></pre> <p>Out:</p> <pre><code>Sampling rate (nodes per micron of cable) before resampling: 1.57\nSampling rate (nodes per micron of cable) after resampling: 0.50\n</code></pre> <p>Get the combined nodes table:</p> <pre><code>nodes = resampled.nodes\nnodes.head()\n</code></pre> label node_id parent_id radius type x y z neuron 0 3 23 3681 0.360400 branch -5.994568 614.076123 19.583313 601506507 1 3 3681 3682 0.373750 slab -5.870639 613.586572 19.553024 601506507 2 3 3682 3683 0.387101 slab -5.746710 613.097021 19.522736 601506507 3 3 3683 3684 0.400267 slab -5.525498 612.648052 19.540078 601506507 4 3 3684 3685 0.413362 slab -5.267001 612.214638 19.575677 601506507 <p>Now we can plot the distribution of cable lengths for our neurons:</p> <pre><code>import seaborn as sns\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n# Plot the neurons again, re-using the function we defined above\nfig, ax = plot_neurons(nl)\n\n# Add a new axis to the right of the main plot\ndivider = make_axes_locatable(ax)\nax_hist = divider.append_axes(\"right\", size=0.75, pad=0.05)\n\n# Add histograms\n# For axon:\nsns.kdeplot(\n    data=nodes[nodes.label == 2], y=\"y\", ax=ax_hist, color=\"magenta\", linewidth=1.5\n)\n# For the rest:\nsns.kdeplot(\n    data=nodes[nodes.label != 2], y=\"y\", ax=ax_hist, color=\"purple\", linewidth=1.5\n)\n\n# Add soma positions\nsoma_pos = nl.soma_pos.reshape(-1, 3)\nax_hist.scatter([0] * len(soma_pos), soma_pos[:, 1], color=\"black\", s=10, clip_on=False)\n\n# Set same axis limits as the main plot\nax_hist.set_ylim(-10, 930)\n\n# Hide axes\nax_hist.set_axis_off()\n</code></pre> <p></p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_06_cortex/#acknowledgements","title":"Acknowledgements","text":"<p>We thank Staci Sorensen and Casey Schneider-Mizell from the Allen Institute for Brain Science for helping with extra information and data for this tutorial!</p> <p>Total running time of the script: ( 0 minutes  15.965 seconds)</p> <p> Download Python source code: tutorial_plotting_06_cortex.py</p> <p> Download Jupyter notebook: tutorial_plotting_06_cortex.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_07_xkcd/","title":"XKCD Style","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/1_plotting/tutorial_plotting_07_xkcd/#xkcd-style","title":"XKCD Style","text":"<p>This example demonstrates how to plot neurons in xkcd style.</p> <p>If you don't already know: <code>matplotlib</code> has a xkcd mode that produces plots that look like they were drawn by hand. This can be a fun way to visualize neurons:</p> <pre><code>import navis\nimport matplotlib.pyplot as plt\n\nn = navis.example_neurons(1, kind=\"skeleton\")\n\n# Plot in xkcd style\nwith plt.xkcd(scale=5, randomness=10, length=200):\n    fig, ax = navis.plot2d(\n        n, method=\"2d\", c=\"k\", view=(\"x\", \"-z\"), radius=False, lw=1.5\n    )\n\nplt.tight_layout()\n</code></pre> <p></p> <p>Out:</p> <pre><code>findfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\n</code></pre> <p>Get a few more example neurons and a volume</p> <pre><code>nl = navis.example_neurons()\nneuropil = navis.example_volume(\"neuropil\")\n\n# Make the neuropil mostly transparent\nneuropil.color = (0, 0, 0, 0.02)\n\n# Plot in xkcd style\nwith plt.xkcd(scale=5, randomness=10, length=200):\n    fig, ax = navis.plot2d(\n        [nl, neuropil],\n        method=\"2d\",\n        c=\"k\",\n        view=(\"x\", \"-z\"),\n        lw=1.2,\n        volume_outlines=\"both\",\n        radius=False,\n    )\n\n    ax.grid(False)\n\nplt.tight_layout()\n</code></pre> <p></p> <p>Out:</p> <pre><code>findfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\n</code></pre> <p>Total running time of the script: ( 0 minutes  2.608 seconds)</p> <p> Download Python source code: tutorial_plotting_07_xkcd.py</p> <p> Download Jupyter notebook: tutorial_plotting_07_xkcd.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/2_morpho/mg_execution_times/","title":"Computation times","text":""},{"location":"generated/gallery/2_morpho/mg_execution_times/#computation-times","title":"Computation times","text":"<p>00:15.924 total execution time for generated_gallery_2_morpho files:</p> <p>+-------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_morpho_00_manipulate (docs/examples/2_morpho/tutorial_morpho_00_manipulate.py) | 00:13.943 | 0.0 MB | +-------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_morpho_01_analyze (docs/examples/2_morpho/tutorial_morpho_01_analyze.py)          | 00:01.981 | 0.0 MB | +-------------------------------------------------------------------------------------------------------------------------------+-----------+--------+</p>"},{"location":"generated/gallery/2_morpho/tutorial_morpho_00_manipulate/","title":"Manipulate Morphology","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/2_morpho/tutorial_morpho_00_manipulate/#manipulate-morphology","title":"Manipulate Morphology","text":"<p>This tutorial will give you an impression of how to process and manipulate your neurons' morphology.</p> <p>See the API reference for a complete list of available functions.</p> <p>As you might imagine some manipulations (e.g. smoothing or simplification) will work on all/most neuron types while others will only work on specific types. For example rerooting only makes sense on a <code>navis.TreeNeuron</code>.</p> <p>The rule of thumb is this: if a function is called e.g. <code>downsample_neuron</code> it should work with multiple, if not all, neuron types while specialized functions will be called e.g. <code>reroot_skeleton</code>. So depending on what data you are working with and what you want to get out of it, you might have to explicitly convert between neuron types. See the respective function's docstring for details!</p>"},{"location":"generated/gallery/2_morpho/tutorial_morpho_00_manipulate/#rerooting","title":"Rerooting","text":"<p><code>navis.TreeNeurons</code> are hierarchical trees and as such typically have a single \"root\" node (fragmented neurons will have multiple roots). The root is important because it is used as the reference/origin for a bunch of analyses such as Strahler order. Typically, you want the root to be the soma. Because the root is so important, <code>TreeNeuron</code> can be rerooted:</p> <pre><code>import navis\n\nn = navis.example_neurons(1, kind=\"skeleton\")\nprint(n.soma)\n</code></pre> <p>Out:</p> <pre><code>4177\n</code></pre> <p><code>.soma</code> returns the node ID of the soma (if there is one) and can be used to reroot</p> <pre><code>navis.reroot_skeleton(n, n.soma, inplace=True)\n</code></pre> type navis.TreeNeuron name DA1_lPN_R id 1734350788 n_nodes 4465 n_connectors 2705 n_branches 598 n_leafs 619 cable_length 266476.875 soma 4177 units 8 nanometer created_at 2026-03-01 17:22:40.897097 origin /home/runner/work/navis/navis/navis/data/swc/1... file 1734350788.swc <p>Note</p> <p>The root is implicitly also important for <code>navis.MeshNeuron</code> because we're using their skeleton representations for a couple operations/analyses!</p>"},{"location":"generated/gallery/2_morpho/tutorial_morpho_00_manipulate/#simplifying","title":"Simplifying","text":"<p>If you work with large lists of neurons you may want to downsample/simplifiy before e.g. trying to plot them. This is one of the things that - in principle work - with all neuron types. The implementation, however, depends on the neuron type. Lookup the respective function's help (e.g. via the <code>API</code>) for details.</p> <p>For <code>TreeNeurons</code> downsampling means skipping N nodes (here 10):</p> <pre><code>sk = navis.example_neurons(n=1, kind=\"skeleton\")\nprint(sk.n_nodes)\n</code></pre> <p>Out:</p> <pre><code>4465\n</code></pre> <pre><code>sk_downsampled = navis.downsample_neuron(sk, downsampling_factor=10, inplace=False)\nprint(sk_downsampled.n_nodes)\n</code></pre> <p>Out:</p> <pre><code>1304\n</code></pre> <p>For <code>MeshNeurons</code> downsampling will reduce the number of faces by a factor of N:</p> <pre><code>me = navis.example_neurons(n=1, kind=\"mesh\")\nprint(me.n_faces)\n</code></pre> <p>Out:</p> <pre><code>13054\n</code></pre> <pre><code>me_downsampled = navis.downsample_neuron(me, downsampling_factor=10, inplace=False)\nprint(me_downsampled.n_faces)\n</code></pre> <p>Out:</p> <pre><code>1459\n</code></pre> <p>Note</p> <p>Under the hood <code>downsample_neuron</code> calls <code>navis.simplify_mesh</code> for <code>MeshNeurons</code>. That function then requires one of the supported backends for mesh operations to be installed: Blender 3D, <code>pymeshlab</code> or <code>open3d</code>. If none is available, it will prompt you to install one.</p>"},{"location":"generated/gallery/2_morpho/tutorial_morpho_00_manipulate/#resampling","title":"Resampling","text":"<p><code>TreeNeurons</code> can also be \"resampled\" (up or down) to a given resolution (i.e. distance between nodes):</p> <pre><code>sk = navis.example_neurons(n=1, kind=\"skeleton\")\nprint(sk.sampling_resolution * sk.units)\n</code></pre> <p>Out:</p> <pre><code>477.4501647949219 nanometer\n</code></pre> <p>Note that we can provide a unit (\"1 micron\") here because our neuron has units set:</p> <pre><code>sk_resampled = navis.resample_skeleton(sk, resample_to=\"1 micron\", inplace=False)\nprint(sk_resampled.sampling_resolution * sk_resampled.units)\n</code></pre> <p>Out:</p> <pre><code>1072.234130859375 nanometer\n</code></pre> <p>Let's visualize what we did there:</p> <pre><code>import matplotlib.pyplot as plt\n\nnodes_original = sk.nodes[[\"x\", \"y\", \"z\"]].values\nnodes_downsampled = sk_downsampled.nodes[[\"x\", \"y\", \"z\"]].values\nnodes_resampled = sk_resampled.nodes[[\"x\", \"y\", \"z\"]].values\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n_ = navis.plot2d(\n    nodes_original,\n    method=\"2d\",\n    view=(\"x\", \"-z\"),\n    scatter_kws=dict(c=\"blue\"),\n    ax=axes[0],\n)\n_ = navis.plot2d(\n    nodes_resampled,\n    method=\"2d\",\n    view=(\"x\", \"-z\"),\n    scatter_kws=dict(c=\"red\"),\n    ax=axes[1],\n)\n_ = navis.plot2d(\n    nodes_downsampled,\n    method=\"2d\",\n    view=(\"x\", \"-z\"),\n    scatter_kws=dict(c=\"green\"),\n    ax=axes[2],\n)\n\nfor ax, title in zip(axes, [\"Original\", \"Resampled to 1um\", \"Downsampled 10x\"]):\n    ax.set_title(title, color=\"k\")\n    ax.invert_yaxis()\n    ax.set_axis_off()\n\nplt.tight_layout()\n</code></pre> <p></p> <p>Tip</p> <p>Click on the image to see it in full resolution.</p> <p>As you can see the resampling increased the node density in the backbone and decreased it in the finer neurites to bring things on par. Downsampling just thinned out the nodes across the board.</p> <p>Important</p> <p>Resampling has a caveat you need to be aware of: nodes are not merely moved around to match the desired resolution - they are regenerated from scratch. As consequence, the original node IDs are - with a few exceptions - all gone.</p>"},{"location":"generated/gallery/2_morpho/tutorial_morpho_00_manipulate/#smoothing","title":"Smoothing","text":"<p>Smoothing is one of those things that work on all neurons but the approaches are so vastly different that there are separate functions: <code>navis.smooth_skeleton</code>, <code>navis.smooth_mesh</code> and <code>navis.smooth_voxels</code>:</p> <pre><code># smooth_skeleton uses a rolling window along the linear segments\nsk = navis.example_neurons(n=1, kind=\"skeleton\")\nsk_smoothed = navis.smooth_skeleton(sk, window=5, inplace=False)\n</code></pre> <pre><code># smooth_mesh uses a iterative rounds of Laplacian smoothing\nme = navis.example_neurons(n=1, kind=\"mesh\")\nme_smoothed = navis.smooth_mesh(me, iterations=5, inplace=False)\n</code></pre>"},{"location":"generated/gallery/2_morpho/tutorial_morpho_00_manipulate/#cutting-pruning","title":"Cutting &amp; Pruning","text":"<p>Cutting and pruning work best if there is a sense of topology which implicitly requires a skeleton. Many functions will also work on MeshNeurons though. That's because the operation is performed on their skeleton and changes are propagated back to the mesh. Fair warning though: this may not be perfect (e.g. the resulting mesh might not be watertight) - should be good enough for a first pass though!</p> <p>Let's start with something easy: cutting a skeleton in two at a given node.</p> <pre><code># Load the neuron\nn = navis.example_neurons(1, kind=\"skeleton\")\n\n# Pick a node ID\ncut_node_id = n.nodes.node_id.values[333]\ndistal, proximal = navis.cut_skeleton(n, cut_node_id)\n</code></pre> <p>Plot the two fragments:</p> <pre><code># Note that we are using method='2d' here because that makes annotating the plot easier\nfig, ax = distal.plot2d(color=\"cyan\", method=\"2d\", view=(\"x\", \"-z\"))\nfig, ax = proximal.plot2d(color=\"green\", ax=ax, method=\"2d\", view=(\"x\", \"-z\"))\n\n# Annotate cut point\ncut_coords = distal.nodes.set_index(\"node_id\").loc[distal.root, [\"x\", \"z\"]].values[0]\nax.annotate(\n    \"cut point\",\n    xy=(cut_coords[0], -cut_coords[1]),\n    color=\"lightgrey\",\n    xytext=(cut_coords[0], -cut_coords[1] - 2000),\n    va=\"center\",\n    ha=\"center\",\n    arrowprops=dict(shrink=0.1, width=2, color=\"lightgrey\"),\n)\n\nplt.tight_layout()\n</code></pre> <p></p> <p>If instead of a node ID, you have an x/y/z coordinate where you want to cut: use the <code>.snap</code> method to find the closest node to that location:</p> <pre><code>node_id, dist = n.snap([14000, 16200, 12000])\nprint(f\"Closest node: {node_id} at distance {dist * n.units:.2f} {n.units.units}\")\n</code></pre> <p>Out:</p> <pre><code>Closest node: 334 at distance 565.69 nanometer nanometer\n</code></pre> <p>Instead of cutting a neuron in two, we can also just prune bits off:</p> <pre><code>n_pruned = n.prune_distal_to(cut_node_id, inplace=False)\n\ncut_coords = n.nodes.set_index(\"node_id\").loc[cut_node_id, [\"x\", \"z\"]].values\n\n# Plot original neuron in red and with dotted line\nfig, ax = n.plot2d(color=\"red\", method=\"2d\", linestyle=(0, (5, 10)), view=(\"x\", \"-z\"))\n\n# Plot remaining neurites in red\nfig, ax = n_pruned.plot2d(color=\"green\", method=\"2d\", ax=ax, view=(\"x\", \"-z\"), lw=1.2)\n\n# Annotate cut point\nax.annotate(\n    \"cut point\",\n    xy=(cut_coords[0], -cut_coords[1]),\n    color=\"lightgrey\",\n    xytext=(cut_coords[0], -cut_coords[1] - 2000),\n    va=\"center\",\n    ha=\"center\",\n    arrowprops=dict(shrink=0.1, width=2, color=\"lightgrey\"),\n)\n\nplt.tight_layout()\n</code></pre> <p></p> <p><code>navis.cut_skeleton</code> also takes multiple cut nodes, in case you want to chop your neuron into multiple pieces.</p> <p>As an (extreme) example, let's cut a neuron at every single branch point:</p> <pre><code>n = navis.example_neurons(1, kind=\"skeleton\")\n\nbranch_points = n.nodes[n.nodes.type == \"branch\"].node_id.values\n\ncut = navis.cut_skeleton(n, branch_points)\ncut.head()\n</code></pre> type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron DA1_lPN_R 1734350788 4 16 0 2 373.565735 None 8 nanometer 2026-03-01 17:22:43.059897 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 1 navis.TreeNeuron DA1_lPN_R 1734350788 5 10 0 2 431.518494 None 8 nanometer 2026-03-01 17:22:43.059897 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 2 navis.TreeNeuron DA1_lPN_R 1734350788 6 12 0 2 388.590637 None 8 nanometer 2026-03-01 17:22:43.059897 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 3 navis.TreeNeuron DA1_lPN_R 1734350788 8 22 0 2 665.534912 None 8 nanometer 2026-03-01 17:22:43.059897 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 4 navis.TreeNeuron DA1_lPN_R 1734350788 28 8 0 2 1534.385742 None 8 nanometer 2026-03-01 17:22:43.059897 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc <pre><code># Plot neuron fragments\nfig, ax = navis.plot2d(cut, linewidth=1.5, view=(\"x\", \"-z\"))\n\nplt.tight_layout()\n</code></pre> <p></p> <p>Out:</p> <pre><code>Plot neurons:   0%|          | 0/600 [00:00&lt;?, ?it/s]\n\nPlot neurons:  31%|###1      | 187/600 [00:00&lt;00:00, 1861.26it/s]\n\nPlot neurons:  62%|######2   | 374/600 [00:00&lt;00:00, 1858.53it/s]\n\nPlot neurons:  93%|#########3| 560/600 [00:00&lt;00:00, 1844.20it/s]\n</code></pre> <p>Let's try something more sophisticated: pruning by Strahler index:</p> <pre><code># Load a fresh skeleton\nn = navis.example_neurons(1, kind=\"skeleton\")\n\n# Reroot to soma\nn = n.reroot(n.soma)\n\n# This will prune off terminal branches (the lowest two Strahler indices)\nn_pruned = n.prune_by_strahler(to_prune=[1, 2], inplace=False)\n\n# Plot original neurons in red\nfig, ax = n.plot2d(color=\"red\", view=('x', '-z'))\n\n# Plot remaining neurites in green\nfig, ax = n_pruned.plot2d(color=\"green\", ax=ax, linewidth=1, view=(\"x\", \"-z\"))\n\nplt.tight_layout()\n</code></pre> <p></p> <p>We can also turn this around and remove only the higher order branches. Let's use this example to show that we can also do this with <code>MeshNeurons</code>:</p> <p>Load an example mesh neuron</p> <pre><code>m = navis.example_neurons(1, kind=\"mesh\")\n\n# This will prune to the just terminal branches\nm_pruned = navis.prune_by_strahler(m, to_prune=range(3, 100), inplace=False)\n\n# Plot original neuron in cyan\nfig, ax = m.plot2d(color=\"cyan\", figsize=(10, 10), view=(\"x\", \"-z\"))\n\n# Plot remaining neurites red\nfig, ax = m_pruned.plot2d(color=\"red\", ax=ax, view=(\"x\", \"-z\"))\n\nplt.tight_layout()\n</code></pre> <p></p> <p>Alternatively, we can prune terminal branches based on size:</p> <pre><code># This will prune all branches smaller than 10 microns\nm_pruned = navis.prune_twigs(m, size=\"10 microns\", inplace=False)\n\n# Plot original neuron in red\nfig, ax = m.plot2d(color=\"red\", figsize=(10, 10), view=(\"x\", \"-z\"))\n\n# Plot remaining neurites in cyan\nfig, ax = m_pruned.plot2d(\n    color=\"cyan\", ax=ax, linewidth=0.75, alpha=0.5, view=(\"x\", \"-z\")\n)\n\nplt.tight_layout()\n</code></pre> <p></p>"},{"location":"generated/gallery/2_morpho/tutorial_morpho_00_manipulate/#intersecting-with-volumes","title":"Intersecting with Volumes","text":"<p>We can also intersect neurons with <code>navis.Volume</code> (and <code>trimesh.Trimesh</code> for that matter). This is useful if you want to e.g. subset a neuron to a certain brain region. Let's see how this works:</p> <p>Load an example navis.Volume</p> <pre><code>lh = navis.example_volume(\"LH\")\n\n# Prune by volume\nm_lh = navis.in_volume(m, lh, inplace=False)\nm_outside_lh = navis.in_volume(m, lh, mode=\"OUT\", inplace=False)\n</code></pre> <p>And plot!</p> <p>Plot pruned branchs neuron in green</p> <pre><code>fig, ax = navis.plot2d(\n    [m_lh, m_outside_lh, lh], color=[\"red\", \"green\"], figsize=(10, 10), view=(\"x\", \"-z\")\n)\n\nplt.tight_layout()\n</code></pre> <p></p> <p>Does this work with all neuron types? There is no simple answer unfortunately. In theory, anything that works on skeletons should also work on meshes, and vice versa. However, <code>navis.Dotprops</code> <code>navis.VoxelNeuron</code> are so fundamentally different that certain operations just don't make sense. For example we can't cut them but we can subset them to a given volume. Check out the I/O API reference docs for an overview of what works with which neuron type.</p> <pre><code># Note that [`navis.in_volume`][] also works with arbitrary spatial data (i.e. `(N, 3)` arrays of x/y/z locations):\n</code></pre> <p>Get the connectors for one of our above skeletons</p> <pre><code>cn = sk.connectors\n\n# Add a column that tells us which connectors are in the LH volume\ncn[\"in_lh\"] = navis.in_volume(cn[[\"x\", \"y\", \"z\"]].values, lh)\ncn.head()\n</code></pre> connector_id node_id type x y z roi confidence in_lh 0 0 1436 pre 6444 21608 14516 LH(R) 0.959 True 1 1 1436 pre 6457 21634 14474 LH(R) 0.997 True 2 2 2638 pre 4728 23538 14179 LH(R) 0.886 True 3 3 1441 pre 5296 22059 16048 LH(R) 0.967 True 4 4 1872 pre 4838 23114 15146 LH(R) 0.990 True <p>Count the number of connectors (pre and post) in- and outside the LH:</p> <pre><code>cn.groupby([\"type\", \"in_lh\"]).size()\n</code></pre> <p>Out:</p> <pre><code>type  in_lh\npost  False    1978\n      True      106\npre   False     325\n      True      296\ndtype: int64\n</code></pre> <p>About half the presynapses are in the LH (most of the rest will be in the MB calyx). The large majority of postsynapses are outside the LH in the antennal lobe where this neuron has its dendrites.</p> <p>That's it for now! Please see the NBLAST tutorial for morphological comparisons using NBLAST and the API reference for a full list of morphology-related functions.</p> <p>Total running time of the script: ( 0 minutes  13.943 seconds)</p> <p> Download Python source code: tutorial_morpho_00_manipulate.py</p> <p> Download Jupyter notebook: tutorial_morpho_00_manipulate.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/2_morpho/tutorial_morpho_01_analyze/","title":"Analyzing Neuron Morphology","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/2_morpho/tutorial_morpho_01_analyze/#analyzing-neuron-morphology","title":"Analyzing Neuron Morphology","text":"<p>This tutorial will give you an overview of how to analyze neuron morphology.</p> <p>Disclaimer: As you might imagine some properties can be gathered for all/most neuron types while others will only work on specific types. For example, topological properties such as cable length, branch points, etc. are easy to get for a skeleton (i.e. a <code>TreeNeuron</code>) but not for an image (i.e. a <code>VoxelNeuron</code>). Make sure to check the respective function's docstring for details!</p>"},{"location":"generated/gallery/2_morpho/tutorial_morpho_01_analyze/#basic-properties","title":"Basic Properties","text":"<p>NAVis provides some basic morphometrics as neuron properties. Others need to be computed using a specific function (e.g. <code>navis.tortuosity</code>) - mostly because they require/allow some parameters to be set.</p> <p>With that in mind, let's dive right in:</p> <pre><code>import navis\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nsns.set_color_codes(\"muted\")\n</code></pre> <p>Accessing attributes for a single neuron:</p> <pre><code># Load a single skeleton (i.e. a TreeNeuron)\nn = navis.example_neurons(n=1, kind=\"skeleton\")\nprint(f\"This neuron has {n.cable_length} of cable\")\n</code></pre> <p>Out:</p> <pre><code>This neuron has 266476.875 of cable\n</code></pre> <p>If your neuron has its <code>units</code> properties set, you can also combine those:</p> <pre><code>print(f\"This neuron has {(n.cable_length * n.units).to('microns')} of cable\")\n</code></pre> <p>Out:</p> <pre><code>This neuron has 2131.815185546875 micron of cable\n</code></pre> <p>Accessing the same properties via a <code>navis.NeuronList</code> will return an array:</p> <pre><code>nl = navis.example_neurons(kind=\"skeleton\")\nprint(f\"Cable lengths for neurons in this list:\\n{nl.cable_length}\")\n</code></pre> <p>Out:</p> <pre><code>Cable lengths for neurons in this list:\n[266476.88 304332.66 274703.38 286522.47 291265.3 ]\n</code></pre> <p><code>navis.NeuronList.summary</code> is a useful way to collect some of the basic parameters:</p> <pre><code>df = nl.summary()\ndf.head()\n</code></pre> type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron DA1_lPN_R 1734350788 4465 2705 599 618 266476.87500 4177.0 8 nanometer 2026-03-01 17:22:55.477254 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 1 navis.TreeNeuron DA1_lPN_R 1734350908 4847 3042 735 761 304332.65625 6.0 8 nanometer 2026-03-01 17:22:55.483911 /home/runner/work/navis/navis/navis/data/swc/1... 1734350908.swc 2 navis.TreeNeuron DA1_lPN_R 722817260 4332 3136 633 656 274703.37500 NaN 8 nanometer 2026-03-01 17:22:55.490952 /home/runner/work/navis/navis/navis/data/swc/7... 722817260.swc 3 navis.TreeNeuron DA1_lPN_R 754534424 4696 3010 696 726 286522.46875 4.0 8 nanometer 2026-03-01 17:22:55.497327 /home/runner/work/navis/navis/navis/data/swc/7... 754534424.swc 4 navis.TreeNeuron DA1_lPN_R 754538881 4881 2943 626 642 291265.31250 701.0 8 nanometer 2026-03-01 17:22:55.503985 /home/runner/work/navis/navis/navis/data/swc/7... 754538881.swc <p>For <code>MeshNeurons</code> the available properties look different. For example, you can get its volume:</p> <pre><code># Load a single MeshNeuron\nm = navis.example_neurons(n=1, kind=\"mesh\")\nprint(f\"Neuron volume: {m.volume}\")\n\n# Again, we can use the `units` property to convert:\nprint(f\"Neuron volume: {(m.volume * n.units **3).to('microns ** 3')}\")\n</code></pre> <p>Out:</p> <pre><code>Neuron volume: 1291610825.1668386\nNeuron volume: 661.3047424854216 micron ** 3\n</code></pre> <p>For topological properties, we need to convert to skeleton. The fastest way is to simply access the <code>MeshNeuron</code>'s <code>.skeleton</code> property:</p> <pre><code>print(f\"Mesh cable length: {m.skeleton.cable_length * m.units}\")\n</code></pre> <p>Out:</p> <pre><code>Mesh cable length: 1351124.75 nanometer\n</code></pre> <p>Note</p> <p>The above <code>.skeleton</code> property is simply an automatically generated <code>TreeNeuron</code> representation of the mesh. It uses sensible defaults but as said initially: it's good practice to create and check the skeleton yourself via <code>navis.skeletonize</code>.</p> <p>Importantly, some NAVis functions (e.g. <code>navis.segment_analysis</code>, see below) that accept <code>MeshNeurons</code> as input, really use this skeleton representation under-the-hood.</p> <p>The skeleton representation of the mesh lets us access many toplogical properties:</p> <pre><code>m.skeleton.n_leafs\n</code></pre> <p>Out:</p> <pre><code>155\n</code></pre> <pre><code>m.skeleton.n_branches\n</code></pre> <p>Out:</p> <pre><code>122\n</code></pre> <p>You may have already noticed here and in other examples the use of <code>n_{property}</code> (e.g. <code>n.n_leafs</code>). These are in fact generic: you can use any <code>n_...</code> and - assuming that property exists - NAVis will return a count:</p> <pre><code>m.n_vertices\n</code></pre> <p>Out:</p> <pre><code>6309\n</code></pre> <pre><code>m.skeleton.n_nodes\n</code></pre> <p>Out:</p> <pre><code>1058\n</code></pre> <pre><code># Illustrate with a random property\nm.my_counts = [1, 2, 3, 4, 5]\nm.n_my_counts\n</code></pre> <p>Out:</p> <pre><code>5\n</code></pre>"},{"location":"generated/gallery/2_morpho/tutorial_morpho_01_analyze/#segment-analysis","title":"Segment Analysis","text":"<p><code>navis.segment_analysis</code> is a great entry point for collecting a bunch of morphometrics for your neuron(s) of interest. It returns Strahler index, cable length, distance to root, radius and tortuosity for each linear segment:</p> <pre><code>sa = navis.segment_analysis(m)\nsa.head()\n</code></pre> length tortuosity root_dist strahler_index radius_mean radius_min radius_max volume 0 58.374999 1.025037 54367.973139 1 46.284119 3.699945 127.752521 8.407469e+05 1 77.956944 1.002647 52057.294094 1 55.333697 10.817280 103.455875 1.250778e+06 2 52.101703 1.189041 53665.977915 1 43.073208 3.699880 118.119985 6.467733e+05 3 52.914108 1.423845 52295.484201 1 61.993531 13.064078 115.277677 9.208421e+05 4 974.176120 1.317796 54018.878403 1 64.802659 3.266119 93.903433 2.154245e+07 <pre><code># See if segment length correlates with radius\nax = sns.scatterplot(\n    data=sa, x=\"length\", y=\"radius_mean\", size=\"strahler_index\", alpha=0.7\n)\nax.set_xscale(\"log\")\nax.set_yscale(\"log\")\n</code></pre> <p></p>"},{"location":"generated/gallery/2_morpho/tutorial_morpho_01_analyze/#sholl-analysis","title":"Sholl Analysis","text":"<p>For an example of a Sholl analyses, check out the MICrONS tutorial.</p>"},{"location":"generated/gallery/2_morpho/tutorial_morpho_01_analyze/#geodesic-distances","title":"Geodesic Distances","text":"<p>Working with Euclidean distances is straight forward and we won't cover this extensively but here is an example where we are measuring the average distances between a node and its parent (= the sampling rate):</p> <pre><code>import numpy as np\n\n# Get nodes but remove the root (has no parent)\nnodes = nl[0].nodes[nl[0].nodes.parent_id &gt; 0]\n\n# Get the x/y/z coordinates of all nodes (except root)\nnode_locs = nodes[[\"x\", \"y\", \"z\"]].values\n\n# For each node, get its parent's location\nparent_locs = (\n    nl[0].nodes.set_index(\"node_id\").loc[nodes.parent_id.values, [\"x\", \"y\", \"z\"]].values\n)\n\n# Calculate Euclidean distances\ndistances = np.sqrt(np.sum((node_locs - parent_locs) ** 2, axis=1))\n\n# Use the neuron's units to convert into nm\ndistances = distances * nl[0].units\n\nprint(\n    f\"Mean distance between nodes: {np.mean(distances):.2f} (+/- {np.std(distances):.2f})\"\n)\n</code></pre> <p>Out:</p> <pre><code>Mean distance between nodes: 477.56 nanometer (+/- 361.10 nanometer)\n</code></pre> <p>What if you wanted to know the distance between the soma and all terminal nodes? In that case Euclidean distance would be insufficient as the neuron is not a straight line. Instead, you need the geodesic, the \"along-the-arbor\" distance.</p> <p>NAVis comes with a couple functions that help you get geodesic distances. For single node-to-node queries, <code>navis.dist_between</code> should be sufficient:</p> <pre><code>n = nl[0]\n\nend = n.nodes[n.nodes.type == \"end\"].node_id.values[0]\n\nd_geo = navis.dist_between(n, n.soma, end) * n.units\n\nprint(f\"Euclidean distance between soma and terminal node {end}: {d_geo:.2f}\")\n</code></pre> <p>Out:</p> <pre><code>Euclidean distance between soma and terminal node 465: 444096.17 nanometer\n</code></pre> <p>Let's visualize this:</p> <pre><code>import networkx as nx\n\n# First we need to find the path between the soma and the terminal node\npath = nx.shortest_path(n.graph.to_undirected(), n.soma, end)\n\n# Get coordinates for the path\npath_co = n.nodes.set_index(\"node_id\").loc[path, [\"x\", \"y\", \"z\"]].copy()\n\n# Add a small offset\npath_co.x += 500\npath_co.y -= 500\n\n# Plot neuron\nfig, ax = navis.plot2d(n, c=\"blue\", method=\"2d\", view=(\"x\", \"-z\"))\n\n# Add geodesic path\nax.plot(path_co.x, path_co.z, c=\"r\", ls=\"--\")\n\n# Add Euclidean path\nend_loc = n.nodes.set_index(\"node_id\").loc[end, [\"x\", \"y\", \"z\"]]\nsoma_loc = n.nodes.set_index(\"node_id\").loc[n.soma, [\"x\", \"y\", \"z\"]]\nax.plot([soma_loc.x, end_loc.x], [soma_loc.z, end_loc.z], c=\"g\", ls=\"--\")\n\nd_eucl = np.sqrt(np.sum((end_loc - soma_loc) ** 2)) * n.units\n\n# Annotate distances\n_ = ax.text(\n    x=0.1,\n    y=0.3,\n    s=f\"Euclidean distance:\\n{d_eucl.to_compact():.0f}\",\n    transform=ax.transAxes,\n    c=\"g\",\n)\n_ = ax.text(\n    x=0.85,\n    y=0.5,\n    s=f\"Geodesic distance:\\n{d_geo.to_compact():.0f}\",\n    transform=ax.transAxes,\n    c=\"r\",\n    ha=\"right\",\n)\n</code></pre> <p></p> <p>If you want to look at geodesic distances across many nodes, you should consider using <code>navis.geodesic_matrix</code>. To demonstrate, let's generate a geodesic distance matrix between all terminal nodes:</p> <pre><code># Calculate distances from all end nodes to all other nodes\nends = n.nodes[n.nodes.type == \"end\"].node_id.values\nm = navis.geodesic_matrix(n, from_=ends)\n\n# Subset to only end-nodes-to-end_nodes\nm = m.loc[ends, ends]\n\nm.head()\n</code></pre> 465 548 618 683 745 789 832 872 911 949 987 1024 1060 1095 1124 1153 1181 1209 1237 1264 1290 1316 1341 1366 1390 1413 1436 1458 1480 1502 1523 1544 1565 1586 1606 1626 1646 1666 1686 1706 ... 4426 4427 4428 4429 4430 4431 4432 4433 4434 4435 4436 4437 4438 4439 4440 4441 4442 4443 4444 4445 4446 4447 4448 4449 4450 4451 4452 4453 4454 4455 4456 4457 4458 4459 4460 4461 4462 4463 4464 4465 465 0.000000 54395.710938 53556.410156 54489.730469 53685.773438 52679.988281 53139.886719 20944.503906 53065.277344 53873.636719 53739.351562 6767.158691 52967.976562 54020.867188 21164.632812 52185.437500 53729.808594 53460.156250 53023.218750 54051.710938 52663.625000 52178.289062 54002.683594 53184.109375 52079.453125 6315.998535 7808.719727 5656.241211 52686.710938 52757.839844 53003.402344 52983.812500 52802.113281 51182.589844 53735.335938 52967.062500 53235.906250 52307.964844 52413.679688 52355.277344 ... 52492.812500 52492.582031 52813.570312 52441.597656 52303.640625 52297.691406 52312.011719 52208.511719 52498.492188 51904.695312 51867.292969 51785.125000 52274.492188 51760.933594 51623.726562 51596.890625 52182.871094 51514.609375 51560.792969 51640.226562 51751.468750 51603.113281 51434.828125 51565.636719 51212.378906 51195.531250 50818.183594 50537.148438 50555.062500 50795.402344 50402.964844 50363.980469 49936.093750 49895.074219 55185.277344 54742.765625 55158.507812 55950.523438 56111.792969 55266.167969 548 54395.710938 0.000000 8980.969727 10787.025391 6228.693848 6866.540039 8564.446289 37619.902344 8489.836914 6416.556641 10036.644531 53636.648438 5732.234375 10318.162109 40115.664062 5461.810547 6272.729492 6003.076172 8447.777344 10349.005859 8960.916992 7602.846680 6545.604492 3736.930176 8376.746094 51252.976562 49521.375000 52525.734375 8111.272461 9055.134766 8427.961914 8408.371094 8226.673828 6607.148438 2236.750977 5509.983398 5778.826172 5072.226562 8710.974609 7779.836914 ... 7917.371094 7917.141602 9110.863281 7866.156250 7728.200195 4840.611816 4854.934570 4751.431641 8795.787109 8201.990234 2811.261230 4549.386719 8571.788086 7185.492188 4387.987305 7021.448242 8480.166016 3128.714600 4103.715332 7064.787109 7176.027832 7900.406250 6859.389648 7862.931641 7509.673828 7492.823242 5004.737305 4116.092285 4309.074707 7092.696289 4589.518066 5788.540527 5360.653809 5319.634766 12127.771484 11685.260742 12101.000977 12893.017578 13054.287109 12208.661133 618 53556.410156 8980.969727 0.000000 9947.724609 8271.031250 7265.245117 4698.465332 36780.601562 3951.851074 8458.894531 9197.342773 52797.347656 7553.233398 9478.861328 39276.363281 6770.696777 8315.067383 8045.413086 3299.611572 9509.704102 8121.616211 3736.865967 8587.941406 7769.369141 7537.445312 50413.675781 48682.074219 51686.433594 3573.287109 8215.833008 4561.980957 4542.390625 3688.688477 4879.403809 8320.592773 7552.320801 7821.163086 6893.224609 7871.673828 3913.856445 ... 4051.390625 4051.160645 8271.562500 2717.991211 3862.219238 6882.949219 6897.271973 6793.769531 7956.486328 7362.688965 6452.550781 6370.384766 7732.486816 3319.511719 6208.986328 3623.442871 7640.864258 6099.867676 6146.052734 3198.806152 3310.047363 7061.105469 2686.695557 7023.630371 6670.372559 6653.521973 5403.443359 5122.408691 5140.319824 6253.395020 4988.223633 4060.795410 3828.188232 3866.356201 11288.469727 10845.958984 11261.699219 12053.715820 12214.984375 11369.359375 683 54489.730469 10787.025391 9947.724609 0.000000 10077.087891 9071.301758 9531.201172 37713.921875 9456.591797 10264.951172 5057.882812 53730.667969 9359.289062 3253.420898 40209.683594 8576.752930 10121.123047 9851.469727 9414.532227 3284.263672 4781.085938 8569.601562 10393.998047 9575.425781 4946.421387 51346.996094 49615.394531 52619.753906 9078.027344 3605.263184 9394.716797 9375.126953 9193.429688 7573.903809 10126.649414 9358.376953 9627.219727 8699.281250 3732.213379 8746.591797 ... 8884.126953 8883.896484 4132.102051 8832.912109 8694.955078 8689.005859 8703.328125 8599.826172 2517.971680 4022.159180 8258.607422 8176.441406 3593.026855 8152.247559 8015.042480 7988.203613 3501.404297 7905.923828 7952.109375 8031.541992 8142.783203 4218.177246 7826.145020 3149.859375 3827.444580 3810.593994 7209.499512 6928.464844 6946.375977 3987.779053 6794.280273 6755.295898 6327.409180 6286.390137 12221.791992 11779.281250 12195.021484 12987.038086 13148.306641 12302.681641 745 53685.773438 6228.693848 8271.031250 10077.087891 0.000000 6156.601562 7854.507812 36909.964844 7779.898438 4485.483398 9326.706055 52926.710938 5022.296875 9608.224609 39405.726562 4751.872070 2609.798096 4072.002441 7737.838867 9639.067383 8250.979492 6892.909180 4614.530762 5017.093750 7666.808594 50543.039062 48811.437500 51815.796875 7401.334473 8345.196289 7718.023926 7698.433105 7516.735840 5897.210449 5568.317871 3578.909668 3847.752197 4362.288086 8001.037109 7069.899414 ... 7207.433594 7207.203125 8400.925781 7156.218750 7018.261719 2909.538086 2625.473145 2820.357910 8085.849609 7492.052246 3700.275391 3839.448486 7861.850098 6475.554199 3678.049561 6311.510254 7770.227539 3347.592285 2327.137207 6354.848633 6466.089844 7190.468750 6149.451660 7152.993652 6799.735840 6782.885254 4294.799805 3406.154053 3599.136719 6382.758301 3879.580078 5078.602539 4650.715820 4609.696777 11417.833008 10975.322266 11391.062500 12183.079102 12344.347656 11498.722656 <p>5 rows \u00d7 618 columns</p> <p>Let's see if we can visualize any clusters using a <code>seaborn</code> clustermap:</p> <pre><code>import seaborn as sns\n\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.spatial.distance import squareform\n\n# Generate a linkage from the distances\nZ = linkage(squareform(m, checks=False), method=\"ward\")\n\n# Plot\ncm = sns.clustermap(m, cmap=\"Greys\", col_linkage=Z, row_linkage=Z)\n\ncm.ax_heatmap.set_xticks([])\ncm.ax_heatmap.set_yticks([])\n</code></pre> <p></p> <p>Out:</p> <pre><code>[]\n</code></pre> <p>As you can see in the heatmap, the dendrites and the axon nicely separate.</p> <p>That's it for now! Please see the NBLAST tutorial for morphological comparisons using NBLAST and the API reference for a full list of morphology-related functions.</p> <p>Total running time of the script: ( 0 minutes  1.981 seconds)</p> <p> Download Python source code: tutorial_morpho_01_analyze.py</p> <p> Download Jupyter notebook: tutorial_morpho_01_analyze.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/3_interfaces/mg_execution_times/","title":"Computation times","text":""},{"location":"generated/gallery/3_interfaces/mg_execution_times/#computation-times","title":"Computation times","text":"<p>04:46.996 total execution time for generated_gallery_3_interfaces files:</p> <p>+--------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_interfaces_01_neuron2 (docs/examples/3_interfaces/tutorial_interfaces_01_neuron2.py) | 04:37.887 | 0.0 MB | +--------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_interfaces_00_neuron (docs/examples/3_interfaces/tutorial_interfaces_00_neuron.py)    | 00:09.109 | 0.0 MB | +--------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_interfaces_02_blender (docs/examples/3_interfaces/tutorial_interfaces_02_blender.py) | 00:00.000 | 0.0 MB | +--------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+</p>"},{"location":"generated/gallery/3_interfaces/tutorial_interfaces_00_neuron/","title":"NEURON simulator","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/3_interfaces/tutorial_interfaces_00_neuron/#neuron-simulator","title":"NEURON simulator","text":"<p>This tutorial will show you how to simulate neurons and networks thereof using the NEURON simulator.</p> <p>NEURON is a simulation environment to model neurons and networks thereof. <code>NEURON</code> itself is rather complex (neurons are complex things after all) and fairly low-level which results in lots of boiler plate code. There are some libraries (e.g. NetPyNE) that wrap <code>NEURON</code> and provide a higher-level interface to facilitate building models. In my experience these are typically geared towards creating models based on probabilities (e.g. \"create 100 neurons with a 10% chance to be connected to another neuron\") rather than the well defined morphology/connectivity you get from e.g. connectomes.</p> <p>NAVis does not try to emulate a full simulation suite but tries to fill a gap for people wanting to use non-probabalistic data (e.g. from connectomics) by providing an entry point for you to get started with some simple models and take it from there. At this point there are two types of models:</p> <ol> <li><code>CompartmentModel</code> for modeling individual neurons</li> <li><code>PointNetwork</code> for modeling networks from point proceses</li> </ol>"},{"location":"generated/gallery/3_interfaces/tutorial_interfaces_00_neuron/#compartment-models","title":"Compartment models","text":"<p>A <code>CompartmentModel</code> represents a single neuron (although you can connect multiple of these neurons) and is constructed from a skeleton (i.e. <code>navis.TreeNeuron</code>, see also <code>navis.conversion.mesh2skeleton</code>).</p> <pre><code>import navis\nimport neuron\n\nimport navis.interfaces.neuron as nrn\n\n# Load one of the example neurons (a Drosophila projection neuron from the hemibrain connectome)\n# Note the conversion to microns!\nn = navis.example_neurons(1).convert_units(\"um\")\n\n# Here we manually corrected the soma\nn.soma = 20\n\n# Reroot to the soma\nn.reroot(n.soma, inplace=True)\n\n# Create the compartment model\ncmp = nrn.CompartmentModel(n, res=10)\n</code></pre> <p><code>NEURON</code> compartment models are effectively collections of connected linear segments. Each of these segments can have its own set of properties and mechanisms. Here, we will use some biophysical properties from Tobin et al. (2017):</p> <pre><code># Set the specific axial resistivity for the entire neuron in Ohm cm\ncmp.Ra = 266.1\n\n# Set the specific membran capacitance in mF / cm**2\ncmp.cm = 0.8\n\n# Add passive membran properties for the entire neuron\ncmp.insert(\n    \"pas\",\n    g=1\n    / 20800,  # specific leakage conductance = 1/Rm; Rm = specific membran resistance in Ohm cm**2\n    e=-60,  # leakage reverse potential\n)\n</code></pre> <p>At this point we already have a passive model of our Drosophila projection neuron. Next, we will determine what's axon and what's dendrite, and add a Hodgkins &amp; Huxley mechanism so we have some actual action potentials to observe.</p> <p>This will also illustrate one of the features of <code>CompartmentModels</code>: we keep a correspondence to the original skeleton via node (and connector) IDs. This allows you to modify, stimulate at or record from parts of the compartment model by using node IDs.</p> <pre><code># Label axon/dendrite\nnavis.split_axon_dendrite(n, label_only=True, cellbodyfiber=\"soma\")\n\nn.nodes.head()\n</code></pre> node_id label x y z radius parent_id type compartment 0 1 0 126.272003 298.000000 224.496017 0.080000 2 end cellbodyfiber 1 2 0 126.112007 297.840027 224.656006 0.146274 3 slab cellbodyfiber 2 3 0 125.952003 297.520020 224.976013 0.277771 4 slab cellbodyfiber 3 4 0 125.952003 297.200012 225.616013 0.277771 5 slab cellbodyfiber 4 5 0 125.632004 297.040009 225.936005 0.277771 6 slab cellbodyfiber <p>A quick visualization</p> <pre><code>import matplotlib.pyplot as plt\n\nfig, ax = navis.plot2d(\n    n, color_by=\"compartment\", palette=\"tab10\", lw=1.5, method=\"3d\", view=(\"x\", \"-z\")\n)\n\n# Label compartments\nfor l in n.nodes.compartment.unique():\n    loc = n.nodes.loc[n.nodes.compartment == l, [\"x\", \"y\", \"z\"]].values[-1]\n    ax.text(loc[0] + 10, loc[1], loc[2], l)\n\nplt.tight_layout()\n</code></pre> <p></p> <p>Note</p> <p>A little excursion about how <code>NEURON</code> represents neurons before we add the Hodgkins &amp; Huxley (HH) mechanism: neurons consist of linear \"sections\" while individual (continuous) positions along each section are called \"segments\". That distinction is important because our skeleton nodes correspond to segments but a mechanism such as HH is applied for entire sections.</p> <pre><code># Collect axon nodes\naxon_nodes = n.nodes.loc[n.nodes.compartment.isin([\"axon\", \"linker\"]), \"node_id\"].values\n\n# Get the sections for the given nodes\naxon_secs = list(set(cmp.get_node_section(axon_nodes)))\n\n# Insert HH mechanism at the given sections\ncmp.insert(\"hh\", subset=axon_secs)\n</code></pre> <p>Next we actually have to do something with our compartment model. For that we will add three voltage recorders: one at the soma, one at the base of the dendrites and one at the tip of the axon.</p> <p>Recordings and stimulations work via segments (as opposed to sections for mechanisms) -&gt; hence we can use node IDs directly here</p> <p>Let's determine the tip of the axon and base of the dendrites programmatically using the geodesic distance:</p> <pre><code>dists = navis.geodesic_matrix(n, from_=n.soma)\n\n# Sort by distance from soma\ndists = dists.iloc[0].sort_values()\n\ndists.head(10)\n</code></pre> <p>Out:</p> <pre><code>20      0.000000\n21      0.455753\n19      0.633259\n2409    0.836538\n22      1.095768\n18      1.113257\n23      1.415775\n2410    1.474030\n24      1.692905\n17      2.032377\nName: 20, dtype: float32\n</code></pre> <pre><code># Find the closest \"dendrite\" and the most distal \"axon\" node\ndend = n.nodes[n.nodes.compartment == \"dendrite\"].node_id.values\ndend_base = dists.index[dists.index.isin(dend)][0]\nprint(f\"Node at the base of the dendrites: {dend_base}\")\n\naxo = n.nodes[n.nodes.compartment == \"axon\"].node_id.values\naxo_tip = dists.index[dists.index.isin(axo)][-1]\nprint(f\"Node at the tip of the axon: {axo_tip}\")\n</code></pre> <p>Out:</p> <pre><code>Node at the base of the dendrites: 467\nNode at the tip of the axon: 4384\n</code></pre> <pre><code># Add voltage recordings\ncmp.add_voltage_record(dend_base, label=\"dendrite_base\")\ncmp.add_voltage_record(axo_tip, label=\"axon_tip\")\ncmp.add_voltage_record(n.soma, label=\"soma\")\n</code></pre> <p>Last but not least we need to provide some input to our neuron otherwise it will just sit there doing nothing. We can add a current injection, trigger some synaptic currents or add a leak current. Let's simulate some synaptic inputs:</p> <pre><code># Get dendritic postsynapses\npost = n.postsynapses[n.postsynapses.compartment == \"dendrite\"]\npost.head()\n</code></pre> connector_id node_id type x y z roi confidence compartment 137 137 3736 post 135.232 291.104 210.840 AL(R) 0.906454 dendrite 138 138 635 post 132.872 284.448 196.056 AL(R) 0.928960 dendrite 139 139 4124 post 131.432 277.048 200.960 AL(R) 0.605084 dendrite 140 140 3176 post 124.984 286.568 216.480 AL(R) 0.922643 dendrite 219 219 3061 post 128.312 276.848 217.272 AL(R) 0.981077 dendrite <pre><code># Here we will open successively more synapses over 5 stimulations\nfor i in range(5):\n    # Onset for this stimulation\n    start = 50 + i * 200\n    # Number of synapses to activate\n    n_syn = i * 5\n    cmp.add_synaptic_current(\n        where=post.node_id.unique()[0:n_syn], start=start, max_syn_cond=0.1, rev_pot=-10\n    )\n</code></pre> <pre><code># Now we can run our simulation for 1000ms\n# (this is equivalent to neuron.h.finitialize + neuron.h.continuerun)\ncmp.run_simulation(1000, v_init=-60)\n</code></pre> <p>The compartment model has a quick &amp; dirty way of plotting the results:</p> <pre><code># Plot the results\naxes = cmp.plot_results()\n</code></pre> <p></p> <pre><code># Plot again and zoom in on one spike\naxes = cmp.plot_results()\naxes[0].set_xlim(240, 280)\n</code></pre> <p></p> <p>Out:</p> <pre><code>(240.0, 280.0)\n</code></pre> <p>As you can see we get a nice depolarization at the base of the dendrites which elicits an action potential that we can measure in the tips of the axon. Because in our model the cell body fiber (i.e. the neurite that connects the soma to the base of the dendrites) is passive, the depolarization of a single spike attenuates before it reaches the soma.</p> <p>Alternatively, you can access the recorded values directly like so:</p> <pre><code>cmp.records\n</code></pre> <p>Out:</p> <pre><code>{'v': {'dendrite_base': Vector[0], 'axon_tip': Vector[1], 'soma': Vector[2]}}\n</code></pre> <pre><code>cmp.records[\"v\"][\"dendrite_base\"].as_numpy()\n</code></pre> <p>Out:</p> <pre><code>array([-60.        , -60.05746836, -60.10429959, ..., -63.74603203,\n       -63.74603203, -63.74603203], shape=(40001,))\n</code></pre> <p>Next, let's try to simulate some noisy input where the presynaptic neuron spikes multiple times:</p> <pre><code># First we need to reset our model (by re-assigning `cmp` the old model will be garbage-collected)\ncmp = nrn.CompartmentModel(n, res=10)\n\n# Set properties and mechanisms\ncmp.Ra, cmp.cm = 266.1, 0.8\ncmp.insert(\"pas\", g=1 / 20800, e=-60)\naxon_secs = list(set(cmp.get_node_section(axon_nodes)))\ncmp.insert(\"hh\", subset=axon_secs)\n\n# Add recording\ncmp.add_voltage_record(dend_base, label=\"dendrite_base\")\ncmp.add_voltage_record(axo_tip, label=\"axon_tip\")\ncmp.add_voltage_record(n.soma, label=\"soma\")\n\n# Also add a spike counter at the axon\ncmp.add_spike_detector(axo_tip, label=\"axon_tip\")\n\n# Now add a noisy preinput to say 20 dendritic postsynapses\npost = n.connectors[\n    (n.connectors.compartment == \"dendrite\") &amp; (n.connectors.type == \"post\")\n]\ncmp.add_synaptic_input(\n    post.node_id.unique()[0:20],\n    spike_no=20,  # produce 20 presynaptic spikes\n    spike_int=50,  # with an average interval of 50ms: 20 * 50ms = over 1s\n    spike_noise=1,  # very noisy!\n    cn_weight=0.04,\n)\n# Run for 1s\ncmp.run_simulation(1000, v_init=-60)\n</code></pre> <pre><code># Plot results\naxes = cmp.plot_results()\naxes[1].set_ylabel(\"spikes [Hz]\")\n</code></pre> <p></p> <p>Out:</p> <pre><code>Text(94.06944444444443, 0.5, 'spikes [Hz]')\n</code></pre> <p>Note how we still don't see a depolarization in the soma? While that might be a genuine biological feature of this neuron, I suspect there is something wrong with the radii along the cell body fiber - perhaps a pinch point somewhere? This just illustrates that good skeletons are paramount and you should critically inspect the results of your models.</p> <p>Many methods in <code>CompartmentModel</code> try to use sensible defaults to make sure that you get some sort of effect. That said, it's advisable that you adjust parameters as you fit your model to real world data. Check the help to see what you can do:</p> <p>Try this for example </p><pre><code>help(cmp.add_synaptic_input)\n</code></pre><p></p>"},{"location":"generated/gallery/3_interfaces/tutorial_interfaces_00_neuron/#point-networks","title":"Point Networks","text":"<p>While you can link together multiple compartment models to simulate networks this quickly becomes prohibitively slow to run. For larger networks it can be sufficient to model each neuron as a single \"point process\". <code>PointNetwork</code> lets you quickly create such a network from an edge list.</p> <p>In this tutorial we will use toy data but it is just as straight forward to plugin real data:</p> <pre><code># First create a small 3 way network where one of the neurons (B) is inhibitory\nimport pandas as pd\n\nedges = pd.DataFrame([])\nedges[\"source\"] = [\"A\", \"B\"]\nedges[\"target\"] = [\"C\", \"C\"]\nedges[\"weight\"] = [0.5, -1]\nedges\n</code></pre> source target weight 0 A C 0.5 1 B C -1.0 <pre><code># Next initialize network from edge list\nnet = nrn.PointNetwork.from_edge_list(edges, model=\"IntFire4\")\nnet\n</code></pre> <p>Out:</p> <pre><code>PointNetwork&lt;neurons=3,edges=2&gt;\n</code></pre> <p>So far, our network won't do anything because it doesn't have any input. Let's add an input to neurons A and B, and try that out:</p> <pre><code># Add the stimulus\nnet.add_stimulus(\"A\", start=100, stop=1000, frequency=100, randomness=0)\nnet.add_stimulus(\"B\", start=600, stop=800, frequency=100, randomness=0)\n\n# Run simulation\nnet.run_simulation(1000)\n</code></pre> <pre><code># Plot\nax = net.plot_raster(backend=\"matplotlib\", label=True)\n\nax.set_xlim(0, 1000)\n</code></pre> <p></p> <p>Out:</p> <pre><code>(0.0, 1000.0)\n</code></pre> <p>This toy example worked quite well: spikes in neuron <code>A</code> elicit occasional spikes in <code>C</code> via temporal summation. Activity in the inhibitiory neuron <code>B</code> hyperpolarizes <code>C</code> and it stops firing until well after activity in <code>B</code> has ceased.</p> <p>The <code>NEURON</code> interface is a very recent addition and it might well change in the future (or become its own package). Functionality is also still limited and while I don't intend to write a feature-complete wrapper for <code>NEURON</code>, I do welcome feature requests or contributions on Github.</p>"},{"location":"generated/gallery/3_interfaces/tutorial_interfaces_00_neuron/#links","title":"Links","text":"<ul> <li>Model DB contains various published <code>NEURON</code> models and mechanisms</li> <li>NetPyNE wraps <code>NEURON</code> and provides high-level syntax to create models</li> </ul> <p>Total running time of the script: ( 0 minutes  9.109 seconds)</p> <p> Download Python source code: tutorial_interfaces_00_neuron.py</p> <p> Download Jupyter notebook: tutorial_interfaces_00_neuron.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/3_interfaces/tutorial_interfaces_01_neuron2/","title":"Visualize NEURON model","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/3_interfaces/tutorial_interfaces_01_neuron2/#visualize-neuron-model","title":"Visualize NEURON model","text":"<p>In this tutorial you will learn to visualize a compartment neuron model.</p> <p>We will jump right in, so please make sure to have a look at the introductory NEURON tutorial first.</p>"},{"location":"generated/gallery/3_interfaces/tutorial_interfaces_01_neuron2/#setup-the-model","title":"Setup the model","text":"<p>The setup will be similar to the previous tutorial: use one of the example neurons to create a compartment model:</p> <pre><code>import navis\nimport neuron\n\nimport navis.interfaces.neuron as nrn\n\n# Load one of the example neurons (a Drosophila projection neuron from the hemibrain connectome)\n# Note the conversion to microns!\nn = navis.example_neurons(1).convert_units(\"um\")\n\n# Here we manually corrected the soma\nn.soma = 20\n\n# Reroot to the soma\nn.reroot(n.soma, inplace=True)\n\n# Create the compartment model\ncmp = nrn.CompartmentModel(n, res=10)\n\n# Set the specific axial resistivity for the entire neuron in Ohm cm\ncmp.Ra = 266.1\n\n# Set the specific membran capacitance in mF / cm**2\ncmp.cm = 0.8\n\n# Add passive membran properties for the entire neuron\ncmp.insert(\n    \"pas\",\n    g=1\n    / 20800,  # specific leakage conductance = 1/Rm; Rm = specific membran resistance in Ohm cm**2\n    e=-60,  # leakage reverse potential\n)\n\n# Label axon/dendrite\nnavis.split_axon_dendrite(n, label_only=True, cellbodyfiber=\"soma\")\n\n# Collect axon nodes\naxon_nodes = n.nodes.loc[n.nodes.compartment.isin([\"axon\", \"linker\"]), \"node_id\"].values\n\n# Get the sections for the given nodes\naxon_secs = list(set(cmp.get_node_section(axon_nodes)))\n\n# Insert HH mechanism at the given sections\ncmp.insert(\"hh\", subset=axon_secs)\n</code></pre> <p>Next, we will add a voltage recording at every single node of the neuron.</p> <pre><code>cmp.add_voltage_record(n.nodes.node_id.values)\n</code></pre> <p>Last but not least, we will add a synaptic input at some dendritic postsynapses of the neuron.</p> <pre><code># Get dendritic postsynapses\npost = n.postsynapses[n.postsynapses.compartment == \"dendrite\"]\n\n# Add synaptic input to the first 10 postsynapses after 2 ms\ncmp.add_synaptic_current(where=post.node_id.unique()[0:10], start=2, max_syn_cond=0.1, rev_pot=-10)\n</code></pre> <p>Now we can run our simulation for 100ms</p> <pre><code># This is equivalent to neuron.h.finitialize + neuron.h.continuerun\ncmp.run_simulation(100, v_init=-60)\n</code></pre>"},{"location":"generated/gallery/3_interfaces/tutorial_interfaces_01_neuron2/#collect-the-data","title":"Collect the data","text":"<p>To visualize and animate, we will collect the results into a pandas DataFrame</p> <pre><code>import numpy as np\nimport pandas as pd\n\n# Collect the voltage recordings at each node\nrecords = pd.DataFrame(np.vstack([r.as_numpy() for r in cmp.records['v'].values()]), index=list(cmp.records['v'].keys()))\n\n# Reindex to make sure it matches the node table\nrecords = records.reindex(n.nodes.node_id)\n\nrecords.head()\n</code></pre> 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 ... 3961 3962 3963 3964 3965 3966 3967 3968 3969 3970 3971 3972 3973 3974 3975 3976 3977 3978 3979 3980 3981 3982 3983 3984 3985 3986 3987 3988 3989 3990 3991 3992 3993 3994 3995 3996 3997 3998 3999 4000 node_id 1 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 ... -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 2 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 ... -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 3 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 ... -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 4 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 ... -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 5 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 ... -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 -60.0 <p>5 rows \u00d7 4001 columns</p>"},{"location":"generated/gallery/3_interfaces/tutorial_interfaces_01_neuron2/#visualize","title":"Visualize","text":"<p>Let's first visualize a single snapshot of the neuron at time <code>t=5ms</code>:</p> <pre><code># The interval for each step is 0.025ms by default\nprint(neuron.h.dt)\n</code></pre> <p>Out:</p> <pre><code>0.025\n</code></pre> <p>Add a new column to the node table for time <code>t=5ms</code></p> <pre><code>n.nodes['v'] = records.loc[:, int(5 / 0.025)].values\n\n# Plot\nfig, ax = navis.plot2d(\n    n,\n    method=\"2d\",\n    color_by=\"v\",  # color by the voltage column\n    palette=\"viridis\",\n    vmin = -70,\n    vmax = 10,\n    view=('x', '-y')\n)\n\n# Manually add a colorbar\nimport matplotlib.pyplot as plt\nfrom matplotlib.cm import ScalarMappable\nsm = ScalarMappable(norm=plt.Normalize(vmin=-70, vmax=10), cmap='viridis')\n_ = fig.colorbar(sm, ax=ax, fraction=0.075, shrink=0.5, label=\"V\")\n</code></pre> <p></p>"},{"location":"generated/gallery/3_interfaces/tutorial_interfaces_01_neuron2/#animate","title":"Animate","text":"<p>One option to animate the voltage recordings over time is to use matplotlib's animation functionality. For that we have to do a bit of setup:</p> <pre><code># Convert our skeleton to a mesh for nicer visualization\nmesh = navis.conversion.tree2meshneuron(n, warn_missing_radii=False)\n\n# Plot the neuron\nfig, ax = navis.plot2d(mesh, method='2d',color='k', view=('x','-y'))\n\nsm = ScalarMappable(norm=plt.Normalize(vmin=-70, vmax=10), cmap='viridis')\n_ = fig.colorbar(sm, ax=ax, fraction=0.075, shrink=0.5, label=\"V\")\n\n# Add a text in the top right for the timestamp\nt = ax.text(0.02, 0.95, 'ms', ha='left', va='top', transform=ax.transAxes, color='r')\n\n# Get the collection representing our neuron\nc = ax.collections[0]\nc.set_cmap('viridis')\nc.set_norm(plt.Normalize(vmin=-70, vmax=10))\n\n# This function updates the voltages according to the frame\ndef animate(i):\n    # We need to map the voltages at individual nodes to faces in the mesh\n    # First nodes to vertices\n    vert_voltage = records[i].values[mesh.vertex_map]\n    # Then vertices to faces\n    face_voltage = vert_voltage[mesh.faces].mean(axis=1)\n    # Set the values\n    c.set_array(face_voltage)\n    # Also update the timestamp\n    t.set_text(f'{i * 0.025:.2f} ms')\n    return (c, t)\n\nimport matplotlib.animation as animation\nani = animation.FuncAnimation(fig, animate, interval=40, blit=True, repeat=True, frames=400)\n</code></pre> Once Loop Reflect <p>Total running time of the script: ( 4 minutes  37.887 seconds)</p> <p> Download Python source code: tutorial_interfaces_01_neuron2.py</p> <p> Download Jupyter notebook: tutorial_interfaces_01_neuron2.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/3_interfaces/tutorial_interfaces_02_blender/","title":"Blender 3D","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/3_interfaces/tutorial_interfaces_02_blender/#blender-3d","title":"Blender 3D","text":"<p>This tutorial shows how to use NAVis within Blender 3D.</p> <p>NAVis comes with an interface to import neurons into Blender 3D for high quality renderings and videos: <code>navis.interfaces.blender</code>.</p>"},{"location":"generated/gallery/3_interfaces/tutorial_interfaces_02_blender/#installation","title":"Installation","text":"<p>Blender comes with its own Python 3.X distribution! So you need to install NAVis explicitly for this distribution in order to use it within Blender.</p> <p>There are several ways to install additional packages for Blender's built-in Python. The easiest way is probably this:</p> <ol> <li> <p>Find out where Blender's Python lives (this depends on your OS). In    Blender's Python console run this:</p> <pre><code>&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.executable\n[..]/Blender 4.1.app/Contents/Resources/4.1/python/bin/python3.11\n</code></pre> <p></p> </li> <li> <p>Now that we know the Python path we open a normal terminal and check if Blender's Python    already came with the package manager <code>pip</code>.</p> <pre><code>[..]/Blender\\ 4.1.app/Contents/Resources/4.1/python/bin/python3.11 -m pip --version\n</code></pre> <p></p> <p>Warning</p> <p>You may have to escape whitespaces in the path to Blender's Python executable like we did above! On OSX this is done with a backslash <code>\\</code>. On Windows you have to wrap the path in quotes <code>\"</code> if it contains spaces.</p> <p>If the above command throws an error along the lines of <code>\"No module named pip\"</code>: get <code>pip</code> by downloading <code>get-pip.py</code> from here and install by executing with your Python distribution:</p> <pre><code>[..]/Blender\\ 4.1.app/Contents/Resources/4.1/python/bin/python3.11 get-pip.py\n</code></pre> <p>If <code>pip</code> is there but horrendously outdated (the current version is <code>24.4</code>), you can update it like so:</p> <pre><code>[..]/Blender\\ 4.1.app/Contents/Resources/4.1/python/bin/python3.11 -m pip install pip -U\n</code></pre> </li> <li> <p>Use <code>pip</code> to install NAVis (or any other package for that matter). Please note    we have to - again - specify that we want to install for Blender's Python:</p> <pre><code>[..]/Blender\\ 4.1.app/Contents/Resources/4.1/python/bin/python3.11 -m pip install navis\n</code></pre> <p>Important</p> <p>It's possible that this install fails with an error message along the lines of <code>'Python.h' file not found</code>. The reason for this is that Blender ships with a \"Python light\" and you have to manually provide the Python header files:</p> <p>First, find out the exact Blender Python version:</p> <pre><code>[..]/Blender\\ 4.1.app/Contents/Resources/4.1/python/bin/python3.11 -V\n</code></pre> <p>Next point your browser at https://www.python.org/downloads/source/ and download the Gzipped source tarball from the exact same Python version, i.e. <code>Python-3.X.X.tgz</code> and save it to your Downloads directory.</p> <p>Finally you need to copy everything in the <code>Include</code> folder inside that tarball into the corresponding <code>include</code> folder in your Blender's Python. In a terminal run::</p> <pre><code>cd ~/Downloads/\ntar -xzf Python-3.X.X.tgz\ncp Python-3.X.X/Include/* [..]/Blender\\ 4.1.app/Contents/Resources/4.1/python/bin/python3.11\n</code></pre> <p>If the above fails you have one more option: figure out which dependency fails to compile and compile it on your system's Python.</p> <p>a) Install the exact same version of Python as Blender is running on your system b) Download the source code for the offending dependency either from PyPI where it'll likely be some <code>tar.gz</code> file under \"Download files\" or from the Github repository c) Run <code>python setup.py bdist_wheel</code> to compile the dependency into a wheel file (will appear as <code>.whl</code> file in a <code>/dist</code> subdirectory) d) Go back to Blender's Python and install the dependency from that wheel: </p><pre><code>[..]/Blender\\ 4.1.app/Contents/Resources/4.1/python/bin/python3.11 -m pip install &lt;full file name of wheel file with .whl extension&gt;\n</code></pre><p></p> </li> <li> <p>You should now be all set to use NAVis in Blender. Check out Quickstart!</p> </li> </ol>"},{"location":"generated/gallery/3_interfaces/tutorial_interfaces_02_blender/#quickstart","title":"Quickstart","text":"<p><code>navis.interfaces.blender</code> provides a simple interface that lets you add, select and manipulate neurons from within Blender's Python console:</p> <p>First, import and set up NAVis like you are used to.</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; # Get example neurons\n&gt;&gt;&gt; nl = navis.example_neurons()\n</code></pre> <p>Now initialise the interface with Blender and import the neurons.</p> <pre><code>&gt;&gt;&gt; # The blender interface has to be imported explicitly\n&gt;&gt;&gt; import navis.interfaces.blender as b3d\n&gt;&gt;&gt; # Initialise handler\n&gt;&gt;&gt; h = b3d.Handler()\n&gt;&gt;&gt; # Load neurons into scene\n&gt;&gt;&gt; h.add(nl)\n</code></pre> <p></p> <p>The interface lets you manipulate neurons in Blender too.</p> <pre><code>&gt;&gt;&gt; # Colorize neurons\n&gt;&gt;&gt; h.colorize()\n&gt;&gt;&gt; # Change thickness of all neurons\n&gt;&gt;&gt; h.neurons.bevel(.02)\n&gt;&gt;&gt; # Select subset\n&gt;&gt;&gt; subset = h.select(nl[:2])\n&gt;&gt;&gt; # Make subset red\n&gt;&gt;&gt; subset.color(1, 0, 0)\n&gt;&gt;&gt; # Clear all objects\n&gt;&gt;&gt; h.clear()\n</code></pre> <p>Note</p> <p>Blender's Python console does not show all outputs. Please check the terminal if you experience issues. In Windows simply go to <code>Help</code> &gt;&gt; <code>Toggle System Console</code>. In MacOS, right-click Blender in Finder &gt;&gt; <code>Show Package Contents</code> &gt;&gt; <code>MacOS</code> &gt;&gt; double click on <code>blender</code>.</p> <p>Last but not least, here's a little taster of what you can do with Blender:</p>"},{"location":"generated/gallery/3_interfaces/tutorial_interfaces_02_blender/#reference","title":"Reference","text":"<p>The <code>navis.interfaces.blender.Handler</code> is providing the interface between NAVis and Blender.</p> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: tutorial_interfaces_02_blender.py</p> <p> Download Jupyter notebook: tutorial_interfaces_02_blender.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/4_remote/mg_execution_times/","title":"Computation times","text":""},{"location":"generated/gallery/4_remote/mg_execution_times/#computation-times","title":"Computation times","text":"<p>06:49.887 total execution time for generated_gallery_4_remote files:</p> <p>+----------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_remote_04_h01 (docs/examples/4_remote/tutorial_remote_04_h01.py)                         | 04:03.433 | 0.0 MB | +----------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_remote_02_microns (docs/examples/4_remote/tutorial_remote_02_microns.py)             | 01:26.968 | 0.0 MB | +----------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_remote_00_neuprint (docs/examples/4_remote/tutorial_remote_00_neuprint.py)          | 00:42.172 | 0.0 MB | +----------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_remote_03_insect_db (docs/examples/4_remote/tutorial_remote_03_insect_db.py)       | 00:32.376 | 0.0 MB | +----------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_remote_01_cloudvolume (docs/examples/4_remote/tutorial_remote_01_cloudvolume.py) | 00:04.938 | 0.0 MB | +----------------------------------------------------------------------------------------------------------------------------------+-----------+--------+</p>"},{"location":"generated/gallery/4_remote/tutorial_remote_00_neuprint/","title":"neuPrint","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/4_remote/tutorial_remote_00_neuprint/#neuprint","title":"neuPrint","text":"<p>This tutorial shows how to fetch neurons from a neuPrint server.</p> <p>NeuPrint is a service for presenting and analyzing connectomics data. It is used to host, for example, the Janelia EM reconstructions from a Drosophila hemibrain at https://neuprint.janelia.org/.</p> <p>neuprint-python is a Python library that lets you query data directly from a neuPrint server. You can install it from PyPI:</p> <pre><code>pip3 install neuprint-python\n</code></pre> <p><code>navis.interfaces.neuprint</code> wraps <code>neuprint-python</code> and adds a few new functions to fetch and convert data into NAVis objects.</p> <pre><code># Import navis\nimport navis\n\n# Import neuprint wrapper by navis\nimport navis.interfaces.neuprint as neu\n</code></pre> <p>First set up the connection: You can either pass your API token directly or store as <code>NEUPRINT_APPLICATION_CREDENTIALS</code> environment variable. The latter is the recommended way and we will use it here:</p> <pre><code>client = neu.Client(\n    \"https://neuprint.janelia.org/\",\n    # token=\"MYLONGTOKEN\"  # use this to instead pass your token directly\n    dataset=\"hemibrain:v1.2.1\",\n)\n</code></pre> <p>You can use all of neuprint's functions:</p> <pre><code>mbons, roi_info = neu.fetch_neurons(\n    neu.SegmentCriteria(instance=\".*MBON.*\", regex=True)\n)\nmbons.head(3)\n</code></pre> bodyId instance type pre post downstream upstream mito size status cropped statusLabel cellBodyFiber somaRadius somaLocation roiInfo notes inputRois outputRois 0 300972942 MBON14(a3)_R MBON14 543 13634 4336 13634 283 1566415099 Traced False Roughly traced None NaN None {'MB(R)': {'pre': 17, 'post': 13295, 'downstre... None [MB(+ACA)(R), MB(R), SIP(R), SLP(R), SMP(R), S... [MB(+ACA)(R), MB(R), SIP(R), SLP(R), SMP(R), S... 1 394225044 MBON14(a3)_L MBON14 187 5172 768 5172 142 359288547 Traced False Roughly traced None NaN None {'MB(L)': {'pre': 185, 'post': 5143, 'downstre... None [MB(L), SIP(L), SNP(L), aL(L)] [MB(L), SIP(L), SNP(L), aL(L)] 2 422725634 MBON06(B1&gt;a)(AVM07)_L MBON06 1356 21000 9579 21000 765 3157840413 Traced False Roughly traced None NaN None {'MB(R)': {'pre': 777, 'post': 20390, 'downstr... None [CRE(-ROB,-RUB)(R), CRE(R), INP, MB(+ACA)(R), ... [CRE(-ROB,-RUB)(R), CRE(R), INP, MB(+ACA)(R), ... <p>NAVis has added three functions to <code>neu</code>:</p> <ul> <li><code>navis.interfaces.neuprint.fetch_roi</code>: returns a <code>navis.Volume</code> from a ROI</li> <li><code>navis.interfaces.neuprint.fetch_skeletons</code>: returns fully fledged <code>navis.TreeNeurons</code> - nodes, synapses, soma and all</li> <li><code>navis.interfaces.neuprint.fetch_mesh_neuron</code>: returns <code>navis.MeshNeurons</code> - including synapses</li> </ul> <p>Let's start by fetching the mesh for the right mushroom body ROI:</p> <pre><code>mb = neu.fetch_roi(\"MB(R)\")\nmb\n</code></pre> <p>Out:</p> <pre><code>&lt;navis.Volume(name=MB(R), units=1 dimensionless, color=(0.85, 0.85, 0.85, 0.2), vertices.shape=(57913, 3), faces.shape=(115856, 3))&gt;\n</code></pre> <p>Next, let's fetch the skeletons of all right MBONs:</p> <pre><code>mbon_skeletons = neu.fetch_skeletons(\n    neu.SegmentCriteria(instance=\".*MBON.*_R\", regex=True), with_synapses=True\n)\nmbon_skeletons.head()\n</code></pre> <p>Out:</p> <pre><code>  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:00&lt;00:00,  3.35it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  3.35it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:00&lt;00:00,  3.88it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  3.88it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.99it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.99it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  1.34it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  1.34it/s]\n\n\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  3.11it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  3.11it/s]\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  2.63it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  2.63it/s]\n\n\n\n\n\n100%|##########| 1/1 [00:01&lt;00:00,  1.04s/it]\n100%|##########| 1/1 [00:01&lt;00:00,  1.04s/it]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:01&lt;00:00,  1.00s/it]\n100%|##########| 1/1 [00:01&lt;00:00,  1.00s/it]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.74it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.74it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:00&lt;00:00,  1.55it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  1.55it/s]\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  3.81it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  3.81it/s]\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  2.55it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  2.55it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:00&lt;00:00,  6.55it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  6.54it/s]\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  5.45it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  5.45it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  5.14it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  5.14it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n100%|##########| 1/1 [00:01&lt;00:00,  1.15s/it]\n100%|##########| 1/1 [00:01&lt;00:00,  1.15s/it]\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  5.39it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  5.39it/s]\n\n\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  1.76it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  1.76it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n100%|##########| 1/1 [00:01&lt;00:00,  1.22s/it]\n100%|##########| 1/1 [00:01&lt;00:00,  1.22s/it]\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.91it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.91it/s]\n\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  2.97it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  2.97it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:01&lt;00:00,  1.36s/it]\n100%|##########| 1/1 [00:01&lt;00:00,  1.36s/it]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  5.29it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  5.28it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:00&lt;00:00,  2.23it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  2.23it/s]\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  1.76it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  1.76it/s]\n\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  2.24it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  2.24it/s]\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  1.61it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  1.61it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:00&lt;00:00,  1.81it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  1.81it/s]\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  2.22it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  2.22it/s]\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  1.30it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  1.30it/s]\n\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  1.75it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  1.75it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.00it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.00it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.24it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.23it/s]\n\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:00&lt;00:00,  1.95it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  1.95it/s]\n\n\n\n\n\n\n100%|##########| 1/1 [00:01&lt;00:00,  1.33s/it]\n100%|##########| 1/1 [00:01&lt;00:00,  1.33s/it]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  1.77it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  1.77it/s]\n\n\n\n100%|##########| 1/1 [00:01&lt;00:00,  1.17s/it]\n100%|##########| 1/1 [00:01&lt;00:00,  1.17s/it]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  5.36it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  5.36it/s]\n\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  1.86it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  1.86it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:00&lt;00:00,  2.32it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  2.32it/s]\n\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  2.60it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  2.60it/s]\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  1.67it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  1.67it/s]\n\n\n\n\n\n\n100%|##########| 1/1 [00:01&lt;00:00,  1.16s/it]\n100%|##########| 1/1 [00:01&lt;00:00,  1.16s/it]\n\n\n\n100%|##########| 1/1 [00:01&lt;00:00,  1.41s/it]\n100%|##########| 1/1 [00:01&lt;00:00,  1.41s/it]\n</code></pre> type name id n_nodes n_connectors n_branches n_leafs cable_length soma units 0 navis.TreeNeuron MBON23(a2sp)_R 423382015 14570 5241 1224 1253 4.588212e+05 2379 8 nanometer 1 navis.TreeNeuron MBON19(a2p3p)_R 423774471 6944 1783 549 561 2.464995e+05 1781 8 nanometer 2 navis.TreeNeuron MBON15-like(a'1a'2)_R 457175171 9523 2379 587 602 3.380105e+05 9282 8 nanometer 3 navis.TreeNeuron MBON02(B2B'2a)_R 424789697 34243 20709 3040 3095 1.435912e+06 34135 8 nanometer 4 navis.TreeNeuron MBON16-like(a'3a)_R 457196643 10198 4142 933 947 4.218314e+05 6297 8 nanometer <p>Co-visualize the MBONs and the MB volume:</p> <pre><code>navis.plot3d(\n    [mbon_skeletons[0], mb],\n    legend=False,  # Hide the legend (more space for the plot)\n)\n</code></pre> <p>Last (but not least), let's make a 2d plot for the tutorial's thumbnail:</p> <pre><code>import matplotlib.pyplot as plt\n\nfig, ax = navis.plot2d(\n    [mbon_skeletons[0], mb],\n    c=(0, 0, 0, 1),  # Make the neuron black\n    method=\"3d\",\n    connectors=True,\n    linewidth=0.5,  # Make neuron a bit thinner to emphasize the synapses\n    view=(\"x\", \"-z\"),\n)\n\nplt.tight_layout()\n</code></pre> <p></p> <p>All NAVis functions for analysis &amp; visualization should work on these neurons. If not, please open an issue on Github.</p> <p>Total running time of the script: ( 0 minutes  42.172 seconds)</p> <p> Download Python source code: tutorial_remote_00_neuprint.py</p> <p> Download Jupyter notebook: tutorial_remote_00_neuprint.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/4_remote/tutorial_remote_01_cloudvolume/","title":"Neuroglancer & CloudVolume","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/4_remote/tutorial_remote_01_cloudvolume/#neuroglancer-cloudvolume","title":"Neuroglancer &amp; CloudVolume","text":"<p>This tutorial will show you how to pull data from <code>Neuroglancer</code> using <code>CloudVolume</code>.</p> <p>Neuroglancer is a WebGL-based viewer for volumetric data. You may have used it to browse some of the recent large EM datasets. If you want to programmatically access/download these data, you need CloudVolume. <code>CloudVolume</code> is an excellent Python library developed by William Silversmith (Seung lab, Princeton) and others. While <code>CloudVolume</code> is not directly related to <code>Neuroglancer</code>, it shares much of its functionality. As a rule of thumb: if you can view a dataset in <code>Neuroglancer</code>, you can download that data using <code>CloudVolume</code>. For example:</p> <ol> <li>FlyWire is a segmentation of an entire Drosophila brain. This dataset is very much work in progress and you    will to register and apply for access. Check out FAFBseg for a fairly mature interface built on    top of NAVis.</li> <li>Google's flood-filling segmentation of an entire Drosophila brain.</li> <li>The Allen Institute's MICrONs datasets. We have a separate tutorial on this!</li> <li>The Janelia hemibrain connectome.</li> </ol> <p>You can find the find the source for the data you want to access by right-clicking on the layer in question and selecting the \"Source\" tab on the right:</p> <p></p> <p><code>CloudVolume</code> supports pretty much all the backends/data formats that neuroglancer does. You can use it to programmatically query the segmentation itself, and to fetch meshes and skeletons (if available). NAVis &amp; friends provide simple interfaces for some of the datasets (see e.g. the neuPrint and the MICrONs tutorials) but there is also some lower-level option to pull neurons into NAVis via <code>CloudVolume</code>.</p> <p>First of all, you will want to make sure to <code>cloud-volume</code> is installed and up-to-date:</p> <pre><code>pip install cloud-volume -U\n</code></pre> <p>Once that's done we can start pulling data using <code>cloud-volume</code>. In this example here, we will use the Google segmentation of the FAFB dataset:</p> <pre><code>import navis\nimport cloudvolume as cv\n</code></pre> <p>Before we connect to the datasource we have to \"monkey patch\" <code>cloudvolume</code> using <code>navis.patch_cloudvolume</code>. That will teach <code>cloudvolume</code> to return NAVis neurons:</p> <pre><code># This needs to be run only once at the beginning of each session\nnavis.patch_cloudvolume()\n</code></pre> <p>Now we can connect to our data source. Here we connect to the Google segmentation of the FAFB dataset:</p> <pre><code># Don't forget to set `use_https=True` to avoid having to setup Google credentials!\nvol = cv.CloudVolume(\n    \"precomputed://gs://fafb-ffn1-20200412/segmentation\", use_https=True, progress=False\n)\n</code></pre> <p>Fetch neuron meshes:</p> <pre><code># Setting `as_navis=True` we will get us MeshNeurons\nm = vol.mesh.get([4335355146, 2913913713, 2137190164, 2268989790], as_navis=True, lod=3)\nm\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 4 neurons (1.8MiB) type name id units n_vertices n_faces 0 navis.MeshNeuron None 2137190164 1 nanometer 10085 17612 1 navis.MeshNeuron None 4335355146 1 nanometer 17066 30420 2 navis.MeshNeuron None 2268989790 1 nanometer 10980 19189 3 navis.MeshNeuron None 2913913713 1 nanometer 18224 31213 <p>Shortcut</p> <p>Instead of <code>vol.mesh.get(..., as_navis=True)</code> you can also use the shortcut <code>vol.mesh.get_navis(...)</code> which is equivalent.</p> <p>Plot!</p> <pre><code>navis.plot3d(\n    m,\n    legend_orientation=\"h\",  # few neurons, so we can afford a horizontal legend\n)\n</code></pre> <pre><code># And one 2D plot (for the tutorial thumbnail)\nimport matplotlib.pyplot as plt\n\nfig, ax = navis.plot2d(m[1], method=\"2d\", view=(\"x\", \"-y\"))\nax.set_axis_off()\nax.grid(False)\nplt.tight_layout()\n</code></pre> <p></p> <p>This also works for skeletons:</p> <pre><code>sk = vol.skeleton.get([4335355146, 2913913713, 2137190164, 2268989790], as_navis=True)\nsk\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 4 neurons (2.1MiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin 0 navis.TreeNeuron SWC 4335355146 27460 None 2184 2192 8219293.0 None 1 nanometer 2026-03-01 17:28:34.025383 string 1 navis.TreeNeuron SWC 2913913713 28640 None 2369 2381 8557470.0 None 1 nanometer 2026-03-01 17:28:34.226026 string 2 navis.TreeNeuron SWC 2137190164 15404 None 867 870 4712362.0 None 1 nanometer 2026-03-01 17:28:34.336834 string 3 navis.TreeNeuron SWC 2268989790 18105 None 945 946 5545407.5 None 1 nanometer 2026-03-01 17:28:34.464759 string <p>Note that not all datasets contain precomputed skeletons! In that case you could download the meshes and use <code>navis.skeletonize</code> to skeletonize them.</p> <p>Try it out!</p> <p>If you are working a lot with NeuroGlancer and need to e.g. generated or parse URLs, you might want to check out the <code>nglscenes</code> package.</p> <p>Total running time of the script: ( 0 minutes  4.938 seconds)</p> <p> Download Python source code: tutorial_remote_01_cloudvolume.py</p> <p> Download Jupyter notebook: tutorial_remote_01_cloudvolume.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/4_remote/tutorial_remote_02_microns/","title":"The MICrONS Datasets","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/4_remote/tutorial_remote_02_microns/#the-microns-datasets","title":"The MICrONS Datasets","text":"<p>In this tutorial we will explore the MICrONS datasets.</p> <p>The Allen Institute for Brain Science in collaboration with Princeton University, and Baylor College of Medicine released two large connecotmics dataset:</p> <ol> <li>A \"Cortical mm<sup>3</sup>\" of mouse visual cortex. This one is broken into two portions: \"65\" and \"35\"</li> <li>A smaller \"Layer 2/3\" dataset of mouse visual cortex.</li> </ol> <p>All of these can be browsed via the MICrONS Explorer using neuroglancer. These data are public and thanks to the excellent <code>cloud-volume</code> and <code>caveclient</code> libraries, developed by William Silversmith, Forrest Collman, Sven Dorkenwald, Casey Schneider-Mizell and others, we can easily fetch neurons and their connectivity.</p> <p>For easier interaction, NAVis ships with a small interface to these datasets. To use it, we will have to make sure <code>caveclient</code> (and with it <code>cloud-volume</code>) is installed:</p> <pre><code>pip install caveclient cloud-volume -U\n</code></pre> <p>The first time you run below code, you might have to get and set a client secret. Simply follow the instructions in the terminal and when in doubt, check out the section about authentication in the <code>caveclient</code> docs.</p> <p>Let's get started:</p> <pre><code>import navis\nimport navis.interfaces.microns as mi\n</code></pre> <p>You will find that most functions in the interface accept a <code>datastack</code> parameter. At the time of writing, the available stacks are:</p> <ul> <li><code>cortex65</code> (also called \"minnie65\") is the anterior portion of the cortical mm<sup>3</sup> dataset</li> <li><code>cortex35</code> (also called \"minnie35\") is the (smaller) posterior portion of the cortical mm<sup>3</sup> dataset</li> <li><code>layer 2/3</code> (also called \"pinky\") is the earlier, smaller cortical dataset</li> </ul> <p>If not specified, the default is <code>cortex65</code>. Both <code>cortex65</code> and <code>cortex35</code> always map to the most recent version of that dataset. You can use <code>get_datastacks</code> to see all available datastacks:</p> <pre><code>mi.get_datastacks()\n</code></pre> <p>Out:</p> <pre><code>['minnie35_public_v0', 'pinky_sandbox', 'minnie65_sandbox', 'minnie65_public']\n</code></pre> <p>Let's start with some basic queries using the <code>caveclient</code> directly:</p> <pre><code># Initialize the client for the 65 part of cortical mm^3 (i.e. \"Minnie\")\nclient = mi.get_cave_client(datastack=\"cortex65\")\n\n# Fetch available annotation tables\nclient.materialize.get_tables()\n</code></pre> <p>Out:</p> <pre><code>['baylor_gnn_cell_type_fine_model_v2', 'vortex_manual_myelination_v0', 'synapse_target_predictions_ssa', 'aibs_metamodel_celltypes_v661', 'nucleus_alternative_points', 'allen_column_mtypes_v2', 'bodor_pt_cells', 'aibs_metamodel_mtypes_v661_v2', 'vortex_microglia_proofreading_status', 'proofreading_status_and_strategy', 'allen_v1_column_types_slanted_ref', 'aibs_column_nonneuronal_ref', 'nucleus_ref_neuron_svm', 'apl_functional_coreg_vess_fwd', 'vortex_axon_backtrace_column', 'vortex_compartment_targets', 'baylor_log_reg_cell_type_coarse_v1', 'vortex_synapse_reattachment', 'gamlin_2023_mcs_met_types', 'gamlin_2023_mcs', 'l5et_column', 'pt_synapse_targets', 'coregistration_manual_v4', 'cg_cell_type_calls', 'synapses_pni_2', 'nucleus_detection_v0', 'vortex_manual_nodes_of_ranvier', 'bodor_pt_target_proofread', 'nucleus_functional_area_assignment', 'coregistration_auto_phase3_fwd_apl_vess_combined_v2', 'vortex_thalamic_proofreading_status', 'multi_input_spine_predictions_ssa', 'synapse_target_structure', 'myelin_auto_tags_2points', 'coregistration_auto_phase3_fwd_v2', 'vortex_peptidergic_proofreading_status', 'digital_twin_properties_bcm_coreg_v4', 'vortex_astrocyte_proofreading_status', 'digital_twin_properties_bcm_coreg_auto_phase3_fwd_v2', 'digital_twin_properties_bcm_coreg_apl_vess_fwd']\n</code></pre> <p>These are the available public tables which we can use to fetch meta data. Let's check out <code>baylor_log_reg_cell_type_coarse_v1</code>. Note that there is also a <code>baylor_gnn_cell_type_fine_model_v2</code> table which contains more detailed cell types.</p> <pre><code># Get cell type table\nct = client.materialize.query_table(\"baylor_log_reg_cell_type_coarse_v1\")\nct.head()\n</code></pre> id_ref created_ref valid_ref volume pt_supervoxel_id pt_root_id id created valid target_id classification_system cell_type pt_position bb_start_position bb_end_position 0 17115 2020-09-28 22:41:18.237823+00:00 True 268.646484 75934403318291307 864691135635239593 25718 2023-03-22 18:05:52.744496+00:00 True 17115 baylor_log_reg_cell_type_coarse inhibitory [80992, 109360, 15101] [&lt;NA&gt;, &lt;NA&gt;, &lt;NA&gt;] [&lt;NA&gt;, &lt;NA&gt;, &lt;NA&gt;] 1 17816 2020-09-28 22:42:54.932823+00:00 True 264.795593 75090047309035210 864691135618175635 25581 2023-03-22 18:05:52.650844+00:00 True 17816 baylor_log_reg_cell_type_coarse inhibitory [74880, 110032, 16883] [&lt;NA&gt;, &lt;NA&gt;, &lt;NA&gt;] [&lt;NA&gt;, &lt;NA&gt;, &lt;NA&gt;] 2 18023 2020-09-28 22:43:00.306675+00:00 True 264.791321 75934266147628505 864691135207734905 5033 2023-03-22 18:04:23.575096+00:00 True 18023 baylor_log_reg_cell_type_coarse inhibitory [81008, 108240, 16995] [&lt;NA&gt;, &lt;NA&gt;, &lt;NA&gt;] [&lt;NA&gt;, &lt;NA&gt;, &lt;NA&gt;] 3 18312 2020-09-28 22:44:09.407821+00:00 True 221.584747 75441272688753483 864691135758479438 32294 2023-03-22 18:06:11.872068+00:00 True 18312 baylor_log_reg_cell_type_coarse inhibitory [77392, 105280, 17650] [&lt;NA&gt;, &lt;NA&gt;, &lt;NA&gt;] [&lt;NA&gt;, &lt;NA&gt;, &lt;NA&gt;] 4 255686 2020-09-28 22:40:42.632533+00:00 True 297.846039 88954888800920543 864691135568539372 2693 2023-03-22 18:04:21.985021+00:00 True 255686 baylor_log_reg_cell_type_coarse excitatory [175760, 126480, 15504] [&lt;NA&gt;, &lt;NA&gt;, &lt;NA&gt;] [&lt;NA&gt;, &lt;NA&gt;, &lt;NA&gt;] <pre><code>ct.cell_type.value_counts()\n</code></pre> <p>Out:</p> <pre><code>cell_type\nexcitatory    49208\ninhibitory     5855\nName: count, dtype: Int64\n</code></pre> <p>Important</p> <p>Not all neurons in the dataset have been proofread. In theory, you can check if a neuron has been proofread using the corresponding annotation table: </p><pre><code>table = client.materialize.query_table('proofreading_status_public_release')#\nfully_proofread = table[\n      table.status_dendrite.isin(['extented', 'clean']) &amp;\n      table.status_axon.isin(['extented', 'clean'])\n  ].pt_root_id.values\n</code></pre> However, it appears that the proofreading status table may be outdated at the moment.<p></p> <p>Let's fetch one of the excitatory neurons:</p> <pre><code>n = mi.fetch_neurons(\n    ct[ct.cell_type == \"excitatory\"].pt_root_id.values[0], with_synapses=False\n)[0]\nn\n</code></pre> type navis.MeshNeuron name None id 864691135568539372 units 1 nanometer n_vertices 1508981 n_faces 3034247 <p>Neuron IDs</p> <p>The neuron IDs in MICrONS are called \"root IDs\" because they represent collections of supervoxels - or rather hierarchical layers of chunks of which the lowest layer are supervoxel IDs.</p> <p>MICrONS neurons can be fairly large, i.e. have lots of faces. You can try using using a higher <code>lod</code> (\"level of detail\", higher = coarser) but not all datastacks actually support multi-resolution meshes. If they don't (like this one) the <code>lod</code> parameter is silently ignored.</p> <p>For visualization in this documentation we will simplify the neuron a little. For this, you need either <code>open3d</code> (<code>pip3 install open3d</code>), <code>pymeshlab</code> (<code>pip3 install pymeshlab</code>) or Blender 3D on your computer.</p> <pre><code># Reduce face counts to 1/3 of the original\nn_ds = navis.simplify_mesh(n, F=1 / 3)\n\n# Inspect (note the lower face/vertex counts)\nn_ds\n</code></pre> type navis.MeshNeuron name None id 864691135568539372 units 1 nanometer n_vertices 502005 n_faces 1011415 <p>Plot the downsample neuron (again: the downsampling is mostly for the sake of this documentation)</p> <pre><code>navis.plot3d(\n    n_ds,\n    radius=False,\n    color=\"r\",\n    legend=False,  # hide the legend (more space for the plot)\n)\n</code></pre> <p>Nice! Now let's run a bit of analysis.</p>"},{"location":"generated/gallery/4_remote/tutorial_remote_02_microns/#sholl-analysis","title":"Sholl Analysis","text":"<p>Sholl analysis is a simple way to quantify the complexity of a neuron's arbor. It counts the number of intersections a neuron's arbor makes with concentric spheres around a center (typically the soma). The number of intersections is then plotted against the radius of the spheres.</p> <pre><code>import numpy as np\n\n# The neuron mesh will automatically be skeletonized for this analysis\n# Note: were defining radii from 0 to 160 microns in 5 micron steps\nsha = navis.sholl_analysis(n, center=\"soma\", radii=np.arange(0, 160_000, 5_000))\n</code></pre> <p>Plot the results</p> <pre><code>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\nsha.intersections.plot(c=\"r\")\n\nax.set_xlabel(\"radius [nm]\")\nax.set_ylabel(\"# of intersections\")\nax.patch.set_color((0, 0, 0, 0))  # Make background transparent\nfig.patch.set_color((0, 0, 0, 0))\n\nplt.tight_layout()\n</code></pre> <p></p> <p>See <code>navis.sholl_analysis</code> for ways to fine tune the analysis. Last but not least a quick visualization with the neuron:</p> <pre><code>from matplotlib.colors import Normalize\nfrom matplotlib.cm import ScalarMappable\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n# Plot one of the excitatory neurons\nfig, ax = navis.plot2d(n, view=(\"x\", \"y\"), figsize=(10, 10), c=\"k\", method=\"2d\")\n\ncmap = plt.get_cmap(\"viridis\")\n\n# Plot Sholl circles and color by number of intersections\ncenter = n.soma_pos\n# Drop the outer Sholl circles where there are no intersections\nnorm = Normalize(vmin=0, vmax=(sha.intersections.max() + 1))\nfor r in sha.index.values:\n    ints = sha.loc[r, \"intersections\"]\n    ints_norm = norm(ints)\n    color = cmap(ints_norm)\n\n    c = plt.Circle(center[:2], r, ec=color, fc=\"none\")\n    ax.add_patch(c)\n\n# Add colorbar\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n_ = plt.colorbar(\n    ScalarMappable(norm=norm, cmap=cmap), cax=cax, label=\"# of intersections\"\n)\n</code></pre> <p></p>"},{"location":"generated/gallery/4_remote/tutorial_remote_02_microns/#render-videos","title":"Render Videos","text":"<p>Beautiful data like the MICrONS datasets lend themselves to visualizations. For making high quality videos (and renderings) I recommend you check out the tutorial on navis' Blender interface. Here's a little taster:</p> <p>Total running time of the script: ( 1 minutes  26.968 seconds)</p> <p> Download Python source code: tutorial_remote_02_microns.py</p> <p> Download Jupyter notebook: tutorial_remote_02_microns.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/4_remote/tutorial_remote_03_insect_db/","title":"Insect Brain DB","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/4_remote/tutorial_remote_03_insect_db/#insect-brain-db","title":"Insect Brain DB","text":"<p>In this example we will show you how to fetch data from the Insect Brain DB.</p> <p>The insect brain database (https://insectbraindb.org) is an online repository for neuron morphologies, brain regions and experimental data across various insect species. At the time of writing Insect Brain DB features close to 400 neuronal cell types from well over 30 insect species. Check out Heinze et al. (2021) for details!</p> <p>While the website features a comprehensive search and some nifty analyses, it can be useful to download these data to run your own analyses or compare to other data sets. For that purpose, NAVis provides an interface to Insect Brain DB that wraps parts of their API:</p> <p>Import navis</p> <pre><code>import navis\n\n# Import the actual Insect Brain DB interface\nimport navis.interfaces.insectbrain_db as ibdb\n</code></pre>"},{"location":"generated/gallery/4_remote/tutorial_remote_03_insect_db/#fetching-meta-data","title":"Fetching meta data","text":"<p>First, fetch a list of available species:</p> <pre><code>species = ibdb.get_available_species()\nspecies.head()\n</code></pre> id scientific_name common_name uuid date_created date_modified persistent_records description 0 40 Abantiades atripalpis Rain moth 8076326a-dd57-49b6-83e1-68c0c5153e6d 2021-06-30T12:21:21.094792+02:00 2025-11-12T15:14:48.047864+01:00 [20.500.12158/SIN-0000040.1] \u201cTrictena atripalpis, also known as bardee (ba... 1 24 Aedes aegypti Yellow fever mosquito 628dbce3-20f4-4af8-8c78-f9d3bc0c6654 2019-03-04T23:28:27.697821+01:00 2022-05-19T18:01:37.298876+02:00 [20.500.12158/SIN-0000024.2, 20.500.12158/SIN-... Aedes aegypti, the yellow fever mosquito, is t... 2 2 Agrotis infusa Bogong moth a6d4eadd-2e82-4393-96ab-fdfedaf42536 2016-07-10T10:37:09.361502+02:00 2022-05-19T18:02:16.115941+02:00 [20.500.12158/SIN-0000002.1, 20.500.12158/SIN-... \"The nocturnal Bogong moth (Agrotis infusa) is... 3 21 Agrotis segetum Turnip moth ae12170d-d655-4794-9f38-54ca4f1b3c50 2017-04-26T12:55:52.000344+02:00 2022-05-19T18:02:49.110675+02:00 [20.500.12158/SIN-0000021.1, 20.500.12158/SIN-... The turnip moth, Agrotis segetum Denis and Sch... 4 7 Apis mellifera Honeybee 8881609d-1929-410a-be38-ca47da5718d5 2016-07-10T10:37:09.372889+02:00 2023-02-08T21:04:16.732190+01:00 [20.500.12158/SIN-0000007.1, 20.500.12158/SIN-... The western honey bee or European honey bee (A... <p>Fetch info for a given species (you can use the scientific or common name, or an ID):</p> <pre><code>spec_info = ibdb.get_species_info('Schistocerca gregaria')\nspec_info\n</code></pre> <p>Out:</p> <pre><code>id                                                                        9\nuuid                                   795f4fd5-cad7-4de0-853f-89406a64aa98\nimages                    [{'id': 925, 'tag': 'SPECIES_IMAGE', 'hash': N...\nprefix                                                                   sg\npublic                                                                 True\napproved                                                               True\ncurators                  [{'uuid': 'c51a2d7e-6314-487a-9b8b-384db7ff79e...\nhost_lab                                    Uwe Homberg, Marburg University\ncreated_by                {'uuid': '89c53715-561d-4942-81ab-aa4a02237fdd...\nsearchable                                                             True\nstructures                [{'id': 1, 'name': 'Antennal Lobe', 'type': 's...\ncommon_name                                                   Desert Locust\ndescription               The desert locust (Schistocerca gregaria) is a...\nexperiments                                                              []\ndate_created                                    2016-07-10T08:37:09.454013Z\ndate_modified                                   2021-07-20T06:55:06.812019Z\nconfocal_stacks                                                          []\nreconstructions           [{'id': 66, 'sex': 'UNKNOWN', 'uuid': '8b10333...\nscientific_name                                       Schistocerca gregaria\nneuron_publications       [{'id': 340, 'doi': 'https://doi.org/10.1002/c...\nspecies_publications      [{'id': 327, 'doi': '10.3389/fnbeh.2015.00346'...\nsemi_schematic_default                                                 None\ndtype: object\n</code></pre>"},{"location":"generated/gallery/4_remote/tutorial_remote_03_insect_db/#fetching-meshes","title":"Fetching meshes","text":"<p>Fetch neuropil meshes for the Locust brain:</p> <pre><code># `combine=True` would produce a single combined mesh but here we want a list of individual neuropils\nlocust_brain = ibdb.get_brain_meshes('Desert Locust', combine=False)\nlocust_brain[:2]\n</code></pre> <p>Out:</p> <pre><code>[&lt;navis.Volume(name=Nodulus (left), units=1 dimensionless, color=(0.8392156862745098, 1.0, 0.8, 0.5), vertices.shape=(302, 3), faces.shape=(600, 3))&gt;, &lt;navis.Volume(name=Lateral Horn (left), units=1 dimensionless, color=(0.25882352941176473, 0.8901960784313725, 0.6901960784313725, 0.5), vertices.shape=(1002, 3), faces.shape=(2000, 3))&gt;]\n</code></pre> <p>Plot neuropils</p> <pre><code>navis.plot3d(locust_brain, volume_legend=True)\n</code></pre> <pre><code># This is for the tutorial thumbail:\nimport matplotlib.pyplot as plt\nfig, ax = navis.plot2d(locust_brain, method='2d')\nax.set_axis_off()\nax.grid(False)\nplt.tight_layout()\n</code></pre> <p></p>"},{"location":"generated/gallery/4_remote/tutorial_remote_03_insect_db/#fetch-neurons","title":"Fetch neurons","text":"<p>First we need to know what neurons are available. Just like on the website you can set all kinds of different search parameters. Here we will stick with our Locust:</p> <pre><code>locust_neurons = ibdb.search_neurons(species='Desert Locust')\nlocust_neurons.head()\n</code></pre> id name full_name short_name persistent_records uploaded_by nin publications date_uploaded reconstruction_creator group_head experimenter public hemisphere archived archived_notes species template_type superseding_record super_type neuron_family neuron_class 0 426 CL1a-L1 sg-CL1a-L1(L) CL1a-L1 [20.500.12158/NIN-0000426.1] {'id': 41, 'first_name': 'Frederick', 'last_na... NIN-0000426 [10.1002/cne.21842, 10.1073/pnas.2005192117] 2021-07-01T02:00:00+02:00 Uwe Homberg Frederick Zittrell True LEFT False None 9 1 None 31.0 8.0 1.0 1 429 CL1a-L3 sg-CL1a-L3(L) CL1a-L3 [20.500.12158/NIN-0000429.1] {'id': 41, 'first_name': 'Frederick', 'last_na... NIN-0000429 [10.1002/cne.21842, 10.1073/pnas.2005192117] 2021-07-01T02:00:00+02:00 Uwe Homberg Frederick Zittrell True LEFT False None 9 1 None 31.0 8.0 1.0 2 440 CL1a-L4 sg-CL1a-L4(L) CL1a-L4 [20.500.12158/NIN-0000440.1] {'id': 41, 'first_name': 'Frederick', 'last_na... NIN-0000440 [10.1002/cne.21842, 10.1073/pnas.2005192117] 2021-07-01T02:00:00+02:00 Uwe Homberg Frederick Zittrell True LEFT False None 9 1 None 31.0 8.0 1.0 3 425 CL1a-L5 sg-CL1a-L5(L) CL1a-L5 [20.500.12158/NIN-0000425.1] {'id': 41, 'first_name': 'Frederick', 'last_na... NIN-0000425 [10.1002/cne.21842, 10.1073/pnas.2005192117] 2021-07-01T02:00:00+02:00 Uwe Homberg Frederick Zittrell True LEFT False None 9 1 None 31.0 8.0 1.0 4 423 CL1a-R2 sg-CL1a-R2(R) CL1a [20.500.12158/NIN-0000423.1] {'id': 71, 'first_name': 'Ronja', 'last_name':... NIN-0000423 [10.1002/cne.25209] 2021-06-19T02:00:00+02:00 Stefanie Jahn Uwe Homberg Ronja Hensgen True RIGHT False None 9 1 None 6.0 8.0 1.0 <p>Let's fetch skeletons (\"reconstructions\") for some of the above neurons. Note that not all neurons have skeletons (see the \"reconstruction_creator\" column)!</p> <pre><code># You can use IDs or names, or a combination thereof to fetch skeletons\nsk = ibdb.get_skeletons('CL1a-R2')\nsk\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 1 neurons (4.2MiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units 0 navis.TreeNeuron CL1190516SkeletonTreeRight.swc 423 78559 None 330 340 9918.345703 None 1 micrometer <p>Plot the neuron - note that most neurons appear to have radii information</p> <pre><code>navis.plot3d(sk, radius=True)\n</code></pre> <p>Check out the API reference for further details.</p> <p>Total running time of the script: ( 0 minutes  32.376 seconds)</p> <p> Download Python source code: tutorial_remote_03_insect_db.py</p> <p> Download Jupyter notebook: tutorial_remote_03_insect_db.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/4_remote/tutorial_remote_04_h01/","title":"H01 Dataset","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/4_remote/tutorial_remote_04_h01/#h01-dataset","title":"H01 Dataset","text":"<p>In this notebook, you can learn how to work with the H01 dataset using NAVis.</p> <p>The H01 dataset contains 57,000 cells and 150 million synapses from a cubic millimeter of the human temporal cortex, which is proofread using the CAVE ecoystem.</p> <p>With this interface, you can access both a snapshot of the proofread dataset and the latest dataset using <code>caveclient</code>:</p> <pre><code>pip install caveclient -U\n</code></pre> <p>Authentication</p> <p>If this is your first time using <code>CAVEclient</code> to access the <code>H01</code> dataset, you might have to get and set your authentication token:</p> <ol> <li>Go to: https://https//global.brain-wire-test.org/auth/api/v1/create_token to create a new token.</li> <li>Log in with your Google credentials and copy the token shown afterward.</li> <li>Save it to your computer with:    <pre><code>from caveclient import CAVEclient\nclient = CAVEclient(server_address=\"https://global.brain-wire-test.org\", datastack_name='h01_c3_flat', auth_token=\"PASTE_YOUR_TOKEN_HERE\")\nclient.auth.save_token(token=\"PASTE_YOUR_TOKEN_HERE\")\n</code></pre></li> </ol> <p>Note that the H01 dataset uses a server address that is different from the default <code>CAVEClient</code> server. Also be aware that creating a new token by finishing step 2 will invalidate the previous token!</p> <pre><code>import navis\nfrom navis.interfaces import h01\n\n# Initialize the client\nclient = h01.get_cave_client()\n</code></pre>"},{"location":"generated/gallery/4_remote/tutorial_remote_04_h01/#query-tables","title":"Query Tables","text":"<pre><code>client.materialize.get_versions()\n</code></pre> <p>Out:</p> <pre><code>[1035, 1012, 1036, 1037, 1026, 1031]\n</code></pre> <pre><code>client.materialize.get_tables()\n</code></pre> <p>Out:</p> <pre><code>['synapses_using_sample_voxel_coords_fixed', 'cells', 'nucleus', 'synapses', 'proofreading_status_test', 'synapses_using_sample_voxel_coords']\n</code></pre>"},{"location":"generated/gallery/4_remote/tutorial_remote_04_h01/#query-materialized-synapse-table","title":"Query Materialized Synapse Table","text":"<p>Query the first few rows in the table</p> <pre><code>client.materialize.synapse_query(limit=10)\n</code></pre> <p>Out:</p> <pre><code>201 - \"Limited query to 10 rows\n</code></pre> id created superceded_id valid size pre_pt_supervoxel_id pre_pt_root_id post_pt_supervoxel_id post_pt_root_id pre_pt_position post_pt_position ctr_pt_position 0 40971523 2024-04-12 00:14:40.110821+00:00 &lt;NA&gt; True &lt;NA&gt; 134364238863925557 864691128621846602 134293870119747633 864691131861340864 [453256, 225950, 1879] [453086, 226004, 1838] [453312, 225966, 1883] 1 40971525 2024-04-12 00:14:40.110821+00:00 &lt;NA&gt; True &lt;NA&gt; 122744257927708975 864691132202350435 123448011605934094 864691132349699414 [369038, 157894, 4777] [373770, 158410, 12] [368610, 158562, 4703] 2 40971526 2024-04-12 00:14:40.110821+00:00 &lt;NA&gt; True &lt;NA&gt; 106917542483722336 864691132270145118 106776805062476142 864691132270173278 [253634, 204356, 1947] [252889, 204590, 2104] [253668, 204394, 1970] 3 40971531 2024-04-12 00:14:40.110821+00:00 &lt;NA&gt; True &lt;NA&gt; 123171415779180930 864691132169548413 123238142525309068 864691132275498000 [371948, 194706, 231] [372276, 167562, 393] [373837, 195793, 656] 4 40971532 2024-04-12 00:14:40.110821+00:00 &lt;NA&gt; True &lt;NA&gt; 112057964830916834 864691132186081100 123873454087733399 864691132406664067 [291270, 230762, 692] [376984, 182427, 422] [291536, 230572, 743] 5 40971533 2024-04-12 00:14:40.110821+00:00 &lt;NA&gt; True &lt;NA&gt; 123247282417042025 864691131368353049 123106476209209491 864691132146145331 [372432, 235870, 836] [371386, 235338, 800] [372404, 235926, 878] 6 40971543 2024-04-12 00:14:40.110821+00:00 &lt;NA&gt; True &lt;NA&gt; 111615894450340136 864691131150038069 110277925298765887 864691132256098281 [288132, 82436, 4516] [278270, 75460, 2688] [288126, 82449, 4534] 7 40971544 2024-04-12 00:14:40.110821+00:00 &lt;NA&gt; True &lt;NA&gt; 97420786213584924 864691132283064473 98476316705161844 864691132291684761 [184568, 226882, 2954] [192388, 226910, 1720] [186353, 228578, 2425] 8 40971558 2024-04-12 00:14:40.110821+00:00 &lt;NA&gt; True &lt;NA&gt; 106124452210671785 864691130242818215 106686508676678327 864691132254753632 [247898, 62853, 4234] [251917, 56124, 4088] [247909, 62819, 4232] 9 40971570 2024-04-12 00:14:40.110821+00:00 &lt;NA&gt; True &lt;NA&gt; 120781010190205127 864691131982166038 123383897072337185 864691132339647570 [354624, 210728, 2843] [373366, 204850, 1449] [354762, 211287, 2855] <p>Query specific pre- and/or postsynaptic IDs</p> <pre><code>syn = client.materialize.synapse_query(\n    post_ids=[864691131861340864],\n    # pre_ids=[ADD YOUR ROOT ID],\n)\nsyn.head()\n</code></pre> id created superceded_id valid size pre_pt_supervoxel_id pre_pt_root_id post_pt_supervoxel_id post_pt_root_id pre_pt_position post_pt_position ctr_pt_position 0 40882209 2024-04-12 00:14:37.031216+00:00 &lt;NA&gt; True &lt;NA&gt; 134364238863925573 864691128621846858 134293870119747633 864691131861340864 [453528, 225898, 1883] [453086, 226004, 1838] [453539, 225957, 1897] 1 40899780 2024-04-12 00:14:37.548445+00:00 &lt;NA&gt; True &lt;NA&gt; 134434607608102964 864691130486857485 134293870119747633 864691131861340864 [453872, 225934, 1847] [453086, 226004, 1838] [453854, 226063, 1851] 2 40924493 2024-04-12 00:14:38.495865+00:00 &lt;NA&gt; True &lt;NA&gt; 134223432656093315 864691132015686175 134293870119747633 864691131861340864 [452262, 225346, 1846] [453086, 226004, 1838] [453124, 225933, 1852] 3 40935748 2024-04-12 00:14:38.723502+00:00 &lt;NA&gt; True &lt;NA&gt; 134434676327579728 864691131840536421 134293870119747633 864691131861340864 [453838, 226378, 1849] [453086, 226004, 1838] [453786, 226050, 1873] 4 40954321 2024-04-12 00:14:39.566756+00:00 &lt;NA&gt; True &lt;NA&gt; 134293732747903463 864691132191805487 134293870119747633 864691131861340864 [453008, 224834, 1980] [453086, 226004, 1838] [453459, 225953, 1891] <pre><code>print(len(syn))\n</code></pre> <p>Out:</p> <pre><code>8\n</code></pre>"},{"location":"generated/gallery/4_remote/tutorial_remote_04_h01/#live-synapse-queries","title":"Live Synapse Queries","text":"<pre><code>import datetime as dt\n\n# Check if root ID is the most recent root ID\nroot_id = 864691131861340864\nnow = dt.datetime.now(dt.timezone.utc)\nis_latest = client.chunkedgraph.is_latest_roots([root_id], timestamp=now)\nlatest_id = client.chunkedgraph.get_latest_roots(root_id, timestamp=now)\nprint(is_latest, latest_id)\n</code></pre> <p>Out:</p> <pre><code>[ True] [864691131861340864]\n</code></pre> <pre><code>synapse_table = client.info.get_datastack_info()[\"synapse_table\"]\ndf = client.materialize.query_table(\n    synapse_table,\n    timestamp=dt.datetime.now(dt.timezone.utc),\n    filter_equal_dict={\"post_pt_root_id\": latest_id[0]},\n)\ndf.head()\n</code></pre> id created superceded_id valid size pre_pt_supervoxel_id pre_pt_root_id post_pt_supervoxel_id post_pt_root_id pre_pt_position post_pt_position ctr_pt_position 0 40882209 2024-04-12 00:14:37.031216+00:00 &lt;NA&gt; True &lt;NA&gt; 134364238863925573 864691128621846858 134293870119747633 864691131861340864 [453528, 225898, 1883] [453086, 226004, 1838] [453539, 225957, 1897] 1 40899780 2024-04-12 00:14:37.548445+00:00 &lt;NA&gt; True &lt;NA&gt; 134434607608102964 864691130486857485 134293870119747633 864691131861340864 [453872, 225934, 1847] [453086, 226004, 1838] [453854, 226063, 1851] 2 40924493 2024-04-12 00:14:38.495865+00:00 &lt;NA&gt; True &lt;NA&gt; 134223432656093315 864691132015686175 134293870119747633 864691131861340864 [452262, 225346, 1846] [453086, 226004, 1838] [453124, 225933, 1852] 3 40935748 2024-04-12 00:14:38.723502+00:00 &lt;NA&gt; True &lt;NA&gt; 134434676327579728 864691131840536421 134293870119747633 864691131861340864 [453838, 226378, 1849] [453086, 226004, 1838] [453786, 226050, 1873] 4 40954321 2024-04-12 00:14:39.566756+00:00 &lt;NA&gt; True &lt;NA&gt; 134293732747903463 864691132191805487 134293870119747633 864691131861340864 [453008, 224834, 1980] [453086, 226004, 1838] [453459, 225953, 1891]"},{"location":"generated/gallery/4_remote/tutorial_remote_04_h01/#query-cells-table","title":"Query Cells Table","text":"<pre><code>ct = client.materialize.query_table(table=\"cells\")\nct.head()\n</code></pre> id created superceded_id valid classification_system cell_type pt_supervoxel_id pt_root_id pt_position 0 1 2023-06-15 19:27:23.586264+00:00 &lt;NA&gt; True 0, 0, 0 UNKNOWN0 0 0 [0, 0, 0] 1 2 2023-06-15 19:27:23.587056+00:00 &lt;NA&gt; True 0, 0, 0 UNKNOWN0 0 0 [0, 0, 0] 2 3 2023-06-15 19:27:23.587786+00:00 &lt;NA&gt; True 0, 0, 0 UNKNOWN0 0 0 [0, 0, 0] 3 4 2023-06-15 19:27:23.588481+00:00 &lt;NA&gt; True 0, 0, 0 UNKNOWN0 0 0 [0, 0, 0] 4 5 2023-06-15 19:27:23.589166+00:00 &lt;NA&gt; True 0, 0, 0 UNKNOWN0 0 0 [0, 0, 0] <pre><code>ct.cell_type.unique()\n</code></pre> <p>Out:</p> <pre><code>&lt;StringArray&gt;\n[               'UNKNOWN0',               'PYRAMIDAL',\n      'UNCLASSIFIEDNEURON',                'UNKNOWN2',\n                   'OLIGO',               'ASTROCYTE',\n                  'MG_OPC',             'INTERNEURON',\n         'BLOODVESSELCELL',                'UNKNOWN1',\n 'C-SHAPEDCELL_AKA_MG_OPC']\nLength: 11, dtype: string\n</code></pre>"},{"location":"generated/gallery/4_remote/tutorial_remote_04_h01/#filter-by-cell-type","title":"Filter by cell type","text":"<pre><code># Get the first 50 interneurons\ninterneuron_ids = ct[ct.cell_type == \"INTERNEURON\"].pt_root_id.values[:50]\n\n# Remove 0 IDs\ninterneuron_ids = interneuron_ids[interneuron_ids != 0]\n\n# What's left?\ninterneuron_ids\n</code></pre> <p>Out:</p> <pre><code>&lt;IntegerArray&gt;\n[864691132294086751, 864691132460017477, 864691132234414123,\n 864691132301485893, 864691132238148606, 864691132382438988,\n 864691132512567744]\nLength: 7, dtype: Int64\n</code></pre>"},{"location":"generated/gallery/4_remote/tutorial_remote_04_h01/#fetch-neuron-meshes","title":"Fetch Neuron Meshes","text":"<pre><code>interneurons = h01.fetch_neurons(interneuron_ids, lod=2, with_synapses=False)\ninterneurons_ds = navis.simplify_mesh(interneurons, F=1 / 3)\ninterneurons_ds\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 7 neurons (47.3MiB) type name id units n_vertices n_faces 0 navis.MeshNeuron None 864691132301485893 1 nanometer 65943 130220 1 navis.MeshNeuron None 864691132294086751 1 nanometer 125418 242575 ... ... ... ... ... ... ... 5 navis.MeshNeuron None 864691132382438988 1 nanometer 131846 264694 6 navis.MeshNeuron None 864691132512567744 1 nanometer 168372 330422 <pre><code># Plot\nimport seaborn as sns\n\ncolors = {n.id: sns.color_palette(\"Reds\", 7)[i] for i, n in enumerate(interneurons_ds)}\nnavis.plot3d([interneurons_ds], color=colors)\n</code></pre>"},{"location":"generated/gallery/4_remote/tutorial_remote_04_h01/#fetch-skeletons","title":"Fetch Skeletons","text":"<pre><code>interneurons_sk = navis.skeletonize(interneurons, parallel=True)\ninterneurons_sk\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 7 neurons (33.6MiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units 0 navis.TreeNeuron None 864691132301485893 15981 None 155 6223 1045000.75 9467 1 nanometer 1 navis.TreeNeuron None 864691132294086751 15318 None 290 3582 1916836.75 9271 1 nanometer ... ... ... ... ... ... ... ... ... ... ... 5 navis.TreeNeuron None 864691132382438988 23380 None 370 8285 2203973.75 19206 1 nanometer 6 navis.TreeNeuron None 864691132512567744 35993 None 355 14852 1970944.50 24725 1 nanometer <pre><code># Plot\nnavis.plot3d([interneurons_sk[0], interneurons[0]], color=[(1, 0, 0), (1, 1, 1, 0.5)])\n</code></pre> <p>Total running time of the script: ( 4 minutes  3.433 seconds)</p> <p> Download Python source code: tutorial_remote_04_h01.py</p> <p> Download Jupyter notebook: tutorial_remote_04_h01.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/5_nblast/mg_execution_times/","title":"Computation times","text":""},{"location":"generated/gallery/5_nblast/mg_execution_times/#computation-times","title":"Computation times","text":"<p>00:45.002 total execution time for generated_gallery_5_nblast files:</p> <p>+-------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_nblast_00_intro (docs/examples/5_nblast/tutorial_nblast_00_intro.py)                            | 00:44.085 | 0.0 MB | +-------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_nblast_03_smat (docs/examples/5_nblast/tutorial_nblast_03_smat.py)                               | 00:00.917 | 0.0 MB | +-------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | zzz_tutorial_nblast_01_flycircuit (docs/examples/5_nblast/zzz_tutorial_nblast_01_flycircuit.py) | 00:00.000 | 0.0 MB | +-------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | zzz_tutorial_nblast_02_hemibrain (docs/examples/5_nblast/zzz_tutorial_nblast_02_hemibrain.py)    | 00:00.000 | 0.0 MB | +-------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+</p>"},{"location":"generated/gallery/5_nblast/tutorial_nblast_00_intro/","title":"NBLAST","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/5_nblast/tutorial_nblast_00_intro/#nblast","title":"NBLAST","text":"<p>This tutorial will introduce you to NBLAST (Costa et al., 2016), a method to compare neurons based on their morphology.</p>"},{"location":"generated/gallery/5_nblast/tutorial_nblast_00_intro/#what-is-nblast","title":"What is NBLAST?","text":"<p>A brief introduction (modified from Jefferis lab's website):</p> <p>NBLAST works by decomposing neurons into point and tangent vector representations - so called \"dotprops\". Similarity between a given query and a given target neuron is determined by:</p> <ol> <li> <p>Nearest-neighbor search:</p> <p>For each point + tangent vector \\(u_{i}\\) of the query neuron, find the closest point + tangent vector \\(v_{i}\\) on the target neuron (this is a simple nearest-neighbor search using Euclidean distance).</p> <p></p> </li> <li> <p>Calculate a raw score:</p> <p>The raw score is a <code>weighted</code> product from the distance \\(d_{i}\\) between the points in each pair and the absolute dot product of the two tangent vectors \\(| \\vec{u_i} \\cdot \\vec{v_i} |\\).</p> <p>The absolute dot product is used because the orientation of the tangent vectors typically has no meaning in our data representation.</p> <p>A suitable scoring function \\(f\\) was determined empirically (see the NBLAST paper) and is shipped with NAVis as scoring matrices:</p> <p></p> <p>Importantly, these matrices were created using Drosophila neurons from the FlyCircuit light-level dataset which are in microns. Consequently, you should make sure your neurons are also in micrometer units for NBLAST! If you are working on non-insect neurons you might have to play around with the scaling to improve results. Alternatively, you can also produce your own scoring function (see this tutorial).</p> </li> <li> <p>Produce a per-pair score:</p> <p>This is done by simply summing up the raw scores over all point + tangent vector pairs for a given query-target neuron pair.</p> </li> <li> <p>Normalize raw score</p> <p>This step is optional but highly recommended: normalizing the raw score by dividing by the raw score of a self-self comparison of the query neuron.</p> </li> </ol> <p>Putting it all together, the formula for the raw score \\(S\\) is:</p> \\[ S(query,target)=\\sum_{i=1}^{n}f(d_{i}, |\\vec{u_i} \\cdot \\vec{v_i}|) \\] <p>The direction of the comparison matters!</p> <p>Consider two very different neurons - one large, one small - that overlap in space. If the small neuron is the query, you will always find a close-by nearest-neighbour among the many points of the large target neuron. Consequently, this small  large comparison will produce a decent NBLAST score. By contrast, the other way around (large  small) will likely produce a bad NBLAST score because many points in the large neuron are far away from the closest point in the small neuron. In practice, we typically use the mean between those forward and the reverse scores. This is done either by running two NBLASTs (query  target and target  query), or by passing e.g. <code>scores=\"mean\"</code> to the respective NBLAST function.</p>"},{"location":"generated/gallery/5_nblast/tutorial_nblast_00_intro/#running-nblast","title":"Running NBLAST","text":"<p>Broadly speaking, there are two applications for NBLAST:</p> <ol> <li>Matching neurons neurons between two datasets</li> <li>Clustering neurons into morphologically similar groups</li> </ol> <p>Before we get our feet wet, two things to keep in mind:</p> <ul> <li>neurons should be in microns as this is what NBLAST's scoring matrices have been optimized for (see above)</li> <li>neurons should have similar sampling resolution (i.e. points per unit of cable)</li> </ul> Speeding up NBLAST <p>For a ~2x speed boost, install the pykdtree library: <code>pip3 install pykdtree</code>.</p> <p>If you installed NAVis with the <code>pip install navis[all]</code> option you should already have it.</p> <p>OK, let's get started!</p> <p>We will use the example neurons that come with NAVis. These are all of the same type, so we don't expect to find very useful clusters - good enough to demo though!</p> <p>Load example neurons</p> <pre><code>import navis\n\nnl = navis.example_neurons()\n</code></pre> <p>NBLAST works on dotprops - these consist of points and tangent vectors decribing the shape of a neuron and are represented by the <code>navis.Dotprops</code> class in NAVis. You can generate those dotprops from skeletons (i.e. <code>TreeNeurons</code>), meshes (i.e. <code>MeshNeurons</code>) (see <code>navis.make_dotprops</code> for details) or straight from image data (see <code>navis.read_nrrd</code> and <code>navis.read_tiff</code>) - e.g. confocal stacks.</p> <pre><code># Convert neurons into microns (they are 8nm)\nnl_um = nl / (1000 / 8)\n\n# Generate dotprops\ndps = navis.make_dotprops(nl_um, k=4, resample=False)\n\n# Run the actual NBLAST: the first two vs the last two neurons\nnbl = navis.nblast(dps[:2], dps[2:], progress=False)\nnbl\n</code></pre> target 722817260 754534424 754538881 query 1734350788 0.751944 0.734036 0.770660 1734350908 0.679263 0.763096 0.753379 <p>Painless, wasn't it? The <code>nbl</code> scores dataframe has the query neurons as rows and the target neurons as columns.</p> <p>Let's run an all-by-all NBLAST next:</p> <pre><code>aba = navis.nblast_allbyall(dps, progress=False)\naba\n</code></pre> target 1734350788 1734350908 722817260 754534424 754538881 query 1734350788 1.000000 0.744841 0.751944 0.734036 0.770660 1734350908 0.732854 1.000000 0.679263 0.763096 0.753379 722817260 0.773451 0.741444 1.000000 0.742954 0.793883 754534424 0.766740 0.766891 0.714837 1.000000 0.785283 754538881 0.761127 0.751072 0.753679 0.762648 1.000000 <p>This demonstrates two things:</p> <ol> <li>The forward and reverse scores are never exactly the same (as noted above).</li> <li>The diagonal is always 1 because it is a self-self comparison (i.e. a perfect match) and we normalize against that.</li> </ol> <p>Let's run some quick &amp; dirty analysis just to illustrate things.</p> <p>For hierarchical clustering we need the matrix to be symmetrical - which our all-by-all matrix is not. We will therefore use the mean of forward and reverse scores (you could also use e.g. the minimum or the maximum):</p> <pre><code>aba_mean = (aba + aba.T) / 2\n</code></pre> <p>We also need distances instead of similarities!</p> <p>Invert to get distances Because our scores are normalized, we know the max similarity is 1</p> <pre><code>aba_dist = 1 - aba_mean\naba_dist\n</code></pre> target 1734350788 1734350908 722817260 754534424 754538881 query 1734350788 0.000000 0.261152 0.237303 0.249612 0.234106 1734350908 0.261152 0.000000 0.289647 0.235006 0.247775 722817260 0.237303 0.289647 0.000000 0.271104 0.226219 754534424 0.249612 0.235006 0.271104 0.000000 0.226034 754538881 0.234106 0.247775 0.226219 0.226034 0.000000 <p>Now we can use scipy's hierarchical clustering to generate a dendrogram</p> <pre><code>from scipy.spatial.distance import squareform\nfrom scipy.cluster.hierarchy import linkage, dendrogram, set_link_color_palette\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcl\nimport seaborn as sns\n\nset_link_color_palette([mcl.to_hex(c) for c in sns.color_palette(\"muted\", 10)])\n\n# To generate a linkage, we have to bring the matrix from square-form to vector-form\naba_vec = squareform(aba_dist, checks=False)\n\n# Generate linkage\nZ = linkage(aba_vec, method=\"ward\")\n\n# Plot a dendrogram\ndn = dendrogram(Z, labels=aba_mean.columns)\n\nax = plt.gca()\nax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha=\"right\")\n\nsns.despine(trim=True, bottom=True)\nplt.tight_layout()\n</code></pre> <p></p> <p>We'll leave it at that for now but just to have it mentioned: there is also a <code>navis.nblast_smart</code> function which tries to cut some corners and may be useful if you want to run very large NBLASTs.</p> <p>These are the functions we seen so far:</p> <ul> <li><code>navis.nblast</code>: classic query  target NBLAST</li> <li><code>navis.nblast_allbyall</code>: pairwise, all-by-all NBLAST</li> <li><code>navis.nblast_smart</code>: a \"smart\" version of NBLAST</li> </ul>"},{"location":"generated/gallery/5_nblast/tutorial_nblast_00_intro/#another-flavour-synblast","title":"Another flavour: syNBLAST","text":"<p>SyNBLAST is synapse-based NBLAST: instead of turning neurons into dotprops, we use their synapses to perform NBLAST (minus the vector component). This is generally faster because we can skip generating dotprops and calculating vector dotproducts. It also focusses the attention on the synapse-bearing axons and dendrites, effectively ignoring the backbone. This changes the question from \"Do neurons look the same?\" to \"Do neurons have in- and output in the same area?\". See <code>navis.synblast</code> for details.</p> <p>Let's try the above but with syNBLAST:</p> <pre><code># Importantly, we still want to use data in microns!\nsynbl = navis.synblast(nl_um, nl_um, by_type=True, progress=False)\nsynbl\n</code></pre> target 1734350788 1734350908 722817260 754534424 754538881 query 1734350788 1.000000 0.831690 0.828234 0.820261 0.849559 1734350908 0.831721 1.000000 0.741676 0.857372 0.833785 722817260 0.842672 0.827759 1.000000 0.815891 0.853005 754534424 0.862494 0.853088 0.793840 1.000000 0.866798 754538881 0.845366 0.833463 0.829094 0.844647 1.000000 <p>The same as above, we can run an all-by-all synNBLAST and generate a dendrogram:</p> <pre><code>aba_vec = squareform(((synbl + synbl.T) / 2 - 1) * -1, checks=False)\n\nZ = linkage(aba_vec, method=\"ward\")\n\ndn = dendrogram(Z, labels=synbl.columns)\n\nax = plt.gca()\nax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha=\"right\")\n\nsns.despine(trim=True, bottom=True)\nplt.tight_layout()\n</code></pre> <p></p>"},{"location":"generated/gallery/5_nblast/tutorial_nblast_00_intro/#a-real-world-example","title":"A real-world example","text":"<p>The toy data above is not really suited to demonstrate NBLAST because these neurons are of the same type (i.e. we do not expect to see differences).</p> <p>Let's try something more elaborate and pull some hemibrain neurons from neuPrint. For this you need to install the <code>neuprint-python</code> package (<code>pip3 install neuprint-python</code>), make a neuPrint account and generate/set an authentication token. Sounds complicated but is all pretty painless - see the neuPrint documentation for details. There is also a separate NAVis tutorial on neuprint here.</p> <p>Once that's done we can get started by importing the neuPrint interface from NAVis:</p> <pre><code>import navis.interfaces.neuprint as neu\n\n# Set a client\nclient = neu.Client(\"https://neuprint.janelia.org\", dataset=\"hemibrain:v1.2.1\")\n</code></pre> <p>Next we will fetch all olfactory projection neurons of the lateral lineage using a regex pattern.</p> <pre><code>pns = neu.fetch_skeletons(\n    neu.NeuronCriteria(type=\".*lPN.*\", regex=True), with_synapses=True, client=client\n)\n\n# Drop neurons on the left hand side\npns = pns[[not n.name.endswith(\"_L\") for n in pns]]\n\npns.head()\n</code></pre> <p>Out:</p> <pre><code>  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.44it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.43it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.48it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.48it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.38it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.38it/s]\n\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.65it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.64it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:00&lt;00:00,  2.86it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  2.86it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  5.28it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  5.28it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  5.36it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  5.36it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  2.52it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  2.52it/s]\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  1.30it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  1.30it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:00&lt;00:00,  3.28it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  3.28it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.51it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.50it/s]\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.52it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.51it/s]\n\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  5.78it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  5.77it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.46it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.45it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  6.50it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  6.50it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:00&lt;00:00,  6.21it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  6.20it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  5.57it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  5.57it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.91it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.91it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  3.57it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  3.57it/s]\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  5.04it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  5.03it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  6.10it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  6.09it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:00&lt;00:00,  2.71it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  2.71it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  5.28it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  5.28it/s]\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.79it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.78it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  5.91it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  5.91it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.91it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.90it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.79it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.78it/s]\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.29it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.29it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  5.49it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  5.48it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.38it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.38it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  6.15it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  6.14it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  5.36it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  5.36it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  5.30it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  5.29it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  5.75it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  5.74it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  5.46it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  5.46it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  3.81it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  3.81it/s]\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  5.99it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  5.99it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  3.26it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  3.26it/s]\n\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  5.31it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  5.31it/s]\n\n\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.63it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.63it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:00&lt;00:00,  5.43it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  5.42it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  3.91it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  3.91it/s]\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  3.92it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  3.92it/s]\n\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.50it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.50it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:00&lt;00:00,  3.30it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  3.30it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.91it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.90it/s]\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  6.26it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  6.25it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  5.92it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  5.92it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.08it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.08it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.47it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.46it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:00&lt;00:00,  6.22it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  6.21it/s]\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.57it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.57it/s]\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.62it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.62it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  3.94it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  3.93it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n100%|##########| 1/1 [00:00&lt;00:00,  4.82it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  4.81it/s]\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  5.34it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  5.33it/s]\n\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  2.71it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  2.71it/s]\n\n\n100%|##########| 1/1 [00:00&lt;00:00,  2.96it/s]\n100%|##########| 1/1 [00:00&lt;00:00,  2.96it/s]\n</code></pre> type name id n_nodes n_connectors n_branches n_leafs cable_length soma units 0 navis.TreeNeuron VA5_lPN_R 574032862 4509 2849 739 763 302345.37500 9.0 8 nanometer 1 navis.TreeNeuron VA5_lPN_R 574037281 4365 2891 719 756 299413.90625 19.0 8 nanometer 2 navis.TreeNeuron VA5_lPN_R 574374051 4480 3000 751 786 309890.12500 6.0 8 nanometer 3 navis.TreeNeuron VP1l+VP3_ilPN 603785283 8920 2345 887 915 396369.37500 NaN 8 nanometer 4 navis.TreeNeuron VC1_lPN_R 606090268 7247 6862 1556 1603 559409.12500 7096.0 8 nanometer <p>Generate dotprops</p> <pre><code># These neurons are in 8x8x8nm (voxel) resolution\npns_um = pns / (1000 / 8)  # convert to microns\npns_dps = navis.make_dotprops(pns_um, k=5)\npns_dps\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 54 neurons (7.6MiB) type name id k units n_points 0 navis.Dotprops VA5_lPN_R 574032862 5 1.0 micrometer 4509 1 navis.Dotprops VA5_lPN_R 574037281 5 1.0 micrometer 4365 ... ... ... ... ... ... ... 52 navis.Dotprops VA7m_lPN_R 5813055180 5 1.0 micrometer 3739 53 navis.Dotprops DM2_lPN_R 5901222910 5 1.0 micrometer 6484 <p>Run an all-by-all NBLAST and synNBLAST</p> <pre><code>pns_nbl = navis.nblast_allbyall(pns_dps, progress=False)\npns_synbl = navis.synblast(pns_um, pns_um, by_type=True, progress=False)\n\n# Generate the linear vectors\nnbl_vec = squareform(((pns_nbl + pns_nbl.T) / 2 - 1) * -1, checks=False)\nsynbl_vec = squareform(((pns_synbl + pns_synbl.T) / 2 - 1) * -1, checks=False)\n\n# Generate linkages\nZ_nbl = linkage(nbl_vec, method=\"ward\", optimal_ordering=True)\nZ_synbl = linkage(synbl_vec, method=\"ward\", optimal_ordering=True)\n\n# Plot dendrograms\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\ndn1 = dendrogram(Z_nbl, no_labels=True, color_threshold=1, ax=axes[0])\ndn2 = dendrogram(Z_synbl, no_labels=True, color_threshold=1, ax=axes[1])\n\naxes[0].set_title(\"NBLAST\")\naxes[1].set_title(\"synNBLAST\")\n\nsns.despine(trim=True, bottom=True)\n</code></pre> <p></p> <p>While we don't know which leaf is which, the structure in both dendrograms looks similar. If we wanted to take it further than that, we could use tanglegram to line up the two clusterings and compare them.</p> <p>But let's save that for another day and instead do some plotting:</p> <p>Generate clusters</p> <pre><code>from scipy.cluster.hierarchy import fcluster\n\ncl = fcluster(Z_synbl, t=1, criterion=\"distance\")\ncl\n</code></pre> <p>Out:</p> <pre><code>array([ 6,  6,  6, 14,  3,  8,  8,  8,  8, 15,  2,  2,  7,  2, 13, 10, 10,\n       14,  1,  1, 13,  3,  1, 10, 11, 10,  2,  2,  7,  2,  7,  7,  9,  7,\n        9,  4, 11,  3,  9,  5,  9, 12,  9, 14, 15, 14, 11, 11, 12,  2, 13,\n        5,  5,  8], dtype=int32)\n</code></pre> <p>Now plot each cluster. For simplicity we are plotting in 2D here:</p> <pre><code>import math\n\nn_clusters = max(cl)\nrows = 4\ncols = math.ceil(n_clusters / 4)\nfig, axes = plt.subplots(rows, cols, figsize=(20, 5 * cols))\n# Flatten axes\naxes = [ax for l in axes for ax in l]\n\n# Generate colors\npal = sns.color_palette(\"muted\", n_clusters)\n\nfor i in range(n_clusters):\n    ax = axes[i]\n    ax.set_title(f\"cluster {i + 1}\")\n    # Get the neurons in this cluster\n    this = pns[cl == (i + 1)]\n\n    navis.plot2d(\n        this, method=\"2d\", ax=ax, color=pal[i], lw=1.5, view=(\"x\", \"-z\"), alpha=0.5\n    )\n\nfor ax in axes:\n    ax.set_aspect(\"equal\")\n    ax.set_axis_off()\n\n    # Set all axes to the same limits\n    bbox = pns.bbox\n    ax.set_xlim(bbox[0][0], bbox[0][1])\n    ax.set_ylim(bbox[2][1], bbox[2][0])\n\nplt.tight_layout()\n</code></pre> <p></p> <p>Note how clusters 3 and 8 look a bit odd? That's because these likely still contain more than one type of neuron. We should probably have gone with a slightly finer clustering. But this little demo should be enough to get you started!</p> <p>Total running time of the script: ( 0 minutes  44.085 seconds)</p> <p> Download Python source code: tutorial_nblast_00_intro.py</p> <p> Download Jupyter notebook: tutorial_nblast_00_intro.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/5_nblast/tutorial_nblast_03_smat/","title":"Custom score matrices","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/5_nblast/tutorial_nblast_03_smat/#custom-score-matrices","title":"Custom score matrices","text":"<p>This tutorial shows how to train a custom score matrix for NBLAST.</p> <p>The core of the NBLAST algorithm is a function which converts point matches (defined by a distance and the dot product of the tangent vectors) into a score expressing how likely they are to have come from the same cell type. This function is typically a 2D lookup table, referred to as the \"score matrix\", generated by \"training\" it on sets of neurons known to be either matching or non-matching.</p> <p>NAVis provides (and uses by default) the score matrix used in the original publication (Costa et al., 2016) which is based on FlyCircuit, a light-level database of Drosophila neurons. Let's quickly visualize this:</p> <pre><code>import navis\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsmat = navis.nbl.smat.smat_fcwb().to_dataframe().T\nax = sns.heatmap(smat, cbar_kws=dict(label=\"raw score\"))\nax.set_xlabel(\"distance [um]\")\nax.set_ylabel(\"dot product\")\nplt.tight_layout()\n</code></pre> <p></p> <p>This scoring matrix works suprisingly well in many cases! However, how appropriate it is for your data depends on a number of factors:</p> <ul> <li>How big your neurons are (commonly addressed by scaling the distance axis of the built-in score matrix)</li> <li>How you have pre-processed your neurons (pruning dendrites, resampling etc.)</li> <li>Your actual task (matching left-right pairs, finding lineages etc.)</li> <li>How distinct you expect your matches and non-matches to be (e.g. how large a body volume you're drawing neurons from)</li> </ul> <p>Utilities in <code>navis.nbl.smat</code> allow you to train your own score matrix.</p> <p>Let's first have a look at the relevant classes:</p> <pre><code>from navis.nbl.smat import Lookup2d, Digitizer, LookupDistDotBuilder\n</code></pre> <ul> <li><code>navis.nbl.smat.Lookup2d</code>: this is the lookup table, it can be passed to any nblast-related class or function</li> <li><code>navis.nbl.smat.Digitizer</code>: this converts continuous values into discrete indices which are used to look up score values in an array, we need one for each axis of the lookup table</li> <li><code>navis.nbl.smat.LookupDistDotBuilder</code> is the helper class which generates <code>navis.nbl.smat.Lookup2d</code> instances from training data</li> </ul> <p>First, we need some training data. Let's augment our set of example neurons by randomly mutating them:</p> <pre><code>import numpy as np\n\n# Use a replicable random number generator\nRNG = np.random.default_rng(2021)\n\n\ndef augment_neuron(\n    nrn: navis.TreeNeuron, scale_sigma=0.1, translation_sigma=50, jitter_sigma=10\n):\n    \"\"\"Mutate a neuron by translating, scaling and jittering its nodes.\"\"\"\n    nrn = nrn.copy(deepcopy=True)\n    nrn.name += \"_aug\"\n    dims = list(\"xyz\")\n    coords = nrn.nodes[dims].to_numpy()\n\n    # translate whole neuron\n    coords += RNG.normal(scale=translation_sigma, size=coords.shape[-1])\n    # jitter individual coordinates\n    coords += RNG.normal(scale=jitter_sigma, size=coords.shape)\n    # rescale\n    mean = np.mean(coords, axis=0)\n    coords -= mean\n    coords *= RNG.normal(loc=1.0, scale=scale_sigma)\n    coords += mean\n\n    nrn.nodes[dims] = coords\n    return nrn\n\n\noriginal = list(navis.example_neurons())\njittered = [augment_neuron(n) for n in original]\n\ndotprops = [navis.make_dotprops(n, k=5, resample=False) for n in original + jittered]\nmatching_pairs = [[idx, idx + len(original)] for idx in range(len(original))]\n</code></pre> <p>The score matrix builder needs some neurons as a list of <code>navis.Dotprops</code> objects, and then to know which neurons should match with each other as indices into that list. It's assumed that matches are relatively rare among the total set of possible pairings, so non-matching pairs are drawn randomly (although a non-matching list can be given explicitly).</p> <p>Then it needs to know where to draw the boundaries between bins in the output lookup table. These can be given explicitly as a list of 2 <code>Digitizer</code>s, or can be inferred from the data: bins will be drawn to evenly partition the matching neuron scores.</p> <p>The resulting <code>Lookup2d</code> can be imported/exported as a <code>pandas.DataFrame</code> for ease of viewing and storing.</p> <pre><code>builder = LookupDistDotBuilder(\n    dotprops, matching_pairs, use_alpha=True, seed=2021\n).with_bin_counts([8, 5])\nsmat = builder.build()\nas_table = smat.to_dataframe()\nas_table\n</code></pre> <p>Out:</p> <pre><code>Drawing non-matching pairs:   0%|          | 0/46442 [00:00&lt;?, ?it/s]\n\n\n\nComparing matching pairs:   0%|          | 0/10 [00:00&lt;?, ?it/s]\n\n\n\nComparing non-matching pairs:   0%|          | 0/11 [00:00&lt;?, ?it/s]\n</code></pre> [0.0,0.14620110690593718) [0.14620110690593718,0.298941045999527) [0.298941045999527,0.48067988157272357) [0.48067988157272357,0.7352141737937927) [0.7352141737937927,0.9988406300544739) [2.014085054397583,57.849366188049316) 1.071521 0.962062 0.889086 0.928131 1.135020 [57.849366188049316,81.31283569335938) 0.879772 0.847659 0.878059 0.831803 0.926530 [81.31283569335938,104.08576202392578) 0.622404 0.626709 0.592930 0.636826 1.182405 [104.08576202392578,128.14104461669922) 0.538608 0.494753 0.412895 0.521149 1.428743 [128.14104461669922,155.36119651794434) 0.147198 0.223463 0.139783 0.472255 1.815169 [155.36119651794434,202.6728515625) -0.529167 -0.365049 -0.428977 -0.206057 1.615743 [202.6728515625,395.9569091796875) -1.499912 -1.320891 -1.316638 -1.178487 -0.164314 [395.9569091796875,4709.61474609375) -1.382498 -1.230513 -1.164390 -0.959355 0.095531 <p>Now that we can have this score matrix, we can use it for a problem which can be solved by NBLAST: we've mixed up a bag of neurons which look very similar to some of our examples, and need to know which they match with.</p> <pre><code>original_dps = dotprops[: len(original)]\nnew_dps = [\n    navis.make_dotprops(augment_neuron(n), k=5, resample=False) for n in original\n]\nRNG.shuffle(new_dps)\n\nresult = navis.nblast(\n    original_dps,\n    new_dps,\n    use_alpha=True,\n    scores=\"mean\",\n    normalized=True,\n    smat=smat,\n    n_cores=1,\n)\nresult.index = [dp.name for dp in original_dps]\nresult.columns = [dp.name for dp in new_dps]\nresult\n</code></pre> <p>Out:</p> <pre><code>Preparing:   0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\nNBlasting:   0%|          | 0/5 [00:00&lt;?, ?it/s]\n</code></pre> DA1_lPN_R_aug DA1_lPN_R_aug DA1_lPN_R_aug DA1_lPN_R_aug DA1_lPN_R_aug DA1_lPN_R -0.237900 0.740553 -0.247653 -0.536301 -0.196978 DA1_lPN_R -0.287879 -0.248290 -0.288717 -0.266280 -0.350840 DA1_lPN_R -0.136732 -0.216223 -0.328166 -0.529397 0.424477 DA1_lPN_R -0.171776 -0.234457 0.485958 -0.530730 -0.326475 DA1_lPN_R 0.121249 -0.059504 -0.084433 -0.450605 -0.023573 <p>You can see that while there aren't any particularly good scores (this is a very small amount of training data, and one would normally preprocess the neurons), in each case the original's best match is its augmented partner.</p> <p>Total running time of the script: ( 0 minutes  0.917 seconds)</p> <p> Download Python source code: tutorial_nblast_03_smat.py</p> <p> Download Jupyter notebook: tutorial_nblast_03_smat.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/5_nblast/zzz_tutorial_nblast_01_flycircuit/","title":"NBLAST against FlyCircuit","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/5_nblast/zzz_tutorial_nblast_01_flycircuit/#nblast-against-flycircuit","title":"NBLAST against FlyCircuit","text":"<p>This example demonstrates how to run an NBLAST against the entire FlyCircuit dataset.</p> <p>This example is not executed</p> <p>In contrast to almost all other tutorials, this one is not executed when the documentation is built. Consequently, it also does not show any code output or figures. That's because this example requires downloading a large dataset (~850Mb) and running an NBLAST against it, it is simply not feasible to run this as part of the documentation build process.</p> <p>If you work with Drosophila, chances are you have heard of FlyCircuit. It's a collection of ~16k single neuron clones published by Chiang et al. (2010).</p> <p>For R, there is a package containing dotprops and meta data for these neurons: <code>nat.flycircuit</code>. This does not yet exist for Python. However, it's still pretty straight forward to run an NBLAST against the entire flycircuit dataset in Python!</p> <p>First you need to download flycircuit dotprops from Zenodo (~850Mb). Next, unzip the archive containing the dotprops as CSV files. Now we we need to load them into NAVis (adjust filepaths as required):</p> <pre><code>import navis\nimport pandas as pd\n\nfrom pathlib import Path\nfrom tqdm import tqdm\n\ndef load_dotprops_csv(fp):\n    \"\"\"Load dotprops from CSV files in filepath `fp`.\"\"\"\n    # Turn into a path object\n    fp = Path(fp).expanduser()\n\n    # Go over each CSV file\n    files = list(fp.glob('*.csv'))\n    dotprops = []\n    for f in tqdm(files, desc='Loading dotprops', leave=False):\n        # Read file\n        csv = pd.read_csv(f)\n\n        # Each row is a point with associated vector\n        pts = csv[['pt_x', 'pt_y', 'pt_z']].values\n        vect = csv[['vec_x', 'vec_y', 'vec_z']].values\n\n        # Turn into a Dotprops\n        dp = navis.Dotprops(points=pts, k=20, vect=vect, units='1 micron')\n\n        # Use filename as ID/name\n        dp.name = dp.id = f.name[:-4]\n\n        # Add this dotprop to the list before moving on to the next\n        dotprops.append(dp)\n\n    return navis.NeuronList(dotprops)\n</code></pre> <p>Load dotprops from the filepath</p> <pre><code>fc_dps = load_dotprops_csv('flycircuit_dotprops')\nfc_dps\n</code></pre> <p>Note</p> <p>To avoid having to re-generate these dotprops, you could consider pickling them: </p><pre><code>import pickle\nwith open('flycircuit_dps.pkl', 'wb') as f:\n    pickle.dump(fc_dps, f)\n</code></pre><p></p> <p>In the future you can then reload the dotprops like so (much faster than loading from CSV): </p><pre><code>with open('flycircuit_dps.pkl', 'rb') as f:\n    fc_dps = pickle.load(f)\n</code></pre><p></p> <p>Note: The names/ids correspond to the flycircuit gene + clone names:</p> <pre><code>fc_dps[0]\n</code></pre> <p>In case your query neurons are in a different brain space, you can use flybrains to convert them to flycircuit's <code>FCWB</code> space.</p> <p>For demonstration purposes we will use the example neurons - olfactory DA1 projection neurons from the hemibrain connectome - that ship with NAVis and try to find their closest match in the FlyCircuit dataset.</p> <pre><code># Load some of the example neurons\nn = navis.example_neurons(3)\n\n# Convert from hemibrain (JRCFIB2018Fraw) to FCWB space\nimport flybrains\n\nn_fcwb = navis.xform_brain(n, source='JRCFIB2018Fraw', target='FCWB')\n</code></pre> <p>A sanity check to make sure the transform worked</p> <pre><code>fig, ax = navis.plot2d([n_fcwb, flybrains.FCWB])\n</code></pre> <p>FlyCircuit neurons are all on the left side of the brain. We need mirror our neurons from the right to the left to match that.</p> <pre><code>n_fcwb_mirr = navis.mirror_brain(n_fcwb, template='FCWB')\n</code></pre> <p>Convert our neurons to dotprops</p> <pre><code>n_dps = navis.make_dotprops(n_fcwb_mirr, resample=1, k=None)\n</code></pre> <p>Run a \"smart\" NBLAST to get the top hits</p> <pre><code>scores = navis.nblast_smart(query=n_dps, target=fc_dps, scores='mean', progress=False)\nscores.head()\n</code></pre> <p>Note</p> <p>If you get a warning about data not being in microns: that's a rounding error from the transform and can be safely ignored.</p> <p>Find the top hits for each of our query neurons</p> <pre><code>import numpy as np\n\nfor dp in n_dps:\n    hit = scores.columns[np.argmax(scores.loc[dp.id].values)]\n    sc = scores.loc[dp.id].values.max()\n    print(f'Top hit for {dp.id}: {hit} ({sc:.3f})')\n</code></pre> <p>All of our query neurons should have the same top match (they are all from the same cell type after all): <code>FruMARCM-F001496_seg001</code></p> <p>Let's co-visualize: Queries in red, hit in black</p> <pre><code>fig, ax = navis.plot2d([n_fcwb_mirr, fc_dps.idx['FruMARCM-F001496_seg001']],\n                       color=['r'] * len(n_fcwb_mirr) + ['k'])\n</code></pre> <p>Looking good!</p> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: zzz_tutorial_nblast_01_flycircuit.py</p> <p> Download Jupyter notebook: zzz_tutorial_nblast_01_flycircuit.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/5_nblast/zzz_tutorial_nblast_02_hemibrain/","title":"NBLAST using light-level data","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/5_nblast/zzz_tutorial_nblast_02_hemibrain/#nblast-using-light-level-data","title":"NBLAST using light-level data","text":"<p>This example demonstrates how to use NBLAST to match light-level neurons against EM skeletons.</p> <p>This example is not executed</p> <p>In contrast to almost all other tutorials, this one is not executed when the documentation is built. Consequently, it also does not show any code output or figures. That's because this example requires downloading a large dataset (~1.8GB) and running an NBLAST against it, it is simply not feasible to run this as part of the documentation build process.</p> <p>One of the applications of NBLAST is to match neurons across data sets. Here we will illustrate this by taking a light-level, confocal image stack and finding the same neuron in an EM connectome.</p> <p>Specifically, we will use an image from Janelia's collection of split-Gal4 driver lines and match it against the neurons in the hemibrain connectome.</p> <p>Before we get started make sure that:</p> <ul> <li>NAVis is installed and up-to-date</li> <li><code>flybrains</code> is installed and you have downloaded the Saalfeld lab's and VFB bridging    transforms (see <code>flybrains.download_...</code> functions)</li> <li>download and extract hemibrain-v1.2-skeletons.tar   (kindly provided by Stuart Berg, Janelia Research Campus)</li> </ul> <p>Next we need to pick an image stack to use as query. You can browse the expression patterns of the Janelia split-Gal4 lines here. I ended up picking <code>LH1112</code> which is a very clean line containing a couple of WED projection neurons. Among other data, you can download these stacks as \"gendered\" (i.e. female or male) or \"unisex\" space. Unfortunately, all image stacks are in Janelia's <code>.h5j</code> format which I haven't figured out how to read straight into Python.</p> <p>Two options:</p> <ol> <li>Load them into Fiji and save the GFP signal channel as <code>.nrrd</code> file.</li> <li>Go to VirtualFlyBrain, search for your line of interested LH1112 (not all lines are be available on VFB)    and download the \"Signal(NRRD)\" at the bottom of Term Info panel on the right hand side.</li> </ol> <p>I went for option 2 here and downloaded a <code>VFB_001013cg.nrrd</code>. This is the neuron we'll be searching for:</p> <p></p> <p>Let's get started!</p> <pre><code>import navis\n</code></pre> <p>First we need to load the image stack and turn it into dotprops:</p> <pre><code>query = navis.read_nrrd(\"VFB_001013cg.nrrd\", output=\"dotprops\", k=20, threshold=100)\nquery.id = \"LH1112\"  # manually set the ID to the Janelia identifier\nquery\n</code></pre> <pre><code># Inspect the results\nnavis.plot3d(query)\n</code></pre> <p>Note</p> <p>Depending on your image you will have to play around with the <code>threshold</code> parameter to get a decent dotprop representation.</p> <p>Next we need to load the hemibrain skeletons and convert them to dotprops:</p> <pre><code># Make sure to adjust the path to where you extracted the skeletons\nsk = navis.read_swc(\n    \"hemibrain-v1.2-skeletons/\", include_subdirs=True, fmt=\"{name,id:int}.swc\"\n)\n</code></pre> <p>These 97k skeletons include lots of small fragments - there are only ~25k proper neurons or substantial fragments thereof in the hemibrain dataset. So to make our life a little easier, we will keep only the largest 30k skeletons:</p> <pre><code>sk.sort_values(\"n_nodes\")\nsk = sk[:30_000]\nsk\n</code></pre> <p>Next up: turning those skeletons into dotprops:</p> <p>Note that we are resampling to 1 point for every micron of cable Because these skeletons are in 8nm voxels we have to use 1000/8</p> <pre><code>targets = navis.make_dotprops(sk, k=5, parallel=True, resample=1000 / 8)\n</code></pre> <p>Note</p> <p>Making the dotprops may take a while (mostly because of the resampling). You can dedicate more cores via the <code>n_cores</code> parameter. It may also make sense to save the dotprops for future use e.g. by pickling them.</p> <p>Last but not least we need to convert the image's dotprops from their current brain space (<code>JRC2018U</code>, <code>U</code> for \"unisex\") to hemibrain (<code>JRCFIB2018Fraw</code>, <code>raw</code> for voxels) space.</p> <pre><code>import flybrains\n\nquery_xf = navis.xform_brain(query, source=\"JRC2018U\", target=\"JRCFIB2018Fraw\")\n</code></pre> <p>Now we can run the actual NBLAST:</p> <pre><code># Note that we convert from the JRCFIB2018F voxel (8x8x8nm) space to microns\nscores = navis.nblast(query_xf / 125, targets / 125, scores=\"mean\")\nscores.head()\n</code></pre> <p>Note</p> <p>You can greatly speed up NBLAST by installing the optional dependency <code>pykdtree</code>: </p><pre><code>pip3 install pykdtree\n</code></pre><p></p> <p>Now we can sort those scores to find the top matches:</p> <pre><code>scores.loc[\"LH1112\"].sort_values(ascending=False)\n</code></pre> <p>So we did get a couple of hits here. Let's visualize the top 3:</p> <pre><code>fig = navis.plot3d([query_xf, targets.idx[[2030342003, 2214504597, 1069223047]]])\n</code></pre> <p>On a final note: the scores for those matches are rather low (1 = perfect match).</p> <p>The main reason for this is that our image stack contains two neurons (the left and the right version) so half of our <code>query</code> won't have a match in any of the individual hemibrain neurons. We could have fixed that by subsetting the query to the approximate hemibrain bounding box. This is also a good idea for bilateral neurons that have parts of their arbour outside the hemibrain volume:</p> <p>Remove the left-hand-side neuron based on the position along the x-axis (this is just one of the possible approaches) This is the approximate LHS boundary of the volume</p> <pre><code>flybrains.JRCFIB2018Fraw.mesh.vertices[:, 0].max()\n</code></pre> <pre><code>query_ss = navis.subset_neuron(query_xf, query_xf.points[:, 0] &lt;= 35_000)\nquery_ss\n</code></pre> <p>Using <code>query_ss</code> should yield much improved scores.</p> <p>Another potential pitfall is the generation of dotprops from the image itself: if you compare the image- against the skeleton-derived dotprops, you might notice that the latter have fewer and less dense points. That's a natural consequence the image containing multiple individuals of the same cell type but we could have tried to ameliorate this by some pre-processing (e.g. downsampling or thinning the image).</p> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: zzz_tutorial_nblast_02_hemibrain.py</p> <p> Download Jupyter notebook: zzz_tutorial_nblast_02_hemibrain.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/6_misc/mg_execution_times/","title":"Computation times","text":""},{"location":"generated/gallery/6_misc/mg_execution_times/#computation-times","title":"Computation times","text":"<p>00:19.082 total execution time for generated_gallery_6_misc files:</p> <p>+-----------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_misc_00_multiprocess (docs/examples/6_misc/tutorial_misc_00_multiprocess.py) | 00:14.133 | 0.0 MB | +-----------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_misc_01_transforms (docs/examples/6_misc/tutorial_misc_01_transforms.py)       | 00:04.948 | 0.0 MB | +-----------------------------------------------------------------------------------------------------------------------------+-----------+--------+</p>"},{"location":"generated/gallery/6_misc/tutorial_misc_00_multiprocess/","title":"Multiprocessing","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/6_misc/tutorial_misc_00_multiprocess/#multiprocessing","title":"Multiprocessing","text":"<p>This notebook will show you how to use parallel processing with <code>navis</code>.</p> <p>By default, most NAVis functions use only a single thread/process (although some third-party functions used under the hood might). Distributing expensive computations across multiple cores can speed things up considerable.</p> <p>Many NAVis functions natively support parallel processing. This notebook will illustrate various ways to use parallelism. Before we get start: NAVis uses <code>pathos</code> for multiprocessing - if you installed NAVis with <code>pip install navis[all]</code> you should be all set. If not, you can install <code>pathos</code> separately:</p> <pre><code>pip install pathos -U\n</code></pre>"},{"location":"generated/gallery/6_misc/tutorial_misc_00_multiprocess/#running-navis-functions-in-parallel","title":"Running NAVis functions in parallel","text":"<p>Since version <code>0.6.0</code> many NAVis functions accept a <code>parallel=True</code> and an (optional) <code>n_cores</code> parameter:</p> <pre><code>import time\nimport navis\n\ndef time_func(func, *args, **kwargs):\n    \"\"\"A function to time the execution of a function.\"\"\"\n    start = time.time()\n    func(*args, **kwargs)\n    print(f\"Execution time: {round(time.time() - start, 2)}s\")\n\n# Load example neurons\nnl = navis.example_neurons()\n</code></pre> <p>Important</p> <p>This documentation is built on Github Actions where the number of cores can be as low as 2. The speedup on your machine should be more pronounced than what you see below. That said: parallel processing has some overhead and for small tasks the overhead can be larger than the speed-up.</p> <p>Without parallel processing:</p> <pre><code>time_func (\n    navis.resample_skeleton,\n    nl,\n    resample_to=125\n)\n</code></pre> <p>Out:</p> <pre><code>Execution time: 1.38s\n</code></pre> <p>With parallel processing:</p> <pre><code>time_func (\n    navis.resample_skeleton,\n    nl,\n    resample_to=125,\n    parallel=True\n)\n</code></pre> <p>Out:</p> <pre><code>Execution time: 1.04s\n</code></pre> <p>The same also works for neuron methods!</p> <p>Without parallel processing:</p> <pre><code>time_func (\n    nl.resample, 125\n)\n</code></pre> <p>Out:</p> <pre><code>Execution time: 1.4s\n</code></pre> <p>With parallel processing:</p> <pre><code>time_func (\n    nl.resample, 125, parallel=True\n)\n</code></pre> <p>Out:</p> <pre><code>Execution time: 1.03s\n</code></pre> <p>By default <code>parallel=True</code> will use half the available CPU cores. You can adjust that behaviour using the <code>n_cores</code> parameter:</p> <pre><code>time_func (\n    nl.resample, 125, parallel=True, n_cores=2\n)\n</code></pre> <p>Out:</p> <pre><code>Execution time: 1.05s\n</code></pre> <p>Note</p> <p>The name <code>n_cores</code> is actually a bit misleading as it determines the number of parallel processes that NAVis will spawn. There is nothing stopping you from setting <code>n_cores</code> to a number higher than the number of available CPU cores. However, doing so will likely over-subscribe your CPU and end up slowing things down.</p>"},{"location":"generated/gallery/6_misc/tutorial_misc_00_multiprocess/#parallelizing-generic-functions","title":"Parallelizing generic functions","text":"<p>For non-NAVis function you can use <code>NeuronList.apply</code> to parallelize them.</p> <p>First, let's write a mock function that simply waits one second and then returns the number of nodes:</p> <pre><code>def my_func(x):\n    import time\n    time.sleep(1)\n    return x.n_nodes\n</code></pre> <pre><code># Without parallel processing\ntime_func (\n    nl.apply, my_func\n)\n</code></pre> <p>Out:</p> <pre><code>Execution time: 5.0s\n</code></pre> <pre><code># With parallel processing\ntime_func (\n    nl.apply, my_func, parallel=True\n)\n</code></pre> <p>Out:</p> <pre><code>Execution time: 3.17s\n</code></pre> <p>Total running time of the script: ( 0 minutes  14.133 seconds)</p> <p> Download Python source code: tutorial_misc_00_multiprocess.py</p> <p> Download Jupyter notebook: tutorial_misc_00_multiprocess.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/6_misc/tutorial_misc_01_transforms/","title":"Transformations","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/6_misc/tutorial_misc_01_transforms/#transformations","title":"Transformations","text":"<p>This tutorial will show you how to transform and mirror neurons.</p>"},{"location":"generated/gallery/6_misc/tutorial_misc_01_transforms/#introduction","title":"Introduction","text":"<p>As of version <code>0.5.0</code>, NAVis includes functions that let you transform and mirror spatial data (e.g. neurons). This new functionality splits into high- and low-level functions. In this tutorial, we will start by exploring the higher-level functions that most users will use and then take a sneak peak at the low-level functions.</p> <p>At the moment, navis supports the following transform types:</p> <ul> <li>CMTK warp transforms</li> <li>Hdf5 deformation fields</li> <li>Elastix transforms</li> <li>landmark-based thin-plate spline transforms</li> <li>affine transforms</li> </ul>"},{"location":"generated/gallery/6_misc/tutorial_misc_01_transforms/#flybrains","title":"flybrains","text":"<p>Since NAVis brings the utility but does not ship with any transforms, we have to either generate those ourselves or get them elsewhere. Here, we will showcase the flybrains library that provides a number of different transforms directly to NAVis. Setting up and registering your own custom transforms will be discussed further down.</p> <p>First, you need to get flybrains. Please follow the instructions to install and download the bridging registrations before you continue.</p> <pre><code>import flybrains\n</code></pre> <p>Importing <code>flybrains</code> automatically registers the transforms with NAVis. This in turn allows NAVis to plot a sequence of bridging transformations to map between any connected template spaces.</p> <p></p> <p>In addition to those bridging transforms, <code>flybrains</code> also contains mirror registrations (we will cover those later), meta data and meshes for the template brains:</p> <pre><code># This is the Janelia \"hemibrain\" template brain\nprint(flybrains.JRCFIB2018F)\n</code></pre> <p>Out:</p> <pre><code>Template brain\n--------------\nName: JRCFIB2018F\nShort Name: JRCFIB2018F\nType: None\nSex:  F\nDimensions: 34432 x 39552 x 41408 voxels\nVoxel size:\n  x = 8 nanometers\n  y = 8 nanometers\n  z = 8 nanometers\nBounding box (nanometers):\n  x = 0, y = 0, z = 0,\n  x = 275456, y = 316416, z = 331264,\nDescription: Nanometer-calibrated version of Janelia FIB hemibrain dataset.\nDOI: https://doi.org/10.1101/2020.01.21.911859\n</code></pre> <pre><code>import navis\nimport matplotlib.pyplot as plt\n\n# This is the hemibrain neuropil surface mesh\nfig, ax = navis.plot2d(flybrains.JRCFIB2018F, view=(\"x\", \"-z\"))\nplt.tight_layout()\n</code></pre> <p></p> <p>You can check the registered transforms like so:</p> <pre><code>navis.transforms.registry.summary()\n\n# !!! note\n#     The documentation is built in an environment with a minimal number of transforms registered. If you have installed\n#     and imported `flybrains`, you should see a lot more than what is shown above.\n</code></pre> source target transform type invertible weight weight_inv 0 JRC2018F JRCFIB2018Fum &lt;navis.transforms.h5reg.H5transform object at ... bridging True 1.0 1.0 1 JRC2018F FAFBum &lt;navis.transforms.h5reg.H5transform object at ... bridging True 1.0 1.0 2 FAFB14 FAFB14sym &lt;navis.transforms.thinplate.TPStransform objec... bridging True 1.0 1.0 3 FLYWIRE FLYWIREsym &lt;navis.transforms.thinplate.TPStransform objec... bridging True 1.0 1.0 4 FAFB14 JRCFIB2022M &lt;navis.transforms.thinplate.TPStransform objec... bridging True 1.0 1.0 5 FLYWIRE JRCFIB2022M &lt;navis.transforms.thinplate.TPStransform objec... bridging True 1.0 1.0 6 JRCFIB2022Mplot JRCFIB2022M &lt;navis.transforms.thinplate.TPStransform objec... bridging True 1.0 1.0 7 MANC FANC &lt;navis.transforms.thinplate.TPStransform objec... bridging True 1.0 1.0 8 BANC JRCFIB2022M &lt;navis.transforms.thinplate.TPStransform objec... bridging True 1.0 1.0 9 FANC FANCum_fixed &lt;navis.transforms.base.FunctionTransform objec... bridging False 0.1 0.1 10 FANCum_fixed JRCVNC2018F_reflected &lt;navis.transforms.elastix.ElastixTransform obj... bridging False 1.0 1.0 11 JRCVNC2018F_reflected JRCVNC2018F &lt;navis.transforms.base.FunctionTransform objec... bridging False 0.1 0.1 12 JRCVNC2018F JRCVNC2018F_reflected &lt;navis.transforms.base.FunctionTransform objec... bridging False 0.1 0.1 13 JRCVNC2018F_reflected FANCum_fixed &lt;navis.transforms.elastix.ElastixTransform obj... bridging False 1.0 1.0 14 FANCum_fixed FANC &lt;navis.transforms.base.FunctionTransform objec... bridging False 0.1 0.1 15 JRCFIB2022Mraw JRCFIB2022M &lt;navis.transforms.affine.AffineTransform objec... bridging True 0.1 0.1 16 MANCraw MANC &lt;navis.transforms.affine.AffineTransform objec... bridging True 0.1 0.1 17 JRCFIB2018Fraw JRCFIB2018F &lt;navis.transforms.affine.AffineTransform objec... bridging True 0.1 0.1 18 FLYWIREraw FLYWIRE &lt;navis.transforms.affine.AffineTransform objec... bridging True 0.1 0.1 19 FAFB14raw FAFB14 &lt;navis.transforms.affine.AffineTransform objec... bridging True 0.1 0.1 20 FANCraw FANC &lt;navis.transforms.affine.AffineTransform objec... bridging True 0.1 0.1 21 JFRC2 JFRC2010 &lt;navis.transforms.affine.AffineTransform objec... bridging True 0.1 0.1 22 FAFB14um FAFB14 &lt;navis.transforms.affine.AffineTransform objec... bridging True 0.1 0.1 23 FLYWIREum FLYWIRE &lt;navis.transforms.affine.AffineTransform objec... bridging True 0.1 0.1 24 MANCum MANC &lt;navis.transforms.affine.AffineTransform objec... bridging True 0.1 0.1 25 JRCFIB2018Fum JRCFIB2018F &lt;navis.transforms.affine.AffineTransform objec... bridging True 0.1 0.1 26 FANCum FANC &lt;navis.transforms.affine.AffineTransform objec... bridging True 0.1 0.1 27 JRCFIB2022Mum JRCFIB2022M &lt;navis.transforms.affine.AffineTransform objec... bridging True 0.1 0.1 28 BANCum BANC &lt;navis.transforms.affine.AffineTransform objec... bridging True 0.1 0.1 29 JRCFIB2022M None &lt;navis.transforms.thinplate.TPStransform objec... mirror True 1.0 1.0 30 JRCFIB2022Mraw None &lt;navis.transforms.thinplate.TPStransform objec... mirror True 1.0 1.0 31 FANC None &lt;navis.transforms.thinplate.TPStransform objec... mirror True 1.0 1.0 32 FLYWIRE None &lt;navis.transforms.thinplate.TPStransform objec... mirror True 1.0 1.0 33 FAFB14 None &lt;navis.transforms.thinplate.TPStransform objec... mirror True 1.0 1.0 34 FAFB None &lt;navis.transforms.thinplate.TPStransform objec... mirror True 1.0 1.0 35 BANC None &lt;navis.transforms.thinplate.TPStransform objec... mirror True 1.0 1.0 36 AEDES None &lt;navis.transforms.thinplate.TPStransform objec... mirror True 1.0 1.0 37 BANCum JRC2018F &lt;navis.transforms.elastix.ElastixTransform obj... bridging False 1.0 1.0 38 JRC2018F BANCum &lt;navis.transforms.elastix.ElastixTransform obj... bridging False 1.0 1.0 39 BANCum JRCVNC2018F &lt;navis.transforms.elastix.ElastixTransform obj... bridging False 1.0 1.0 40 JRCVNC2018F BANCum &lt;navis.transforms.elastix.ElastixTransform obj... bridging False 1.0 1.0 41 hemibrain JRCFIB2018F &lt;navis.transforms.base.AliasTransform object a... bridging True 0.0 0.0 42 hemibrainraw JRCFIB2018Fraw &lt;navis.transforms.base.AliasTransform object a... bridging True 0.0 0.0 43 hemibrainum JRCFIB2018Fum &lt;navis.transforms.base.AliasTransform object a... bridging True 0.0 0.0 44 FAFB FAFB14 &lt;navis.transforms.base.AliasTransform object a... bridging True 0.0 0.0 45 FAFBum FAFB14um &lt;navis.transforms.base.AliasTransform object a... bridging True 0.0 0.0 46 FAFBnm FAFB14nm &lt;navis.transforms.base.AliasTransform object a... bridging True 0.0 0.0 47 FANC FANCnm &lt;navis.transforms.base.AliasTransform object a... bridging True 0.0 0.0 48 JRCFIB2022M JRCFIB2022Mnm &lt;navis.transforms.base.AliasTransform object a... bridging True 0.0 0.0 49 MANC MANCnm &lt;navis.transforms.base.AliasTransform object a... bridging True 0.0 0.0 50 FLYWIRE FLYWIREnm &lt;navis.transforms.base.AliasTransform object a... bridging True 0.0 0.0"},{"location":"generated/gallery/6_misc/tutorial_misc_01_transforms/#using-xform_brain","title":"Using <code>xform_brain</code>","text":"<p>For high-level transforming, you will want to use <code>navis.xform_brain</code>. This function takes a <code>source</code> and <code>target</code> argument and tries to find a bridging sequence that gets you to where you want. Let's try it out:</p> <p>Info</p> <p>Incidentally, the example neurons that NAVis ships with are from the Janelia hemibrain project and are therefore in <code>JRCFIB2018raw</code> space (\"raw\" means uncalibrated voxel space which is 8x8x8nm for this dataset). We will be using those but there is nothing stopping you from using the NAVis interface with neuPrint (the tutorials on interfaces) to fetch your favourite hemibrain neurons and transform those.</p> <pre><code># Load the example hemibrain neurons (JRCFIB2018raw space)\nnl = navis.example_neurons()\nnl\n</code></pre>      &lt;class 'navis.core.neuronlist.NeuronList'&gt; containing 5 neurons (1.4MiB) type name id n_nodes n_connectors n_branches n_leafs cable_length soma units created_at origin file 0 navis.TreeNeuron DA1_lPN_R 1734350788 4465 2705 599 618 266476.87500 4177.0 8 nanometer 2026-03-01 17:35:44.733731 /home/runner/work/navis/navis/navis/data/swc/1... 1734350788.swc 1 navis.TreeNeuron DA1_lPN_R 1734350908 4847 3042 735 761 304332.65625 6.0 8 nanometer 2026-03-01 17:35:44.740654 /home/runner/work/navis/navis/navis/data/swc/1... 1734350908.swc ... ... ... ... ... ... ... ... ... ... ... ... ... ... 3 navis.TreeNeuron DA1_lPN_R 754534424 4696 3010 696 726 286522.46875 4.0 8 nanometer 2026-03-01 17:35:44.754110 /home/runner/work/navis/navis/navis/data/swc/7... 754534424.swc 4 navis.TreeNeuron DA1_lPN_R 754538881 4881 2943 626 642 291265.31250 701.0 8 nanometer 2026-03-01 17:35:44.760989 /home/runner/work/navis/navis/navis/data/swc/7... 754538881.swc <pre><code>fig, ax = navis.plot2d([nl, flybrains.JRCFIB2018Fraw], view=(\"x\", \"-z\"))\nplt.tight_layout()\n</code></pre> <p></p> <p>Let's say we want these neurons in <code>JRC2018F</code> template space. Before we do the actual transform it's useful to quickly check above bridging graph to see what we expect to happen:</p> What is JRC2018F? <p><code>JRC2018F</code> is a standard brain made from averaging over multiple fly brains. See Bogovic et al., 2020 for details.</p> <p>Have a look at the bridging graph above: first, we know that we are starting in <code>JRCFIB2018Fraw</code> space. From there, it's two simple affine transforms to go from voxels to nanometers and from nanometers to micrometers. Once we are in <code>JRCFIB2018Fum</code> space, we can use a Hdf5 transform generated by the Saalfeld lab to map to <code>JRC2018F</code>. Note that the arrows in the bridging graph indicate the transforms' forward directions but they can all be inversed to traverse the graph.</p> <p>Let's give this a shot:</p> <pre><code>xf = navis.xform_brain(nl, source=\"JRCFIB2018Fraw\", target=\"JRC2018F\")\n</code></pre> <p>Out:</p> <pre><code>Transform path: JRCFIB2018Fraw -&gt; JRCFIB2018F -&gt; JRCFIB2018Fum -&gt; JRC2018F\n</code></pre> <p>Painless, wasn't it? Let's see if it worked:</p> <p>Plot the transformed neurons and the JRC2018F template brain</p> <pre><code>fig, ax = navis.plot2d([xf, flybrains.JRC2018F], color=\"r\", view=(\"x\", \"-y\"))\nplt.tight_layout()\n</code></pre> <p></p> <p>That worked like a charm! I highly recommend you read through the documentation for <code>navis.xform_brain</code> and check out the parameters you can use to fine-tune it.</p>"},{"location":"generated/gallery/6_misc/tutorial_misc_01_transforms/#using-mirror_brain","title":"Using <code>mirror_brain</code>","text":"<p>Another useful type of transform is mirroring using <code>navis.mirror_brain</code> to e.g. mirror neurons from the left to the right side of a given brain. The way this works is this:</p> <ol> <li>Reflect coordinates about the midpoint of the mirror axis (affine transformation)</li> <li>Apply a warping transformation to compensate for e.g. left/right asymmetries</li> </ol> <p>For the first step, we need to know the length of the mirror axis. This is why - similar to having registered transforms - we need to have meta data about the template space (i.e. the bounding box) available to NAVis.</p> <p>The second step is optional. For example, <code>JRC2018F</code> and <code>JRC2018U</code> are templates generate from averaging multiple fly brains and are therefore already mirror symmetrical, meaning we don't need the additional warping transform. <code>flybrains</code> does include some mirror transforms though: e.g. for <code>FCWB</code>, <code>VNCIS1</code> or <code>JFRC2</code>!</p> <p>Since our neurons are already in <code>JRC2018F</code> space, let's try mirroring them:</p> <pre><code>mirrored = navis.mirror_brain(xf, template=\"JRC2018F\")\n</code></pre> <pre><code>fig, ax = navis.plot2d(\n    [xf, mirrored, flybrains.JRC2018F], color=[\"r\"] * 5 + [\"g\"] * 5, view=(\"x\", \"-y\")\n)\nplt.tight_layout()\n</code></pre> <p></p> <p>Out:</p> <pre><code>Plot neurons:   0%|          | 0/10 [00:00&lt;?, ?it/s]\n</code></pre> <p>Perfect! As noted above, this only works if the <code>template</code> is registered with NAVis and if it contains info about its bounding box. If you only have the bounding box at hand but no template brain, check out the lower level function <code>navis.transforms.mirror</code>.</p>"},{"location":"generated/gallery/6_misc/tutorial_misc_01_transforms/#low-level-functions","title":"Low-level functions","text":""},{"location":"generated/gallery/6_misc/tutorial_misc_01_transforms/#adding-your-own-transforms","title":"Adding your own transforms","text":"<p>Let's assume you want to add your own transforms. There are four different transform types:</p> <ul> <li><code>navis.transforms.affine.AffineTransform</code></li> <li><code>navis.transforms.cmtk.CMTKtransform</code></li> <li><code>navis.transforms.h5reg.H5transform</code></li> <li><code>navis.transforms.thinplate.TPStransform</code></li> </ul> <p>To show you how to use them, we will create a new thin plate spline transform using <code>TPStransform</code>. If you look at the bridging graph again, you might note the <code>\"FAFB14\"</code> template brain: it stands for <code>\"Full Adult Fly Brain\"</code> (the <code>14</code> is a version number for the alignment). We will use landmarks to generate a mapping between this 14th and the previous 13th iteration.</p> <p>First we will grab the landmarks from the Saalfeld's lab elm repository:</p> <pre><code>import pandas as pd\n\n# These landmarks map betweet FAFB (v14 and v13) and a light level template\n# We will use only the v13 and v14 landmarks\nlandmarks_v14 = pd.read_csv(\n    \"https://github.com/saalfeldlab/elm/raw/master/lm-em-landmarks_v14.csv\", header=None\n)\nlandmarks_v13 = pd.read_csv(\n    \"https://github.com/saalfeldlab/elm/raw/master/lm-em-landmarks_v13.csv\", header=None\n)\n\n# Name the columns\nlandmarks_v14.columns = landmarks_v13.columns = [\n    \"label\",\n    \"use\",\n    \"lm_x\",\n    \"lm_y\",\n    \"lm_z\",\n    \"fafb_x\",\n    \"fafb_y\",\n    \"fafb_z\",\n]\n\nlandmarks_v13.head()\n</code></pre> label use lm_x lm_y lm_z fafb_x fafb_y fafb_z 0 Pt-1 True 571.400083 38.859963 287.059544 525666.465856 172470.413167 80994.733289 1 Pt-2 True 715.811344 213.299356 217.393493 595391.597008 263523.121958 84156.773677 2 Pt-3 True 513.002196 198.001970 217.794090 501716.347872 253223.667163 98413.701578 3 Pt-6 True 867.012542 31.919253 276.223437 670999.903156 179097.916778 67561.691416 4 Pt-7 True 935.210895 234.229522 351.518068 702703.909963 251846.384054 127865.886146 <p>Now we can use those landmarks to generate a thin plate spine transform:</p> <pre><code>from navis.transforms.thinplate import TPStransform\n\ntr = TPStransform(\n    landmarks_source=landmarks_v14[[\"fafb_x\", \"fafb_y\", \"fafb_z\"]].values,\n    landmarks_target=landmarks_v13[[\"fafb_x\", \"fafb_y\", \"fafb_z\"]].values,\n)\n# note: navis.transforms.MovingLeastSquaresTransform has similar properties\n</code></pre> <p>The transform has a method that we can use to transform points but first we need some data in <code>FAFB14</code> space:</p> <pre><code># Transform our neurons into FAFB 14 space\nxf_fafb14 = navis.xform_brain(nl, source=\"JRCFIB2018Fraw\", target=\"FAFB14\")\n</code></pre> <p>Out:</p> <pre><code>Transform path: JRCFIB2018Fraw -&gt; JRCFIB2018F -&gt; JRCFIB2018Fum -&gt; JRC2018F -&gt; FAFBum = FAFB14um -&gt; FAFB14\n</code></pre> <p>Now let's see if we can use the v14v13 transform:</p> <pre><code># Transform the nodes of the first two neurons\npts_v14 = xf_fafb14[:2].nodes[[\"x\", \"y\", \"z\"]].values\npts_v13 = tr.xform(pts_v14)\n</code></pre> <p>Out:</p> <pre><code>/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/morphops/tps.py:157: DeprecationWarning:\n\n`row_stack` alias is deprecated. Use `np.vstack` directly.\n</code></pre> <p>Quick check how the v14 and v13 coordinates compare:</p> <pre><code># Original in black, transformed in red\nfig, ax = navis.plot2d(pts_v14, scatter_kws=dict(c=\"k\"), view=(\"x\", \"-y\"))\n_ = navis.plot2d(pts_v13, scatter_kws=dict(c=\"r\"), ax=ax, view=(\"x\", \"-y\"))\n</code></pre> <p></p> <p>So that did... something. To be honest, I'm not sure what to expect for the FAFB 1413 transform but let's assume this is correct and move on.</p> <p>Next, we will register this new transform with NAVis so that we can use it with higher level functions:</p> <pre><code># Register the transform\nnavis.transforms.registry.register_transform(\n    tr, source=\"FAFB14\", target=\"FAFB13\", transform_type=\"bridging\"\n)\n</code></pre> <p>Now that's done we can use <code>FAFB13</code> with <code>navis.xform_brain</code>:</p> <pre><code># Transform our neurons into FAFB 14 space\nxf_fafb13 = navis.xform_brain(xf_fafb14, source=\"FAFB14\", target=\"FAFB13\")\n</code></pre> <p>Out:</p> <pre><code>Transform path: FAFB14 -&gt; FAFB13\n</code></pre> <pre><code>fig, ax = navis.plot2d(xf_fafb14, c='k', view=(\"x\", \"-y\"))\n_ = navis.plot2d(xf_fafb13, c='r', ax=ax)\n</code></pre> <p></p>"},{"location":"generated/gallery/6_misc/tutorial_misc_01_transforms/#registering-template-brains","title":"Registering Template Brains","text":"<p>For completeness, lets also have a quick look at registering additional template brains.</p> <p>Template brains are represented in navis as <code>navis.transforms.templates.TemplateBrain</code> and there is currently no canonical way of constructing them: you can associate as much or as little data with them as you like. However, for them to be useful they should have a <code>name</code>, a <code>label</code> and a <code>boundingbox</code> property.</p> <p>Minimally, you could do something like this:</p> <pre><code># Construct template brain from base class\nmy_brain = navis.transforms.templates.TemplateBrain(\n    name=\"My template brain\",\n    label=\"my_brain\",\n    boundingbox=[[0, 100], [0, 100], [0, 100]],\n)\n\n# Register with navis\nnavis.transforms.registry.register_templatebrain(my_brain)\n\n# Now you can use it with mirror_brain:\nimport numpy as np\n\npts = np.array([[10, 10, 10]])\npts_mirrored = navis.mirror_brain(pts, template=\"my_brain\")\n\n# Plot the points\nfig, ax = plt.subplots()\nax.scatter(pts[:, 0], pts[:, 1], c=\"k\", alpha=1, s=50, label=\"Original\")\nax.scatter(\n    pts_mirrored[:, 0], pts_mirrored[:, 1], c=\"r\", alpha=1, s=50, label=\"Mirrored\"\n)\nax.legend()\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;matplotlib.legend.Legend object at 0x7f416364ae10&gt;\n</code></pre> <p>While this is a working solution, it's not very pretty: for example, <code>my_brain</code> does have the default docstring and no fancy string representation (e.g. for <code>print(my_brain)</code>). I highly recommend you take a look at how flybrains constructs and packages the templates.</p>"},{"location":"generated/gallery/6_misc/tutorial_misc_01_transforms/#acknowledgments","title":"Acknowledgments","text":"<p>Much of the transform module is modelled after functions written by Greg Jefferis for the natverse. Likewise, flybrains is a port of data collected by Greg Jefferis for <code>nat.flybrains</code> and <code>nat.jrcbrains</code>.</p> <p>Total running time of the script: ( 0 minutes  4.948 seconds)</p> <p> Download Python source code: tutorial_misc_01_transforms.py</p> <p> Download Jupyter notebook: tutorial_misc_01_transforms.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>navis<ul> <li>config</li> <li>connectivity<ul> <li>adjacency</li> <li>similarity</li> </ul> </li> <li>conversion<ul> <li>meshing</li> </ul> </li> <li>core<ul> <li>base</li> <li>core_utils</li> <li>skeleton</li> </ul> </li> <li>graph<ul> <li>graph_utils</li> <li>clinic</li> </ul> </li> <li>meshes<ul> <li>b3d</li> <li>mesh_utils</li> <li>o3d</li> <li>operations</li> </ul> </li> <li>models<ul> <li>network_models</li> </ul> </li> <li>morpho<ul> <li>ivscc</li> <li>mmetrics</li> <li>persistence</li> <li>subset</li> </ul> </li> <li>nbl<ul> <li>ablast_funcs</li> <li>nblast_funcs</li> <li>smat</li> <li>synblast_funcs</li> <li>utils</li> </ul> </li> <li>plotting<ul> <li>colors</li> <li>dd</li> <li>ddd</li> <li>plot_utils</li> <li>settings</li> <li>vispy<ul> <li>viewer</li> </ul> </li> <li>k3d<ul> <li>k3d_objects</li> </ul> </li> <li>plotly<ul> <li>graph_objs</li> </ul> </li> </ul> </li> <li>transforms<ul> <li>align</li> <li>affine</li> <li>base</li> <li>cmtk</li> <li>elastix</li> <li>factory</li> <li>h5reg</li> <li>templates</li> <li>thinplate</li> </ul> </li> <li>utils<ul> <li>cv</li> <li>decorators</li> </ul> </li> <li>intersection<ul> <li>intersect</li> </ul> </li> <li>interfaces<ul> <li>blender</li> <li>cave_utils</li> <li>h01</li> <li>insectbrain_db</li> <li>microns</li> <li>neuprint</li> <li>neuromorpho</li> <li>neuron<ul> <li>comp</li> <li>network</li> </ul> </li> </ul> </li> <li>io<ul> <li>base</li> <li>hdf_io</li> <li>mesh_io</li> <li>nmx_io</li> <li>nrrd_io</li> <li>precomputed_io</li> <li>rda_io</li> <li>swc_io</li> <li>tiff_io</li> </ul> </li> <li>sampling<ul> <li>downsampling</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/navis/","title":"navis","text":""},{"location":"reference/navis/#navis.BaseNeuron","title":"<code>navis.BaseNeuron</code>","text":"<p>Base class for all neurons.</p> Source code in <code>navis/core/base.py</code> <pre><code>class BaseNeuron(UnitObject):\n    \"\"\"Base class for all neurons.\"\"\"\n\n    name: Optional[str]\n    id: Union[int, str, uuid.UUID]\n\n    #: Unit space for this neuron. Some functions, like soma detection are\n    #: sensitive to units (if provided)\n    #: Default = micrometers\n    units: Union[pint.Unit, pint.Quantity]\n\n    volume: Union[int, float]\n\n    connectors: Optional[pd.DataFrame]\n\n    #: Attributes used for neuron summary\n    SUMMARY_PROPS = [\"type\", \"name\", \"units\"]\n\n    #: Attributes to be used when comparing two neurons.\n    EQ_ATTRIBUTES = [\"name\"]\n\n    #: Temporary attributes that need clearing when neuron data changes\n    TEMP_ATTR = [\"_memory_usage\"]\n\n    #: Core data table(s) used to calculate hash\n    CORE_DATA = []\n\n    def __init__(self, **kwargs):\n        # Set a random ID -&gt; may be replaced later\n        self.id = uuid.uuid4()\n\n        # Make a copy of summary and temp props so that if we register\n        # additional properties we don't change this for every single neuron\n        self.SUMMARY_PROPS = self.SUMMARY_PROPS.copy()\n        self.TEMP_ATTR = self.TEMP_ATTR.copy()\n\n        self._lock = 0\n        for k, v in kwargs.items():\n            self._register_attr(name=k, value=v)\n\n        # Base neurons has no data\n        self._current_md5 = None\n\n    def __getattr__(self, key):\n        \"\"\"Get attribute.\"\"\"\n        if key.startswith(\"has_\"):\n            key = key[key.index(\"_\") + 1 :]\n            if hasattr(self, key):\n                data = getattr(self, key)\n                if isinstance(data, pd.DataFrame):\n                    if not data.empty:\n                        return True\n                    else:\n                        return False\n                # This is necessary because np.any does not like strings\n                elif isinstance(data, str):\n                    if data == \"NA\" or not data:\n                        return False\n                    return True\n                elif utils.is_iterable(data) and len(data) &gt; 0:\n                    return True\n                elif data:\n                    return True\n            return False\n        elif key.startswith(\"n_\"):\n            key = key[key.index(\"_\") + 1 :]\n            if hasattr(self, key):\n                data = getattr(self, key, None)\n                if isinstance(data, pd.DataFrame):\n                    return data.shape[0]\n                elif utils.is_iterable(data):\n                    return len(data)\n                elif isinstance(data, str) and data == \"NA\":\n                    return \"NA\"\n            return None\n\n        raise AttributeError(f'Attribute \"{key}\" not found')\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __repr__(self):\n        return str(self.summary())\n\n    def __copy__(self):\n        return self.copy(deepcopy=False)\n\n    def __deepcopy__(self, memo):\n        result = self.copy(deepcopy=True)\n        memo[id(self)] = result\n        return result\n\n    def __eq__(self, other):\n        \"\"\"Implement neuron comparison.\"\"\"\n        if isinstance(other, BaseNeuron):\n            # We will do this sequentially and stop as soon as we find a\n            # discrepancy -&gt; this saves tons of time!\n            for at in self.EQ_ATTRIBUTES:\n                comp = getattr(self, at, None) == getattr(other, at, None)\n                if isinstance(comp, np.ndarray) and not all(comp):\n                    return False\n                elif comp is False:\n                    return False\n            # If all comparisons have passed, return True\n            return True\n        else:\n            return NotImplemented\n\n    def __hash__(self):\n        \"\"\"Generate a hashable value.\"\"\"\n        # We will simply use the neuron's memory address\n        return id(self)\n\n    def __add__(self, other):\n        \"\"\"Implement addition.\"\"\"\n        if isinstance(other, BaseNeuron):\n            return core.NeuronList([self, other])\n        return NotImplemented\n\n    def __imul__(self, other):\n        \"\"\"Multiplication with assignment (*=).\"\"\"\n        return self.__mul__(other, copy=False)\n\n    def __itruediv__(self, other):\n        \"\"\"Division with assignment (/=).\"\"\"\n        return self.__truediv__(other, copy=False)\n\n    def __iadd__(self, other):\n        \"\"\"Addition with assignment (+=).\"\"\"\n        return self.__add__(other, copy=False)\n\n    def __isub__(self, other):\n        \"\"\"Subtraction with assignment (-=).\"\"\"\n        return self.__sub__(other, copy=False)\n\n    def _repr_html_(self):\n        frame = self.summary().to_frame()\n        frame.columns = [\"\"]\n        # return self._gen_svg_thumbnail() + frame._repr_html_()\n        return frame._repr_html_()\n\n    def _gen_svg_thumbnail(self):\n        \"\"\"Generate 2D plot for thumbnail.\"\"\"\n        import matplotlib.pyplot as plt\n\n        # Store some previous states\n        prev_level = logger.getEffectiveLevel()\n        prev_pbar = config.pbar_hide\n        prev_int = plt.isinteractive()\n\n        plt.ioff()  # turn off interactive mode\n        logger.setLevel(\"WARNING\")\n        config.pbar_hide = True\n        fig = plt.figure(figsize=(2, 2))\n        ax = fig.add_subplot(111)\n        fig, ax = self.plot2d(connectors=False, ax=ax)\n        output = StringIO()\n        fig.savefig(output, format=\"svg\")\n\n        if prev_int:\n            plt.ion()  # turn on interactive mode\n        logger.setLevel(prev_level)\n        config.pbar_hide = prev_pbar\n        _ = plt.clf()\n        return output.getvalue()\n\n    def _clear_temp_attr(self, exclude: list = []) -&gt; None:\n        \"\"\"Clear temporary attributes.\"\"\"\n        if self.is_locked:\n            logger.debug(f\"Neuron {self.id} at {hex(id(self))} locked.\")\n            return\n\n        # Must set checksum before recalculating e.g. node types\n        # -&gt; otherwise we run into a recursive loop\n        self._current_md5 = self.core_md5\n        self._stale = False\n\n        for a in [at for at in self.TEMP_ATTR if at not in exclude]:\n            try:\n                delattr(self, a)\n                logger.debug(f\"Neuron {self.id} {hex(id(self))}: attribute {a} cleared\")\n            except AttributeError:\n                logger.debug(\n                    f'Neuron {self.id} at {hex(id(self))}: Unable to clear temporary attribute \"{a}\"'\n                )\n            except BaseException:\n                raise\n\n    def _register_attr(self, name, value, summary=True, temporary=False):\n        \"\"\"Set and register attribute.\n\n        Use this if you want an attribute to be used for the summary or cleared\n        when temporary attributes are cleared.\n        \"\"\"\n        setattr(self, name, value)\n\n        # If this is an easy to summarize attribute, add to summary\n        if summary and name not in self.SUMMARY_PROPS:\n            if isinstance(value, (numbers.Number, str, bool, np.bool_, type(None))):\n                self.SUMMARY_PROPS.append(name)\n            else:\n                logger.error(\n                    f'Attribute \"{name}\" of type \"{type(value)}\" '\n                    \"can not be added to summary\"\n                )\n\n        if temporary:\n            self.TEMP_ATTR.append(name)\n\n    def _unregister_attr(self, name):\n        \"\"\"Remove and unregister attribute.\"\"\"\n        if name in self.SUMMARY_PROPS:\n            self.SUMMARY_PROPS.remove(name)\n\n        if name in self.TEMP_ATTR:\n            self.TEMP_ATTR.remove(name)\n\n        delattr(self, name)\n\n    @property\n    def core_md5(self) -&gt; str:\n        \"\"\"MD5 checksum of core data.\n\n        Generated from `.CORE_DATA` properties.\n\n        Returns\n        -------\n        md5 :   string\n                MD5 checksum of core data. `None` if no core data.\n\n        \"\"\"\n        hash = \"\"\n        for prop in self.CORE_DATA:\n            cols = None\n            # See if we need to parse props into property and columns\n            # e.g. \"nodes:node_id,parent_id,x,y,z\"\n            if \":\" in prop:\n                prop, cols = prop.split(\":\")\n                cols = cols.split(\",\")\n\n            if hasattr(self, prop):\n                data = getattr(self, prop)\n                if isinstance(data, pd.DataFrame):\n                    if cols:\n                        data = data[cols]\n                    data = data.values\n\n                data = np.ascontiguousarray(data)\n\n                if xxhash:\n                    hash += xxhash.xxh128(data).hexdigest()\n                else:\n                    hash += hashlib.md5(data).hexdigest()\n\n        return hash if hash else None\n\n    @property\n    def datatables(self) -&gt; List[str]:\n        \"\"\"Names of all DataFrames attached to this neuron.\"\"\"\n        return [k for k, v in self.__dict__.items() if isinstance(v, pd.DataFrame)]\n\n    @property\n    def extents(self) -&gt; np.ndarray:\n        \"\"\"Extents of neuron in x/y/z direction (includes connectors).\"\"\"\n        if not hasattr(self, \"bbox\"):\n            raise ValueError(\n                \"Neuron must implement `.bbox` (bounding box) \"\n                \"property to calculate extents.\"\n            )\n        bbox = self.bbox\n        return bbox[:, 1] - bbox[:, 0]\n\n    @property\n    def id(self) -&gt; Any:\n        \"\"\"ID of the neuron.\n\n        Must be hashable. If not set, will assign a random unique identifier.\n        Can be indexed by using the `NeuronList.idx[]` locator.\n        \"\"\"\n        return getattr(self, \"_id\", None)\n\n    @id.setter\n    def id(self, value):\n        try:\n            hash(value)\n        except BaseException:\n            raise ValueError(\"id must be hashable\")\n        self._id = value\n\n    @property\n    def label(self) -&gt; str:\n        \"\"\"Label (e.g. for legends).\"\"\"\n        # If explicitly set return that label\n        if getattr(self, \"_label\", None):\n            return self._label\n\n        # If no label set, produce one from name + id (optional)\n        name = getattr(self, \"name\", None)\n        id = getattr(self, \"id\", None)\n\n        # If no name, use type\n        if not name:\n            name = self.type\n\n        label = name\n\n        # Use ID only if not a UUID\n        if not isinstance(id, uuid.UUID):\n            # And if it can be turned into a string\n            try:\n                id = str(id)\n            except BaseException:\n                id = \"\"\n\n            # Only use ID if it is not the same as name\n            if id and name != id:\n                label += f\" ({id})\"\n\n        return label\n\n    @label.setter\n    def label(self, value: str):\n        if not isinstance(value, str):\n            raise TypeError(f'label must be string, got \"{type(value)}\"')\n        self._label = value\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Neuron name.\"\"\"\n        return getattr(self, \"_name\", None)\n\n    @name.setter\n    def name(self, value: str):\n        self._name = value\n\n    @property\n    def connectors(self) -&gt; pd.DataFrame:\n        \"\"\"Connector table. If none, will return `None`.\"\"\"\n        return getattr(self, \"_connectors\", None)\n\n    @connectors.setter\n    def connectors(self, v):\n        if isinstance(v, type(None)):\n            self._connectors = None\n        else:\n            self._connectors = utils.validate_table(\n                v, required=[\"x\", \"y\", \"z\"], rename=True, restrict=False\n            )\n\n    @property\n    def presynapses(self):\n        \"\"\"Table with presynapses (filtered from connectors table).\n\n        Requires a \"type\" column in connector table. Will look for type labels\n        that include \"pre\" or that equal 0 or \"0\".\n        \"\"\"\n        if not isinstance(getattr(self, \"connectors\", None), pd.DataFrame):\n            raise ValueError(\"No connector table found.\")\n        # Make an educated guess what presynapses are\n        types = self.connectors[\"type\"].unique()\n        pre = [t for t in types if \"pre\" in str(t).lower() or t in [0, \"0\"]]\n\n        if len(pre) == 0:\n            logger.debug(f\"Unable to find presynapses in types: {types}\")\n            return self.connectors.iloc[0:0]  # return empty DataFrame\n        elif len(pre) &gt; 1:\n            raise ValueError(f\"Found ambigous presynapse labels: {pre}\")\n\n        return self.connectors[self.connectors[\"type\"] == pre[0]]\n\n    @property\n    def postsynapses(self):\n        \"\"\"Table with postsynapses (filtered from connectors table).\n\n        Requires a \"type\" column in connector table. Will look for type labels\n        that include \"post\" or that equal 1 or \"1\".\n        \"\"\"\n        if not isinstance(getattr(self, \"connectors\", None), pd.DataFrame):\n            raise ValueError(\"No connector table found.\")\n        # Make an educated guess what presynapses are\n        types = self.connectors[\"type\"].unique()\n        post = [t for t in types if \"post\" in str(t).lower() or t in [1, \"1\"]]\n\n        if len(post) == 0:\n            logger.debug(f\"Unable to find postsynapses in types: {types}\")\n            return self.connectors.iloc[0:0]  # return empty DataFrame\n        elif len(post) &gt; 1:\n            raise ValueError(f\"Found ambigous postsynapse labels: {post}\")\n\n        return self.connectors[self.connectors[\"type\"] == post[0]]\n\n    @property\n    def is_stale(self) -&gt; bool:\n        \"\"\"Test if temporary attributes might be outdated.\"\"\"\n        # If we know we are stale, just return True\n        if getattr(self, \"_stale\", False):\n            return True\n        else:\n            # Only check if we believe we are not stale\n            self._stale = self._current_md5 != self.core_md5\n        return self._stale\n\n    @property\n    def is_locked(self):\n        \"\"\"Test if neuron is locked.\"\"\"\n        return getattr(self, \"_lock\", 0) &gt; 0\n\n    @property\n    def type(self) -&gt; str:\n        \"\"\"Neuron type.\"\"\"\n        return \"navis.BaseNeuron\"\n\n    @property\n    def soma(self):\n        \"\"\"The soma of the neuron (if any).\"\"\"\n        raise NotImplementedError(f\"`soma` property not implemented for {type(self)}.\")\n\n    @property\n    def bbox(self) -&gt; np.ndarray:\n        \"\"\"Bounding box of neuron.\"\"\"\n        raise NotImplementedError(f\"Bounding box not implemented for {type(self)}.\")\n\n    def convert_units(\n        self, to: Union[pint.Unit, str], inplace: bool = False\n    ) -&gt; Optional[\"BaseNeuron\"]:\n        \"\"\"Convert coordinates to different unit.\n\n        Only works if neuron's `.units` is not dimensionless.\n\n        Parameters\n        ----------\n        to :        pint.Unit | str\n                    Units to convert to. If string, must be parsable by pint.\n                    See examples.\n        inplace :   bool, optional\n                    If True will convert in place. If not will return a\n                    copy.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import navis\n        &gt;&gt;&gt; n = navis.example_neurons(1)\n        &gt;&gt;&gt; n.units\n        &lt;Quantity(8, 'nanometer')&gt;\n        &gt;&gt;&gt; n.cable_length\n        266476.8\n        &gt;&gt;&gt; n2 = n.convert_units('um')\n        &gt;&gt;&gt; n2.units\n        &lt;Quantity(1.0, 'micrometer')&gt;\n        &gt;&gt;&gt; n2.cable_length\n        2131.8\n\n        \"\"\"\n        if not isinstance(self.units, (pint.Unit, pint.Quantity)):\n            raise ValueError(\"Unable to convert: neuron has no units set.\")\n\n        n = self.copy() if not inplace else self\n\n        # Catch pint's UnitStrippedWarning\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            # Get factor by which we have to multiply to get to target units\n            conv = n.units.to(to).magnitude\n            # Multiply by conversion factor\n            n *= conv\n\n        n._clear_temp_attr(exclude=[\"classify_nodes\"])\n\n        return n\n\n    def copy(self, deepcopy=False) -&gt; \"BaseNeuron\":\n        \"\"\"Return a copy of the neuron.\"\"\"\n        copy_fn = copy.deepcopy if deepcopy else copy.copy\n        # Attributes not to copy\n        no_copy = [\"_lock\"]\n        # Generate new empty neuron\n        x = self.__class__()\n        # Override with this neuron's data\n        x.__dict__.update(\n            {k: copy_fn(v) for k, v in self.__dict__.items() if k not in no_copy}\n        )\n\n        return x\n\n    def summary(self, add_props=None) -&gt; pd.Series:\n        \"\"\"Get a summary of this neuron.\"\"\"\n        # Do not remove the list -&gt; otherwise we might change the original!\n        props = list(self.SUMMARY_PROPS)\n\n        # Make sure ID is always in second place\n        if \"id\" in props and props.index(\"id\") != 2:\n            props.remove(\"id\")\n            props.insert(2, \"id\")\n        # Add .id to summary if not a generic UUID\n        elif not isinstance(self.id, uuid.UUID) and \"id\" not in props:\n            props.insert(2, \"id\")\n\n        if add_props:\n            props, ix = np.unique(np.append(props, add_props), return_inverse=True)\n            props = props[ix]\n\n        # This is to catch an annoying \"UnitStrippedWarning\" with pint\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            s = pd.Series([getattr(self, at, \"NA\") for at in props], index=props)\n\n        return s\n\n    def plot2d(self, **kwargs):\n        \"\"\"Plot neuron using [`navis.plot2d`][].\n\n        Parameters\n        ----------\n        **kwargs\n                Will be passed to [`navis.plot2d`][].\n                See `help(navis.plot2d)` for a list of keywords.\n\n        See Also\n        --------\n        [`navis.plot2d`][]\n                    Function called to generate 2d plot.\n\n        \"\"\"\n        from ..plotting import plot2d\n\n        return plot2d(self, **kwargs)\n\n    def plot3d(self, **kwargs):\n        \"\"\"Plot neuron using [`navis.plot3d`][].\n\n        Parameters\n        ----------\n        **kwargs\n                Keyword arguments. Will be passed to [`navis.plot3d`][].\n                See `help(navis.plot3d)` for a list of keywords.\n\n        See Also\n        --------\n        [`navis.plot3d`][]\n                    Function called to generate 3d plot.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import navis\n        &gt;&gt;&gt; nl = navis.example_neurons()\n        &gt;&gt;&gt; #Plot with connectors\n        &gt;&gt;&gt; viewer = nl.plot3d(connectors=True)\n\n        \"\"\"\n        from ..plotting import plot3d\n\n        return plot3d(core.NeuronList(self, make_copy=False), **kwargs)\n\n    def map_units(\n        self,\n        units: Union[pint.Unit, str],\n        on_error: Union[Literal[\"raise\"], Literal[\"ignore\"]] = \"raise\",\n    ) -&gt; Union[int, float]:\n        \"\"\"Convert units to match neuron space.\n\n        Only works if neuron's `.units` is isometric and not dimensionless.\n\n        Parameters\n        ----------\n        units :     number | str | pint.Quantity | pint.Units\n                    The units to convert to neuron units. Simple numbers are just\n                    passed through.\n        on_error :  \"raise\" | \"ignore\"\n                    What to do if an error occurs (e.g. because `neuron` does not\n                    have units specified). If \"ignore\" will simply return `units`\n                    unchanged.\n\n        See Also\n        --------\n        [`navis.core.to_neuron_space`][]\n                    The base function for this method.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import navis\n        &gt;&gt;&gt; # Example neurons are in 8x8x8nm voxel space\n        &gt;&gt;&gt; n = navis.example_neurons(1)\n        &gt;&gt;&gt; n.map_units('1 nanometer')\n        0.125\n        &gt;&gt;&gt; # Numbers are passed-through\n        &gt;&gt;&gt; n.map_units(1)\n        1\n        &gt;&gt;&gt; # For neuronlists\n        &gt;&gt;&gt; nl = navis.example_neurons(3)\n        &gt;&gt;&gt; nl.map_units('1 nanometer')\n        [0.125, 0.125, 0.125]\n\n        \"\"\"\n        return core.core_utils.to_neuron_space(units, neuron=self, on_error=on_error)\n\n    def memory_usage(self, deep=False, estimate=False):\n        \"\"\"Return estimated memory usage of this neuron.\n\n        Works by going over attached data (numpy arrays and pandas DataFrames\n        such as vertices, nodes, etc) and summing up their size in memory.\n\n        Parameters\n        ----------\n        deep :      bool\n                    Passed to pandas DataFrames. If True will also inspect\n                    memory footprint of `object` dtypes.\n        estimate :  bool\n                    If True, we will only estimate the size. This is\n                    considerably faster but will slightly underestimate the\n                    memory usage.\n\n        Returns\n        -------\n        int\n                    Memory usage in bytes.\n\n        \"\"\"\n        # We will use a very simply caching here\n        # We don't check whether neuron is stale because that causes\n        # additional overhead and we want this function to be as fast\n        # as possible\n        if hasattr(self, \"_memory_usage\"):\n            mu = self._memory_usage\n            if mu[\"deep\"] == deep and mu[\"estimate\"] == estimate:\n                return mu[\"size\"]\n\n        size = 0\n        if not estimate:\n            for k, v in self.__dict__.items():\n                if isinstance(v, np.ndarray):\n                    size += v.nbytes\n                elif isinstance(v, pd.DataFrame):\n                    size += v.memory_usage(deep=deep).sum()\n                elif isinstance(v, pd.Series):\n                    size += v.memory_usage(deep=deep)\n        else:\n            for k, v in self.__dict__.items():\n                if isinstance(v, np.ndarray):\n                    size += v.dtype.itemsize * v.size\n                elif isinstance(v, pd.DataFrame):\n                    for dt in v.dtypes.values:\n                        if isinstance(dt, pd.CategoricalDtype):\n                            size += len(dt.categories) * dt.itemsize\n                        else:\n                            size += dt.itemsize * v.shape[0]\n                elif isinstance(v, pd.Series):\n                    if isinstance(v.dtype, pd.CategoricalDtype):\n                        size += len(dt.categories) * dt.itemsize\n                    else:\n                        size += v.dtype.itemsize * v.shape[0]\n\n        self._memory_usage = {\"deep\": deep, \"estimate\": estimate, \"size\": size}\n\n        return size\n</code></pre>"},{"location":"reference/navis/#navis.BaseNeuron.bbox","title":"<code>bbox: np.ndarray</code>  <code>property</code>","text":"<p>Bounding box of neuron.</p>"},{"location":"reference/navis/#navis.BaseNeuron.connectors","title":"<code>connectors: pd.DataFrame</code>  <code>property</code> <code>writable</code>","text":"<p>Connector table. If none, will return <code>None</code>.</p>"},{"location":"reference/navis/#navis.BaseNeuron.core_md5","title":"<code>core_md5: str</code>  <code>property</code>","text":"<p>MD5 checksum of core data.</p> <p>Generated from <code>.CORE_DATA</code> properties.</p> RETURNS DESCRIPTION <code>md5</code> <p>MD5 checksum of core data. <code>None</code> if no core data.</p> <p> TYPE: <code>string</code> </p>"},{"location":"reference/navis/#navis.BaseNeuron.datatables","title":"<code>datatables: List[str]</code>  <code>property</code>","text":"<p>Names of all DataFrames attached to this neuron.</p>"},{"location":"reference/navis/#navis.BaseNeuron.extents","title":"<code>extents: np.ndarray</code>  <code>property</code>","text":"<p>Extents of neuron in x/y/z direction (includes connectors).</p>"},{"location":"reference/navis/#navis.BaseNeuron.id","title":"<code>id: Any</code>  <code>property</code> <code>writable</code>","text":"<p>ID of the neuron.</p> <p>Must be hashable. If not set, will assign a random unique identifier. Can be indexed by using the <code>NeuronList.idx[]</code> locator.</p>"},{"location":"reference/navis/#navis.BaseNeuron.is_locked","title":"<code>is_locked</code>  <code>property</code>","text":"<p>Test if neuron is locked.</p>"},{"location":"reference/navis/#navis.BaseNeuron.is_stale","title":"<code>is_stale: bool</code>  <code>property</code>","text":"<p>Test if temporary attributes might be outdated.</p>"},{"location":"reference/navis/#navis.BaseNeuron.label","title":"<code>label: str</code>  <code>property</code> <code>writable</code>","text":"<p>Label (e.g. for legends).</p>"},{"location":"reference/navis/#navis.BaseNeuron.name","title":"<code>name: str</code>  <code>property</code> <code>writable</code>","text":"<p>Neuron name.</p>"},{"location":"reference/navis/#navis.BaseNeuron.postsynapses","title":"<code>postsynapses</code>  <code>property</code>","text":"<p>Table with postsynapses (filtered from connectors table).</p> <p>Requires a \"type\" column in connector table. Will look for type labels that include \"post\" or that equal 1 or \"1\".</p>"},{"location":"reference/navis/#navis.BaseNeuron.presynapses","title":"<code>presynapses</code>  <code>property</code>","text":"<p>Table with presynapses (filtered from connectors table).</p> <p>Requires a \"type\" column in connector table. Will look for type labels that include \"pre\" or that equal 0 or \"0\".</p>"},{"location":"reference/navis/#navis.BaseNeuron.soma","title":"<code>soma</code>  <code>property</code>","text":"<p>The soma of the neuron (if any).</p>"},{"location":"reference/navis/#navis.BaseNeuron.type","title":"<code>type: str</code>  <code>property</code>","text":"<p>Neuron type.</p>"},{"location":"reference/navis/#navis.BaseNeuron.convert_units","title":"<code>convert_units</code>","text":"<p>Convert coordinates to different unit.</p> <p>Only works if neuron's <code>.units</code> is not dimensionless.</p> PARAMETER DESCRIPTION <code>to</code> <pre><code>    Units to convert to. If string, must be parsable by pint.\n    See examples.\n</code></pre> <p> TYPE: <code>       pint.Unit | str</code> </p> <code>inplace</code> <pre><code>    If True will convert in place. If not will return a\n    copy.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; n.units\n&lt;Quantity(8, 'nanometer')&gt;\n&gt;&gt;&gt; n.cable_length\n266476.8\n&gt;&gt;&gt; n2 = n.convert_units('um')\n&gt;&gt;&gt; n2.units\n&lt;Quantity(1.0, 'micrometer')&gt;\n&gt;&gt;&gt; n2.cable_length\n2131.8\n</code></pre> Source code in <code>navis/core/base.py</code> <pre><code>def convert_units(\n    self, to: Union[pint.Unit, str], inplace: bool = False\n) -&gt; Optional[\"BaseNeuron\"]:\n    \"\"\"Convert coordinates to different unit.\n\n    Only works if neuron's `.units` is not dimensionless.\n\n    Parameters\n    ----------\n    to :        pint.Unit | str\n                Units to convert to. If string, must be parsable by pint.\n                See examples.\n    inplace :   bool, optional\n                If True will convert in place. If not will return a\n                copy.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; n.units\n    &lt;Quantity(8, 'nanometer')&gt;\n    &gt;&gt;&gt; n.cable_length\n    266476.8\n    &gt;&gt;&gt; n2 = n.convert_units('um')\n    &gt;&gt;&gt; n2.units\n    &lt;Quantity(1.0, 'micrometer')&gt;\n    &gt;&gt;&gt; n2.cable_length\n    2131.8\n\n    \"\"\"\n    if not isinstance(self.units, (pint.Unit, pint.Quantity)):\n        raise ValueError(\"Unable to convert: neuron has no units set.\")\n\n    n = self.copy() if not inplace else self\n\n    # Catch pint's UnitStrippedWarning\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        # Get factor by which we have to multiply to get to target units\n        conv = n.units.to(to).magnitude\n        # Multiply by conversion factor\n        n *= conv\n\n    n._clear_temp_attr(exclude=[\"classify_nodes\"])\n\n    return n\n</code></pre>"},{"location":"reference/navis/#navis.BaseNeuron.copy","title":"<code>copy</code>","text":"<p>Return a copy of the neuron.</p> Source code in <code>navis/core/base.py</code> <pre><code>def copy(self, deepcopy=False) -&gt; \"BaseNeuron\":\n    \"\"\"Return a copy of the neuron.\"\"\"\n    copy_fn = copy.deepcopy if deepcopy else copy.copy\n    # Attributes not to copy\n    no_copy = [\"_lock\"]\n    # Generate new empty neuron\n    x = self.__class__()\n    # Override with this neuron's data\n    x.__dict__.update(\n        {k: copy_fn(v) for k, v in self.__dict__.items() if k not in no_copy}\n    )\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.BaseNeuron.map_units","title":"<code>map_units</code>","text":"<p>Convert units to match neuron space.</p> <p>Only works if neuron's <code>.units</code> is isometric and not dimensionless.</p> PARAMETER DESCRIPTION <code>units</code> <pre><code>    The units to convert to neuron units. Simple numbers are just\n    passed through.\n</code></pre> <p> TYPE: <code>    number | str | pint.Quantity | pint.Units</code> </p> <code>on_error</code> <pre><code>    What to do if an error occurs (e.g. because `neuron` does not\n    have units specified). If \"ignore\" will simply return `units`\n    unchanged.\n</code></pre> <p> TYPE: <code> \"raise\" | \"ignore\"</code> DEFAULT: <code>'raise'</code> </p> See Also <p><code>navis.core.to_neuron_space</code>             The base function for this method.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; # Example neurons are in 8x8x8nm voxel space\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; n.map_units('1 nanometer')\n0.125\n&gt;&gt;&gt; # Numbers are passed-through\n&gt;&gt;&gt; n.map_units(1)\n1\n&gt;&gt;&gt; # For neuronlists\n&gt;&gt;&gt; nl = navis.example_neurons(3)\n&gt;&gt;&gt; nl.map_units('1 nanometer')\n[0.125, 0.125, 0.125]\n</code></pre> Source code in <code>navis/core/base.py</code> <pre><code>def map_units(\n    self,\n    units: Union[pint.Unit, str],\n    on_error: Union[Literal[\"raise\"], Literal[\"ignore\"]] = \"raise\",\n) -&gt; Union[int, float]:\n    \"\"\"Convert units to match neuron space.\n\n    Only works if neuron's `.units` is isometric and not dimensionless.\n\n    Parameters\n    ----------\n    units :     number | str | pint.Quantity | pint.Units\n                The units to convert to neuron units. Simple numbers are just\n                passed through.\n    on_error :  \"raise\" | \"ignore\"\n                What to do if an error occurs (e.g. because `neuron` does not\n                have units specified). If \"ignore\" will simply return `units`\n                unchanged.\n\n    See Also\n    --------\n    [`navis.core.to_neuron_space`][]\n                The base function for this method.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; # Example neurons are in 8x8x8nm voxel space\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; n.map_units('1 nanometer')\n    0.125\n    &gt;&gt;&gt; # Numbers are passed-through\n    &gt;&gt;&gt; n.map_units(1)\n    1\n    &gt;&gt;&gt; # For neuronlists\n    &gt;&gt;&gt; nl = navis.example_neurons(3)\n    &gt;&gt;&gt; nl.map_units('1 nanometer')\n    [0.125, 0.125, 0.125]\n\n    \"\"\"\n    return core.core_utils.to_neuron_space(units, neuron=self, on_error=on_error)\n</code></pre>"},{"location":"reference/navis/#navis.BaseNeuron.memory_usage","title":"<code>memory_usage</code>","text":"<p>Return estimated memory usage of this neuron.</p> <p>Works by going over attached data (numpy arrays and pandas DataFrames such as vertices, nodes, etc) and summing up their size in memory.</p> PARAMETER DESCRIPTION <code>deep</code> <pre><code>    Passed to pandas DataFrames. If True will also inspect\n    memory footprint of `object` dtypes.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>False</code> </p> <code>estimate</code> <pre><code>    If True, we will only estimate the size. This is\n    considerably faster but will slightly underestimate the\n    memory usage.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>int</code> <p>Memory usage in bytes.</p> Source code in <code>navis/core/base.py</code> <pre><code>def memory_usage(self, deep=False, estimate=False):\n    \"\"\"Return estimated memory usage of this neuron.\n\n    Works by going over attached data (numpy arrays and pandas DataFrames\n    such as vertices, nodes, etc) and summing up their size in memory.\n\n    Parameters\n    ----------\n    deep :      bool\n                Passed to pandas DataFrames. If True will also inspect\n                memory footprint of `object` dtypes.\n    estimate :  bool\n                If True, we will only estimate the size. This is\n                considerably faster but will slightly underestimate the\n                memory usage.\n\n    Returns\n    -------\n    int\n                Memory usage in bytes.\n\n    \"\"\"\n    # We will use a very simply caching here\n    # We don't check whether neuron is stale because that causes\n    # additional overhead and we want this function to be as fast\n    # as possible\n    if hasattr(self, \"_memory_usage\"):\n        mu = self._memory_usage\n        if mu[\"deep\"] == deep and mu[\"estimate\"] == estimate:\n            return mu[\"size\"]\n\n    size = 0\n    if not estimate:\n        for k, v in self.__dict__.items():\n            if isinstance(v, np.ndarray):\n                size += v.nbytes\n            elif isinstance(v, pd.DataFrame):\n                size += v.memory_usage(deep=deep).sum()\n            elif isinstance(v, pd.Series):\n                size += v.memory_usage(deep=deep)\n    else:\n        for k, v in self.__dict__.items():\n            if isinstance(v, np.ndarray):\n                size += v.dtype.itemsize * v.size\n            elif isinstance(v, pd.DataFrame):\n                for dt in v.dtypes.values:\n                    if isinstance(dt, pd.CategoricalDtype):\n                        size += len(dt.categories) * dt.itemsize\n                    else:\n                        size += dt.itemsize * v.shape[0]\n            elif isinstance(v, pd.Series):\n                if isinstance(v.dtype, pd.CategoricalDtype):\n                    size += len(dt.categories) * dt.itemsize\n                else:\n                    size += v.dtype.itemsize * v.shape[0]\n\n    self._memory_usage = {\"deep\": deep, \"estimate\": estimate, \"size\": size}\n\n    return size\n</code></pre>"},{"location":"reference/navis/#navis.BaseNeuron.plot2d","title":"<code>plot2d</code>","text":"<p>Plot neuron using <code>navis.plot2d</code>.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <pre><code>Will be passed to [`navis.plot2d`][].\nSee `help(navis.plot2d)` for a list of keywords.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> See Also <p><code>navis.plot2d</code>             Function called to generate 2d plot.</p> Source code in <code>navis/core/base.py</code> <pre><code>def plot2d(self, **kwargs):\n    \"\"\"Plot neuron using [`navis.plot2d`][].\n\n    Parameters\n    ----------\n    **kwargs\n            Will be passed to [`navis.plot2d`][].\n            See `help(navis.plot2d)` for a list of keywords.\n\n    See Also\n    --------\n    [`navis.plot2d`][]\n                Function called to generate 2d plot.\n\n    \"\"\"\n    from ..plotting import plot2d\n\n    return plot2d(self, **kwargs)\n</code></pre>"},{"location":"reference/navis/#navis.BaseNeuron.plot3d","title":"<code>plot3d</code>","text":"<p>Plot neuron using <code>navis.plot3d</code>.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <pre><code>Keyword arguments. Will be passed to [`navis.plot3d`][].\nSee `help(navis.plot3d)` for a list of keywords.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> See Also <p><code>navis.plot3d</code>             Function called to generate 3d plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons()\n&gt;&gt;&gt; #Plot with connectors\n&gt;&gt;&gt; viewer = nl.plot3d(connectors=True)\n</code></pre> Source code in <code>navis/core/base.py</code> <pre><code>def plot3d(self, **kwargs):\n    \"\"\"Plot neuron using [`navis.plot3d`][].\n\n    Parameters\n    ----------\n    **kwargs\n            Keyword arguments. Will be passed to [`navis.plot3d`][].\n            See `help(navis.plot3d)` for a list of keywords.\n\n    See Also\n    --------\n    [`navis.plot3d`][]\n                Function called to generate 3d plot.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons()\n    &gt;&gt;&gt; #Plot with connectors\n    &gt;&gt;&gt; viewer = nl.plot3d(connectors=True)\n\n    \"\"\"\n    from ..plotting import plot3d\n\n    return plot3d(core.NeuronList(self, make_copy=False), **kwargs)\n</code></pre>"},{"location":"reference/navis/#navis.BaseNeuron.summary","title":"<code>summary</code>","text":"<p>Get a summary of this neuron.</p> Source code in <code>navis/core/base.py</code> <pre><code>def summary(self, add_props=None) -&gt; pd.Series:\n    \"\"\"Get a summary of this neuron.\"\"\"\n    # Do not remove the list -&gt; otherwise we might change the original!\n    props = list(self.SUMMARY_PROPS)\n\n    # Make sure ID is always in second place\n    if \"id\" in props and props.index(\"id\") != 2:\n        props.remove(\"id\")\n        props.insert(2, \"id\")\n    # Add .id to summary if not a generic UUID\n    elif not isinstance(self.id, uuid.UUID) and \"id\" not in props:\n        props.insert(2, \"id\")\n\n    if add_props:\n        props, ix = np.unique(np.append(props, add_props), return_inverse=True)\n        props = props[ix]\n\n    # This is to catch an annoying \"UnitStrippedWarning\" with pint\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        s = pd.Series([getattr(self, at, \"NA\") for at in props], index=props)\n\n    return s\n</code></pre>"},{"location":"reference/navis/#navis.Dotprops","title":"<code>navis.Dotprops</code>","text":"<p>Neuron represented as points + local vectors.</p> <p>Dotprops consist of points with x/y/z coordinates, a tangent vector and an alpha value describing the immediate neighbourhood (see also references).</p> <p>Typically constructed using <code>navis.make_dotprops</code>.</p> References <p>Masse N.Y., Cachero S., Ostrovsky A., and Jefferis G.S.X.E. (2012). A mutual information approach to automate identification of neuronal clusters in Drosophila brain images. Frontiers in Neuroinformatics 6 (00021). doi: 10.3389/fninf.2012.00021</p> PARAMETER DESCRIPTION <code>points</code> <pre><code>        (N, 3) array of x/y/z coordinates.\n</code></pre> <p> TYPE: <code>       numpy array</code> </p> <code>k</code> <pre><code>        Number of nearest neighbors for tangent vector calculation.\n        This can be `None` or `0` but then vectors must be\n        provided on initialization and can subsequently not be\n        re-calculated. Typical values here are `k=20` for dense\n        (e.g. from light level data) and `k=5` for sparse\n        (e.g. from skeletons) point clouds.\n</code></pre> <p> TYPE: <code>            int</code> </p> <code>vect</code> <pre><code>        (N, 3) array of vectors. If not provided will\n        recalculate both `vect` and `alpha` using `k`.\n</code></pre> <p> TYPE: <code>         numpy array</code> DEFAULT: <code>None</code> </p> <code>alpha</code> <pre><code>        (N, ) array of alpha values. If not provided will\n        recalculate both `alpha` and `vect` using `k`.\n</code></pre> <p> TYPE: <code>        numpy array</code> DEFAULT: <code>None</code> </p> <code>units</code> <pre><code>        Units for coordinates. Defaults to `None` (dimensionless).\n        Strings must be parsable by pint: e.g. \"nm\", \"um\",\n        \"micrometer\" or \"8 nanometers\".\n</code></pre> <p> TYPE: <code>        str | pint.Units | pint.Quantity</code> DEFAULT: <code>None</code> </p> <code>**metadata</code> <pre><code>        Any additional data to attach to neuron.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> Source code in <code>navis/core/dotprop.py</code> <pre><code>class Dotprops(BaseNeuron):\n    \"\"\"Neuron represented as points + local vectors.\n\n    Dotprops consist of points with x/y/z coordinates, a tangent vector and an\n    alpha value describing the immediate neighbourhood (see also references).\n\n    Typically constructed using [`navis.make_dotprops`][].\n\n    References\n    ----------\n    Masse N.Y., Cachero S., Ostrovsky A., and Jefferis G.S.X.E. (2012). A mutual\n    information approach to automate identification of neuronal clusters in\n    Drosophila brain images. Frontiers in Neuroinformatics 6 (00021).\n    doi: 10.3389/fninf.2012.00021\n\n    Parameters\n    ----------\n    points :        numpy array\n                    (N, 3) array of x/y/z coordinates.\n    k :             int, optional\n                    Number of nearest neighbors for tangent vector calculation.\n                    This can be `None` or `0` but then vectors must be\n                    provided on initialization and can subsequently not be\n                    re-calculated. Typical values here are `k=20` for dense\n                    (e.g. from light level data) and `k=5` for sparse\n                    (e.g. from skeletons) point clouds.\n    vect :          numpy array, optional\n                    (N, 3) array of vectors. If not provided will\n                    recalculate both `vect` and `alpha` using `k`.\n    alpha :         numpy array, optional\n                    (N, ) array of alpha values. If not provided will\n                    recalculate both `alpha` and `vect` using `k`.\n    units :         str | pint.Units | pint.Quantity\n                    Units for coordinates. Defaults to `None` (dimensionless).\n                    Strings must be parsable by pint: e.g. \"nm\", \"um\",\n                    \"micrometer\" or \"8 nanometers\".\n    **metadata\n                    Any additional data to attach to neuron.\n\n    \"\"\"\n\n    connectors: Optional[pd.DataFrame]\n\n    points: np.ndarray\n    alpha: np.ndarray\n    vect:  np.ndarray\n    k: Optional[int]\n\n    soma: Optional[Union[list, np.ndarray]]\n\n    #: Attributes used for neuron summary\n    SUMMARY_PROPS = ['type', 'name', 'k', 'units', 'n_points']\n\n    #: Attributes to be used when comparing two neurons.\n    EQ_ATTRIBUTES = ['name', 'n_points', 'k']\n\n    #: Temporary attributes that need clearing when neuron data changes\n    TEMP_ATTR = ['_memory_usage']\n\n    #: Core data table(s) used to calculate hash\n    _CORE_DATA = ['points', 'vect']\n\n    def __init__(self,\n                 points: np.ndarray,\n                 k: int,\n                 vect: Optional[np.ndarray] = None,\n                 alpha: Optional[np.ndarray] = None,\n                 units: Union[pint.Unit, str] = None,\n                 **metadata\n                 ):\n        \"\"\"Initialize Dotprops Neuron.\"\"\"\n        super().__init__()\n\n        self.k = k\n        self.points = points\n        self.alpha = alpha\n        self.vect = vect\n\n        self.soma = None\n\n        for k, v in metadata.items():\n            try:\n                setattr(self, k, v)\n            except AttributeError:\n                raise AttributeError(f\"Unable to set neuron's `{k}` attribute.\")\n\n        self.units = units\n\n    def __truediv__(self, other, copy=True):\n        \"\"\"Implement division for coordinates.\"\"\"\n        if isinstance(other, numbers.Number) or utils.is_iterable(other):\n            # If a number, consider this an offset for coordinates\n            n = self.copy() if copy else self\n            _ = np.divide(n.points, other, out=n.points, casting='unsafe')\n            if n.has_connectors:\n                n.connectors.loc[:, ['x', 'y', 'z']] /= other\n\n            # Force recomputing of KDTree\n            if hasattr(n, '_tree'):\n                delattr(n, '_tree')\n\n            # Convert units\n            # Note: .to_compact() throws a RuntimeWarning and returns unchanged\n            # values  when `units` is a iterable\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                n.units = (n.units * other).to_compact()\n\n            return n\n        return NotImplemented\n\n    def __mul__(self, other, copy=True):\n        \"\"\"Implement multiplication for coordinates.\"\"\"\n        if isinstance(other, numbers.Number) or utils.is_iterable(other):\n            # If a number, consider this an offset for coordinates\n            n = self.copy() if copy else self\n            _ = np.multiply(n.points, other, out=n.points, casting='unsafe')\n            if n.has_connectors:\n                n.connectors.loc[:, ['x', 'y', 'z']] *= other\n\n            # Force recomputing of KDTree\n            if hasattr(n, '_tree'):\n                delattr(n, '_tree')\n\n            # Convert units\n            # Note: .to_compact() throws a RuntimeWarning and returns unchanged\n            # values  when `units` is a iterable\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                n.units = (n.units / other).to_compact()\n\n            return n\n        return NotImplemented\n\n    def __add__(self, other, copy=True):\n        \"\"\"Implement addition for coordinates.\"\"\"\n        if isinstance(other, numbers.Number) or utils.is_iterable(other):\n            # If a number, consider this an offset for coordinates\n            n = self.copy() if copy else self\n            _ = np.add(n.points, other, out=n.points, casting='unsafe')\n            if n.has_connectors:\n                n.connectors.loc[:, ['x', 'y', 'z']] += other\n\n            # Force recomputing of KDTree\n            if hasattr(n, '_tree'):\n                delattr(n, '_tree')\n\n            return n\n        # If another neuron, return a list of neurons\n        elif isinstance(other, BaseNeuron):\n            return core.NeuronList([self, other])\n        return NotImplemented\n\n    def __sub__(self, other, copy=True):\n        \"\"\"Implement subtraction for coordinates.\"\"\"\n        if isinstance(other, numbers.Number) or utils.is_iterable(other):\n            # If a number, consider this an offset for coordinates\n            n = self.copy() if copy else self\n            _ = np.subtract(n.points, other, out=n.points, casting='unsafe')\n            if n.has_connectors:\n                n.connectors.loc[:, ['x', 'y', 'z']] -= other\n\n            # Force recomputing of KDTree\n            if hasattr(n, '_tree'):\n                delattr(n, '_tree')\n\n            return n\n        return NotImplemented\n\n    def __getstate__(self):\n        \"\"\"Get state (used e.g. for pickling).\"\"\"\n        state = {k: v for k, v in self.__dict__.items() if not callable(v)}\n\n        # The KDTree from pykdtree does not like being pickled\n        # We will have to remove it which will force it to be regenerated\n        # after unpickling\n        if '_tree' in state:\n            if 'pykdtree' in str(type(state['_tree'])):\n                _ = state.pop('_tree')\n\n        return state\n\n    def __len__(self):\n        return len(self.points)\n\n    @property\n    def alpha(self):\n        \"\"\"Alpha values for tangent vectors.\n\n        High alpha indicates that the local neighborhood is pointing in the same direction.\n        Low alpha indicates that the tangent vectors are pointing in different directions.\n\n        \"\"\"\n        if isinstance(self._alpha, type(None)):\n            if isinstance(self.k, type(None)) or (self.k &lt;= 0):\n                raise ValueError('Unable to calculate `alpha` for Dotprops not '\n                                 'generated using k-nearest-neighbors.')\n\n            self.recalculate_tangents(self.k, inplace=True)\n        return self._alpha\n\n    @alpha.setter\n    def alpha(self, value):\n        if not isinstance(value, type(None)):\n            value = np.asarray(value)\n            if value.ndim != 1:\n                raise ValueError(f'alpha must be (N, ) array, got {value.shape}')\n        self._alpha = value\n\n    @property\n    def bbox(self) -&gt; np.ndarray:\n        \"\"\"Bounding box (includes connectors).\"\"\"\n        mn = np.min(self.points, axis=0)\n        mx = np.max(self.points, axis=0)\n\n        if self.has_connectors:\n            cn_mn = np.min(self.connectors[['x', 'y', 'z']].values, axis=0)\n            cn_mx = np.max(self.connectors[['x', 'y', 'z']].values, axis=0)\n\n            mn = np.min(np.vstack((mn, cn_mn)), axis=0)\n            mx = np.max(np.vstack((mx, cn_mx)), axis=0)\n\n        return np.vstack((mn, mx)).T\n\n    @property\n    def datatables(self) -&gt; List[str]:\n        \"\"\"Names of all DataFrames attached to this neuron.\"\"\"\n        return [k for k, v in self.__dict__.items() if isinstance(v, pd.DataFrame, np.ndarray)]\n\n    @property\n    def kdtree(self):\n        \"\"\"KDTree for points.\"\"\"\n        if not getattr(self, '_tree', None):\n            self._tree = KDTree(self.points)\n        return self._tree\n\n    @property\n    def points(self):\n        \"\"\"Center of tangent vectors.\"\"\"\n        return self._points\n\n    @points.setter\n    def points(self, value):\n        if isinstance(value, type(None)):\n            value = np.zeros((0, 3))\n        value = np.asarray(value)\n        if value.ndim != 2 or value.shape[1] != 3:\n            raise ValueError(f'points must be (N, 3) array, got {value.shape}')\n        self._points = value\n        # Also reset KDtree\n        self._tree = None\n\n    @property\n    def vect(self):\n        \"\"\"Tangent vectors.\"\"\"\n        if isinstance(self._vect, type(None)):\n            self.recalculate_tangents(self.k, inplace=True)\n        return self._vect\n\n    @vect.setter\n    def vect(self, value):\n        if not isinstance(value, type(None)):\n            value = np.asarray(value)\n            if value.ndim != 2 or value.shape[1] != 3:\n                raise ValueError(f'vectors must be (N, 3) array, got {value.shape}')\n        self._vect = value\n\n    @property\n    def sampling_resolution(self):\n        \"\"\"Mean distance between points.\"\"\"\n        dist, _ = self.kdtree.query(self.points, k=2)\n        return np.mean(dist[:, 1])\n\n    @property\n    def soma(self) -&gt; Optional[int]:\n        \"\"\"Index of soma point.\n\n        `None` if no soma. You can assign either a function that accepts a\n        Dotprops as input or a fix value. Default is None.\n        \"\"\"\n        if callable(self._soma):\n            soma = self._soma.__call__()  # type: ignore  # say int not callable\n        else:\n            soma = self._soma\n\n        # Sanity check to make sure that the soma node actually exists\n        if isinstance(soma, type(None)):\n            # Return immmediately without expensive checks\n            return soma\n        elif utils.is_iterable(soma):\n            if not any(soma):\n                soma = None\n            elif any(np.array(soma) &lt; 0) or any(np.array(soma) &gt; self.points.shape[0]):\n                logger.warning(f'Soma(s) {soma} not found in points.')\n                soma = None\n        else:\n            if 0 &lt; soma &lt; self.points.shape[0]:\n                logger.warning(f'Soma {soma} not found in node table.')\n                soma = None\n\n        return soma\n\n    @soma.setter\n    def soma(self, value: Union[Callable, int, None]) -&gt; None:\n        \"\"\"Set soma.\"\"\"\n        if hasattr(value, '__call__'):\n            self._soma = types.MethodType(value, self)\n        elif isinstance(value, type(None)):\n            self._soma = None\n        elif isinstance(value, bool) and not value:\n            self._soma = None\n        else:\n            if 0 &lt; value &lt; self.points.shape[0]:\n                self._soma = value\n            else:\n                raise ValueError('Soma must be function, None or a valid node index.')\n\n    @property\n    def type(self) -&gt; str:\n        \"\"\"Neuron type.\"\"\"\n        return 'navis.Dotprops'\n\n    def dist_dots(self,\n                  other: 'Dotprops',\n                  alpha: bool = False,\n                  distance_upper_bound: Optional[float] = None,\n                  **kwargs) -&gt; Union[\n                      Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, np.ndarray]\n                    ]:\n        \"\"\"Query this Dotprops against another.\n\n        This function is mainly for `navis.nblast`.\n\n        Parameters\n        ----------\n        other :                 Dotprops\n        alpha :                 bool\n                                If True, will also return the product of the\n                                `alpha` values of matched points.\n        distance_upper_bound :  non-negative float, optional\n                                If provided, we will stop the nearest neighbor\n                                search at this distance which can vastly speed\n                                up the query. For points with no hit within this\n                                distance, `dist` will be set to\n                                `distance_upper_bound`, and `dotprods` and\n                                `alpha_prod` will be set to 0.\n        kwargs\n                                Keyword arguments are passed to the KDTree's\n                                `query()` method. Note that we are using\n                                `pykdtree.kdtree.KDTree` if available and fall\n                                back to `scipy.spatial.cKDTree` if pykdtree is\n                                not installed.\n\n        Returns\n        -------\n        dist :          np.ndarray\n                        For each point in `self`, the distance to the closest\n                        point in `other`.\n        dotprods :      np.ndarray\n                        Dotproduct of each pair of closest points between\n                        `self` and `other`.\n        alpha_prod :    np.ndarray\n                        Dotproduct of each pair of closest points between\n                        `self` and `other`. Only returned if `alpha=True`.\n\n        \"\"\"\n        if not isinstance(other, Dotprops):\n            raise TypeError(f'Expected Dotprops, got \"{type(other)}\"')\n\n        # If we are using pykdtree we need to make sure that self.points is\n        # of the same dtype as other.points - not a problem with scipy but\n        # the overhead is typically only a few micro seconds anyway\n        points = self.points.astype(other.points.dtype, copy=False)\n\n        # Scipy's KDTree does not like the distance to be None\n        diub = distance_upper_bound if distance_upper_bound else np.inf\n        fast_dists, fast_idxs = other.kdtree.query(points,\n                                                   distance_upper_bound=diub,\n                                                   **kwargs)\n\n        # If upper distance we have to worry about infinite distances\n        if distance_upper_bound:\n            no_nn = fast_dists == np.inf\n            fast_dists[no_nn] = distance_upper_bound\n\n            # Temporarily give those nodes a match\n            fast_idxs[no_nn] = 0\n\n        fast_dotprods = np.abs((self.vect * other.vect[fast_idxs]).sum(axis=1))\n\n        if distance_upper_bound:\n            fast_dotprods[no_nn] = 0\n\n        if not alpha:\n            return fast_dists, fast_dotprods\n\n        fast_alpha = self.alpha * other.alpha[fast_idxs]\n\n        if distance_upper_bound:\n            fast_alpha[no_nn] = 0\n\n        return fast_dists, fast_dotprods, fast_alpha\n\n    def downsample(self, factor=5, inplace=False, **kwargs):\n        \"\"\"Downsample the neuron by given factor.\n\n        Parameters\n        ----------\n        factor :                int, optional\n                                Factor by which to downsample the neurons.\n                                Default = 5.\n        inplace :               bool, optional\n                                If True, operation will be performed on\n                                itself. If False, operation is performed on\n                                copy which is then returned.\n        **kwargs\n                                Additional arguments passed to\n                                [`navis.downsample_neuron`][].\n\n        See Also\n        --------\n        [`navis.downsample_neuron`][]\n            Base function. See for details and examples.\n\n        \"\"\"\n        if inplace:\n            x = self\n        else:\n            x = self.copy()\n\n        sampling.downsample_neuron(x, factor, inplace=True, **kwargs)\n\n        if not inplace:\n            return x\n        return None\n\n    def copy(self) -&gt; 'Dotprops':\n        \"\"\"Return a copy of the dotprops.\n\n        Returns\n        -------\n        Dotprops\n\n        \"\"\"\n        # Don't copy the KDtree - when using pykdtree, copy.copy throws an\n        # error and the construction is super fast anyway\n        no_copy = ['_lock', '_tree']\n        # Generate new empty neuron - note we pass vect and alpha to\n        # prevent calculation on initialization\n        x = self.__class__(points=np.zeros((0, 3)), k=1,\n                           vect=np.zeros((0, 3)), alpha=np.zeros(0))\n        # Populate with this neuron's data\n        x.__dict__.update({k: copy.copy(v) for k, v in self.__dict__.items() if k not in no_copy})\n\n        return x\n\n    def drop_fluff(self, epsilon, keep_size: int = None, n_largest: int = None, inplace=False):\n        \"\"\"Remove fluff from neuron.\n\n        By default, this function will remove all but the largest connected\n        component from the neuron. You can change that behavior using the\n        `keep_size` and `n_largest` parameters.\n\n        Parameters\n        ----------\n        epsilon :   float\n                    Distance at which to consider two points to be connected.\n                    If `None`, will use the default value of 5 times the average\n                    node distance (`self.sampling_resolution`).\n        keep_size : float, optional\n                    Use this to set a size (in number of points) for small\n                    bits to keep. If `keep_size` &lt; 1 it will be intepreted as\n                    fraction of total nodes/vertices/points.\n        n_largest : int, optional\n                    If set, will keep the `n_largest` connected components. Note:\n                    if provided, `keep_size` will be applied first!\n        inplace :   bool, optional\n                    If False, will return a copy and leave the original data\n                    unmodified.\n\n        Returns\n        -------\n        Dotprops\n                    Only if `inplace=False`.\n\n        See Also\n        --------\n        [`navis.drop_fluff`][]\n            Base function. See for details and examples.\n\n        \"\"\"\n        x = morpho.drop_fluff(self, epsilon=epsilon, keep_size=keep_size, n_largest=n_largest, inplace=inplace)\n\n        if not inplace:\n            return x\n\n    def recalculate_tangents(self, k: int, inplace=False):\n        \"\"\"Recalculate tangent vectors and alpha with a new `k`.\n\n        Parameters\n        ----------\n        k :         int\n                    Number of nearest neighbours to use for tangent vector\n                    calculation.\n        inplace :   bool\n                    If False, will return a copy and leave the original data\n                    unmodified.\n\n        Returns\n        -------\n        Dotprops\n                    Only if `inplace=False`.\n\n        \"\"\"\n        if not inplace:\n            x = self.copy()\n        else:\n            x = self\n\n        if isinstance(k, type(None)) or k &lt; 1:\n            raise ValueError(f'`k` must be integer &gt;= 1, got \"{k}\"')\n\n        # Checks and balances\n        n_points = x.points.shape[0]\n        if n_points &lt; k:\n            raise ValueError(f\"Too few points ({n_points}) to calculate {k} \"\n                             \"nearest-neighbors\")\n\n        # Create the KDTree and get the k-nearest neighbors for each point\n        dist, ix = self.kdtree.query(x.points, k=k)\n\n        # Get points: array of (N, k, 3)\n        pt = x.points[ix]\n\n        # Generate centers for each cloud of k nearest neighbors\n        centers = np.mean(pt, axis=1)\n\n        # Generate vector from center\n        cpt = pt - centers.reshape((pt.shape[0], 1, 3))\n\n        # Get inertia (N, 3, 3)\n        inertia = cpt.transpose((0, 2, 1)) @ cpt\n\n        # Extract vector and alpha\n        u, s, vh = np.linalg.svd(inertia)\n        x.vect = vh[:, 0, :]\n        x.alpha = (s[:, 0] - s[:, 1]) / np.sum(s, axis=1)\n\n        # Keep track of k\n        x.k = k\n\n        if not inplace:\n            return x\n\n    def snap(self, locs, to='points'):\n        \"\"\"Snap xyz location(s) to closest point or synapse.\n\n        Parameters\n        ----------\n        locs :      (N, 3) array | (3, ) array\n                    Either single or multiple XYZ locations.\n        to :        \"points\" | \"connectors\"\n                    Whether to snap to points or connectors.\n\n        Returns\n        -------\n        ix :        int | list of int\n                    Index/Indices of the closest point/connector.\n        dist :      float | list of float\n                    Distance(s) to the closest point/connector.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import navis\n        &gt;&gt;&gt; n = navis.example_neurons(1)\n        &gt;&gt;&gt; dp = navis.make_dotprops(n, k=5)\n        &gt;&gt;&gt; ix, dist = dp.snap([0, 0, 0])\n        &gt;&gt;&gt; ix\n        1123\n\n        \"\"\"\n        locs = np.asarray(locs).astype(np.float64)\n\n        is_single = (locs.ndim == 1 and len(locs) == 3)\n        is_multi = (locs.ndim == 2 and locs.shape[1] == 3)\n        if not is_single and not is_multi:\n            raise ValueError('Expected a single (x, y, z) location or a '\n                             '(N, 3) array of multiple locations')\n\n        if to not in ['points', 'connectors']:\n            raise ValueError('`to` must be \"points\" or \"connectors\", '\n                             f'got {to}')\n\n        # Generate tree\n        tree = graph.neuron2KDTree(self, data=to)\n\n        # Find the closest node\n        dist, ix = tree.query(locs)\n\n        return ix, dist\n\n    def to_skeleton(self,\n                    scale_vec: Union[float, Literal['auto']] = 'auto'\n                    ) -&gt; core.TreeNeuron:\n        \"\"\"Turn Dotprop into a TreeNeuron.\n\n        This does *not* skeletonize the neuron but rather generates a line\n        segment for each point based on the tangent vector. This is mainly\n        used under the hood for plotting. Also note that only minimal meta\n        data is carried over.\n\n        For proper skeletonization see [`navis.skeletonize`][].\n\n        Parameters\n        ----------\n        scale_vec :     \"auto\" | float\n                        Factor by which to scale each tangent vector when\n                        generating the line segments. If \"auto\" (default for\n                        plotting) will use the sampling resolution (median\n                        distance between points) to determine a suitable\n                        values.\n\n        Returns\n        -------\n        TreeNeuron\n\n        \"\"\"\n        if not isinstance(scale_vec, numbers.Number) and scale_vec != 'auto':\n            raise ValueError('`scale_vect` must be \"auto\" or a number, '\n                             f'got {scale_vec}')\n\n        if scale_vec == 'auto':\n            scale_vec = self.sampling_resolution * .8\n\n        # Prepare segments - this is based on nat:::plot3d.dotprops\n        halfvect = self.vect / 2 * scale_vec\n        starts = self.points - halfvect\n        ends = self.points + halfvect\n\n        # Interweave starts and ends\n        segs = np.zeros((starts.shape[0] * 2, 3))\n        segs[::2] = starts\n        segs[1::2] = ends\n\n        # Generate node table\n        nodes = pd.DataFrame(segs, columns=['x', 'y', 'z'])\n        nodes['node_id'] = nodes.index\n        nodes['parent_id'] = -1\n        nodes.loc[1::2, 'parent_id'] = nodes.index.values[::2]\n\n        # Produce a minimal TreeNeuron\n        tn = core.TreeNeuron(nodes, units=self.units, id=self.id)\n\n        # Carry over the label\n        if getattr(self, '_label', None):\n            tn._label = self._label\n\n        # Add some other relevant attributes directly\n        if self.has_connectors:\n            tn._connectors = self._connectors\n        tn._soma = self._soma\n\n        return tn\n</code></pre>"},{"location":"reference/navis/#navis.Dotprops.alpha","title":"<code>alpha</code>  <code>property</code> <code>writable</code>","text":"<p>Alpha values for tangent vectors.</p> <p>High alpha indicates that the local neighborhood is pointing in the same direction. Low alpha indicates that the tangent vectors are pointing in different directions.</p>"},{"location":"reference/navis/#navis.Dotprops.bbox","title":"<code>bbox: np.ndarray</code>  <code>property</code>","text":"<p>Bounding box (includes connectors).</p>"},{"location":"reference/navis/#navis.Dotprops.datatables","title":"<code>datatables: List[str]</code>  <code>property</code>","text":"<p>Names of all DataFrames attached to this neuron.</p>"},{"location":"reference/navis/#navis.Dotprops.kdtree","title":"<code>kdtree</code>  <code>property</code>","text":"<p>KDTree for points.</p>"},{"location":"reference/navis/#navis.Dotprops.points","title":"<code>points</code>  <code>property</code> <code>writable</code>","text":"<p>Center of tangent vectors.</p>"},{"location":"reference/navis/#navis.Dotprops.sampling_resolution","title":"<code>sampling_resolution</code>  <code>property</code>","text":"<p>Mean distance between points.</p>"},{"location":"reference/navis/#navis.Dotprops.soma","title":"<code>soma: Optional[int]</code>  <code>property</code> <code>writable</code>","text":"<p>Index of soma point.</p> <p><code>None</code> if no soma. You can assign either a function that accepts a Dotprops as input or a fix value. Default is None.</p>"},{"location":"reference/navis/#navis.Dotprops.type","title":"<code>type: str</code>  <code>property</code>","text":"<p>Neuron type.</p>"},{"location":"reference/navis/#navis.Dotprops.vect","title":"<code>vect</code>  <code>property</code> <code>writable</code>","text":"<p>Tangent vectors.</p>"},{"location":"reference/navis/#navis.Dotprops.__init__","title":"<code>__init__</code>","text":"<p>Initialize Dotprops Neuron.</p> Source code in <code>navis/core/dotprop.py</code> <pre><code>def __init__(self,\n             points: np.ndarray,\n             k: int,\n             vect: Optional[np.ndarray] = None,\n             alpha: Optional[np.ndarray] = None,\n             units: Union[pint.Unit, str] = None,\n             **metadata\n             ):\n    \"\"\"Initialize Dotprops Neuron.\"\"\"\n    super().__init__()\n\n    self.k = k\n    self.points = points\n    self.alpha = alpha\n    self.vect = vect\n\n    self.soma = None\n\n    for k, v in metadata.items():\n        try:\n            setattr(self, k, v)\n        except AttributeError:\n            raise AttributeError(f\"Unable to set neuron's `{k}` attribute.\")\n\n    self.units = units\n</code></pre>"},{"location":"reference/navis/#navis.Dotprops.copy","title":"<code>copy</code>","text":"<p>Return a copy of the dotprops.</p> RETURNS DESCRIPTION <code>Dotprops</code> Source code in <code>navis/core/dotprop.py</code> <pre><code>def copy(self) -&gt; 'Dotprops':\n    \"\"\"Return a copy of the dotprops.\n\n    Returns\n    -------\n    Dotprops\n\n    \"\"\"\n    # Don't copy the KDtree - when using pykdtree, copy.copy throws an\n    # error and the construction is super fast anyway\n    no_copy = ['_lock', '_tree']\n    # Generate new empty neuron - note we pass vect and alpha to\n    # prevent calculation on initialization\n    x = self.__class__(points=np.zeros((0, 3)), k=1,\n                       vect=np.zeros((0, 3)), alpha=np.zeros(0))\n    # Populate with this neuron's data\n    x.__dict__.update({k: copy.copy(v) for k, v in self.__dict__.items() if k not in no_copy})\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.Dotprops.dist_dots","title":"<code>dist_dots</code>","text":"<p>Query this Dotprops against another.</p> <p>This function is mainly for <code>navis.nblast</code>.</p> PARAMETER DESCRIPTION <code>other</code> <p> TYPE: <code>                Dotprops</code> </p> <code>alpha</code> <pre><code>                If True, will also return the product of the\n                `alpha` values of matched points.\n</code></pre> <p> TYPE: <code>                bool</code> DEFAULT: <code>False</code> </p> <code>distance_upper_bound</code> <pre><code>                If provided, we will stop the nearest neighbor\n                search at this distance which can vastly speed\n                up the query. For points with no hit within this\n                distance, `dist` will be set to\n                `distance_upper_bound`, and `dotprods` and\n                `alpha_prod` will be set to 0.\n</code></pre> <p> TYPE: <code> non-negative float</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <pre><code>                Keyword arguments are passed to the KDTree's\n                `query()` method. Note that we are using\n                `pykdtree.kdtree.KDTree` if available and fall\n                back to `scipy.spatial.cKDTree` if pykdtree is\n                not installed.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>dist</code> <p>For each point in <code>self</code>, the distance to the closest point in <code>other</code>.</p> <p> TYPE: <code>np.ndarray</code> </p> <code>dotprods</code> <p>Dotproduct of each pair of closest points between <code>self</code> and <code>other</code>.</p> <p> TYPE: <code>np.ndarray</code> </p> <code>alpha_prod</code> <p>Dotproduct of each pair of closest points between <code>self</code> and <code>other</code>. Only returned if <code>alpha=True</code>.</p> <p> TYPE: <code>np.ndarray</code> </p> Source code in <code>navis/core/dotprop.py</code> <pre><code>def dist_dots(self,\n              other: 'Dotprops',\n              alpha: bool = False,\n              distance_upper_bound: Optional[float] = None,\n              **kwargs) -&gt; Union[\n                  Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, np.ndarray]\n                ]:\n    \"\"\"Query this Dotprops against another.\n\n    This function is mainly for `navis.nblast`.\n\n    Parameters\n    ----------\n    other :                 Dotprops\n    alpha :                 bool\n                            If True, will also return the product of the\n                            `alpha` values of matched points.\n    distance_upper_bound :  non-negative float, optional\n                            If provided, we will stop the nearest neighbor\n                            search at this distance which can vastly speed\n                            up the query. For points with no hit within this\n                            distance, `dist` will be set to\n                            `distance_upper_bound`, and `dotprods` and\n                            `alpha_prod` will be set to 0.\n    kwargs\n                            Keyword arguments are passed to the KDTree's\n                            `query()` method. Note that we are using\n                            `pykdtree.kdtree.KDTree` if available and fall\n                            back to `scipy.spatial.cKDTree` if pykdtree is\n                            not installed.\n\n    Returns\n    -------\n    dist :          np.ndarray\n                    For each point in `self`, the distance to the closest\n                    point in `other`.\n    dotprods :      np.ndarray\n                    Dotproduct of each pair of closest points between\n                    `self` and `other`.\n    alpha_prod :    np.ndarray\n                    Dotproduct of each pair of closest points between\n                    `self` and `other`. Only returned if `alpha=True`.\n\n    \"\"\"\n    if not isinstance(other, Dotprops):\n        raise TypeError(f'Expected Dotprops, got \"{type(other)}\"')\n\n    # If we are using pykdtree we need to make sure that self.points is\n    # of the same dtype as other.points - not a problem with scipy but\n    # the overhead is typically only a few micro seconds anyway\n    points = self.points.astype(other.points.dtype, copy=False)\n\n    # Scipy's KDTree does not like the distance to be None\n    diub = distance_upper_bound if distance_upper_bound else np.inf\n    fast_dists, fast_idxs = other.kdtree.query(points,\n                                               distance_upper_bound=diub,\n                                               **kwargs)\n\n    # If upper distance we have to worry about infinite distances\n    if distance_upper_bound:\n        no_nn = fast_dists == np.inf\n        fast_dists[no_nn] = distance_upper_bound\n\n        # Temporarily give those nodes a match\n        fast_idxs[no_nn] = 0\n\n    fast_dotprods = np.abs((self.vect * other.vect[fast_idxs]).sum(axis=1))\n\n    if distance_upper_bound:\n        fast_dotprods[no_nn] = 0\n\n    if not alpha:\n        return fast_dists, fast_dotprods\n\n    fast_alpha = self.alpha * other.alpha[fast_idxs]\n\n    if distance_upper_bound:\n        fast_alpha[no_nn] = 0\n\n    return fast_dists, fast_dotprods, fast_alpha\n</code></pre>"},{"location":"reference/navis/#navis.Dotprops.downsample","title":"<code>downsample</code>","text":"<p>Downsample the neuron by given factor.</p> PARAMETER DESCRIPTION <code>factor</code> <pre><code>                Factor by which to downsample the neurons.\n                Default = 5.\n</code></pre> <p> TYPE: <code>               int</code> DEFAULT: <code>5</code> </p> <code>inplace</code> <pre><code>                If True, operation will be performed on\n                itself. If False, operation is performed on\n                copy which is then returned.\n</code></pre> <p> TYPE: <code>              bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <pre><code>                Additional arguments passed to\n                [`navis.downsample_neuron`][].\n</code></pre> <p> DEFAULT: <code>{}</code> </p> See Also <p><code>navis.downsample_neuron</code>     Base function. See for details and examples.</p> Source code in <code>navis/core/dotprop.py</code> <pre><code>def downsample(self, factor=5, inplace=False, **kwargs):\n    \"\"\"Downsample the neuron by given factor.\n\n    Parameters\n    ----------\n    factor :                int, optional\n                            Factor by which to downsample the neurons.\n                            Default = 5.\n    inplace :               bool, optional\n                            If True, operation will be performed on\n                            itself. If False, operation is performed on\n                            copy which is then returned.\n    **kwargs\n                            Additional arguments passed to\n                            [`navis.downsample_neuron`][].\n\n    See Also\n    --------\n    [`navis.downsample_neuron`][]\n        Base function. See for details and examples.\n\n    \"\"\"\n    if inplace:\n        x = self\n    else:\n        x = self.copy()\n\n    sampling.downsample_neuron(x, factor, inplace=True, **kwargs)\n\n    if not inplace:\n        return x\n    return None\n</code></pre>"},{"location":"reference/navis/#navis.Dotprops.drop_fluff","title":"<code>drop_fluff</code>","text":"<p>Remove fluff from neuron.</p> <p>By default, this function will remove all but the largest connected component from the neuron. You can change that behavior using the <code>keep_size</code> and <code>n_largest</code> parameters.</p> PARAMETER DESCRIPTION <code>epsilon</code> <pre><code>    Distance at which to consider two points to be connected.\n    If `None`, will use the default value of 5 times the average\n    node distance (`self.sampling_resolution`).\n</code></pre> <p> TYPE: <code>  float</code> </p> <code>keep_size</code> <pre><code>    Use this to set a size (in number of points) for small\n    bits to keep. If `keep_size` &lt; 1 it will be intepreted as\n    fraction of total nodes/vertices/points.\n</code></pre> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>n_largest</code> <pre><code>    If set, will keep the `n_largest` connected components. Note:\n    if provided, `keep_size` will be applied first!\n</code></pre> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>inplace</code> <pre><code>    If False, will return a copy and leave the original data\n    unmodified.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Dotprops</code> <p>Only if <code>inplace=False</code>.</p> See Also <p><code>navis.drop_fluff</code>     Base function. See for details and examples.</p> Source code in <code>navis/core/dotprop.py</code> <pre><code>def drop_fluff(self, epsilon, keep_size: int = None, n_largest: int = None, inplace=False):\n    \"\"\"Remove fluff from neuron.\n\n    By default, this function will remove all but the largest connected\n    component from the neuron. You can change that behavior using the\n    `keep_size` and `n_largest` parameters.\n\n    Parameters\n    ----------\n    epsilon :   float\n                Distance at which to consider two points to be connected.\n                If `None`, will use the default value of 5 times the average\n                node distance (`self.sampling_resolution`).\n    keep_size : float, optional\n                Use this to set a size (in number of points) for small\n                bits to keep. If `keep_size` &lt; 1 it will be intepreted as\n                fraction of total nodes/vertices/points.\n    n_largest : int, optional\n                If set, will keep the `n_largest` connected components. Note:\n                if provided, `keep_size` will be applied first!\n    inplace :   bool, optional\n                If False, will return a copy and leave the original data\n                unmodified.\n\n    Returns\n    -------\n    Dotprops\n                Only if `inplace=False`.\n\n    See Also\n    --------\n    [`navis.drop_fluff`][]\n        Base function. See for details and examples.\n\n    \"\"\"\n    x = morpho.drop_fluff(self, epsilon=epsilon, keep_size=keep_size, n_largest=n_largest, inplace=inplace)\n\n    if not inplace:\n        return x\n</code></pre>"},{"location":"reference/navis/#navis.Dotprops.recalculate_tangents","title":"<code>recalculate_tangents</code>","text":"<p>Recalculate tangent vectors and alpha with a new <code>k</code>.</p> PARAMETER DESCRIPTION <code>k</code> <pre><code>    Number of nearest neighbours to use for tangent vector\n    calculation.\n</code></pre> <p> TYPE: <code>        int</code> </p> <code>inplace</code> <pre><code>    If False, will return a copy and leave the original data\n    unmodified.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Dotprops</code> <p>Only if <code>inplace=False</code>.</p> Source code in <code>navis/core/dotprop.py</code> <pre><code>def recalculate_tangents(self, k: int, inplace=False):\n    \"\"\"Recalculate tangent vectors and alpha with a new `k`.\n\n    Parameters\n    ----------\n    k :         int\n                Number of nearest neighbours to use for tangent vector\n                calculation.\n    inplace :   bool\n                If False, will return a copy and leave the original data\n                unmodified.\n\n    Returns\n    -------\n    Dotprops\n                Only if `inplace=False`.\n\n    \"\"\"\n    if not inplace:\n        x = self.copy()\n    else:\n        x = self\n\n    if isinstance(k, type(None)) or k &lt; 1:\n        raise ValueError(f'`k` must be integer &gt;= 1, got \"{k}\"')\n\n    # Checks and balances\n    n_points = x.points.shape[0]\n    if n_points &lt; k:\n        raise ValueError(f\"Too few points ({n_points}) to calculate {k} \"\n                         \"nearest-neighbors\")\n\n    # Create the KDTree and get the k-nearest neighbors for each point\n    dist, ix = self.kdtree.query(x.points, k=k)\n\n    # Get points: array of (N, k, 3)\n    pt = x.points[ix]\n\n    # Generate centers for each cloud of k nearest neighbors\n    centers = np.mean(pt, axis=1)\n\n    # Generate vector from center\n    cpt = pt - centers.reshape((pt.shape[0], 1, 3))\n\n    # Get inertia (N, 3, 3)\n    inertia = cpt.transpose((0, 2, 1)) @ cpt\n\n    # Extract vector and alpha\n    u, s, vh = np.linalg.svd(inertia)\n    x.vect = vh[:, 0, :]\n    x.alpha = (s[:, 0] - s[:, 1]) / np.sum(s, axis=1)\n\n    # Keep track of k\n    x.k = k\n\n    if not inplace:\n        return x\n</code></pre>"},{"location":"reference/navis/#navis.Dotprops.snap","title":"<code>snap</code>","text":"<p>Snap xyz location(s) to closest point or synapse.</p> PARAMETER DESCRIPTION <code>locs</code> <pre><code>    Either single or multiple XYZ locations.\n</code></pre> <p> TYPE: <code>     (N, 3) array | (3, ) array</code> </p> <code>to</code> <pre><code>    Whether to snap to points or connectors.\n</code></pre> <p> TYPE: <code>       \"points\" | \"connectors\"</code> DEFAULT: <code>'points'</code> </p> RETURNS DESCRIPTION <code>ix</code> <p>Index/Indices of the closest point/connector.</p> <p> TYPE: <code>int | list of int</code> </p> <code>dist</code> <p>Distance(s) to the closest point/connector.</p> <p> TYPE: <code>float | list of float</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; dp = navis.make_dotprops(n, k=5)\n&gt;&gt;&gt; ix, dist = dp.snap([0, 0, 0])\n&gt;&gt;&gt; ix\n1123\n</code></pre> Source code in <code>navis/core/dotprop.py</code> <pre><code>def snap(self, locs, to='points'):\n    \"\"\"Snap xyz location(s) to closest point or synapse.\n\n    Parameters\n    ----------\n    locs :      (N, 3) array | (3, ) array\n                Either single or multiple XYZ locations.\n    to :        \"points\" | \"connectors\"\n                Whether to snap to points or connectors.\n\n    Returns\n    -------\n    ix :        int | list of int\n                Index/Indices of the closest point/connector.\n    dist :      float | list of float\n                Distance(s) to the closest point/connector.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; dp = navis.make_dotprops(n, k=5)\n    &gt;&gt;&gt; ix, dist = dp.snap([0, 0, 0])\n    &gt;&gt;&gt; ix\n    1123\n\n    \"\"\"\n    locs = np.asarray(locs).astype(np.float64)\n\n    is_single = (locs.ndim == 1 and len(locs) == 3)\n    is_multi = (locs.ndim == 2 and locs.shape[1] == 3)\n    if not is_single and not is_multi:\n        raise ValueError('Expected a single (x, y, z) location or a '\n                         '(N, 3) array of multiple locations')\n\n    if to not in ['points', 'connectors']:\n        raise ValueError('`to` must be \"points\" or \"connectors\", '\n                         f'got {to}')\n\n    # Generate tree\n    tree = graph.neuron2KDTree(self, data=to)\n\n    # Find the closest node\n    dist, ix = tree.query(locs)\n\n    return ix, dist\n</code></pre>"},{"location":"reference/navis/#navis.Dotprops.to_skeleton","title":"<code>to_skeleton</code>","text":"<p>Turn Dotprop into a TreeNeuron.</p> <p>This does not skeletonize the neuron but rather generates a line segment for each point based on the tangent vector. This is mainly used under the hood for plotting. Also note that only minimal meta data is carried over.</p> <p>For proper skeletonization see <code>navis.skeletonize</code>.</p> PARAMETER DESCRIPTION <code>scale_vec</code> <pre><code>        Factor by which to scale each tangent vector when\n        generating the line segments. If \"auto\" (default for\n        plotting) will use the sampling resolution (median\n        distance between points) to determine a suitable\n        values.\n</code></pre> <p> TYPE: <code>    \"auto\" | float</code> DEFAULT: <code>'auto'</code> </p> RETURNS DESCRIPTION <code>TreeNeuron</code> Source code in <code>navis/core/dotprop.py</code> <pre><code>def to_skeleton(self,\n                scale_vec: Union[float, Literal['auto']] = 'auto'\n                ) -&gt; core.TreeNeuron:\n    \"\"\"Turn Dotprop into a TreeNeuron.\n\n    This does *not* skeletonize the neuron but rather generates a line\n    segment for each point based on the tangent vector. This is mainly\n    used under the hood for plotting. Also note that only minimal meta\n    data is carried over.\n\n    For proper skeletonization see [`navis.skeletonize`][].\n\n    Parameters\n    ----------\n    scale_vec :     \"auto\" | float\n                    Factor by which to scale each tangent vector when\n                    generating the line segments. If \"auto\" (default for\n                    plotting) will use the sampling resolution (median\n                    distance between points) to determine a suitable\n                    values.\n\n    Returns\n    -------\n    TreeNeuron\n\n    \"\"\"\n    if not isinstance(scale_vec, numbers.Number) and scale_vec != 'auto':\n        raise ValueError('`scale_vect` must be \"auto\" or a number, '\n                         f'got {scale_vec}')\n\n    if scale_vec == 'auto':\n        scale_vec = self.sampling_resolution * .8\n\n    # Prepare segments - this is based on nat:::plot3d.dotprops\n    halfvect = self.vect / 2 * scale_vec\n    starts = self.points - halfvect\n    ends = self.points + halfvect\n\n    # Interweave starts and ends\n    segs = np.zeros((starts.shape[0] * 2, 3))\n    segs[::2] = starts\n    segs[1::2] = ends\n\n    # Generate node table\n    nodes = pd.DataFrame(segs, columns=['x', 'y', 'z'])\n    nodes['node_id'] = nodes.index\n    nodes['parent_id'] = -1\n    nodes.loc[1::2, 'parent_id'] = nodes.index.values[::2]\n\n    # Produce a minimal TreeNeuron\n    tn = core.TreeNeuron(nodes, units=self.units, id=self.id)\n\n    # Carry over the label\n    if getattr(self, '_label', None):\n        tn._label = self._label\n\n    # Add some other relevant attributes directly\n    if self.has_connectors:\n        tn._connectors = self._connectors\n    tn._soma = self._soma\n\n    return tn\n</code></pre>"},{"location":"reference/navis/#navis.MeshNeuron","title":"<code>navis.MeshNeuron</code>","text":"<p>Neuron represented as mesh with vertices and faces.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Data to construct neuron from:\n         - any object that has `.vertices` and `.faces`\n           properties (e.g. a trimesh.Trimesh)\n         - a tuple `(vertices, faces)`\n         - a dictionary `{\"vertices\": (N, 3), \"faces\": (M, 3)}`\n         - filepath to a file that can be read by `trimesh.load`\n         - `None` will initialize an empty MeshNeuron\n         - `skeletor.Skeleton` will use the mesh and the skeleton\n           (including the vertex to node map)\n</code></pre> <p> TYPE: <code>            mesh-like | tuple | dictionary | filepath | None</code> </p> <code>units</code> <pre><code>        Units for coordinates. Defaults to `None` (dimensionless).\n        Strings must be parsable by pint: e.g. \"nm\", \"um\",\n        \"micrometer\" or \"8 nanometers\".\n</code></pre> <p> TYPE: <code>        str | pint.Units | pint.Quantity</code> DEFAULT: <code>None</code> </p> <code>process</code> <pre><code>        If True (default and highly recommended), will remove NaN\n        and infinite values, and merge duplicate vertices.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>True</code> </p> <code>validate</code> <pre><code>        If True, will try to fix some common problems with\n        meshes. See `navis.fix_mesh` for details.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>False</code> </p> <code>**metadata</code> <pre><code>        Any additional data to attach to neuron.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> Source code in <code>navis/core/mesh.py</code> <pre><code>class MeshNeuron(BaseNeuron):\n    \"\"\"Neuron represented as mesh with vertices and faces.\n\n    Parameters\n    ----------\n    x :             mesh-like | tuple | dictionary | filepath | None\n                    Data to construct neuron from:\n                     - any object that has `.vertices` and `.faces`\n                       properties (e.g. a trimesh.Trimesh)\n                     - a tuple `(vertices, faces)`\n                     - a dictionary `{\"vertices\": (N, 3), \"faces\": (M, 3)}`\n                     - filepath to a file that can be read by `trimesh.load`\n                     - `None` will initialize an empty MeshNeuron\n                     - `skeletor.Skeleton` will use the mesh and the skeleton\n                       (including the vertex to node map)\n\n    units :         str | pint.Units | pint.Quantity\n                    Units for coordinates. Defaults to `None` (dimensionless).\n                    Strings must be parsable by pint: e.g. \"nm\", \"um\",\n                    \"micrometer\" or \"8 nanometers\".\n    process :       bool\n                    If True (default and highly recommended), will remove NaN\n                    and infinite values, and merge duplicate vertices.\n    validate :      bool\n                    If True, will try to fix some common problems with\n                    meshes. See `navis.fix_mesh` for details.\n    **metadata\n                    Any additional data to attach to neuron.\n\n    \"\"\"\n\n    connectors: Optional[pd.DataFrame]\n\n    vertices: np.ndarray\n    faces: np.ndarray\n\n    soma: Optional[Union[list, np.ndarray]]\n\n    #: Attributes used for neuron summary\n    SUMMARY_PROPS = ['type', 'name', 'units', 'n_vertices', 'n_faces']\n\n    #: Attributes to be used when comparing two neurons.\n    EQ_ATTRIBUTES = ['name', 'n_vertices', 'n_faces']\n\n    #: Temporary attributes that need clearing when neuron data changes\n    TEMP_ATTR = ['_memory_usage', '_trimesh', '_skeleton', '_igraph', '_graph_nx']\n\n    #: Core data table(s) used to calculate hash\n    CORE_DATA = ['vertices', 'faces']\n\n    def __init__(self,\n                 x,\n                 units: Union[pint.Unit, str] = None,\n                 process: bool = True,\n                 validate: bool = False,\n                 **metadata\n                 ):\n        \"\"\"Initialize Mesh Neuron.\"\"\"\n        super().__init__()\n\n        # Lock neuron during initialization\n        self._lock = 1\n        self._trimesh = None  # this is required to avoid recursion during init\n\n        if isinstance(x, MeshNeuron):\n            self.__dict__.update(x.copy().__dict__)\n            self.vertices, self.faces = x.vertices, x.faces\n        elif hasattr(x, 'faces') and hasattr(x, 'vertices'):\n            self.vertices, self.faces = x.vertices, x.faces\n        elif isinstance(x, dict):\n            if 'faces' not in x or 'vertices' not in x:\n                raise ValueError('Dictionary must contain \"vertices\" and \"faces\"')\n            self.vertices, self.faces = x['vertices'], x['faces']\n        elif isinstance(x, str) and os.path.isfile(x):\n            m = tm.load(x)\n            self.vertices, self.faces = m.vertices, m.faces\n        elif isinstance(x, type(None)):\n            # Empty neuron\n            self.vertices, self.faces = np.zeros((0, 3)), np.zeros((0, 3))\n        elif isinstance(x, sk.Skeleton):\n            self.vertices, self.faces = x.mesh.vertices, x.mesh.faces\n            self._skeleton = TreeNeuron(x)\n        elif isinstance(x, tuple):\n            if len(x) != 2 or any([not isinstance(v, np.ndarray) for v in x]):\n                raise TypeError('Expect tuple to be two arrays: (vertices, faces)')\n            self.vertices, self.faces = x[0], x[1]\n        else:\n            raise utils.ConstructionError(f'Unable to construct MeshNeuron from \"{type(x)}\"')\n\n        for k, v in metadata.items():\n            try:\n                setattr(self, k, v)\n            except AttributeError:\n                raise AttributeError(f\"Unable to set neuron's `{k}` attribute.\")\n\n        if process and self.vertices.shape[0]:\n            # For some reason we can't do self._trimesh at this stage\n            _trimesh = tm.Trimesh(self.vertices, self.faces,\n                                  process=process,\n                                  validate=validate)\n            self.vertices = _trimesh.vertices\n            self.faces = _trimesh.faces\n\n        self._lock = 0\n\n        if validate:\n            self.validate()\n\n        self.units = units\n\n    def __getattr__(self, key):\n        \"\"\"We will use this magic method to calculate some attributes on-demand.\"\"\"\n        # Note that we're mixing @property and __getattr__ which causes problems:\n        # if a @property raises an Exception, Python falls back to __getattr__\n        # and traceback is lost!\n\n        # Last ditch effort - maybe the base class knows the key?\n        return super().__getattr__(key)\n\n    def __getstate__(self):\n        \"\"\"Get state (used e.g. for pickling).\"\"\"\n        state = {k: v for k, v in self.__dict__.items() if not callable(v)}\n\n        # We don't need the trimesh object\n        if '_trimesh' in state:\n            _ = state.pop('_trimesh')\n\n        return state\n\n    def __setstate__(self, d):\n        \"\"\"Update state (used e.g. for pickling).\"\"\"\n        self.__dict__.update(d)\n\n    def __truediv__(self, other, copy=True):\n        \"\"\"Implement division for coordinates (vertices, connectors).\"\"\"\n        if isinstance(other, numbers.Number) or utils.is_iterable(other):\n            # If a number, consider this an offset for coordinates\n            n = self.copy() if copy else self\n            _ = np.divide(n.vertices, other, out=n.vertices, casting='unsafe')\n            if n.has_connectors:\n                n.connectors.loc[:, ['x', 'y', 'z']] /= other\n\n            # Convert units\n            # Note: .to_compact() throws a RuntimeWarning and returns unchanged\n            # values  when `units` is a iterable\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                n.units = (n.units * other).to_compact()\n\n            self._clear_temp_attr()\n\n            return n\n        return NotImplemented\n\n    def __mul__(self, other, copy=True):\n        \"\"\"Implement multiplication for coordinates (vertices, connectors).\"\"\"\n        if isinstance(other, numbers.Number) or utils.is_iterable(other):\n            # If a number, consider this an offset for coordinates\n            n = self.copy() if copy else self\n            _ = np.multiply(n.vertices, other, out=n.vertices, casting='unsafe')\n            if n.has_connectors:\n                n.connectors.loc[:, ['x', 'y', 'z']] *= other\n\n            # Convert units\n            # Note: .to_compact() throws a RuntimeWarning and returns unchanged\n            # values  when `units` is a iterable\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                n.units = (n.units / other).to_compact()\n\n            self._clear_temp_attr()\n\n            return n\n        return NotImplemented\n\n    def __add__(self, other, copy=True):\n        \"\"\"Implement addition for coordinates (vertices, connectors).\"\"\"\n        if isinstance(other, numbers.Number) or utils.is_iterable(other):\n            n = self.copy() if copy else self\n            _ = np.add(n.vertices, other, out=n.vertices, casting='unsafe')\n            if n.has_connectors:\n                n.connectors.loc[:, ['x', 'y', 'z']] += other\n\n            self._clear_temp_attr()\n\n            return n\n        # If another neuron, return a list of neurons\n        elif isinstance(other, BaseNeuron):\n            return NeuronList([self, other])\n        return NotImplemented\n\n    def __sub__(self, other, copy=True):\n        \"\"\"Implement subtraction for coordinates (vertices, connectors).\"\"\"\n        if isinstance(other, numbers.Number) or utils.is_iterable(other):\n            n = self.copy() if copy else self\n            _ = np.subtract(n.vertices, other, out=n.vertices, casting='unsafe')\n            if n.has_connectors:\n                n.connectors.loc[:, ['x', 'y', 'z']] -= other\n\n            self._clear_temp_attr()\n\n            return n\n        return NotImplemented\n\n    @property\n    def bbox(self) -&gt; np.ndarray:\n        \"\"\"Bounding box (includes connectors).\"\"\"\n        mn = np.min(self.vertices, axis=0)\n        mx = np.max(self.vertices, axis=0)\n\n        if self.has_connectors:\n            cn_mn = np.min(self.connectors[['x', 'y', 'z']].values, axis=0)\n            cn_mx = np.max(self.connectors[['x', 'y', 'z']].values, axis=0)\n\n            mn = np.min(np.vstack((mn, cn_mn)), axis=0)\n            mx = np.max(np.vstack((mx, cn_mx)), axis=0)\n\n        return np.vstack((mn, mx)).T\n\n    @property\n    def vertices(self):\n        \"\"\"Vertices making up the neuron.\"\"\"\n        return self._vertices\n\n    @vertices.setter\n    def vertices(self, verts):\n        if not isinstance(verts, np.ndarray):\n            raise TypeError(f'Vertices must be numpy array, got \"{type(verts)}\"')\n        if verts.ndim != 2:\n            raise ValueError('Vertices must be 2-dimensional array')\n        self._vertices = verts\n        self._clear_temp_attr()\n\n    @property\n    def faces(self):\n        \"\"\"Faces making up the neuron.\"\"\"\n        return self._faces\n\n    @faces.setter\n    def faces(self, faces):\n        if not isinstance(faces, np.ndarray):\n            raise TypeError(f'Faces must be numpy array, got \"{type(faces)}\"')\n        if faces.ndim != 2:\n            raise ValueError('Faces must be 2-dimensional array')\n        self._faces = faces\n        self._clear_temp_attr()\n\n    @property\n    @temp_property\n    def igraph(self) -&gt; 'igraph.Graph':\n        \"\"\"iGraph representation of the vertex connectivity.\"\"\"\n        # If igraph does not exist, create and return\n        if not hasattr(self, '_igraph'):\n            # This also sets the attribute\n            self._igraph = graph.neuron2igraph(self, raise_not_installed=False)\n        return self._igraph\n\n    @property\n    @temp_property\n    def graph(self) -&gt; nx.DiGraph:\n        \"\"\"Networkx Graph representation of the vertex connectivity.\"\"\"\n        # If graph does not exist, create and return\n        if not hasattr(self, '_graph_nx'):\n            # This also sets the attribute\n            self._graph_nx = graph.neuron2nx(self)\n        return self._graph_nx\n\n    @property\n    def sampling_resolution(self) -&gt; float:\n        \"\"\"Average distance between vertices.\"\"\"\n        return float(self.trimesh.edges_unique_length.mean())\n\n    @property\n    @add_units(compact=True, power=3)\n    def volume(self) -&gt; float:\n        \"\"\"Volume of the neuron.\n\n        Calculated from the surface integral. Garbage if neuron is not\n        watertight.\n\n        \"\"\"\n        return float(self.trimesh.volume)\n\n    @property\n    @temp_property\n    def skeleton(self) -&gt; 'TreeNeuron':\n        \"\"\"Skeleton representation of this neuron.\n\n        Uses [`navis.conversion.mesh2skeleton`][].\n\n        \"\"\"\n        if not hasattr(self, '_skeleton'):\n            self._skeleton = self.skeletonize()\n        return self._skeleton\n\n    @skeleton.setter\n    def skeleton(self, s):\n        \"\"\"Attach skeleton respresentation for this neuron.\"\"\"\n        if isinstance(s, sk.Skeleton):\n            s = TreeNeuron(s, id=self.id, name=self.name)\n        elif not isinstance(s, TreeNeuron):\n            raise TypeError(f'`.skeleton` must be a TreeNeuron, got \"{type(s)}\"')\n        self._skeleton = s\n\n    @property\n    def soma(self):\n        \"\"\"Not implemented for MeshNeurons - use `.soma_pos`.\"\"\"\n        raise AttributeError(\"MeshNeurons have a soma position (`.soma_pos`), not a soma.\")\n\n    @property\n    def soma_pos(self):\n        \"\"\"X/Y/Z position of the soma.\n\n        Returns `None` if no soma.\n        \"\"\"\n        return getattr(self, '_soma_pos', None)\n\n    @soma_pos.setter\n    def soma_pos(self, value):\n        \"\"\"Set soma by position.\"\"\"\n        if value is None:\n            self._soma_pos = None\n            return\n\n        try:\n            value = np.asarray(value).astype(np.float64).reshape(3)\n        except BaseException:\n            raise ValueError(f'Unable to convert soma position \"{value}\" '\n                             f'to numeric (3, ) numpy array.')\n\n        self._soma_pos = value\n\n    @property\n    def type(self) -&gt; str:\n        \"\"\"Neuron type.\"\"\"\n        return 'navis.MeshNeuron'\n\n    @property\n    @temp_property\n    def trimesh(self):\n        \"\"\"Trimesh representation of the neuron.\"\"\"\n        if not getattr(self, '_trimesh', None):\n            self._trimesh = tm.Trimesh(vertices=self._vertices,\n                                       faces=self._faces,\n                                       process=False)\n        return self._trimesh\n\n    def copy(self) -&gt; 'MeshNeuron':\n        \"\"\"Return a copy of the neuron.\"\"\"\n        no_copy = ['_lock']\n\n        # Generate new neuron\n        x = self.__class__(None)\n        # Override with this neuron's data\n        x.__dict__.update({k: copy.copy(v) for k, v in self.__dict__.items() if k not in no_copy})\n\n        return x\n\n    def snap(self, locs, to='vertices'):\n        \"\"\"Snap xyz location(s) to closest vertex or synapse.\n\n        Parameters\n        ----------\n        locs :      (N, 3) array | (3, ) array\n                    Either single or multiple XYZ locations.\n        to :        \"vertices\" | \"connectors\"\n                    Whether to snap to vertex or connector.\n\n        Returns\n        -------\n        ix :        int | list of int\n                    Index/indices of the closest vertex/connector.\n        dist :      float | list of float\n                    Distance(s) to the closest vertex/connector.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import navis\n        &gt;&gt;&gt; n = navis.example_neurons(1, kind='mesh')\n        &gt;&gt;&gt; ix, dist = n.snap([0, 0, 0])\n        &gt;&gt;&gt; ix\n        4134\n\n        \"\"\"\n        locs = np.asarray(locs).astype(self.vertices.dtype)\n\n        is_single = (locs.ndim == 1 and len(locs) == 3)\n        is_multi = (locs.ndim == 2 and locs.shape[1] == 3)\n        if not is_single and not is_multi:\n            raise ValueError('Expected a single (x, y, z) location or a '\n                             '(N, 3) array of multiple locations')\n\n        if to not in ('vertices', 'vertex', 'connectors', 'connectors'):\n            raise ValueError('`to` must be \"vertices\" or \"connectors\", '\n                             f'got {to}')\n\n        # Generate tree\n        tree = scipy.spatial.cKDTree(data=self.vertices)\n\n        # Find the closest node\n        dist, ix = tree.query(locs)\n\n        return ix, dist\n\n    def skeletonize(self, method='wavefront', heal=True, inv_dist=None, **kwargs) -&gt; 'TreeNeuron':\n        \"\"\"Skeletonize mesh.\n\n        See [`navis.conversion.mesh2skeleton`][] for details.\n\n        Parameters\n        ----------\n        method :    \"wavefront\" | \"teasar\"\n                    Method to use for skeletonization.\n        heal :      bool\n                    Whether to heal a fragmented skeleton after skeletonization.\n        inv_dist :  int | float\n                    Only required for method \"teasar\": invalidation distance for\n                    the traversal. Smaller `inv_dist` captures smaller features\n                    but is slower and vice versa. A good starting value is around\n                    2-5 microns.\n        **kwargs\n                    Additional keyword are passed through to\n                    [`navis.conversion.mesh2skeleton`][].\n\n        Returns\n        -------\n        skeleton :  navis.TreeNeuron\n\n        \"\"\"\n        return conversion.mesh2skeleton(self, method=method, heal=heal,\n                                        inv_dist=inv_dist, **kwargs)\n\n    def validate(self, inplace=False):\n        \"\"\"Use trimesh to try and fix some common mesh issues.\n\n        See [`navis.fix_mesh`][] for details.\n\n        \"\"\"\n        return meshes.fix_mesh(self, inplace=inplace)\n</code></pre>"},{"location":"reference/navis/#navis.MeshNeuron.bbox","title":"<code>bbox: np.ndarray</code>  <code>property</code>","text":"<p>Bounding box (includes connectors).</p>"},{"location":"reference/navis/#navis.MeshNeuron.faces","title":"<code>faces</code>  <code>property</code> <code>writable</code>","text":"<p>Faces making up the neuron.</p>"},{"location":"reference/navis/#navis.MeshNeuron.graph","title":"<code>graph: nx.DiGraph</code>  <code>property</code>","text":"<p>Networkx Graph representation of the vertex connectivity.</p>"},{"location":"reference/navis/#navis.MeshNeuron.igraph","title":"<code>igraph: igraph.Graph</code>  <code>property</code>","text":"<p>iGraph representation of the vertex connectivity.</p>"},{"location":"reference/navis/#navis.MeshNeuron.sampling_resolution","title":"<code>sampling_resolution: float</code>  <code>property</code>","text":"<p>Average distance between vertices.</p>"},{"location":"reference/navis/#navis.MeshNeuron.skeleton","title":"<code>skeleton: TreeNeuron</code>  <code>property</code> <code>writable</code>","text":"<p>Skeleton representation of this neuron.</p> <p>Uses <code>navis.conversion.mesh2skeleton</code>.</p>"},{"location":"reference/navis/#navis.MeshNeuron.soma","title":"<code>soma</code>  <code>property</code>","text":"<p>Not implemented for MeshNeurons - use <code>.soma_pos</code>.</p>"},{"location":"reference/navis/#navis.MeshNeuron.soma_pos","title":"<code>soma_pos</code>  <code>property</code> <code>writable</code>","text":"<p>X/Y/Z position of the soma.</p> <p>Returns <code>None</code> if no soma.</p>"},{"location":"reference/navis/#navis.MeshNeuron.trimesh","title":"<code>trimesh</code>  <code>property</code>","text":"<p>Trimesh representation of the neuron.</p>"},{"location":"reference/navis/#navis.MeshNeuron.type","title":"<code>type: str</code>  <code>property</code>","text":"<p>Neuron type.</p>"},{"location":"reference/navis/#navis.MeshNeuron.vertices","title":"<code>vertices</code>  <code>property</code> <code>writable</code>","text":"<p>Vertices making up the neuron.</p>"},{"location":"reference/navis/#navis.MeshNeuron.volume","title":"<code>volume: float</code>  <code>property</code>","text":"<p>Volume of the neuron.</p> <p>Calculated from the surface integral. Garbage if neuron is not watertight.</p>"},{"location":"reference/navis/#navis.MeshNeuron.__init__","title":"<code>__init__</code>","text":"<p>Initialize Mesh Neuron.</p> Source code in <code>navis/core/mesh.py</code> <pre><code>def __init__(self,\n             x,\n             units: Union[pint.Unit, str] = None,\n             process: bool = True,\n             validate: bool = False,\n             **metadata\n             ):\n    \"\"\"Initialize Mesh Neuron.\"\"\"\n    super().__init__()\n\n    # Lock neuron during initialization\n    self._lock = 1\n    self._trimesh = None  # this is required to avoid recursion during init\n\n    if isinstance(x, MeshNeuron):\n        self.__dict__.update(x.copy().__dict__)\n        self.vertices, self.faces = x.vertices, x.faces\n    elif hasattr(x, 'faces') and hasattr(x, 'vertices'):\n        self.vertices, self.faces = x.vertices, x.faces\n    elif isinstance(x, dict):\n        if 'faces' not in x or 'vertices' not in x:\n            raise ValueError('Dictionary must contain \"vertices\" and \"faces\"')\n        self.vertices, self.faces = x['vertices'], x['faces']\n    elif isinstance(x, str) and os.path.isfile(x):\n        m = tm.load(x)\n        self.vertices, self.faces = m.vertices, m.faces\n    elif isinstance(x, type(None)):\n        # Empty neuron\n        self.vertices, self.faces = np.zeros((0, 3)), np.zeros((0, 3))\n    elif isinstance(x, sk.Skeleton):\n        self.vertices, self.faces = x.mesh.vertices, x.mesh.faces\n        self._skeleton = TreeNeuron(x)\n    elif isinstance(x, tuple):\n        if len(x) != 2 or any([not isinstance(v, np.ndarray) for v in x]):\n            raise TypeError('Expect tuple to be two arrays: (vertices, faces)')\n        self.vertices, self.faces = x[0], x[1]\n    else:\n        raise utils.ConstructionError(f'Unable to construct MeshNeuron from \"{type(x)}\"')\n\n    for k, v in metadata.items():\n        try:\n            setattr(self, k, v)\n        except AttributeError:\n            raise AttributeError(f\"Unable to set neuron's `{k}` attribute.\")\n\n    if process and self.vertices.shape[0]:\n        # For some reason we can't do self._trimesh at this stage\n        _trimesh = tm.Trimesh(self.vertices, self.faces,\n                              process=process,\n                              validate=validate)\n        self.vertices = _trimesh.vertices\n        self.faces = _trimesh.faces\n\n    self._lock = 0\n\n    if validate:\n        self.validate()\n\n    self.units = units\n</code></pre>"},{"location":"reference/navis/#navis.MeshNeuron.copy","title":"<code>copy</code>","text":"<p>Return a copy of the neuron.</p> Source code in <code>navis/core/mesh.py</code> <pre><code>def copy(self) -&gt; 'MeshNeuron':\n    \"\"\"Return a copy of the neuron.\"\"\"\n    no_copy = ['_lock']\n\n    # Generate new neuron\n    x = self.__class__(None)\n    # Override with this neuron's data\n    x.__dict__.update({k: copy.copy(v) for k, v in self.__dict__.items() if k not in no_copy})\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.MeshNeuron.skeletonize","title":"<code>skeletonize</code>","text":"<p>Skeletonize mesh.</p> <p>See <code>navis.conversion.mesh2skeleton</code> for details.</p> PARAMETER DESCRIPTION <code>method</code> <pre><code>    Method to use for skeletonization.\n</code></pre> <p> TYPE: <code>   \"wavefront\" | \"teasar\"</code> DEFAULT: <code>'wavefront'</code> </p> <code>heal</code> <pre><code>    Whether to heal a fragmented skeleton after skeletonization.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> <code>inv_dist</code> <pre><code>    Only required for method \"teasar\": invalidation distance for\n    the traversal. Smaller `inv_dist` captures smaller features\n    but is slower and vice versa. A good starting value is around\n    2-5 microns.\n</code></pre> <p> TYPE: <code> int | float</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <pre><code>    Additional keyword are passed through to\n    [`navis.conversion.mesh2skeleton`][].\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>skeleton</code> <p> TYPE: <code>navis.TreeNeuron</code> </p> Source code in <code>navis/core/mesh.py</code> <pre><code>def skeletonize(self, method='wavefront', heal=True, inv_dist=None, **kwargs) -&gt; 'TreeNeuron':\n    \"\"\"Skeletonize mesh.\n\n    See [`navis.conversion.mesh2skeleton`][] for details.\n\n    Parameters\n    ----------\n    method :    \"wavefront\" | \"teasar\"\n                Method to use for skeletonization.\n    heal :      bool\n                Whether to heal a fragmented skeleton after skeletonization.\n    inv_dist :  int | float\n                Only required for method \"teasar\": invalidation distance for\n                the traversal. Smaller `inv_dist` captures smaller features\n                but is slower and vice versa. A good starting value is around\n                2-5 microns.\n    **kwargs\n                Additional keyword are passed through to\n                [`navis.conversion.mesh2skeleton`][].\n\n    Returns\n    -------\n    skeleton :  navis.TreeNeuron\n\n    \"\"\"\n    return conversion.mesh2skeleton(self, method=method, heal=heal,\n                                    inv_dist=inv_dist, **kwargs)\n</code></pre>"},{"location":"reference/navis/#navis.MeshNeuron.snap","title":"<code>snap</code>","text":"<p>Snap xyz location(s) to closest vertex or synapse.</p> PARAMETER DESCRIPTION <code>locs</code> <pre><code>    Either single or multiple XYZ locations.\n</code></pre> <p> TYPE: <code>     (N, 3) array | (3, ) array</code> </p> <code>to</code> <pre><code>    Whether to snap to vertex or connector.\n</code></pre> <p> TYPE: <code>       \"vertices\" | \"connectors\"</code> DEFAULT: <code>'vertices'</code> </p> RETURNS DESCRIPTION <code>ix</code> <p>Index/indices of the closest vertex/connector.</p> <p> TYPE: <code>int | list of int</code> </p> <code>dist</code> <p>Distance(s) to the closest vertex/connector.</p> <p> TYPE: <code>float | list of float</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1, kind='mesh')\n&gt;&gt;&gt; ix, dist = n.snap([0, 0, 0])\n&gt;&gt;&gt; ix\n4134\n</code></pre> Source code in <code>navis/core/mesh.py</code> <pre><code>def snap(self, locs, to='vertices'):\n    \"\"\"Snap xyz location(s) to closest vertex or synapse.\n\n    Parameters\n    ----------\n    locs :      (N, 3) array | (3, ) array\n                Either single or multiple XYZ locations.\n    to :        \"vertices\" | \"connectors\"\n                Whether to snap to vertex or connector.\n\n    Returns\n    -------\n    ix :        int | list of int\n                Index/indices of the closest vertex/connector.\n    dist :      float | list of float\n                Distance(s) to the closest vertex/connector.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1, kind='mesh')\n    &gt;&gt;&gt; ix, dist = n.snap([0, 0, 0])\n    &gt;&gt;&gt; ix\n    4134\n\n    \"\"\"\n    locs = np.asarray(locs).astype(self.vertices.dtype)\n\n    is_single = (locs.ndim == 1 and len(locs) == 3)\n    is_multi = (locs.ndim == 2 and locs.shape[1] == 3)\n    if not is_single and not is_multi:\n        raise ValueError('Expected a single (x, y, z) location or a '\n                         '(N, 3) array of multiple locations')\n\n    if to not in ('vertices', 'vertex', 'connectors', 'connectors'):\n        raise ValueError('`to` must be \"vertices\" or \"connectors\", '\n                         f'got {to}')\n\n    # Generate tree\n    tree = scipy.spatial.cKDTree(data=self.vertices)\n\n    # Find the closest node\n    dist, ix = tree.query(locs)\n\n    return ix, dist\n</code></pre>"},{"location":"reference/navis/#navis.MeshNeuron.validate","title":"<code>validate</code>","text":"<p>Use trimesh to try and fix some common mesh issues.</p> <p>See <code>navis.fix_mesh</code> for details.</p> Source code in <code>navis/core/mesh.py</code> <pre><code>def validate(self, inplace=False):\n    \"\"\"Use trimesh to try and fix some common mesh issues.\n\n    See [`navis.fix_mesh`][] for details.\n\n    \"\"\"\n    return meshes.fix_mesh(self, inplace=inplace)\n</code></pre>"},{"location":"reference/navis/#navis.Neuron","title":"<code>navis.Neuron</code>","text":"<p>Constructor for Neuron objects. Depending on the input, either a <code>TreeNeuron</code> or a <code>MeshNeuron</code> is returned.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            Anything that can construct a [`navis.TreeNeuron`][]\n            or [`navis.MeshNeuron`][].\n</code></pre> <p> TYPE: <code>Union[nx.DiGraph, str, pd.DataFrame, TreeNeuron, MeshNeuron]</code> </p> <code>**metadata</code> <pre><code>            Any additional data to attach to neuron.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> See Also <p><code>navis.read_swc</code>                     Gives you more control over how data is extracted from                     SWC file. <code>navis.example_neurons</code>                     Loads some example neurons provided.</p> Source code in <code>navis/core/base.py</code> <pre><code>def Neuron(\n    x: Union[nx.DiGraph, str, pd.DataFrame, \"TreeNeuron\", \"MeshNeuron\"], **metadata\n):\n    \"\"\"Constructor for Neuron objects. Depending on the input, either a\n    `TreeNeuron` or a `MeshNeuron` is returned.\n\n    Parameters\n    ----------\n    x\n                        Anything that can construct a [`navis.TreeNeuron`][]\n                        or [`navis.MeshNeuron`][].\n    **metadata\n                        Any additional data to attach to neuron.\n\n    See Also\n    --------\n    [`navis.read_swc`][]\n                        Gives you more control over how data is extracted from\n                        SWC file.\n    [`navis.example_neurons`][]\n                        Loads some example neurons provided.\n\n    \"\"\"\n    try:\n        return core.TreeNeuron(x, **metadata)\n    except utils.ConstructionError:\n        try:\n            return core.MeshNeuron(x, **metadata)\n        except utils.ConstructionError:\n            pass\n        except BaseException:\n            raise\n    except BaseException:\n        raise\n\n    raise utils.ConstructionError(f'Unable to construct neuron from \"{type(x)}\"')\n</code></pre>"},{"location":"reference/navis/#navis.NeuronConnector","title":"<code>navis.NeuronConnector</code>","text":"<p>Class which creates a connectivity graph from a set of neurons.</p> <p>Connectivity is determined by shared IDs in the <code>connectors</code> table.</p> <p>Add neurons with the <code>add_neuron</code> and <code>add_neurons</code> methods. Alternatively, supply an iterable of neurons in the constructor. Neurons must have unique names.</p> <p>See the <code>to_(multi)digraph</code> method for output.</p> Source code in <code>navis/connectivity/adjacency.py</code> <pre><code>class NeuronConnector:\n    \"\"\"Class which creates a connectivity graph from a set of neurons.\n\n    Connectivity is determined by shared IDs in the `connectors` table.\n\n    Add neurons with the `add_neuron` and `add_neurons` methods.\n    Alternatively, supply an iterable of neurons in the constructor.\n    Neurons must have unique names.\n\n    See the `to_(multi)digraph` method for output.\n    \"\"\"\n\n    def __init__(self, nrns: Optional[Iterable[TreeNeuron]] = None) -&gt; None:\n        self.neurons = dict()\n        self.connector_xyz = dict()\n        # connectors and the treenodes presynaptic to them\n        self.conn_inputs = dict()\n        # connectors and the treenodes postsynaptic to them\n        self.conn_outputs = dict()\n\n        if nrns is not None:\n            self.add_neurons(nrns)\n\n    def __len__(self) -&gt; int:\n        return len(self.neurons)\n\n    def add_neurons(self, nrns: Iterable[TreeNeuron]):\n        \"\"\"Add several neurons to the connector.\n\n        All neurons must have unique names.\n\n        Parameters\n        ----------\n        nrns : Iterable[TreeNeuron]\n\n        Returns\n        -------\n        Modified connector.\n        \"\"\"\n        for nrn in nrns:\n            self.add_neuron(nrn)\n        return self\n\n    def add_neuron(self, nrn: TreeNeuron):\n        \"\"\"Add a single neuron to the connector.\n\n        All neurons must have unique names.\n\n        Parameters\n        ----------\n        nrn : TreeNeuron\n\n        Returns\n        -------\n        Modified connector.\n        \"\"\"\n        if nrn.name in self.neurons:\n            logger.warning(\n                \"Neuron with name %s has already been added to NeuronConnector. \"\n                \"These will occupy the same node in the graph, \"\n                \"but have connectors from both.\",\n                nrn.name\n            )\n\n        self.neurons[nrn.name] = nrn\n        if nrn.connectors is None:\n            logger.warning(\"Neuron with name %s has no connector information\", nrn.name)\n            return self\n\n        for row in nrn.connectors.itertuples():\n            # connector_id, node_id, x, y, z, is_input\n            self.connector_xyz[row.connector_id] = (row.x, row.y, row.z)\n            if row.type == 1:\n                self.conn_outputs.setdefault(row.connector_id, []).append((nrn.name, row.node_id))\n            elif row.type == 0:\n                if row.connector_id in self.conn_inputs:\n                    logger.warning(\n                        \"Connector with ID %s has multiple inputs: \"\n                        \"connector tables are probably inconsistent\",\n                        row.connector_id\n                    )\n                self.conn_inputs[row.connector_id] = (nrn.name, row.node_id)\n\n        return self\n\n    def edges(self, include_other=True) -&gt; Iterable[Edge]:\n        \"\"\"Iterate through all synapse edges.\n\n        Parameters\n        ----------\n        include_other : bool, optional\n            Include edges for which only one partner is known, by default True.\n            If included, the name of the unknown partner will be `\"__OTHER__\"`,\n            and the treenode ID will be None.\n\n        Yields\n        ------\n        tuple[int, str, str, int, int]\n            Connector ID, source name, target name, source treenode, target treenode.\n        \"\"\"\n        for conn_id in set(self.conn_inputs).union(self.conn_outputs):\n            src, src_node = self.conn_inputs.get(conn_id, (OTHER, None))\n            if src_node is None and not include_other:\n                continue\n            for tgt, tgt_node in self.conn_outputs.get(conn_id, [(OTHER, None)]):\n                if tgt_node is None and not include_other:\n                    continue\n                yield Edge(conn_id, src, tgt, src_node, tgt_node)\n\n    def to_adjacency(self, include_other=True) -&gt; pd.DataFrame:\n        \"\"\"Create an adjacency matrix of neuron connectivity.\n\n        Parameters\n        ----------\n        include_other : bool, optional\n            Whether to include a node called `\"__OTHER__\"`,\n            which represents all unknown partners.\n            By default True.\n            This can be helpful when calculating a neuron's input fraction,\n            but cannot be used for output fractions if synapses are polyadic.\n\n        Returns\n        -------\n        pandas.DataFrame\n            Row index is source neuron name,\n            column index is target neuron name,\n            cells are the number of synapses from source to target.\n        \"\"\"\n        index = list(self.neurons)\n        if include_other:\n            index.append(OTHER)\n        data = np.zeros((len(index), len(index)), np.uint64)\n        df = pd.DataFrame(data, index, index)\n        for _, src, tgt, _, _ in self.edges(include_other):\n            df.loc[src, tgt] += 1\n\n        return df\n\n    def to_digraph(self, include_other=True) -&gt; nx.DiGraph:\n        \"\"\"Create a graph of neuron connectivity.\n\n        Parameters\n        ----------\n        include_other : bool, optional\n            Whether to include a node called `\"__OTHER__\"`,\n            which represents all unknown partners.\n            By default True.\n            This can be helpful when calculating a neuron's input fraction,\n            but cannot be used for output fractions if synapses are polyadic.\n\n        Returns\n        -------\n        nx.DiGraph\n            The graph has data `{\"connector_xyz\": {connector_id: (x, y, z), ...}}`.\n            The nodes have data `{\"neuron\": tree_neuron}`.\n            The edges have data `{\"connectors\": data_frame, \"weight\": n_connectors}`,\n            where the connectors data frame has columns\n            \"connector_id\", \"pre_node\", \"post_node\".\n        \"\"\"\n        g = nx.DiGraph()\n        g.add_nodes_from((k, {\"neuron\": v}) for k, v in self.neurons.items())\n        if include_other:\n            g.add_node(OTHER, neuron=None)\n\n        g.graph[\"connector_xyz\"] = self.connector_xyz\n        headers = {\n            \"connector_id\": pd.UInt64Dtype(),\n            \"pre_node\": pd.UInt64Dtype(),\n            \"post_node\": pd.UInt64Dtype(),\n        }\n        edges = dict()\n        for conn_id, src, tgt, src_node, tgt_node in self.edges(include_other):\n            edges.setdefault((src, tgt), []).append([conn_id, src_node, tgt_node])\n\n        for (src, tgt), rows in edges.items():\n            df_tmp = pd.DataFrame(rows, columns=list(headers), dtype=object)\n            df = df_tmp.astype(headers, copy=False)\n            g.add_edge(src, tgt, connectors=df, weight=len(df))\n\n        return g\n\n    def to_multidigraph(self, include_other=True) -&gt; nx.MultiDiGraph:\n        \"\"\"Create a graph of neuron connectivity where each synapse is an edge.\n\n        Parameters\n        ----------\n        include_other : bool, optional\n            Whether to include a node called `\"__OTHER__\"`,\n            which represents all unknown partners.\n            By default True.\n            This can be helpful when calculating a neuron's input fraction,\n            but cannot be used for output fractions if synapses are polyadic.\n\n        Returns\n        -------\n        nx.MultiDiGraph\n            The nodes have data `{\"neuron\": tree_neuron}`.\n            The edges have data\n            `{\"pre_node\": presyn_treenode_id, \"post_node\": postsyn_treenode_id, \"xyz\": connector_location, \"connector_id\": conn_id}`.\n        \"\"\"\n        g = nx.MultiDiGraph()\n        g.add_nodes_from((k, {\"neuron\": v}) for k, v in self.neurons.items())\n        if include_other:\n            g.add_node(OTHER, neuron=None)\n\n        for conn_id, src, tgt, src_node, tgt_node in self.edges(include_other):\n            g.add_edge(\n                src,\n                tgt,\n                pre_node=src_node,\n                post_node=tgt_node,\n                xyz=self.connector_xyz[conn_id],\n                connector_id=conn_id,\n            )\n\n        return g\n</code></pre>"},{"location":"reference/navis/#navis.NeuronConnector.add_neuron","title":"<code>add_neuron</code>","text":"<p>Add a single neuron to the connector.</p> <p>All neurons must have unique names.</p> PARAMETER DESCRIPTION <code>nrn</code> <p> TYPE: <code>TreeNeuron</code> </p> RETURNS DESCRIPTION <code>Modified connector.</code> Source code in <code>navis/connectivity/adjacency.py</code> <pre><code>def add_neuron(self, nrn: TreeNeuron):\n    \"\"\"Add a single neuron to the connector.\n\n    All neurons must have unique names.\n\n    Parameters\n    ----------\n    nrn : TreeNeuron\n\n    Returns\n    -------\n    Modified connector.\n    \"\"\"\n    if nrn.name in self.neurons:\n        logger.warning(\n            \"Neuron with name %s has already been added to NeuronConnector. \"\n            \"These will occupy the same node in the graph, \"\n            \"but have connectors from both.\",\n            nrn.name\n        )\n\n    self.neurons[nrn.name] = nrn\n    if nrn.connectors is None:\n        logger.warning(\"Neuron with name %s has no connector information\", nrn.name)\n        return self\n\n    for row in nrn.connectors.itertuples():\n        # connector_id, node_id, x, y, z, is_input\n        self.connector_xyz[row.connector_id] = (row.x, row.y, row.z)\n        if row.type == 1:\n            self.conn_outputs.setdefault(row.connector_id, []).append((nrn.name, row.node_id))\n        elif row.type == 0:\n            if row.connector_id in self.conn_inputs:\n                logger.warning(\n                    \"Connector with ID %s has multiple inputs: \"\n                    \"connector tables are probably inconsistent\",\n                    row.connector_id\n                )\n            self.conn_inputs[row.connector_id] = (nrn.name, row.node_id)\n\n    return self\n</code></pre>"},{"location":"reference/navis/#navis.NeuronConnector.add_neurons","title":"<code>add_neurons</code>","text":"<p>Add several neurons to the connector.</p> <p>All neurons must have unique names.</p> PARAMETER DESCRIPTION <code>nrns</code> <p> TYPE: <code>Iterable[TreeNeuron]</code> </p> RETURNS DESCRIPTION <code>Modified connector.</code> Source code in <code>navis/connectivity/adjacency.py</code> <pre><code>def add_neurons(self, nrns: Iterable[TreeNeuron]):\n    \"\"\"Add several neurons to the connector.\n\n    All neurons must have unique names.\n\n    Parameters\n    ----------\n    nrns : Iterable[TreeNeuron]\n\n    Returns\n    -------\n    Modified connector.\n    \"\"\"\n    for nrn in nrns:\n        self.add_neuron(nrn)\n    return self\n</code></pre>"},{"location":"reference/navis/#navis.NeuronConnector.edges","title":"<code>edges</code>","text":"<p>Iterate through all synapse edges.</p> PARAMETER DESCRIPTION <code>include_other</code> <p>Include edges for which only one partner is known, by default True. If included, the name of the unknown partner will be <code>\"__OTHER__\"</code>, and the treenode ID will be None.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> YIELDS DESCRIPTION <code>tuple[int, str, str, int, int]</code> <p>Connector ID, source name, target name, source treenode, target treenode.</p> Source code in <code>navis/connectivity/adjacency.py</code> <pre><code>def edges(self, include_other=True) -&gt; Iterable[Edge]:\n    \"\"\"Iterate through all synapse edges.\n\n    Parameters\n    ----------\n    include_other : bool, optional\n        Include edges for which only one partner is known, by default True.\n        If included, the name of the unknown partner will be `\"__OTHER__\"`,\n        and the treenode ID will be None.\n\n    Yields\n    ------\n    tuple[int, str, str, int, int]\n        Connector ID, source name, target name, source treenode, target treenode.\n    \"\"\"\n    for conn_id in set(self.conn_inputs).union(self.conn_outputs):\n        src, src_node = self.conn_inputs.get(conn_id, (OTHER, None))\n        if src_node is None and not include_other:\n            continue\n        for tgt, tgt_node in self.conn_outputs.get(conn_id, [(OTHER, None)]):\n            if tgt_node is None and not include_other:\n                continue\n            yield Edge(conn_id, src, tgt, src_node, tgt_node)\n</code></pre>"},{"location":"reference/navis/#navis.NeuronConnector.to_adjacency","title":"<code>to_adjacency</code>","text":"<p>Create an adjacency matrix of neuron connectivity.</p> PARAMETER DESCRIPTION <code>include_other</code> <p>Whether to include a node called <code>\"__OTHER__\"</code>, which represents all unknown partners. By default True. This can be helpful when calculating a neuron's input fraction, but cannot be used for output fractions if synapses are polyadic.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>pandas.DataFrame</code> <p>Row index is source neuron name, column index is target neuron name, cells are the number of synapses from source to target.</p> Source code in <code>navis/connectivity/adjacency.py</code> <pre><code>def to_adjacency(self, include_other=True) -&gt; pd.DataFrame:\n    \"\"\"Create an adjacency matrix of neuron connectivity.\n\n    Parameters\n    ----------\n    include_other : bool, optional\n        Whether to include a node called `\"__OTHER__\"`,\n        which represents all unknown partners.\n        By default True.\n        This can be helpful when calculating a neuron's input fraction,\n        but cannot be used for output fractions if synapses are polyadic.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Row index is source neuron name,\n        column index is target neuron name,\n        cells are the number of synapses from source to target.\n    \"\"\"\n    index = list(self.neurons)\n    if include_other:\n        index.append(OTHER)\n    data = np.zeros((len(index), len(index)), np.uint64)\n    df = pd.DataFrame(data, index, index)\n    for _, src, tgt, _, _ in self.edges(include_other):\n        df.loc[src, tgt] += 1\n\n    return df\n</code></pre>"},{"location":"reference/navis/#navis.NeuronConnector.to_digraph","title":"<code>to_digraph</code>","text":"<p>Create a graph of neuron connectivity.</p> PARAMETER DESCRIPTION <code>include_other</code> <p>Whether to include a node called <code>\"__OTHER__\"</code>, which represents all unknown partners. By default True. This can be helpful when calculating a neuron's input fraction, but cannot be used for output fractions if synapses are polyadic.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>nx.DiGraph</code> <p>The graph has data <code>{\"connector_xyz\": {connector_id: (x, y, z), ...}}</code>. The nodes have data <code>{\"neuron\": tree_neuron}</code>. The edges have data <code>{\"connectors\": data_frame, \"weight\": n_connectors}</code>, where the connectors data frame has columns \"connector_id\", \"pre_node\", \"post_node\".</p> Source code in <code>navis/connectivity/adjacency.py</code> <pre><code>def to_digraph(self, include_other=True) -&gt; nx.DiGraph:\n    \"\"\"Create a graph of neuron connectivity.\n\n    Parameters\n    ----------\n    include_other : bool, optional\n        Whether to include a node called `\"__OTHER__\"`,\n        which represents all unknown partners.\n        By default True.\n        This can be helpful when calculating a neuron's input fraction,\n        but cannot be used for output fractions if synapses are polyadic.\n\n    Returns\n    -------\n    nx.DiGraph\n        The graph has data `{\"connector_xyz\": {connector_id: (x, y, z), ...}}`.\n        The nodes have data `{\"neuron\": tree_neuron}`.\n        The edges have data `{\"connectors\": data_frame, \"weight\": n_connectors}`,\n        where the connectors data frame has columns\n        \"connector_id\", \"pre_node\", \"post_node\".\n    \"\"\"\n    g = nx.DiGraph()\n    g.add_nodes_from((k, {\"neuron\": v}) for k, v in self.neurons.items())\n    if include_other:\n        g.add_node(OTHER, neuron=None)\n\n    g.graph[\"connector_xyz\"] = self.connector_xyz\n    headers = {\n        \"connector_id\": pd.UInt64Dtype(),\n        \"pre_node\": pd.UInt64Dtype(),\n        \"post_node\": pd.UInt64Dtype(),\n    }\n    edges = dict()\n    for conn_id, src, tgt, src_node, tgt_node in self.edges(include_other):\n        edges.setdefault((src, tgt), []).append([conn_id, src_node, tgt_node])\n\n    for (src, tgt), rows in edges.items():\n        df_tmp = pd.DataFrame(rows, columns=list(headers), dtype=object)\n        df = df_tmp.astype(headers, copy=False)\n        g.add_edge(src, tgt, connectors=df, weight=len(df))\n\n    return g\n</code></pre>"},{"location":"reference/navis/#navis.NeuronConnector.to_multidigraph","title":"<code>to_multidigraph</code>","text":"<p>Create a graph of neuron connectivity where each synapse is an edge.</p> PARAMETER DESCRIPTION <code>include_other</code> <p>Whether to include a node called <code>\"__OTHER__\"</code>, which represents all unknown partners. By default True. This can be helpful when calculating a neuron's input fraction, but cannot be used for output fractions if synapses are polyadic.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>nx.MultiDiGraph</code> <p>The nodes have data <code>{\"neuron\": tree_neuron}</code>. The edges have data <code>{\"pre_node\": presyn_treenode_id, \"post_node\": postsyn_treenode_id, \"xyz\": connector_location, \"connector_id\": conn_id}</code>.</p> Source code in <code>navis/connectivity/adjacency.py</code> <pre><code>def to_multidigraph(self, include_other=True) -&gt; nx.MultiDiGraph:\n    \"\"\"Create a graph of neuron connectivity where each synapse is an edge.\n\n    Parameters\n    ----------\n    include_other : bool, optional\n        Whether to include a node called `\"__OTHER__\"`,\n        which represents all unknown partners.\n        By default True.\n        This can be helpful when calculating a neuron's input fraction,\n        but cannot be used for output fractions if synapses are polyadic.\n\n    Returns\n    -------\n    nx.MultiDiGraph\n        The nodes have data `{\"neuron\": tree_neuron}`.\n        The edges have data\n        `{\"pre_node\": presyn_treenode_id, \"post_node\": postsyn_treenode_id, \"xyz\": connector_location, \"connector_id\": conn_id}`.\n    \"\"\"\n    g = nx.MultiDiGraph()\n    g.add_nodes_from((k, {\"neuron\": v}) for k, v in self.neurons.items())\n    if include_other:\n        g.add_node(OTHER, neuron=None)\n\n    for conn_id, src, tgt, src_node, tgt_node in self.edges(include_other):\n        g.add_edge(\n            src,\n            tgt,\n            pre_node=src_node,\n            post_node=tgt_node,\n            xyz=self.connector_xyz[conn_id],\n            connector_id=conn_id,\n        )\n\n    return g\n</code></pre>"},{"location":"reference/navis/#navis.NeuronList","title":"<code>navis.NeuronList</code>","text":"<p>Collection of neurons.</p> <p>Gives quick access to neurons' attributes and functions.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            Data to construct neuronlist from. Can be either:\n\n            1. Tree/MeshNeuron(s) or Dotprops\n            2. NeuronList(s)\n            3. Anything that constructs a Tree/MeshNeuron\n            4. List of the above\n</code></pre> <p> TYPE: <code>                list | array | TreeNeuron | MeshNeuron | Dotprops | NeuronList</code> </p> <code>make_copy</code> <pre><code>            If True, Neurons are deepcopied before being\n            assigned to the NeuronList.\n</code></pre> <p> TYPE: <code>        bool</code> DEFAULT: <code>False</code> </p> <code>make_using</code> <pre><code>            Function or class used to construct neurons from\n            elements in `x` if they aren't already neurons.\n            By default, will use `navis.Neuron` to try to infer\n            what kind of neuron can be constructed.\n</code></pre> <p> TYPE: <code>       function | class</code> DEFAULT: <code>None</code> </p> <code>parallel</code> <pre><code>            If True, will use parallel threads when initialising the\n            NeuronList. Should be slightly up to a lot faster\n            depending on the numbers of cores and the input data.\n</code></pre> <p> TYPE: <code>         bool</code> DEFAULT: <code>False</code> </p> <code>n_cores</code> <pre><code>            Number of cores to use for when `parallel=True`.\n            Defaults to half the available cores.\n</code></pre> <p> TYPE: <code>          int</code> DEFAULT: <code>os.cpu_count() // 2</code> </p> <code>**kwargs</code> <pre><code>            Will be passed to constructor of Tree/MeshNeuron (see\n            `make_using`).\n</code></pre> <p> DEFAULT: <code>{}</code> </p> Source code in <code>navis/core/neuronlist.py</code> <pre><code>class NeuronList:\n    \"\"\"Collection of neurons.\n\n    Gives quick access to neurons' attributes and functions.\n\n    Parameters\n    ----------\n    x :                 list | array | TreeNeuron | MeshNeuron | Dotprops | NeuronList\n                        Data to construct neuronlist from. Can be either:\n\n                        1. Tree/MeshNeuron(s) or Dotprops\n                        2. NeuronList(s)\n                        3. Anything that constructs a Tree/MeshNeuron\n                        4. List of the above\n\n    make_copy :         bool, optional\n                        If True, Neurons are deepcopied before being\n                        assigned to the NeuronList.\n    make_using :        function | class, optional\n                        Function or class used to construct neurons from\n                        elements in `x` if they aren't already neurons.\n                        By default, will use `navis.Neuron` to try to infer\n                        what kind of neuron can be constructed.\n    parallel :          bool\n                        If True, will use parallel threads when initialising the\n                        NeuronList. Should be slightly up to a lot faster\n                        depending on the numbers of cores and the input data.\n    n_cores :           int\n                        Number of cores to use for when `parallel=True`.\n                        Defaults to half the available cores.\n    **kwargs\n                        Will be passed to constructor of Tree/MeshNeuron (see\n                        `make_using`).\n\n    \"\"\"\n\n    neurons: List['core.NeuronObject']\n\n    cable_length: Sequence[float]\n\n    soma: Sequence[int]\n    root: Sequence[int]\n\n    graph: 'nx.DiGraph'\n    igraph: 'igraph.Graph'  # type: ignore  # doesn't know iGraph\n\n    def __init__(self,\n                 x: Union[Iterable[Union[core.BaseNeuron,\n                                         'NeuronList',\n                                         pd.DataFrame]],\n                          'NeuronList',\n                          core.BaseNeuron,\n                          pd.DataFrame],\n                 make_copy: bool = False,\n                 make_using: Optional[type] = None,\n                 parallel: bool = False,\n                 n_cores: int = os.cpu_count() // 2,\n                 **kwargs):\n        # If below parameter is True, most calculations will be parallelized\n        # which speeds them up quite a bit. Unfortunately, this uses A TON of\n        # memory - for large lists this might make your system run out of\n        # memory. In these cases, leave this property at False\n        self.parallel = parallel\n        self.n_cores = n_cores\n\n        # Determines if subsetting this NeuronList will copy the neurons\n        self.copy_on_subset: bool = False\n\n        if isinstance(x, NeuronList):\n            # We can't simply say self.neurons = x.neurons b/c that way\n            # changes in the list would backpropagate\n            self.neurons = [n for n in x.neurons]\n        elif utils.is_iterable(x):\n            # If x is a list of mixed objects we need to unpack/flatten that\n            # E.g. x = [NeuronList, NeuronList, core.TreeNeuron]\n            # We need to make sure the order is retained though (important for\n            # e.g. plotting)\n            self.neurons = []\n            for n in x:\n                # Unpack neuronlists\n                if isinstance(n, NeuronList):\n                    self.neurons += n.neurons\n                # Everything else is just appended - will throw error later\n                else:\n                    self.neurons.append(n)\n        elif isinstance(x, type(None)):\n            # Empty Neuronlist\n            self.neurons = []\n        else:\n            # Any other datatype will simply be assumed to be accepted by\n            # core.Neuron() - if not this will throw an error\n            self.neurons = [x]  # type: ignore\n\n        # Now convert and/or make copies if necessary\n        to_convert = []\n        for i, n in enumerate(self.neurons):\n            if not isinstance(n, core.BaseNeuron) or make_copy is True:\n                # The `i` keeps track of the original index so that after\n                # conversion to Neurons, the objects will occupy the same\n                # position\n                to_convert.append((n, i))\n\n        if to_convert:\n            if not make_using:\n                make_using = core.Neuron\n            elif not isinstance(make_using, type) and not callable(make_using):\n                make_using = make_using.__class__\n\n            if self.parallel:\n                with ThreadPoolExecutor(max_workers=self.n_cores) as e:\n                    futures = e.map(lambda x: make_using(x, **kwargs),\n                                    [n[0] for n in to_convert])\n\n                    converted = [n for n in config.tqdm(futures,\n                                                        total=len(to_convert),\n                                                        desc='Make nrn',\n                                                        disable=config.pbar_hide,\n                                                        leave=config.pbar_leave)]\n\n                    for i, c in enumerate(to_convert):\n                        self.neurons[c[1]] = converted[i]\n\n            else:\n                for n in config.tqdm(to_convert, desc='Make nrn',\n                                     disable=config.pbar_hide or len(to_convert) == 1,\n                                     leave=config.pbar_leave):\n                    self.neurons[n[1]] = make_using(n[0], **kwargs)\n\n        # Add ID-based indexer\n        self.idx = _IdIndexer(self)\n\n    @property\n    def neurons(self):\n        \"\"\"Neurons contained in this NeuronList.\"\"\"\n        return self.__dict__.get('neurons', [])\n\n    @property\n    def is_mixed(self):\n        \"\"\"Return True if contains more than one type of neuron.\"\"\"\n        return len(self.types) &gt; 1\n\n    @property\n    def is_degenerated(self):\n        \"\"\"Return True if contains neurons with non-unique IDs.\"\"\"\n        return len(set(self.id)) &lt; len(self.neurons)\n\n    @property\n    def types(self):\n        \"\"\"Return neuron types present in this list.\"\"\"\n        return tuple(set([type(n) for n in self.neurons]))\n\n    @property\n    def shape(self):\n        \"\"\"Shape of NeuronList (N, ).\"\"\"\n        return (self.__len__(),)\n\n    @property\n    def bbox(self):\n        \"\"\"Bounding box across all neurons in the list.\"\"\"\n        if self.empty:\n            raise ValueError('No bounding box - NeuronList is empty.')\n\n        bboxes = np.hstack([n.bbox for n in self.neurons])\n        mn = np.min(bboxes, axis=1)\n        mx = np.max(bboxes, axis=1)\n        return np.vstack((mn, mx)).T\n\n    @property\n    def empty(self):\n        \"\"\"Return True if NeuronList is empty.\"\"\"\n        return len(self.neurons) == 0\n\n    def __reprframe__(self):\n        \"\"\"Return truncated DataFrame for self representation.\"\"\"\n        if self.empty:\n            return pd.DataFrame([])\n        elif len(self) &lt; 5:\n            return self.summary()\n        else:\n            nl = self[:3] + self[-3:]\n            s = nl.summary()\n            # Fix index\n            s.index = np.append(s.index[:3], np.arange(len(self)-3, len(self)))\n            return s\n\n    def __reprheader__(self, html=False):\n        \"\"\"Generate header for representation.\"\"\"\n        if len(self) &lt;= 2000:\n            size = utils.sizeof_fmt(self.memory_usage(deep=False, estimate=True))\n            head = f'{type(self)} containing {len(self)} neurons ({size})'\n        else:\n            # For larger lists, extrapolate from sampling 10% of the list\n            size = utils.sizeof_fmt(self.memory_usage(deep=False,\n                                                      sample=True,\n                                                      estimate=True))\n            head = f'{type(self)} containing {len(self)} neurons (est. {size})'\n\n        if html:\n            head = head.replace('&lt;', '&amp;lt;').replace('&gt;', '&amp;gt;')\n\n        return head\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __repr__(self):\n        string = self.__reprheader__(html=False)\n        if not self.empty:\n            with pd.option_context(\"display.max_rows\", 4,\n                                   \"display.show_dimensions\", False):\n                string += f'\\n{str(self.__reprframe__())}'\n\n        return string\n\n    def _repr_html_(self):\n        string = self.__reprheader__(html=True)\n        if not self.empty:\n            with pd.option_context(\"display.max_rows\", 4,\n                                   \"display.show_dimensions\", False):\n                string += self.__reprframe__()._repr_html_()\n        return string\n\n    def __iter__(self) -&gt; Iterator['core.NeuronObject']:\n        \"\"\"Iterator instanciates a new class every time it is called.\n        This allows the use of nested loops on the same NeuronList object.\n        \"\"\"\n        class prange_iter(Iterator['core.NeuronObject']):\n            def __init__(self, neurons, start):\n                self.iter = start\n                self.neurons = neurons\n\n            def __next__(self) -&gt; 'core.NeuronObject':\n                if self.iter &gt;= len(self.neurons):\n                    raise StopIteration\n                to_return = self.neurons[self.iter]\n                self.iter += 1\n                return to_return\n\n        return prange_iter(self.neurons, 0)\n\n    def __len__(self):\n        \"\"\"Number of neurons in this list.\"\"\"\n        return len(self.neurons)\n\n    def __dir__(self):\n        \"\"\"Custom __dir__ to add some parameters that we want to make searchable.\"\"\"\n        add_attr = set.union(*[set(dir(n)) for n in self.neurons[:MAX_SEARCH]])\n\n        return list(set(super().__dir__() + list(add_attr)))\n\n    def __getattr__(self, key):\n        if self.empty:\n            raise AttributeError(f'Neuronlist is empty - \"{key}\" not found')\n        # Dynamically check if the requested attribute/function exists in\n        # all neurons\n        values = [getattr(n, key, NotImplemented) for n in self.neurons]\n        is_method = [isinstance(v, types.MethodType) for v in values]\n        # is_none = [isinstance(v, type(None)) for v in values]\n        is_frame = [isinstance(v, pd.DataFrame) for v in values]\n        is_quantity = [isinstance(v, config.ureg.Quantity) for v in values]\n\n        # First check if there is any reason why we can't collect this\n        # attribute across all neurons\n        if all([isinstance(v, type(NotImplemented)) for v in values]):\n            raise AttributeError(f'Attribute \"{key}\" not found in '\n                                 'NeuronList or its neurons')\n        elif any([isinstance(v, type(NotImplemented)) for v in values]):\n            raise AttributeError(f'Attribute or function \"{key}\" missing '\n                                 'for some neurons')\n        elif len(set(is_method)) &gt; 1:\n            raise TypeError('Found both methods and attributes with name '\n                            f'\"{key}\" among neurons.')\n        # Concatenate if dealing with DataFrame\n        elif not all(is_method):\n            if any(is_frame):\n                df = pd.concat([v for v in values if isinstance(v, pd.DataFrame)],\n                               axis=0,\n                               ignore_index=True,\n                               join='outer',\n                               sort=True)\n\n                # For each row label which neuron (id) it belongs to\n                df['neuron'] = None\n                ix = 0\n                for k, v in enumerate(values):\n                    if isinstance(v, pd.DataFrame):\n                        df.iloc[ix:ix + v.shape[0],\n                                df.columns.get_loc('neuron')] = self.neurons[k].id\n                        ix += v.shape[0]\n                return df\n            elif all(is_quantity):\n                # See if units are all compatible\n                is_compatible = [values[0].is_compatible_with(v) for v in values]\n                if all(is_compatible):\n                    # Convert all to the same units\n                    conv = [v.to(values[0]).magnitude for v in values]\n                    # Return pint array\n                    return config.ureg.Quantity(np.array(conv), values[0].units)\n                else:\n                    logger.warning(f'\"{key}\" contains incompatible units. '\n                                   'Returning unitless values.')\n                    return np.array([v.magnitude for v in values])\n            elif any(is_quantity):\n                logger.warning(f'\"{key}\" contains data with and without '\n                               'units. Removing units.')\n                return np.array([getattr(v, 'magnitude', v) for v in values])\n            else:\n                # If the result would be a ragged array specify dtype as object\n                # This avoids a depcrecation warning and future issues\n                dtype = None\n                if any([utils.is_iterable(v) for v in values]):\n                    if not all([utils.is_iterable(v) for v in values]):\n                        dtype = object\n                    elif len(set([len(v) for v in values])) &gt; 1:\n                        dtype = object\n                return np.array(values, dtype=dtype)\n        # If everything is a method\n        else:\n            # To avoid confusion we will not allow calling of magic methods\n            # via the NeuronProcessor as those are generally expected to\n            # be methods of the NeuronList itself\n            if key.startswith('__') and key.endswith('__'):\n                raise AttributeError(f\"'NeuronList' object has no attribute '{key}'\")\n\n            # Delayed import to avoid circular import\n            from .core_utils import NeuronProcessor\n\n            # Return function but wrap it in a function that will show\n            # a progress bar. Note that we do not use parallel processing by\n            # default to avoid errors with `inplace=True`\n            return NeuronProcessor(self,\n                                   values,\n                                   parallel=False,\n                                   desc=key)\n\n    def __setattr__(self, key, value):\n        # We have cater for the situation when we want to replace the whole\n        # dictionary - e.g. when unpickling (see __setstate__)\n        # Below code for setting the dictionary looks complicated and\n        # unnecessary but is really complicated and VERY necessary\n        if key == '__dict__':\n            if not isinstance(value, dict):\n                raise TypeError(f'__dict__ must be dict, got {type(value)}')\n            self.__dict__.clear()\n            for k, v in value.items():\n                self.__dict__[k] = v\n            return\n\n        # Check if this attribute exists in the neurons\n        if any([hasattr(n, key) for n in self.neurons]):\n            logger.warning('It looks like you are trying to add a neuron '\n                           'attribute to a NeuronList. Setting the attribute '\n                           f'\"{key}\" on the NeuronList will not propagated to '\n                           'the neurons it contains! To set neuron attributes '\n                           'use the `NeuronList.set_neuron_attributes()` method.')\n\n        self.__dict__[key] = value\n\n    def __getstate__(self):\n        \"\"\"Get state (used e.g. for pickling).\"\"\"\n        # We have to implement this to make sure that we don't accidentally\n        # call __getstate__ of each neuron via the NeuronProcessor\n        state = {k: v for k, v in self.__dict__.items() if not callable(v)}\n        return state\n\n    def __setstate__(self, d):\n        \"\"\"Set state (used e.g. for unpickling).\"\"\"\n        # We have to implement this to make sure that we don't accidentally\n        # call __setstate__ of each neuron via the NeuronProcessor\n        self.__dict__ = d\n\n    def __contains__(self, x):\n        return x in self.neurons\n\n    def __copy__(self):\n        return self.copy(deepcopy=False)\n\n    def __deepcopy__(self):\n        return self.copy(deepcopy=True)\n\n    def __getitem__(self, key):\n        if utils.is_iterable(key):\n            # This avoids issues when the Series' index is not [0, 1, 2, etc]\n            if isinstance(key, pd.Series):\n                key = key.values\n\n            if all([isinstance(k, (bool, np.bool_)) for k in key]):\n                if len(key) != len(self.neurons):\n                    raise IndexError('boolean index did not match indexed '\n                                     f'NeuronList; dimension is {len(self.neurons)} '\n                                     'but corresponding boolean dimension is '\n                                     f'{len(key)}')\n                subset = [n for i, n in enumerate(self.neurons) if key[i]]\n            else:\n                subset = [self[i] for i in key]\n        elif isinstance(key, str):\n            subset = [n for n in self.neurons if re.fullmatch(key, getattr(n, 'name', ''))]\n\n            # For indexing by name, we expect a match\n            if not subset:\n                raise AttributeError('NeuronList does not contain neuron(s) '\n                                     f'with name: \"{key}\"')\n\n        elif isinstance(key, (int, np.integer, slice)):\n            subset = self.neurons[key]\n        else:\n            raise NotImplementedError(f'Indexing NeuronList by {type(key)} not implemented')\n\n        if isinstance(subset, core.BaseNeuron):\n            return subset\n\n        # Make sure we unpack neurons\n        subset = utils.unpack_neurons(subset)\n\n        return self.__class__(subset, make_copy=self.copy_on_subset)\n\n    def __setitem__(self, key, value):\n        if isinstance(key, str):\n            if not utils.is_iterable(value):\n                for n in self.neurons:\n                    setattr(n, key, value)\n            elif len(value) == len(self.neurons):\n                for n, v in zip(self.neurons, value):\n                    setattr(n, key, v)\n            else:\n                raise ValueError('Length of values does not match number of '\n                                 'neurons in NeuronList.')\n        else:\n            msg = ('Itemsetter can only be used to set attributes of the '\n                   'neurons contained in the NeuronList. For example:\\n'\n                   '  &gt;&gt;&gt; nl = navis.example_neurons(3)\\n'\n                   '  &gt;&gt;&gt; nl[\"propertyA\"] = 1\\n'\n                   '  &gt;&gt;&gt; nl[0].propertyA\\n'\n                   '  1\\n'\n                   '  &gt;&gt;&gt; nl[\"propertyB\"] = [\"a\", \"b\", \"c\"]\\n'\n                   '  &gt;&gt;&gt; nl[2].propertyB\\n'\n                   '  \"c\"')\n            raise NotImplementedError(msg)\n\n    def __missing__(self, key):\n        raise AttributeError('No neuron matching the search criteria.')\n\n    def __add__(self, to_add):\n        \"\"\"Implement addition.\"\"\"\n        if isinstance(to_add, core.BaseNeuron):\n            return self.__class__(self.neurons + [to_add],\n                                  make_copy=self.copy_on_subset)\n        elif isinstance(to_add, NeuronList):\n            return self.__class__(self.neurons + to_add.neurons,\n                                  make_copy=self.copy_on_subset)\n        elif utils.is_iterable(to_add):\n            if False not in [isinstance(n, core.BaseNeuron) for n in to_add]:\n                return self.__class__(self.neurons + list(to_add),\n                                      make_copy=self.copy_on_subset)\n            else:\n                return self.__class__(self.neurons + [core.BaseNeuron[n] for n in to_add],\n                                      make_copy=self.copy_on_subset)\n        else:\n            return NotImplemented\n\n    def __eq__(self, other):\n        \"\"\"Implement equality.\"\"\"\n        if isinstance(other, NeuronList):\n            if len(self) != len(other):\n                return False\n            else:\n                return all([n1 == n2 for n1, n2 in zip(self, other)])\n        else:\n            return NotImplemented\n\n    def __sub__(self, to_sub):\n        \"\"\"Implement substraction.\"\"\"\n        if isinstance(to_sub, core.BaseNeuron):\n            return self.__class__([n for n in self.neurons if n != to_sub],\n                                  make_copy=self.copy_on_subset)\n        elif isinstance(to_sub, NeuronList):\n            return self.__class__([n for n in self.neurons if n not in to_sub],\n                                  make_copy=self.copy_on_subset)\n        else:\n            return NotImplemented\n\n    def __truediv__(self, other):\n        \"\"\"Implements division for coordinates (nodes, connectors).\"\"\"\n        return self.__class__([n / other for n in config.tqdm(self.neurons,\n                                                              desc='Dividing',\n                                                              disable=config.pbar_hide,\n                                                              leave=False)])\n\n\n    def __mul__(self, other):\n        \"\"\"Implement multiplication for coordinates (nodes, connectors).\"\"\"\n        return self.__class__([n * other for n in config.tqdm(self.neurons,\n                                                              desc='Multiplying',\n                                                              disable=config.pbar_hide,\n                                                              leave=False)])\n\n    def __and__(self, other):\n        \"\"\"Implement bitwise AND using the &amp; operator.\"\"\"\n        if isinstance(other, core.BaseNeuron):\n            return self.__class__([n for n in self.neurons if n == other],\n                                  make_copy=self.copy_on_subset)\n        elif isinstance(other, NeuronList):\n            return self.__class__([n for n in self.neurons if n in other],\n                                  make_copy=self.copy_on_subset)\n        else:\n            return NotImplemented\n\n    def __or__(self, other):\n        \"\"\"Implement bitwise OR using the | operator.\"\"\"\n        if isinstance(other, core.BaseNeuron):\n            neurons = self.neurons\n            if not any(n == other for n in neurons):\n                neurons.append(other)\n            return self.__class__(neurons, make_copy=self.copy_on_subset)\n        elif isinstance(other, NeuronList):\n            neurons = self.neurons + [n for n in other.neurons if n not in self]\n            return self.__class__(neurons, make_copy=self.copy_on_subset)\n        else:\n            return NotImplemented\n\n    def append(self, v):\n        \"\"\"Add neuron(s) to this list.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import navis\n        &gt;&gt;&gt; # This is mostly for doctests\n        &gt;&gt;&gt; nl = navis.example_neurons()\n        &gt;&gt;&gt; len(nl)\n        5\n        &gt;&gt;&gt; # Add a single neuron to the list\n        &gt;&gt;&gt; nl.append(nl[0])\n        &gt;&gt;&gt; len(nl)\n        6\n        &gt;&gt;&gt; # Add a list of neurons to the list\n        &gt;&gt;&gt; nl.append(nl)\n        &gt;&gt;&gt; len(nl)\n        12\n\n        \"\"\"\n        if isinstance(v, core.BaseNeuron):\n            self.neurons.append(v)\n        elif isinstance(v, NeuronList):\n            self.neurons += v.neurons\n        else:\n            raise NotImplementedError('Unable to append data of type'\n                                      f'{type(v)} to NeuronList')\n\n    def apply(self,\n              func: Callable,\n              *,\n              parallel: bool = False,\n              n_cores: int = os.cpu_count() // 2,\n              omit_failures: bool = False,\n              **kwargs):\n        \"\"\"Apply function across all neurons in this NeuronList.\n\n        Parameters\n        ----------\n        func :          callable\n                        Function to be applied. Must accept\n                        [`navis.BaseNeuron`][] as first argument.\n        parallel :      bool\n                        If True (default) will use multiprocessing. Spawning the\n                        processes takes time (and memory). Using `parallel=True`\n                        makes only sense if the NeuronList is large or the\n                        function takes a long time to run.\n        n_cores :       int\n                        Number of CPUs to use for multiprocessing. Defaults to\n                        half the available cores.\n        omit_failures : bool\n                        If True, will ignore failures.\n\n        **kwargs\n                    Will be passed to function.\n\n        Returns\n        -------\n        Results\n\n        Examples\n        --------\n        &gt;&gt;&gt; import navis\n        &gt;&gt;&gt; nl = navis.example_neurons()\n        &gt;&gt;&gt; # Apply resampling function\n        &gt;&gt;&gt; nl_rs = nl.apply(navis.resample_skeleton, resample_to=1000, inplace=False)\n\n        \"\"\"\n        if not callable(func):\n            raise TypeError('\"func\" must be callable')\n\n        # Delayed import to avoid circular import\n        from .core_utils import NeuronProcessor\n        proc = NeuronProcessor(self,\n                               func,\n                               parallel=parallel,\n                               n_cores=n_cores,\n                               omit_failures=omit_failures,\n                               desc=f'Apply {func.__name__}')\n\n        return proc(self.neurons, **kwargs)\n\n    def sum(self) -&gt; pd.DataFrame:\n        \"\"\"Return sum numeric and boolean values over all neurons.\"\"\"\n        return self.summary().sum(numeric_only=True)\n\n    def mean(self) -&gt; pd.DataFrame:\n        \"\"\"Return mean numeric and boolean values over all neurons.\"\"\"\n        return self.summary().mean(numeric_only=True)\n\n    def memory_usage(self, deep=False, estimate=False, sample=False):\n        \"\"\"Return estimated size in memory of this NeuronList.\n\n        Works by going over each neuron and summing up their size in memory.\n\n        Parameters\n        ----------\n        deep :          bool\n                        Pass to pandas DataFrames. If True will inspect data of\n                        object type too.\n        estimate :      bool\n                        If True, we will only estimate the size. This is\n                        considerably faster but will slightly underestimate the\n                        memory usage.\n        sample :        bool\n                        If True, we will only sample 10% of the neurons\n                        contained in the list and extrapolate an estimate from\n                        there.\n\n        Returns\n        -------\n        int\n                    Memory usage in bytes.\n\n        \"\"\"\n        if self.empty:\n            return 0\n\n        if not sample:\n            try:\n                return sum([n.memory_usage(deep=deep,\n                                           estimate=estimate) for n in self.neurons])\n            except BaseException:\n                return 0\n        else:\n            try:\n                s = sum([n.memory_usage(deep=deep,\n                                        estimate=estimate) for n in self.neurons[::10]])\n                return s * (len(self.neurons) / len(self.neurons[::10]))\n            except BaseException:\n                return 0\n\n    def sample(self, N: Union[int, float] = 1) -&gt; 'NeuronList':\n        \"\"\"Return random subset of neurons.\"\"\"\n        if N &lt; 1 and N &gt; 0:\n            N = int(len(self.neurons) * N)\n\n        indices = list(range(len(self.neurons)))\n        random.shuffle(indices)\n        return self.__class__([n for i, n in enumerate(self.neurons) if i in indices[:N]],\n                              make_copy=self.copy_on_subset)\n\n    def plot3d(self, **kwargs):\n        \"\"\"Plot neuron in 3D using [`navis.plot3d`][].\n\n        Parameters\n        ----------\n        **kwargs\n                Keyword arguments will be passed to [`navis.plot3d`][].\n                See `help(navis.plot3d)` for a list of keywords.\n\n        See Also\n        --------\n        [`navis.plot3d`][]\n                Base function called to generate 3d plot.\n\n        \"\"\"\n        from ..plotting import plot3d\n\n        return plot3d(self, **kwargs)\n\n    def plot2d(self, **kwargs):\n        \"\"\"Plot neuron in 2D using [`navis.plot2d`][].\n\n        Parameters\n        ----------\n        **kwargs\n                Keyword arguments will be passed to [`navis.plot2d`][].\n                See `help(navis.plot2d)` for a list of accepted keywords.\n\n        See Also\n        --------\n        [`navis.plot2d`][]\n                Base function called to generate 2d plot.\n\n        \"\"\"\n        from ..plotting import plot2d\n\n        return plot2d(self, **kwargs)\n\n    def summary(self,\n                N: Optional[Union[int, slice]] = None,\n                add_props: list = [],\n                progress=False\n                ) -&gt; pd.DataFrame:\n        \"\"\"Get summary over all neurons in this NeuronList.\n\n        Parameters\n        ----------\n        N :         int | slice, optional\n                    If int, get only first N entries.\n        add_props : list, optional\n                    Additional properties to add to summary. If attribute not\n                    available will return 'NA'.\n        progress :  bool\n                    Whether to show a progress bar. Can be useful for very\n                    large list.\n\n        Returns\n        -------\n        pandas DataFrame\n\n        \"\"\"\n        if not self.empty:\n            # Fetch a union of all summary props (keep order)\n            all_props = [p for l in self.SUMMARY_PROPS for p in l]\n            props = np.unique(all_props)\n            props = sorted(props, key=lambda x: all_props.index(x))\n        else:\n            props = []\n\n        # Add ID to properties - unless all are generic UUIDs\n        if any([not isinstance(n.id, uuid.UUID) for n in self.neurons]):\n            # Make sure we don't have two IDs\n            if 'id' in props:\n                props.remove('id')\n            props = np.insert(props, 2, 'id')\n\n        if add_props:\n            props = np.append(props, add_props)\n\n        if not isinstance(N, slice):\n            N = slice(N)\n\n        return pd.DataFrame(data=[[getattr(n, a, 'NA') for a in props]\n                                  for n in config.tqdm(self.neurons[N],\n                                                       desc='Summarizing',\n                                                       leave=False,\n                                                       disable=not progress)],\n                            columns=props)\n\n    def itertuples(self):\n        \"\"\"Helper to mimic `pandas.DataFrame.itertuples()`.\"\"\"\n        return self.neurons\n\n    def add_metadata(self, meta, id_col='id', neuron_id='id', columns=None, register=False, missing='raise'):\n        \"\"\"Add neuron metadata from a DataFrame.\n\n        Parameters\n        ----------\n        meta :      pd.DataFrame | str | Path\n                    DataFrame or filepath to a CSV file containing metadata.\n                    Must contain a column with neuron IDs.\n        id_col :    str\n                    Name of the column containing neuron IDs.\n        neuron_id : str\n                    Name of the attribute in the neuron that corresponds to\n                    the `id_col`.\n        columns :   list, optional\n                    List of columns to add. If None, will add all columns except\n                    for `id_col`.\n        register :  bool\n                    If True, will also register the attribute(s) as properties\n                    that should show up in the summary.\n        missing :   'raise' | 'warn' | 'ignore'\n                    What to do if `meta` is missing a value for a neuron.\n\n        See Also\n        --------\n        navis.NeuronList.set_neuron_attributes\n                    Set individual attributes of neurons contained in the NeuronList.\n\n        \"\"\"\n        assert missing in ('raise', 'warn', 'ignore')\n\n        if isinstance(meta, (str, Path)):\n            meta = pd.read_csv(meta)\n\n        if not isinstance(meta, pd.DataFrame):\n            raise TypeError('`meta` must be a DataFrame or a path to a CSV file, '\n                            f'got {type(meta)}')\n\n        if id_col not in meta.columns:\n            raise KeyError(f'Column \"{id_col}\" not found in metadata.')\n\n        # Index meta data by the neuron_id\n        neuron_id = getattr(self, neuron_id)\n        miss = ~np.isin(neuron_id, meta[id_col].values)\n        if any(miss):\n            msg = f'Metadata is missing entries for IDs: {neuron_id[miss]}'\n            if missing == 'raise':\n                raise KeyError(msg)\n            elif missing == 'warn':\n                logger.warning(msg)\n\n        meta = meta.set_index(id_col).reindex(neuron_id)\n\n        if columns is None:\n            columns = meta.columns\n\n        for c in columns:\n            if c == id_col:\n                continue\n            self.set_neuron_attributes(\n                meta[c].values.tolist(),\n                name=c,\n                register=register\n                )\n\n    def get_neuron_attributes(self, *args, **kwargs):\n        \"\"\"Get attributes of neurons contained in the NeuronList.\n\n        Parameters\n        ----------\n        name :      str\n                    Name of the property to get.\n        default :   any, optional\n                    Default value to return if attribute is not found.\n\n        Returns\n        -------\n        np.ndarray\n                    Array of values for the requested attribute.\n\n        \"\"\"\n        return np.array([getattr(n, *args, **kwargs) for n in self.neurons])\n\n    def set_neuron_attributes(self, x, name, register=False, na='raise'):\n        \"\"\"Set attributes of neurons contained in the NeuronList.\n\n        Parameters\n        ----------\n        x :         any | list | np.ndarray | dict | function\n                    Value of the property:\n                      - lists and arrays are expected to contain a value for\n                        each neuron and hence have to match the length of the\n                        NeuronList\n                      - dict is expected to map `{neuron.id: value}`\n                      - a function is expected to take `neuron.id` as input\n                        and return a value\n        name :      str\n                    Name of the property to set.\n        register :  bool\n                    If True, will also register the attribute(s) as properties\n                    that should show up in the summary.\n        na :        'raise' | 'propagate' | 'skip'\n                    What to do if `x` is a dictionary and does not contain a\n                    value for a neuron:\n                     - 'raise' will raise a KeyError\n                     - 'propagate' will set the attribute to `None`\n                     - 'skip' will not set the attribute\n\n        See Also\n        --------\n        navis.NeuronList.add_metadata\n                    Set metadata from a dataframe.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import navis\n        &gt;&gt;&gt; nl = navis.example_neurons(5)\n        &gt;&gt;&gt; # Set a single value\n        &gt;&gt;&gt; nl.set_neuron_attributes('some_value', name='my_attr')\n        &gt;&gt;&gt; nl[0].my_attr\n        'some_value'\n        &gt;&gt;&gt; # Set individual values as iterable\n        &gt;&gt;&gt; nl.set_neuron_attributes([1, 2, 3, 4, 5], name='my_attr')\n        &gt;&gt;&gt; nl[0].my_attr\n        1\n        &gt;&gt;&gt; nl.my_attr\n        array([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; # Set individual values using a dictionary\n        &gt;&gt;&gt; val_dict = dict(zip(nl.id, ['test', 2, 2.2, 4, 'test2']))\n        &gt;&gt;&gt; nl.set_neuron_attributes(val_dict, name='my_attr')\n        &gt;&gt;&gt; nl[0].my_attr\n        'test'\n\n        \"\"\"\n        utils.eval_param(na, name='na',\n                         allowed_values=('raise', 'propagate', 'skip'))\n        utils.eval_param(name, name='name', allowed_types=(str, ))\n\n        if isinstance(x, dict):\n            if na == 'raise':\n                miss = ~np.isin(self.id, list(x))\n                if any(miss):\n                    raise KeyError('Dictionary `x` is missing entries for IDs: '\n                                   f'{self.id[miss]}')\n            for n in self.neurons:\n                v = x.get(n.id, None)\n                if (v is None) and (na == 'skip'):\n                    continue\n                n._register_attr(name, v, summary=register)\n        elif isinstance(x, (list, np.ndarray)):\n            if len(x) != len(self):\n                raise ValueError(f'Got {len(x)} values for the{len(self)} '\n                                 'neurons in the NeuronList.')\n            for n, v in zip(self.neurons, x):\n                n._register_attr(name, v, summary=register)\n        elif callable(x):\n            for n in self.neurons:\n                n._register_attr(name, x(n.id), summary=register)\n        else:\n            for n in self.neurons:\n                n._register_attr(name, x, summary=register)\n\n    def sort_values(self, key: str, ascending: bool = False):\n        \"\"\"Sort neurons by given key.\n\n        Needs to be an attribute of all neurons: for example `name`.\n        Also works with custom attributes.\n        \"\"\"\n        self.neurons = sorted(self.neurons,\n                              key=lambda x: getattr(x, key),\n                              reverse=ascending is False)\n\n    def copy(self, **kwargs) -&gt; 'NeuronList':\n        \"\"\"Return copy of this NeuronList.\n\n        Parameters\n        ----------\n        **kwargs\n                    Keyword arguments passed to neuron's `.copy()` method::\n\n                    deepcopy :  bool, for TreeNeurons only\n                                If False, `.graph` (NetworkX DiGraphs) will be\n                                returned as views - changes to nodes/edges can\n                                progagate back! `.igraph` (iGraph) - if\n                                available - will always be deepcopied.\n\n        \"\"\"\n        return self.__class__([n.copy(**kwargs) for n in config.tqdm(self.neurons,\n                                                                     desc='Copy',\n                                                                     leave=False,\n                                                                     disable=config.pbar_hide or len(self) &lt; 20)],\n                              make_copy=False)\n\n    def head(self, N: int = 5) -&gt; pd.DataFrame:\n        \"\"\"Return summary for top N neurons.\"\"\"\n        return self.summary(N=N)\n\n    def tail(self, N: int = 5) -&gt; pd.DataFrame:\n        \"\"\"Return summary for bottom N neurons.\"\"\"\n        return self.summary(N=slice(-N, len(self)))\n\n    def remove_duplicates(self,\n                          key: str = 'name',\n                          keep: str = 'first',\n                          inplace: bool = False\n                          ) -&gt; Optional['NeuronList']:\n        \"\"\"Remove duplicate neurons from list.\n\n        Parameters\n        ----------\n        key :       str | list, optional\n                    Attribute(s) by which to identify duplicates. In case of\n                    multiple, all attributes must match to flag a neuron as\n                    duplicate.\n        keep :      str\n                    Which of the duplicated neurons to keep.\n        inplace :   bool, optional\n                    If False will return a copy of the original with\n                    duplicates removed.\n\n        \"\"\"\n        if inplace:\n            x = self\n        else:\n            x = self.copy()\n\n        key = utils.make_iterable(key)\n\n        # Generate pandas DataFrame\n        df = pd.DataFrame([[getattr(n, at) for at in key] for n in x],\n                          columns=key)\n\n        # Find out which neurons to keep\n        to_keep = ~df.duplicated(keep=keep).values\n\n        # Reassign neurons\n        x.neurons = x[to_keep].neurons\n\n        if not inplace:\n            return x\n        return None\n\n    def unmix(self):\n        \"\"\"Split into NeuronLists of the same neuron type.\n\n        Returns\n        -------\n        dict\n                Dictionary of `{Neurontype: NeuronList}`\n\n        \"\"\"\n        return {t: self.__class__([n for n in self.neurons if isinstance(n, t)])\n                for t in self.types}\n</code></pre>"},{"location":"reference/navis/#navis.NeuronList.bbox","title":"<code>bbox</code>  <code>property</code>","text":"<p>Bounding box across all neurons in the list.</p>"},{"location":"reference/navis/#navis.NeuronList.empty","title":"<code>empty</code>  <code>property</code>","text":"<p>Return True if NeuronList is empty.</p>"},{"location":"reference/navis/#navis.NeuronList.is_degenerated","title":"<code>is_degenerated</code>  <code>property</code>","text":"<p>Return True if contains neurons with non-unique IDs.</p>"},{"location":"reference/navis/#navis.NeuronList.is_mixed","title":"<code>is_mixed</code>  <code>property</code>","text":"<p>Return True if contains more than one type of neuron.</p>"},{"location":"reference/navis/#navis.NeuronList.neurons","title":"<code>neurons</code>  <code>property</code>","text":"<p>Neurons contained in this NeuronList.</p>"},{"location":"reference/navis/#navis.NeuronList.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>Shape of NeuronList (N, ).</p>"},{"location":"reference/navis/#navis.NeuronList.types","title":"<code>types</code>  <code>property</code>","text":"<p>Return neuron types present in this list.</p>"},{"location":"reference/navis/#navis.NeuronList.add_metadata","title":"<code>add_metadata</code>","text":"<p>Add neuron metadata from a DataFrame.</p> PARAMETER DESCRIPTION <code>meta</code> <pre><code>    DataFrame or filepath to a CSV file containing metadata.\n    Must contain a column with neuron IDs.\n</code></pre> <p> TYPE: <code>     pd.DataFrame | str | Path</code> </p> <code>id_col</code> <pre><code>    Name of the column containing neuron IDs.\n</code></pre> <p> TYPE: <code>   str</code> DEFAULT: <code>'id'</code> </p> <code>neuron_id</code> <pre><code>    Name of the attribute in the neuron that corresponds to\n    the `id_col`.\n</code></pre> <p> TYPE: <code>str</code> DEFAULT: <code>'id'</code> </p> <code>columns</code> <pre><code>    List of columns to add. If None, will add all columns except\n    for `id_col`.\n</code></pre> <p> TYPE: <code>  list</code> DEFAULT: <code>None</code> </p> <code>register</code> <pre><code>    If True, will also register the attribute(s) as properties\n    that should show up in the summary.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>False</code> </p> <code>missing</code> <pre><code>    What to do if `meta` is missing a value for a neuron.\n</code></pre> <p> TYPE: <code>  'raise' | 'warn' | 'ignore'</code> DEFAULT: <code>'raise'</code> </p> See Also <p>navis.NeuronList.set_neuron_attributes             Set individual attributes of neurons contained in the NeuronList.</p> Source code in <code>navis/core/neuronlist.py</code> <pre><code>def add_metadata(self, meta, id_col='id', neuron_id='id', columns=None, register=False, missing='raise'):\n    \"\"\"Add neuron metadata from a DataFrame.\n\n    Parameters\n    ----------\n    meta :      pd.DataFrame | str | Path\n                DataFrame or filepath to a CSV file containing metadata.\n                Must contain a column with neuron IDs.\n    id_col :    str\n                Name of the column containing neuron IDs.\n    neuron_id : str\n                Name of the attribute in the neuron that corresponds to\n                the `id_col`.\n    columns :   list, optional\n                List of columns to add. If None, will add all columns except\n                for `id_col`.\n    register :  bool\n                If True, will also register the attribute(s) as properties\n                that should show up in the summary.\n    missing :   'raise' | 'warn' | 'ignore'\n                What to do if `meta` is missing a value for a neuron.\n\n    See Also\n    --------\n    navis.NeuronList.set_neuron_attributes\n                Set individual attributes of neurons contained in the NeuronList.\n\n    \"\"\"\n    assert missing in ('raise', 'warn', 'ignore')\n\n    if isinstance(meta, (str, Path)):\n        meta = pd.read_csv(meta)\n\n    if not isinstance(meta, pd.DataFrame):\n        raise TypeError('`meta` must be a DataFrame or a path to a CSV file, '\n                        f'got {type(meta)}')\n\n    if id_col not in meta.columns:\n        raise KeyError(f'Column \"{id_col}\" not found in metadata.')\n\n    # Index meta data by the neuron_id\n    neuron_id = getattr(self, neuron_id)\n    miss = ~np.isin(neuron_id, meta[id_col].values)\n    if any(miss):\n        msg = f'Metadata is missing entries for IDs: {neuron_id[miss]}'\n        if missing == 'raise':\n            raise KeyError(msg)\n        elif missing == 'warn':\n            logger.warning(msg)\n\n    meta = meta.set_index(id_col).reindex(neuron_id)\n\n    if columns is None:\n        columns = meta.columns\n\n    for c in columns:\n        if c == id_col:\n            continue\n        self.set_neuron_attributes(\n            meta[c].values.tolist(),\n            name=c,\n            register=register\n            )\n</code></pre>"},{"location":"reference/navis/#navis.NeuronList.append","title":"<code>append</code>","text":"<p>Add neuron(s) to this list.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; # This is mostly for doctests\n&gt;&gt;&gt; nl = navis.example_neurons()\n&gt;&gt;&gt; len(nl)\n5\n&gt;&gt;&gt; # Add a single neuron to the list\n&gt;&gt;&gt; nl.append(nl[0])\n&gt;&gt;&gt; len(nl)\n6\n&gt;&gt;&gt; # Add a list of neurons to the list\n&gt;&gt;&gt; nl.append(nl)\n&gt;&gt;&gt; len(nl)\n12\n</code></pre> Source code in <code>navis/core/neuronlist.py</code> <pre><code>def append(self, v):\n    \"\"\"Add neuron(s) to this list.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; # This is mostly for doctests\n    &gt;&gt;&gt; nl = navis.example_neurons()\n    &gt;&gt;&gt; len(nl)\n    5\n    &gt;&gt;&gt; # Add a single neuron to the list\n    &gt;&gt;&gt; nl.append(nl[0])\n    &gt;&gt;&gt; len(nl)\n    6\n    &gt;&gt;&gt; # Add a list of neurons to the list\n    &gt;&gt;&gt; nl.append(nl)\n    &gt;&gt;&gt; len(nl)\n    12\n\n    \"\"\"\n    if isinstance(v, core.BaseNeuron):\n        self.neurons.append(v)\n    elif isinstance(v, NeuronList):\n        self.neurons += v.neurons\n    else:\n        raise NotImplementedError('Unable to append data of type'\n                                  f'{type(v)} to NeuronList')\n</code></pre>"},{"location":"reference/navis/#navis.NeuronList.apply","title":"<code>apply</code>","text":"<p>Apply function across all neurons in this NeuronList.</p> PARAMETER DESCRIPTION <code>func</code> <pre><code>        Function to be applied. Must accept\n        [`navis.BaseNeuron`][] as first argument.\n</code></pre> <p> TYPE: <code>         callable</code> </p> <code>parallel</code> <pre><code>        If True (default) will use multiprocessing. Spawning the\n        processes takes time (and memory). Using `parallel=True`\n        makes only sense if the NeuronList is large or the\n        function takes a long time to run.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>False</code> </p> <code>n_cores</code> <pre><code>        Number of CPUs to use for multiprocessing. Defaults to\n        half the available cores.\n</code></pre> <p> TYPE: <code>      int</code> DEFAULT: <code>os.cpu_count() // 2</code> </p> <code>omit_failures</code> <pre><code>        If True, will ignore failures.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <pre><code>    Will be passed to function.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Results</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons()\n&gt;&gt;&gt; # Apply resampling function\n&gt;&gt;&gt; nl_rs = nl.apply(navis.resample_skeleton, resample_to=1000, inplace=False)\n</code></pre> Source code in <code>navis/core/neuronlist.py</code> <pre><code>def apply(self,\n          func: Callable,\n          *,\n          parallel: bool = False,\n          n_cores: int = os.cpu_count() // 2,\n          omit_failures: bool = False,\n          **kwargs):\n    \"\"\"Apply function across all neurons in this NeuronList.\n\n    Parameters\n    ----------\n    func :          callable\n                    Function to be applied. Must accept\n                    [`navis.BaseNeuron`][] as first argument.\n    parallel :      bool\n                    If True (default) will use multiprocessing. Spawning the\n                    processes takes time (and memory). Using `parallel=True`\n                    makes only sense if the NeuronList is large or the\n                    function takes a long time to run.\n    n_cores :       int\n                    Number of CPUs to use for multiprocessing. Defaults to\n                    half the available cores.\n    omit_failures : bool\n                    If True, will ignore failures.\n\n    **kwargs\n                Will be passed to function.\n\n    Returns\n    -------\n    Results\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons()\n    &gt;&gt;&gt; # Apply resampling function\n    &gt;&gt;&gt; nl_rs = nl.apply(navis.resample_skeleton, resample_to=1000, inplace=False)\n\n    \"\"\"\n    if not callable(func):\n        raise TypeError('\"func\" must be callable')\n\n    # Delayed import to avoid circular import\n    from .core_utils import NeuronProcessor\n    proc = NeuronProcessor(self,\n                           func,\n                           parallel=parallel,\n                           n_cores=n_cores,\n                           omit_failures=omit_failures,\n                           desc=f'Apply {func.__name__}')\n\n    return proc(self.neurons, **kwargs)\n</code></pre>"},{"location":"reference/navis/#navis.NeuronList.copy","title":"<code>copy</code>","text":"<p>Return copy of this NeuronList.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <pre><code>    Keyword arguments passed to neuron's `.copy()` method::\n\n    deepcopy :  bool, for TreeNeurons only\n                If False, `.graph` (NetworkX DiGraphs) will be\n                returned as views - changes to nodes/edges can\n                progagate back! `.igraph` (iGraph) - if\n                available - will always be deepcopied.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> Source code in <code>navis/core/neuronlist.py</code> <pre><code>def copy(self, **kwargs) -&gt; 'NeuronList':\n    \"\"\"Return copy of this NeuronList.\n\n    Parameters\n    ----------\n    **kwargs\n                Keyword arguments passed to neuron's `.copy()` method::\n\n                deepcopy :  bool, for TreeNeurons only\n                            If False, `.graph` (NetworkX DiGraphs) will be\n                            returned as views - changes to nodes/edges can\n                            progagate back! `.igraph` (iGraph) - if\n                            available - will always be deepcopied.\n\n    \"\"\"\n    return self.__class__([n.copy(**kwargs) for n in config.tqdm(self.neurons,\n                                                                 desc='Copy',\n                                                                 leave=False,\n                                                                 disable=config.pbar_hide or len(self) &lt; 20)],\n                          make_copy=False)\n</code></pre>"},{"location":"reference/navis/#navis.NeuronList.get_neuron_attributes","title":"<code>get_neuron_attributes</code>","text":"<p>Get attributes of neurons contained in the NeuronList.</p> PARAMETER DESCRIPTION <code>name</code> <pre><code>    Name of the property to get.\n</code></pre> <p> TYPE: <code>     str</code> </p> <code>default</code> <pre><code>    Default value to return if attribute is not found.\n</code></pre> <p> TYPE: <code>  any</code> </p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>Array of values for the requested attribute.</p> Source code in <code>navis/core/neuronlist.py</code> <pre><code>def get_neuron_attributes(self, *args, **kwargs):\n    \"\"\"Get attributes of neurons contained in the NeuronList.\n\n    Parameters\n    ----------\n    name :      str\n                Name of the property to get.\n    default :   any, optional\n                Default value to return if attribute is not found.\n\n    Returns\n    -------\n    np.ndarray\n                Array of values for the requested attribute.\n\n    \"\"\"\n    return np.array([getattr(n, *args, **kwargs) for n in self.neurons])\n</code></pre>"},{"location":"reference/navis/#navis.NeuronList.head","title":"<code>head</code>","text":"<p>Return summary for top N neurons.</p> Source code in <code>navis/core/neuronlist.py</code> <pre><code>def head(self, N: int = 5) -&gt; pd.DataFrame:\n    \"\"\"Return summary for top N neurons.\"\"\"\n    return self.summary(N=N)\n</code></pre>"},{"location":"reference/navis/#navis.NeuronList.itertuples","title":"<code>itertuples</code>","text":"<p>Helper to mimic <code>pandas.DataFrame.itertuples()</code>.</p> Source code in <code>navis/core/neuronlist.py</code> <pre><code>def itertuples(self):\n    \"\"\"Helper to mimic `pandas.DataFrame.itertuples()`.\"\"\"\n    return self.neurons\n</code></pre>"},{"location":"reference/navis/#navis.NeuronList.mean","title":"<code>mean</code>","text":"<p>Return mean numeric and boolean values over all neurons.</p> Source code in <code>navis/core/neuronlist.py</code> <pre><code>def mean(self) -&gt; pd.DataFrame:\n    \"\"\"Return mean numeric and boolean values over all neurons.\"\"\"\n    return self.summary().mean(numeric_only=True)\n</code></pre>"},{"location":"reference/navis/#navis.NeuronList.memory_usage","title":"<code>memory_usage</code>","text":"<p>Return estimated size in memory of this NeuronList.</p> <p>Works by going over each neuron and summing up their size in memory.</p> PARAMETER DESCRIPTION <code>deep</code> <pre><code>        Pass to pandas DataFrames. If True will inspect data of\n        object type too.\n</code></pre> <p> TYPE: <code>         bool</code> DEFAULT: <code>False</code> </p> <code>estimate</code> <pre><code>        If True, we will only estimate the size. This is\n        considerably faster but will slightly underestimate the\n        memory usage.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>False</code> </p> <code>sample</code> <pre><code>        If True, we will only sample 10% of the neurons\n        contained in the list and extrapolate an estimate from\n        there.\n</code></pre> <p> TYPE: <code>       bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>int</code> <p>Memory usage in bytes.</p> Source code in <code>navis/core/neuronlist.py</code> <pre><code>def memory_usage(self, deep=False, estimate=False, sample=False):\n    \"\"\"Return estimated size in memory of this NeuronList.\n\n    Works by going over each neuron and summing up their size in memory.\n\n    Parameters\n    ----------\n    deep :          bool\n                    Pass to pandas DataFrames. If True will inspect data of\n                    object type too.\n    estimate :      bool\n                    If True, we will only estimate the size. This is\n                    considerably faster but will slightly underestimate the\n                    memory usage.\n    sample :        bool\n                    If True, we will only sample 10% of the neurons\n                    contained in the list and extrapolate an estimate from\n                    there.\n\n    Returns\n    -------\n    int\n                Memory usage in bytes.\n\n    \"\"\"\n    if self.empty:\n        return 0\n\n    if not sample:\n        try:\n            return sum([n.memory_usage(deep=deep,\n                                       estimate=estimate) for n in self.neurons])\n        except BaseException:\n            return 0\n    else:\n        try:\n            s = sum([n.memory_usage(deep=deep,\n                                    estimate=estimate) for n in self.neurons[::10]])\n            return s * (len(self.neurons) / len(self.neurons[::10]))\n        except BaseException:\n            return 0\n</code></pre>"},{"location":"reference/navis/#navis.NeuronList.plot2d","title":"<code>plot2d</code>","text":"<p>Plot neuron in 2D using <code>navis.plot2d</code>.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <pre><code>Keyword arguments will be passed to [`navis.plot2d`][].\nSee `help(navis.plot2d)` for a list of accepted keywords.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> See Also <p><code>navis.plot2d</code>         Base function called to generate 2d plot.</p> Source code in <code>navis/core/neuronlist.py</code> <pre><code>def plot2d(self, **kwargs):\n    \"\"\"Plot neuron in 2D using [`navis.plot2d`][].\n\n    Parameters\n    ----------\n    **kwargs\n            Keyword arguments will be passed to [`navis.plot2d`][].\n            See `help(navis.plot2d)` for a list of accepted keywords.\n\n    See Also\n    --------\n    [`navis.plot2d`][]\n            Base function called to generate 2d plot.\n\n    \"\"\"\n    from ..plotting import plot2d\n\n    return plot2d(self, **kwargs)\n</code></pre>"},{"location":"reference/navis/#navis.NeuronList.plot3d","title":"<code>plot3d</code>","text":"<p>Plot neuron in 3D using <code>navis.plot3d</code>.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <pre><code>Keyword arguments will be passed to [`navis.plot3d`][].\nSee `help(navis.plot3d)` for a list of keywords.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> See Also <p><code>navis.plot3d</code>         Base function called to generate 3d plot.</p> Source code in <code>navis/core/neuronlist.py</code> <pre><code>def plot3d(self, **kwargs):\n    \"\"\"Plot neuron in 3D using [`navis.plot3d`][].\n\n    Parameters\n    ----------\n    **kwargs\n            Keyword arguments will be passed to [`navis.plot3d`][].\n            See `help(navis.plot3d)` for a list of keywords.\n\n    See Also\n    --------\n    [`navis.plot3d`][]\n            Base function called to generate 3d plot.\n\n    \"\"\"\n    from ..plotting import plot3d\n\n    return plot3d(self, **kwargs)\n</code></pre>"},{"location":"reference/navis/#navis.NeuronList.remove_duplicates","title":"<code>remove_duplicates</code>","text":"<p>Remove duplicate neurons from list.</p> PARAMETER DESCRIPTION <code>key</code> <pre><code>    Attribute(s) by which to identify duplicates. In case of\n    multiple, all attributes must match to flag a neuron as\n    duplicate.\n</code></pre> <p> TYPE: <code>      str | list</code> DEFAULT: <code>'name'</code> </p> <code>keep</code> <pre><code>    Which of the duplicated neurons to keep.\n</code></pre> <p> TYPE: <code>     str</code> DEFAULT: <code>'first'</code> </p> <code>inplace</code> <pre><code>    If False will return a copy of the original with\n    duplicates removed.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> Source code in <code>navis/core/neuronlist.py</code> <pre><code>def remove_duplicates(self,\n                      key: str = 'name',\n                      keep: str = 'first',\n                      inplace: bool = False\n                      ) -&gt; Optional['NeuronList']:\n    \"\"\"Remove duplicate neurons from list.\n\n    Parameters\n    ----------\n    key :       str | list, optional\n                Attribute(s) by which to identify duplicates. In case of\n                multiple, all attributes must match to flag a neuron as\n                duplicate.\n    keep :      str\n                Which of the duplicated neurons to keep.\n    inplace :   bool, optional\n                If False will return a copy of the original with\n                duplicates removed.\n\n    \"\"\"\n    if inplace:\n        x = self\n    else:\n        x = self.copy()\n\n    key = utils.make_iterable(key)\n\n    # Generate pandas DataFrame\n    df = pd.DataFrame([[getattr(n, at) for at in key] for n in x],\n                      columns=key)\n\n    # Find out which neurons to keep\n    to_keep = ~df.duplicated(keep=keep).values\n\n    # Reassign neurons\n    x.neurons = x[to_keep].neurons\n\n    if not inplace:\n        return x\n    return None\n</code></pre>"},{"location":"reference/navis/#navis.NeuronList.sample","title":"<code>sample</code>","text":"<p>Return random subset of neurons.</p> Source code in <code>navis/core/neuronlist.py</code> <pre><code>def sample(self, N: Union[int, float] = 1) -&gt; 'NeuronList':\n    \"\"\"Return random subset of neurons.\"\"\"\n    if N &lt; 1 and N &gt; 0:\n        N = int(len(self.neurons) * N)\n\n    indices = list(range(len(self.neurons)))\n    random.shuffle(indices)\n    return self.__class__([n for i, n in enumerate(self.neurons) if i in indices[:N]],\n                          make_copy=self.copy_on_subset)\n</code></pre>"},{"location":"reference/navis/#navis.NeuronList.set_neuron_attributes","title":"<code>set_neuron_attributes</code>","text":"<p>Set attributes of neurons contained in the NeuronList.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Value of the property:\n      - lists and arrays are expected to contain a value for\n        each neuron and hence have to match the length of the\n        NeuronList\n      - dict is expected to map `{neuron.id: value}`\n      - a function is expected to take `neuron.id` as input\n        and return a value\n</code></pre> <p> TYPE: <code>        any | list | np.ndarray | dict | function</code> </p> <code>name</code> <pre><code>    Name of the property to set.\n</code></pre> <p> TYPE: <code>     str</code> </p> <code>register</code> <pre><code>    If True, will also register the attribute(s) as properties\n    that should show up in the summary.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>False</code> </p> <code>na</code> <pre><code>    What to do if `x` is a dictionary and does not contain a\n    value for a neuron:\n     - 'raise' will raise a KeyError\n     - 'propagate' will set the attribute to `None`\n     - 'skip' will not set the attribute\n</code></pre> <p> TYPE: <code>       'raise' | 'propagate' | 'skip'</code> DEFAULT: <code>'raise'</code> </p> See Also <p>navis.NeuronList.add_metadata             Set metadata from a dataframe.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(5)\n&gt;&gt;&gt; # Set a single value\n&gt;&gt;&gt; nl.set_neuron_attributes('some_value', name='my_attr')\n&gt;&gt;&gt; nl[0].my_attr\n'some_value'\n&gt;&gt;&gt; # Set individual values as iterable\n&gt;&gt;&gt; nl.set_neuron_attributes([1, 2, 3, 4, 5], name='my_attr')\n&gt;&gt;&gt; nl[0].my_attr\n1\n&gt;&gt;&gt; nl.my_attr\narray([1, 2, 3, 4, 5])\n&gt;&gt;&gt; # Set individual values using a dictionary\n&gt;&gt;&gt; val_dict = dict(zip(nl.id, ['test', 2, 2.2, 4, 'test2']))\n&gt;&gt;&gt; nl.set_neuron_attributes(val_dict, name='my_attr')\n&gt;&gt;&gt; nl[0].my_attr\n'test'\n</code></pre> Source code in <code>navis/core/neuronlist.py</code> <pre><code>def set_neuron_attributes(self, x, name, register=False, na='raise'):\n    \"\"\"Set attributes of neurons contained in the NeuronList.\n\n    Parameters\n    ----------\n    x :         any | list | np.ndarray | dict | function\n                Value of the property:\n                  - lists and arrays are expected to contain a value for\n                    each neuron and hence have to match the length of the\n                    NeuronList\n                  - dict is expected to map `{neuron.id: value}`\n                  - a function is expected to take `neuron.id` as input\n                    and return a value\n    name :      str\n                Name of the property to set.\n    register :  bool\n                If True, will also register the attribute(s) as properties\n                that should show up in the summary.\n    na :        'raise' | 'propagate' | 'skip'\n                What to do if `x` is a dictionary and does not contain a\n                value for a neuron:\n                 - 'raise' will raise a KeyError\n                 - 'propagate' will set the attribute to `None`\n                 - 'skip' will not set the attribute\n\n    See Also\n    --------\n    navis.NeuronList.add_metadata\n                Set metadata from a dataframe.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(5)\n    &gt;&gt;&gt; # Set a single value\n    &gt;&gt;&gt; nl.set_neuron_attributes('some_value', name='my_attr')\n    &gt;&gt;&gt; nl[0].my_attr\n    'some_value'\n    &gt;&gt;&gt; # Set individual values as iterable\n    &gt;&gt;&gt; nl.set_neuron_attributes([1, 2, 3, 4, 5], name='my_attr')\n    &gt;&gt;&gt; nl[0].my_attr\n    1\n    &gt;&gt;&gt; nl.my_attr\n    array([1, 2, 3, 4, 5])\n    &gt;&gt;&gt; # Set individual values using a dictionary\n    &gt;&gt;&gt; val_dict = dict(zip(nl.id, ['test', 2, 2.2, 4, 'test2']))\n    &gt;&gt;&gt; nl.set_neuron_attributes(val_dict, name='my_attr')\n    &gt;&gt;&gt; nl[0].my_attr\n    'test'\n\n    \"\"\"\n    utils.eval_param(na, name='na',\n                     allowed_values=('raise', 'propagate', 'skip'))\n    utils.eval_param(name, name='name', allowed_types=(str, ))\n\n    if isinstance(x, dict):\n        if na == 'raise':\n            miss = ~np.isin(self.id, list(x))\n            if any(miss):\n                raise KeyError('Dictionary `x` is missing entries for IDs: '\n                               f'{self.id[miss]}')\n        for n in self.neurons:\n            v = x.get(n.id, None)\n            if (v is None) and (na == 'skip'):\n                continue\n            n._register_attr(name, v, summary=register)\n    elif isinstance(x, (list, np.ndarray)):\n        if len(x) != len(self):\n            raise ValueError(f'Got {len(x)} values for the{len(self)} '\n                             'neurons in the NeuronList.')\n        for n, v in zip(self.neurons, x):\n            n._register_attr(name, v, summary=register)\n    elif callable(x):\n        for n in self.neurons:\n            n._register_attr(name, x(n.id), summary=register)\n    else:\n        for n in self.neurons:\n            n._register_attr(name, x, summary=register)\n</code></pre>"},{"location":"reference/navis/#navis.NeuronList.sort_values","title":"<code>sort_values</code>","text":"<p>Sort neurons by given key.</p> <p>Needs to be an attribute of all neurons: for example <code>name</code>. Also works with custom attributes.</p> Source code in <code>navis/core/neuronlist.py</code> <pre><code>def sort_values(self, key: str, ascending: bool = False):\n    \"\"\"Sort neurons by given key.\n\n    Needs to be an attribute of all neurons: for example `name`.\n    Also works with custom attributes.\n    \"\"\"\n    self.neurons = sorted(self.neurons,\n                          key=lambda x: getattr(x, key),\n                          reverse=ascending is False)\n</code></pre>"},{"location":"reference/navis/#navis.NeuronList.sum","title":"<code>sum</code>","text":"<p>Return sum numeric and boolean values over all neurons.</p> Source code in <code>navis/core/neuronlist.py</code> <pre><code>def sum(self) -&gt; pd.DataFrame:\n    \"\"\"Return sum numeric and boolean values over all neurons.\"\"\"\n    return self.summary().sum(numeric_only=True)\n</code></pre>"},{"location":"reference/navis/#navis.NeuronList.summary","title":"<code>summary</code>","text":"<p>Get summary over all neurons in this NeuronList.</p> PARAMETER DESCRIPTION <code>N</code> <pre><code>    If int, get only first N entries.\n</code></pre> <p> TYPE: <code>        int | slice</code> DEFAULT: <code>None</code> </p> <code>add_props</code> <pre><code>    Additional properties to add to summary. If attribute not\n    available will return 'NA'.\n</code></pre> <p> TYPE: <code>list</code> DEFAULT: <code>[]</code> </p> <code>progress</code> <pre><code>    Whether to show a progress bar. Can be useful for very\n    large list.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>pandas DataFrame</code> Source code in <code>navis/core/neuronlist.py</code> <pre><code>def summary(self,\n            N: Optional[Union[int, slice]] = None,\n            add_props: list = [],\n            progress=False\n            ) -&gt; pd.DataFrame:\n    \"\"\"Get summary over all neurons in this NeuronList.\n\n    Parameters\n    ----------\n    N :         int | slice, optional\n                If int, get only first N entries.\n    add_props : list, optional\n                Additional properties to add to summary. If attribute not\n                available will return 'NA'.\n    progress :  bool\n                Whether to show a progress bar. Can be useful for very\n                large list.\n\n    Returns\n    -------\n    pandas DataFrame\n\n    \"\"\"\n    if not self.empty:\n        # Fetch a union of all summary props (keep order)\n        all_props = [p for l in self.SUMMARY_PROPS for p in l]\n        props = np.unique(all_props)\n        props = sorted(props, key=lambda x: all_props.index(x))\n    else:\n        props = []\n\n    # Add ID to properties - unless all are generic UUIDs\n    if any([not isinstance(n.id, uuid.UUID) for n in self.neurons]):\n        # Make sure we don't have two IDs\n        if 'id' in props:\n            props.remove('id')\n        props = np.insert(props, 2, 'id')\n\n    if add_props:\n        props = np.append(props, add_props)\n\n    if not isinstance(N, slice):\n        N = slice(N)\n\n    return pd.DataFrame(data=[[getattr(n, a, 'NA') for a in props]\n                              for n in config.tqdm(self.neurons[N],\n                                                   desc='Summarizing',\n                                                   leave=False,\n                                                   disable=not progress)],\n                        columns=props)\n</code></pre>"},{"location":"reference/navis/#navis.NeuronList.tail","title":"<code>tail</code>","text":"<p>Return summary for bottom N neurons.</p> Source code in <code>navis/core/neuronlist.py</code> <pre><code>def tail(self, N: int = 5) -&gt; pd.DataFrame:\n    \"\"\"Return summary for bottom N neurons.\"\"\"\n    return self.summary(N=slice(-N, len(self)))\n</code></pre>"},{"location":"reference/navis/#navis.NeuronList.unmix","title":"<code>unmix</code>","text":"<p>Split into NeuronLists of the same neuron type.</p> RETURNS DESCRIPTION <code>dict</code> <p>Dictionary of <code>{Neurontype: NeuronList}</code></p> Source code in <code>navis/core/neuronlist.py</code> <pre><code>def unmix(self):\n    \"\"\"Split into NeuronLists of the same neuron type.\n\n    Returns\n    -------\n    dict\n            Dictionary of `{Neurontype: NeuronList}`\n\n    \"\"\"\n    return {t: self.__class__([n for n in self.neurons if isinstance(n, t)])\n            for t in self.types}\n</code></pre>"},{"location":"reference/navis/#navis.TreeNeuron","title":"<code>navis.TreeNeuron</code>","text":"<p>Neuron represented as hierarchical tree (i.e. a skeleton).</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Data to construct neuron from:\n         - `pandas.DataFrame` is expected to be a SWC table\n         - `pandas.Series` is expected to have a DataFrame as\n           `.nodes` - additional properties will be attached\n           as meta data\n         - `tuple` of `(vertices, edges)` arrays is passed to\n           [`navis.edges2neuron`][]\n         - `str` is passed to [`navis.read_swc`][]\n         - `BufferedIOBase` e.g. from `open(filename)`\n         - `networkx.DiGraph` parsed by [`navis.nx2neuron`][]\n         - `skeletor.Skeleton`\n         - `TreeNeuron` - in this case we will try to copy every\n           attribute\n         - `None` will initialize an empty neuron\n</code></pre> <p> TYPE: <code>Union[pd.DataFrame, BufferedIOBase, str, TreeNeuron, nx.DiGraph]</code> </p> <code>units</code> <pre><code>        Units for coordinates. Defaults to `None` (dimensionless).\n        Strings must be parsable by pint: e.g. \"nm\", \"um\",\n        \"micrometer\" or \"8 nanometers\".\n</code></pre> <p> TYPE: <code>        str | pint.Units | pint.Quantity</code> DEFAULT: <code>None</code> </p> <code>**metadata</code> <pre><code>        Any additional data to attach to neuron.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> Source code in <code>navis/core/skeleton.py</code> <pre><code>class TreeNeuron(BaseNeuron):\n    \"\"\"Neuron represented as hierarchical tree (i.e. a skeleton).\n\n    Parameters\n    ----------\n    x\n                    Data to construct neuron from:\n                     - `pandas.DataFrame` is expected to be a SWC table\n                     - `pandas.Series` is expected to have a DataFrame as\n                       `.nodes` - additional properties will be attached\n                       as meta data\n                     - `tuple` of `(vertices, edges)` arrays is passed to\n                       [`navis.edges2neuron`][]\n                     - `str` is passed to [`navis.read_swc`][]\n                     - `BufferedIOBase` e.g. from `open(filename)`\n                     - `networkx.DiGraph` parsed by [`navis.nx2neuron`][]\n                     - `skeletor.Skeleton`\n                     - `TreeNeuron` - in this case we will try to copy every\n                       attribute\n                     - `None` will initialize an empty neuron\n    units :         str | pint.Units | pint.Quantity\n                    Units for coordinates. Defaults to `None` (dimensionless).\n                    Strings must be parsable by pint: e.g. \"nm\", \"um\",\n                    \"micrometer\" or \"8 nanometers\".\n    **metadata\n                    Any additional data to attach to neuron.\n\n    \"\"\"\n\n    nodes: pd.DataFrame\n\n    graph: 'nx.DiGraph'\n    igraph: 'igraph.Graph'  # type: ignore  # doesn't know iGraph\n\n    n_branches: int\n    n_leafs: int\n    cable_length: Union[int, float]\n\n    segments: List[list]\n    small_segments: List[list]\n\n    root: np.ndarray\n\n    soma: Optional[Union[int, str]]\n    soma_pos: Optional[Sequence]\n\n    #: Minimum radius for soma detection. Set to `None` if no tag needed.\n    #: Default = 1 micron\n    soma_detection_radius: Union[float, int, pint.Quantity] = 1 * config.ureg.um\n    #: Label for soma detection. Set to `None` if no tag needed. Default = 1.\n    soma_detection_label: Union[float, int, str] = 1\n    #: Soma radius (e.g. for plotting). If string, must be column in nodes\n    #: table. Default = 'radius'.\n    soma_radius: Union[float, int, str] = 'radius'\n    # Set default function for soma finding. Default = [`navis.morpho.find_soma`][]\n    _soma: Union[Callable[['TreeNeuron'], Sequence[int]], int] = morpho.find_soma\n\n    tags: Optional[Dict[str, List[int]]] = None\n\n    #: Attributes to be used when comparing two neurons.\n    EQ_ATTRIBUTES = ['n_nodes', 'n_connectors', 'soma', 'root',\n                     'n_branches', 'n_leafs', 'cable_length', 'name']\n\n    #: Temporary attributes that need to be regenerated when data changes.\n    TEMP_ATTR = ['_igraph', '_graph_nx', '_segments', '_small_segments',\n                 '_geodesic_matrix', 'centrality_method', '_simple',\n                 '_cable_length', '_memory_usage', '_adjacency_matrix']\n\n    #: Attributes used for neuron summary\n    SUMMARY_PROPS = ['type', 'name', 'n_nodes', 'n_connectors', 'n_branches',\n                     'n_leafs', 'cable_length', 'soma', 'units']\n\n    #: Core data table(s) used to calculate hash\n    CORE_DATA = ['nodes:node_id,parent_id,x,y,z']\n\n    def __init__(self,\n                 x: Union[pd.DataFrame,\n                          BufferedIOBase,\n                          str,\n                          'TreeNeuron',\n                          nx.DiGraph],\n                 units: Union[pint.Unit, str] = None,\n                 **metadata\n                 ):\n        \"\"\"Initialize Skeleton Neuron.\"\"\"\n        super().__init__()\n\n        # Lock neuron during construction\n        self._lock = 1\n\n        if isinstance(x, pd.DataFrame):\n            self.nodes = x\n        elif isinstance(x, pd.Series):\n            if not hasattr(x, 'nodes'):\n                raise ValueError('pandas.Series must have `nodes` entry.')\n            elif not isinstance(x.nodes, pd.DataFrame):\n                raise TypeError(f'Nodes must be pandas DataFrame, got \"{type(x.nodes)}\"')\n            self.nodes = x.nodes\n            metadata.update(x.to_dict())\n        elif isinstance(x, nx.Graph):\n            self.nodes = graph.nx2neuron(x).nodes\n        elif isinstance(x, BufferedIOBase) or isinstance(x, str):\n            x = io.read_swc(x)  # type: ignore\n            self.__dict__.update(x.__dict__)\n        elif isinstance(x, sk.Skeleton):\n            self.nodes = x.swc.copy()\n            self.vertex_map = x.mesh_map\n        elif isinstance(x, TreeNeuron):\n            self.__dict__.update(x.copy().__dict__)\n            # Try to copy every attribute\n            for at in self.__dict__:\n                try:\n                    setattr(self, at, copy.copy(getattr(self, at)))\n                except BaseException:\n                    logger.warning(f'Unable to deep-copy attribute \"{at}\"')\n        elif isinstance(x, tuple):\n            # Tuple of vertices and edges\n            if len(x) != 2:\n                raise ValueError('Tuple must have 2 elements: vertices and edges.')\n            self.nodes = graph.edges2neuron(edges=x[1], vertices=x[0]).nodes\n        elif isinstance(x, type(None)):\n            # This is a essentially an empty neuron\n            pass\n        else:\n            raise utils.ConstructionError(f'Unable to construct TreeNeuron from \"{type(x)}\"')\n\n        for k, v in metadata.items():\n            try:\n                setattr(self, k, v)\n            except AttributeError:\n                raise AttributeError(f\"Unable to set neuron's `{k}` attribute.\")\n\n        self.units = units\n        self._current_md5 = self.core_md5\n\n        self._lock = 0\n\n    def __getattr__(self, key):\n        \"\"\"We will use this magic method to calculate some attributes on-demand.\"\"\"\n        # Note that we're mixing @property and __getattr__ which causes problems:\n        # if a @property raises an Exception, Python falls back to __getattr__\n        # and traceback is lost!\n\n        # Last ditch effort - maybe the base class knows the key?\n        return super().__getattr__(key)\n\n    def __truediv__(self, other, copy=True):\n        \"\"\"Implement division for coordinates (nodes, connectors).\"\"\"\n        if isinstance(other, numbers.Number) or utils.is_iterable(other):\n            if utils.is_iterable(other):\n                # If divisor is isotropic use only single value\n                if len(set(other)) == 1:\n                    other == other[0]\n                elif len(other) != 4:\n                    raise ValueError('Division by list/array requires 4 '\n                                     'divisors for x/y/z and radius - '\n                                     f'got {len(other)}')\n\n            # If a number, consider this an offset for coordinates\n            n = self.copy() if copy else self\n            n.nodes[['x', 'y', 'z', 'radius']] /= other\n\n            # At this point we can ditch any 4th unit\n            if utils.is_iterable(other):\n                other = other[:3]\n            if n.has_connectors:\n                n.connectors[['x', 'y', 'z']] /= other\n\n            if hasattr(n, 'soma_radius'):\n                if isinstance(n.soma_radius, numbers.Number):\n                    n.soma_radius /= other\n\n            # Convert units\n            # Note: .to_compact() throws a RuntimeWarning and returns unchanged\n            # values  when `units` is a iterable\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                n.units = (n.units * other).to_compact()\n\n            n._clear_temp_attr(exclude=['classify_nodes'])\n            return n\n        return NotImplemented\n\n    def __mul__(self, other, copy=True):\n        \"\"\"Implement multiplication for coordinates (nodes, connectors).\"\"\"\n        if isinstance(other, numbers.Number) or utils.is_iterable(other):\n            if utils.is_iterable(other):\n                # If multiplicator is isotropic use only single value\n                if len(set(other)) == 1:\n                    other == other[0]\n                elif len(other) != 4:\n                    raise ValueError('Multiplication by list/array requires 4'\n                                     'multipliers for x/y/z and radius - '\n                                     f'got {len(other)}')\n\n            # If a number, consider this an offset for coordinates\n            n = self.copy() if copy else self\n            n.nodes[['x', 'y', 'z', 'radius']] *= other\n\n            # At this point we can ditch any 4th unit\n            if utils.is_iterable(other):\n                other = other[:3]\n            if n.has_connectors:\n                n.connectors[['x', 'y', 'z']] *= other\n\n            if hasattr(n, 'soma_radius'):\n                if isinstance(n.soma_radius, numbers.Number):\n                    n.soma_radius *= other\n\n            # Convert units\n            # Note: .to_compact() throws a RuntimeWarning and returns unchanged\n            # values  when `units` is a iterable\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                n.units = (n.units / other).to_compact()\n\n            n._clear_temp_attr(exclude=['classify_nodes'])\n            return n\n        return NotImplemented\n\n    def __add__(self, other, copy=True):\n        \"\"\"Implement addition for coordinates (nodes, connectors).\"\"\"\n        if isinstance(other, numbers.Number) or utils.is_iterable(other):\n            if utils.is_iterable(other):\n                # If offset isotropic use only single value\n                if len(set(other)) == 1:\n                    other == other[0]\n                elif len(other) != 3:\n                    raise ValueError('Addition by list/array requires 3'\n                                     'multipliers for x/y/z coordinates '\n                                     f'got {len(other)}')\n\n            # If a number, consider this an offset for coordinates\n            n = self.copy() if copy else self\n            n.nodes[['x', 'y', 'z']] += other\n\n            # Do the connectors\n            if n.has_connectors:\n                n.connectors[['x', 'y', 'z']] += other\n\n            n._clear_temp_attr(exclude=['classify_nodes'])\n            return n\n        # If another neuron, return a list of neurons\n        elif isinstance(other, BaseNeuron):\n            return core.NeuronList([self, other])\n        return NotImplemented\n\n    def __sub__(self, other, copy=True):\n        \"\"\"Implement subtraction for coordinates (nodes, connectors).\"\"\"\n        if isinstance(other, numbers.Number) or utils.is_iterable(other):\n            if utils.is_iterable(other):\n                # If offset is isotropic use only single value\n                if len(set(other)) == 1:\n                    other == other[0]\n                elif len(other) != 3:\n                    raise ValueError('Addition by list/array requires 3'\n                                     'multipliers for x/y/z coordinates '\n                                     f'got {len(other)}')\n\n            # If a number, consider this an offset for coordinates\n            n = self.copy() if copy else self\n            n.nodes[['x', 'y', 'z']] -= other\n\n            # Do the connectors\n            if n.has_connectors:\n                n.connectors[['x', 'y', 'z']] -= other\n\n            n._clear_temp_attr(exclude=['classify_nodes'])\n            return n\n        return NotImplemented\n\n    def __getstate__(self):\n        \"\"\"Get state (used e.g. for pickling).\"\"\"\n        state = {k: v for k, v in self.__dict__.items() if not callable(v)}\n\n        # Pickling the graphs actually takes longer than regenerating them\n        # from scratch\n        if '_graph_nx' in state:\n            _ = state.pop('_graph_nx')\n        if '_igraph' in state:\n            _ = state.pop('_igraph')\n\n        return state\n\n    @property\n    @temp_property\n    def adjacency_matrix(self):\n        \"\"\"Adjacency matrix of the skeleton.\"\"\"\n        if not hasattr(self, '_adjacency_matrix'):\n            self._adjacency_matrix = graph.skeleton_adjacency_matrix(self)\n        return self._adjacency_matrix\n\n    @property\n    @requires_nodes\n    def vertices(self) -&gt; np.ndarray:\n        \"\"\"Vertices of the skeleton.\"\"\"\n        return self.nodes[['x', 'y', 'z']].values\n\n    @property\n    @requires_nodes\n    def edges(self) -&gt; np.ndarray:\n        \"\"\"Edges between nodes.\n\n        See Also\n        --------\n        edge_coords\n                Same but with x/y/z coordinates instead of node IDs.\n\n        \"\"\"\n        not_root = self.nodes[self.nodes.parent_id &gt;= 0]\n        return not_root[['node_id', 'parent_id']].values\n\n    @property\n    @requires_nodes\n    def edge_coords(self) -&gt; np.ndarray:\n        \"\"\"Coordinates of edges between nodes.\n\n        See Also\n        --------\n        edges\n                Same but with node IDs instead of x/y/z coordinates.\n\n        \"\"\"\n        locs = self.nodes.set_index('node_id')[['x', 'y', 'z']]\n        edges = self.edges\n        edges_co = np.zeros((edges.shape[0], 2, 3))\n        edges_co[:, 0, :] = locs.loc[edges[:, 0]].values\n        edges_co[:, 1, :] = locs.loc[edges[:, 1]].values\n        return edges_co\n\n    @property\n    @temp_property\n    def igraph(self) -&gt; 'igraph.Graph':\n        \"\"\"iGraph representation of this neuron.\"\"\"\n        # If igraph does not exist, create and return\n        if not hasattr(self, '_igraph'):\n            # This also sets the attribute\n            return self.get_igraph()\n        return self._igraph\n\n    @property\n    @temp_property\n    def graph(self) -&gt; nx.DiGraph:\n        \"\"\"Networkx Graph representation of this neuron.\"\"\"\n        # If graph does not exist, create and return\n        if not hasattr(self, '_graph_nx'):\n            # This also sets the attribute\n            return self.get_graph_nx()\n        return self._graph_nx\n\n    @property\n    @temp_property\n    def geodesic_matrix(self):\n        \"\"\"Matrix with geodesic (along-the-arbor) distance between nodes.\"\"\"\n        # If matrix has not yet been generated or needs update\n        if not hasattr(self, '_geodesic_matrix'):\n            # (Re-)generate matrix\n            self._geodesic_matrix = graph.geodesic_matrix(self)\n\n        return self._geodesic_matrix\n\n    @property\n    @requires_nodes\n    def leafs(self) -&gt; pd.DataFrame:\n        \"\"\"Leaf node table.\"\"\"\n        return self.nodes[self.nodes['type'] == 'end']\n\n    @property\n    @requires_nodes\n    def ends(self):\n        \"\"\"End node table (same as leafs).\"\"\"\n        return self.leafs\n\n    @property\n    @requires_nodes\n    def branch_points(self):\n        \"\"\"Branch node table.\"\"\"\n        return self.nodes[self.nodes['type'] == 'branch']\n\n    @property\n    def nodes(self) -&gt; pd.DataFrame:\n        \"\"\"Node table.\"\"\"\n        return self._get_nodes()\n\n    def _get_nodes(self) -&gt; pd.DataFrame:\n        # Redefine this function in subclass to change how nodes are retrieved\n        return self._nodes\n\n    @nodes.setter\n    def nodes(self, v):\n        \"\"\"Validate and set node table.\"\"\"\n        # We are refering to an extra function to facilitate subclassing:\n        # Redefine _set_nodes() to not break property\n        self._set_nodes(v)\n\n    def _set_nodes(self, v):\n        # Redefine this function in subclass to change validation\n        self._nodes = utils.validate_table(v,\n                                           required=[('node_id', 'rowId', 'node', 'treenode_id', 'PointNo'),\n                                                     ('parent_id', 'link', 'parent', 'Parent'),\n                                                     ('x', 'X'),\n                                                     ('y', 'Y'),\n                                                     ('z', 'Z')],\n                                           rename=True,\n                                           optional={('radius', 'W'): 0},\n                                           restrict=False)\n\n        # Make sure we don't end up with object dtype anywhere as this can\n        # cause problems\n        for c in ('node_id', 'parent_id'):\n            if self._nodes[c].dtype == 'O':\n                self._nodes[c] = self._nodes[c].astype(int)\n\n        graph.classify_nodes(self)\n\n    @property\n    def n_trees(self) -&gt; int:\n        \"\"\"Count number of connected trees in this neuron.\"\"\"\n        return len(self.subtrees)\n\n    @property\n    def is_tree(self) -&gt; bool:\n        \"\"\"Whether neuron is a tree.\n\n        Also returns True if neuron consists of multiple separate trees!\n\n        See also\n        --------\n        networkx.is_forest()\n                    Function used to test whether neuron is a tree.\n        :attr:`TreeNeuron.cycles`\n                    If your neuron is not a tree, this will help you identify\n                    cycles.\n\n        \"\"\"\n        return nx.is_forest(self.graph)\n\n    @property\n    def subtrees(self) -&gt; List[List[int]]:\n        \"\"\"List of subtrees. Sorted by size as sets of node IDs.\"\"\"\n        return sorted(graph._connected_components(self),\n                      key=lambda x: -len(x))\n\n    @property\n    def connectors(self) -&gt; pd.DataFrame:\n        \"\"\"Connector table. If none, will return `None`.\"\"\"\n        return self._get_connectors()\n\n    def _get_connectors(self) -&gt; pd.DataFrame:\n        # Redefine this function in subclass to change how nodes are retrieved\n        return getattr(self, '_connectors', None)\n\n    @connectors.setter\n    def connectors(self, v):\n        \"\"\"Validate and set connector table.\"\"\"\n        # We are refering to an extra function to facilitate subclassing:\n        # Redefine _set_connectors() to not break property\n        self._set_connectors(v)\n\n    def _set_connectors(self, v):\n        # Redefine this function in subclass to change validation\n        if isinstance(v, type(None)):\n            self._connectors = None\n        else:\n            self._connectors = utils.validate_table(v,\n                                                    required=[('connector_id', 'id'),\n                                                              ('node_id', 'rowId', 'node', 'treenode_id'),\n                                                              ('x', 'X'),\n                                                              ('y', 'Y'),\n                                                              ('z', 'Z'),\n                                                              ('type', 'relation', 'label', 'prepost')],\n                                                    rename=True,\n                                                    restrict=False)\n\n    @property\n    @requires_nodes\n    def cycles(self) -&gt; Optional[List[int]]:\n        \"\"\"Cycles in neuron (if any).\n\n        See also\n        --------\n        networkx.find_cycles()\n                    Function used to find cycles.\n\n        \"\"\"\n        try:\n            c = nx.find_cycle(self.graph,\n                              source=self.nodes[self.nodes.type == 'end'].node_id.values)\n            return c\n        except nx.exception.NetworkXNoCycle:\n            return None\n        except BaseException:\n            raise\n\n    @property\n    def simple(self) -&gt; 'TreeNeuron':\n        \"\"\"Simplified representation consisting only of root, branch points and leafs.\"\"\"\n        if not hasattr(self, '_simple'):\n            self._simple = self.copy()\n\n            # Make sure we don't have a soma, otherwise that node will be preserved\n            self._simple.soma = None\n\n            # Downsample\n            self._simple.downsample(float('inf'), inplace=True)\n        return self._simple\n\n    @property\n    def soma(self) -&gt; Optional[Union[str, int]]:\n        \"\"\"Search for soma and return node ID(s).\n\n        `None` if no soma. You can assign either a function that accepts a\n        TreeNeuron as input or a fix value. The default is [`navis.find_soma`][].\n\n        \"\"\"\n        if callable(self._soma):\n            soma = self._soma.__call__()  # type: ignore  # say int not callable\n        else:\n            soma = self._soma\n\n        # Sanity check to make sure that the soma node actually exists\n        if isinstance(soma, type(None)):\n            # Return immmediately without expensive checks\n            return soma\n        elif utils.is_iterable(soma):\n            if all(pd.isnull(soma)):\n                soma = None\n            elif not any(self.nodes.node_id.isin(soma)):\n                logger.warning(f'Soma(s) {soma} not found in node table.')\n                soma = None\n        else:\n            if soma not in self.nodes.node_id.values:\n                logger.warning(f'Soma {soma} not found in node table.')\n                soma = None\n\n        return soma\n\n    @soma.setter\n    def soma(self, value: Union[Callable, int, None]) -&gt; None:\n        \"\"\"Set soma.\"\"\"\n        if hasattr(value, '__call__'):\n            self._soma = types.MethodType(value, self)\n        elif isinstance(value, type(None)):\n            self._soma = None\n        elif isinstance(value, bool) and not value:\n            self._soma = None\n        else:\n            if value in self.nodes.node_id.values:\n                self._soma = value\n            else:\n                raise ValueError('Soma must be function, None or a valid node ID.')\n\n    @property\n    def soma_pos(self) -&gt; Optional[Sequence]:\n        \"\"\"Search for soma and return its position.\n\n        Returns `None` if no soma. You can also use this to assign a soma by\n        position in which case it will snap to the closest node.\n        \"\"\"\n        # Sanity check to make sure that the soma node actually exists\n        soma = self.soma\n        if isinstance(soma, type(None)):\n            return None\n        elif utils.is_iterable(soma):\n            if all(pd.isnull(soma)):\n                return None\n        else:\n            soma = utils.make_iterable(soma)\n\n        return self.nodes.loc[self.nodes.node_id.isin(soma), ['x', 'y', 'z']].values\n\n    @soma_pos.setter\n    def soma_pos(self, value: Sequence) -&gt; None:\n        \"\"\"Set soma by position.\"\"\"\n        if value is None:\n            self.soma = None\n            return\n\n        try:\n            value = np.asarray(value).astype(np.float64).reshape(3)\n        except BaseException:\n            raise ValueError(f'Unable to convert soma position \"{value}\" '\n                             f'to numeric (3, ) numpy array.')\n\n        # Generate tree\n        id, dist = self.snap(value, to='nodes')\n\n        # A sanity check\n        if dist &gt; (self.sampling_resolution * 10):\n            logger.warning(f'New soma position for {self.id} is suspiciously '\n                           f'far away from the closest node: {dist}')\n\n        self.soma = id\n\n    @property\n    @requires_nodes\n    def root(self) -&gt; Sequence:\n        \"\"\"Root node(s).\"\"\"\n        roots = self.nodes[self.nodes.parent_id &lt; 0].node_id.values\n        return roots\n\n    @root.setter\n    def root(self, value: Union[int, List[int]]) -&gt; None:\n        \"\"\"Reroot neuron to given node.\"\"\"\n        self.reroot(value, inplace=True)\n\n    @property\n    def type(self) -&gt; str:\n        \"\"\"Neuron type.\"\"\"\n        return 'navis.TreeNeuron'\n\n    @property\n    @requires_nodes\n    def n_branches(self) -&gt; Optional[int]:\n        \"\"\"Number of branch points.\"\"\"\n        return self.nodes[self.nodes.type == 'branch'].shape[0]\n\n    @property\n    @requires_nodes\n    def n_leafs(self) -&gt; Optional[int]:\n        \"\"\"Number of leaf nodes.\"\"\"\n        return self.nodes[self.nodes.type == 'end'].shape[0]\n\n    @property\n    @temp_property\n    @add_units(compact=True)\n    def cable_length(self) -&gt; Union[int, float]:\n        \"\"\"Cable length.\"\"\"\n        if not hasattr(self, '_cable_length'):\n            self._cable_length = morpho.cable_length(self)\n        return self._cable_length\n\n    @property\n    @add_units(compact=True, power=2)\n    def surface_area(self) -&gt; float:\n        \"\"\"Radius-based lateral surface area.\"\"\"\n        if 'radius' not in self.nodes.columns:\n            raise ValueError(f'Neuron {self.id} does not have radius information')\n\n        if any(self.nodes.radius &lt; 0):\n            logger.warning(f'Neuron {self.id} has negative radii - area will not be correct.')\n\n        if any(self.nodes.radius.isnull()):\n            logger.warning(f'Neuron {self.id} has NaN radii - area will not be correct.')\n\n        # Generate radius dict\n        radii = self.nodes.set_index('node_id').radius.to_dict()\n        # Drop root node(s)\n        not_root = self.nodes.parent_id &gt;= 0\n        # For each cylinder get the height\n        h = morpho.mmetrics.parent_dist(self, root_dist=0)[not_root]\n\n        # Radii for top and bottom of tapered cylinder\n        nodes = self.nodes[not_root]\n        r1 = nodes.node_id.map(radii).values\n        r2 = nodes.parent_id.map(radii).values\n\n        return (np.pi * (r1 + r2) * np.sqrt( (r1-r2)**2 + h**2)).sum()\n\n    @property\n    @add_units(compact=True, power=3)\n    def volume(self) -&gt; float:\n        \"\"\"Radius-based volume.\"\"\"\n        if 'radius' not in self.nodes.columns:\n            raise ValueError(f'Neuron {self.id} does not have radius information')\n\n        if any(self.nodes.radius &lt; 0):\n            logger.warning(f'Neuron {self.id} has negative radii - volume will not be correct.')\n\n        if any(self.nodes.radius.isnull()):\n            logger.warning(f'Neuron {self.id} has NaN radii - volume will not be correct.')\n\n        # Generate radius dict\n        radii = self.nodes.set_index('node_id').radius.to_dict()\n        # Drop root node(s)\n        not_root = self.nodes.parent_id &gt;= 0\n        # For each cylinder get the height\n        h = morpho.mmetrics.parent_dist(self, root_dist=0)[not_root]\n\n        # Radii for top and bottom of tapered cylinder\n        nodes = self.nodes[not_root]\n        r1 = nodes.node_id.map(radii).values\n        r2 = nodes.parent_id.map(radii).values\n\n        return (1/3 * np.pi * (r1**2 + r1 * r2 + r2**2) * h).sum()\n\n    @property\n    def bbox(self) -&gt; np.ndarray:\n        \"\"\"Bounding box (includes connectors).\"\"\"\n        mn = np.min(self.nodes[['x', 'y', 'z']].values, axis=0)\n        mx = np.max(self.nodes[['x', 'y', 'z']].values, axis=0)\n\n        if self.has_connectors:\n            cn_mn = np.min(self.connectors[['x', 'y', 'z']].values, axis=0)\n            cn_mx = np.max(self.connectors[['x', 'y', 'z']].values, axis=0)\n\n            mn = np.min(np.vstack((mn, cn_mn)), axis=0)\n            mx = np.max(np.vstack((mx, cn_mx)), axis=0)\n\n        return np.vstack((mn, mx)).T\n\n    @property\n    def sampling_resolution(self) -&gt; float:\n        \"\"\"Average cable length between child -&gt; parent nodes.\"\"\"\n        res = self.cable_length / self.n_nodes\n\n        if isinstance(res, pint.Quantity):\n            res = res.to_compact()\n\n        return res\n\n    @property\n    @temp_property\n    def segments(self) -&gt; List[list]:\n        \"\"\"Neuron broken down into linear segments (see also `.small_segments`).\"\"\"\n        # Calculate if required\n        if not hasattr(self, '_segments'):\n            # This also sets the attribute\n            self._segments = self._get_segments(how='length')\n        return self._segments\n\n    @property\n    @temp_property\n    def small_segments(self) -&gt; List[list]:\n        \"\"\"Neuron broken down into small linear segments (see also `.segments`).\"\"\"\n        # Calculate if required\n        if not hasattr(self, '_small_segments'):\n            # This also sets the attribute\n            self._small_segments = self._get_segments(how='break')\n        return self._small_segments\n\n    def _get_segments(self,\n                      how: Union[Literal['length'],\n                                 Literal['break']] = 'length'\n                      ) -&gt; List[list]:\n        \"\"\"Generate segments for neuron.\"\"\"\n        if how == 'length':\n            return graph._generate_segments(self)\n        elif how == 'break':\n            return graph._break_segments(self)\n        else:\n            raise ValueError(f'Unknown method: \"{how}\"')\n\n    @property\n    def n_skeletons(self) -&gt; int:\n        \"\"\"Number of seperate skeletons in this neuron.\"\"\"\n        return len(self.root)\n\n    def _clear_temp_attr(self, exclude: list = []) -&gt; None:\n        \"\"\"Clear temporary attributes.\"\"\"\n        super()._clear_temp_attr(exclude=exclude)\n\n        # Remove temporary node values\n        # temp_node_cols = ['flow_centrality', 'strahler_index', 'SI', 'bending_flow']\n        # self._nodes.drop(columns=temp_node_cols, errors='ignore', inplace=True)\n\n        # Remove soma if it was manually assigned and is not present anymore\n        if not callable(self._soma) and not isinstance(self._soma, type(None)):\n            if utils.is_iterable(self._soma):\n                exists = np.isin(self._soma, self.nodes.node_id.values)\n                self._soma = np.asarray(self._soma)[exists]\n                if not np.any(self._soma):\n                    self._soma = None\n            elif self._soma not in self.nodes.node_id.values:\n                self.soma = None\n\n        if 'classify_nodes' not in exclude:\n            # Reclassify nodes\n            graph.classify_nodes(self, inplace=True)\n\n    def copy(self, deepcopy: bool = False) -&gt; 'TreeNeuron':\n        \"\"\"Return a copy of the neuron.\n\n        Parameters\n        ----------\n        deepcopy :  bool, optional\n                    If False, `.graph` (NetworkX DiGraph) will be returned\n                    as view - changes to nodes/edges can progagate back!\n                    `.igraph` (iGraph) - if available - will always be\n                    deepcopied.\n\n        Returns\n        -------\n        TreeNeuron\n\n        \"\"\"\n        no_copy = ['_lock']\n        # Generate new empty neuron\n        x = self.__class__(None)\n        # Populate with this neuron's data\n        x.__dict__.update({k: copy.copy(v) for k, v in self.__dict__.items() if k not in no_copy})\n\n        # Copy graphs only if neuron is not stale\n        if not self.is_stale:\n            if '_graph_nx' in self.__dict__:\n                x._graph_nx = self._graph_nx.copy(as_view=deepcopy is not True)\n            if '_igraph' in self.__dict__:\n                if self._igraph is not None:\n                    # This is pretty cheap, so we will always make a deep copy\n                    x._igraph = self._igraph.copy()\n        else:\n            x._clear_temp_attr()\n\n        return x\n\n    def get_graph_nx(self) -&gt; nx.DiGraph:\n        \"\"\"Calculate and return networkX representation of neuron.\n\n        Once calculated stored as `.graph`. Call function again to update\n        graph.\n\n        See Also\n        --------\n        [`navis.neuron2nx`][]\n\n        \"\"\"\n        self._graph_nx = graph.neuron2nx(self)\n        return self._graph_nx\n\n    def get_igraph(self) -&gt; 'igraph.Graph':  # type: ignore\n        \"\"\"Calculate and return iGraph representation of neuron.\n\n        Once calculated stored as `.igraph`. Call function again to update\n        iGraph.\n\n        Important\n        ---------\n        Returns `None` if igraph is not installed!\n\n        See Also\n        --------\n        [`navis.neuron2igraph`][]\n\n        \"\"\"\n        self._igraph = graph.neuron2igraph(self, raise_not_installed=False)\n        return self._igraph\n\n    @overload\n    def resample(self, resample_to: int, inplace: Literal[False]) -&gt; 'TreeNeuron': ...\n\n    @overload\n    def resample(self, resample_to: int, inplace: Literal[True]) -&gt; None: ...\n\n    def resample(self, resample_to, inplace=False):\n        \"\"\"Resample neuron to given resolution.\n\n        Parameters\n        ----------\n        resample_to :           int\n                                Resolution to which to resample the neuron.\n        inplace :               bool, optional\n                                If True, operation will be performed on\n                                itself. If False, operation is performed on\n                                copy which is then returned.\n\n        See Also\n        --------\n        [`navis.resample_skeleton`][]\n            Base function. See for details and examples.\n\n        \"\"\"\n        if inplace:\n            x = self\n        else:\n            x = self.copy(deepcopy=False)\n\n        sampling.resample_skeleton(x, resample_to, inplace=True)\n\n        # No need to call this as base function does this for us\n        # x._clear_temp_attr()\n\n        if not inplace:\n            return x\n        return None\n\n    @overload\n    def downsample(self,\n                   factor: float,\n                   inplace: Literal[False],\n                   **kwargs) -&gt; 'TreeNeuron': ...\n\n    @overload\n    def downsample(self,\n                   factor: float,\n                   inplace: Literal[True],\n                   **kwargs) -&gt; None: ...\n\n    def downsample(self, factor=5, inplace=False, **kwargs):\n        \"\"\"Downsample the neuron by given factor.\n\n        Parameters\n        ----------\n        factor :                int, optional\n                                Factor by which to downsample the neurons.\n                                Default = 5.\n        inplace :               bool, optional\n                                If True, operation will be performed on\n                                itself. If False, operation is performed on\n                                copy which is then returned.\n        **kwargs\n                                Additional arguments passed to\n                                [`navis.downsample_neuron`][].\n\n        See Also\n        --------\n        [`navis.downsample_neuron`][]\n            Base function. See for details and examples.\n\n        \"\"\"\n        if inplace:\n            x = self\n        else:\n            x = self.copy(deepcopy=False)\n\n        sampling.downsample_neuron(x, factor, inplace=True, **kwargs)\n\n        # Delete outdated attributes\n        x._clear_temp_attr()\n\n        if not inplace:\n            return x\n        return None\n\n    def reroot(self,\n               new_root: Union[int, str],\n               inplace: bool = False) -&gt; Optional['TreeNeuron']:\n        \"\"\"Reroot neuron to given node ID or node tag.\n\n        Parameters\n        ----------\n        new_root :  int | str\n                    Either node ID or node tag.\n        inplace :   bool, optional\n                    If True, operation will be performed on itself. If False,\n                    operation is performed on copy which is then returned.\n\n        See Also\n        --------\n        [`navis.reroot_skeleton`][]\n            Base function. See for details and examples.\n\n        \"\"\"\n        if inplace:\n            x = self\n        else:\n            x = self.copy(deepcopy=False)\n\n        graph.reroot_skeleton(x, new_root, inplace=True)\n\n        # Clear temporary attributes is done by morpho.reroot_skeleton()\n        # x._clear_temp_attr()\n\n        if not inplace:\n            return x\n        return None\n\n    def prune_distal_to(self,\n                        node: Union[str, int],\n                        inplace: bool = False) -&gt; Optional['TreeNeuron']:\n        \"\"\"Cut off nodes distal to given nodes.\n\n        Parameters\n        ----------\n        node :      node ID | node tag\n                    Provide either node ID(s) or a unique tag(s)\n        inplace :   bool, optional\n                    If True, operation will be performed on itself. If False,\n                    operation is performed on copy which is then returned.\n\n        See Also\n        --------\n        [`navis.cut_skeleton`][]\n            Base function. See for details and examples.\n\n        \"\"\"\n        if inplace:\n            x = self\n        else:\n            x = self.copy(deepcopy=False)\n\n        node = utils.make_iterable(node, force_type=None)\n\n        for n in node:\n            prox = graph.cut_skeleton(x, n, ret='proximal')[0]\n            # Reinitialise with proximal data\n            x.__init__(prox)  # type: ignore  # Cannot access \"__init__\" directly\n            # Remove potential \"left over\" attributes (happens if we use a copy)\n            x._clear_temp_attr()\n\n        if not inplace:\n            return x\n        return None\n\n    def prune_proximal_to(self,\n                          node: Union[str, int],\n                          inplace: bool = False) -&gt; Optional['TreeNeuron']:\n        \"\"\"Remove nodes proximal to given node. Reroots neuron to cut node.\n\n        Parameters\n        ----------\n        node :      node_id | node tag\n                    Provide either a node ID or a (unique) tag\n        inplace :   bool, optional\n                    If True, operation will be performed on itself. If False,\n                    operation is performed on copy which is then returned.\n\n        See Also\n        --------\n        [`navis.cut_skeleton`][]\n            Base function. See for details and examples.\n\n        \"\"\"\n        if inplace:\n            x = self\n        else:\n            x = self.copy(deepcopy=False)\n\n        node = utils.make_iterable(node, force_type=None)\n\n        for n in node:\n            dist = graph.cut_skeleton(x, n, ret='distal')[0]\n            # Reinitialise with distal data\n            x.__init__(dist)  # type: ignore  # Cannot access \"__init__\" directly\n            # Remove potential \"left over\" attributes (happens if we use a copy)\n            x._clear_temp_attr()\n\n        # Clear temporary attributes is done by cut_skeleton\n        # x._clear_temp_attr()\n\n        if not inplace:\n            return x\n        return None\n\n    def prune_by_strahler(self,\n                          to_prune: Union[int, List[int], slice],\n                          inplace: bool = False) -&gt; Optional['TreeNeuron']:\n        \"\"\"Prune neuron based on [Strahler order](https://en.wikipedia.org/wiki/Strahler_number).\n\n        Will reroot neuron to soma if possible.\n\n        Parameters\n        ----------\n        to_prune :  int | list | range | slice\n                    Strahler indices to prune. For example:\n\n                    1. `to_prune=1` removes all leaf branches\n                    2. `to_prune=[1, 2]` removes SI 1 and 2\n                    3. `to_prune=range(1, 4)` removes SI 1, 2 and 3\n                    4. `to_prune=slice(1, -1)` removes everything but the\n                       highest SI\n                    5. `to_prune=slice(-1, None)` removes only the highest\n                       SI\n\n        inplace :   bool, optional\n                    If True, operation will be performed on itself. If False,\n                    operation is performed on copy which is then returned.\n\n        See Also\n        --------\n        [`navis.prune_by_strahler`][]\n            This is the base function. See for details and examples.\n\n        \"\"\"\n        if inplace:\n            x = self\n        else:\n            x = self.copy()\n\n        morpho.prune_by_strahler(\n            x, to_prune=to_prune, reroot_soma=True, inplace=True)\n\n        # No need to call this as morpho.prune_by_strahler does this already\n        # self._clear_temp_attr()\n\n        if not inplace:\n            return x\n        return None\n\n    def prune_twigs(self,\n                    size: float,\n                    inplace: bool = False,\n                    recursive: Union[int, bool, float] = False\n                    ) -&gt; Optional['TreeNeuron']:\n        \"\"\"Prune terminal twigs under a given size.\n\n        Parameters\n        ----------\n        size :          int | float\n                        Twigs shorter than this will be pruned.\n        inplace :       bool, optional\n                        If False, pruning is performed on copy of original neuron\n                        which is then returned.\n        recursive :     int | bool | \"inf\", optional\n                        If `int` will undergo that many rounds of recursive\n                        pruning. Use `float(\"inf\")` to prune until no more\n                        twigs under the given size are left.\n\n        See Also\n        --------\n        [`navis.prune_twigs`][]\n            This is the base function. See for details and examples.\n\n        \"\"\"\n        if inplace:\n            x = self\n        else:\n            x = self.copy()\n\n        morpho.prune_twigs(x, size=size, inplace=True)\n\n        if not inplace:\n            return x\n        return None\n\n    def prune_at_depth(self,\n                       depth: Union[float, int],\n                       source: Optional[int] = None,\n                       inplace: bool = False\n                       ) -&gt; Optional['TreeNeuron']:\n        \"\"\"Prune all neurites past a given distance from a source.\n\n        Parameters\n        ----------\n        x :             TreeNeuron | NeuronList\n        depth :         int | float\n                        Distance from source at which to start pruning.\n        source :        int, optional\n                        Source node for depth calculation. If `None`, will use\n                        root. If `x` is a list of neurons then must provide a\n                        source for each neuron.\n        inplace :       bool, optional\n                        If False, pruning is performed on copy of original neuron\n                        which is then returned.\n\n        Returns\n        -------\n        TreeNeuron/List\n                        Pruned neuron(s).\n\n        See Also\n        --------\n        [`navis.prune_at_depth`][]\n            This is the base function. See for details and examples.\n\n        \"\"\"\n        if inplace:\n            x = self\n        else:\n            x = self.copy()\n\n        morpho.prune_at_depth(x, depth=depth, source=source, inplace=True)\n\n        if not inplace:\n            return x\n        return None\n\n    def cell_body_fiber(self,\n                        reroot_soma: bool = True,\n                        inplace: bool = False,\n                        ) -&gt; Optional['TreeNeuron']:\n        \"\"\"Prune neuron to its cell body fiber.\n\n        Parameters\n        ----------\n        reroot_soma :       bool, optional\n                            If True, will reroot to soma.\n        inplace :           bool, optional\n                            If True, operation will be performed on itself.\n                            If False, operation is performed on copy which is\n                            then returned.\n\n        See Also\n        --------\n        [`navis.cell_body_fiber`][]\n            This is the base function. See for details and examples.\n\n        \"\"\"\n        if inplace:\n            x = self\n        else:\n            x = self.copy()\n\n        morpho.cell_body_fiber(x, inplace=True, reroot_soma=reroot_soma)\n\n        # Clear temporary attributes\n        x._clear_temp_attr()\n\n        if not inplace:\n            return x\n        return None\n\n    def prune_by_longest_neurite(self,\n                                 n: int = 1,\n                                 reroot_soma: bool = False,\n                                 inplace: bool = False,\n                                 ) -&gt; Optional['TreeNeuron']:\n        \"\"\"Prune neuron down to the longest neurite.\n\n        Parameters\n        ----------\n        n :                 int, optional\n                            Number of longest neurites to preserve.\n        reroot_soma :       bool, optional\n                            If True, will reroot to soma before pruning.\n        inplace :           bool, optional\n                            If True, operation will be performed on itself.\n                            If False, operation is performed on copy which is\n                            then returned.\n\n        See Also\n        --------\n        [`navis.longest_neurite`][]\n            This is the base function. See for details and examples.\n\n        \"\"\"\n        if inplace:\n            x = self\n        else:\n            x = self.copy()\n\n        graph.longest_neurite(\n            x, n, inplace=True, reroot_soma=reroot_soma)\n\n        # Clear temporary attributes\n        x._clear_temp_attr()\n\n        if not inplace:\n            return x\n        return None\n\n    def prune_by_volume(self,\n                        v: Union[core.Volume,\n                                 List[core.Volume],\n                                 Dict[str, core.Volume]],\n                        mode: Union[Literal['IN'], Literal['OUT']] = 'IN',\n                        prevent_fragments: bool = False,\n                        inplace: bool = False\n                        ) -&gt; Optional['TreeNeuron']:\n        \"\"\"Prune neuron by intersection with given volume(s).\n\n        Parameters\n        ----------\n        v :                 str | navis.Volume | list of either\n                            Volume(s) to check for intersection\n        mode :              'IN' | 'OUT', optional\n                            If 'IN', parts of the neuron inside the volume are\n                            kept.\n        prevent_fragments : bool, optional\n                            If True, will add nodes to `subset` required to\n                            keep neuron from fragmenting.\n        inplace :           bool, optional\n                            If True, operation will be performed on itself. If\n                            False, operation is performed on copy which is then\n                            returned.\n\n        See Also\n        --------\n        [`navis.in_volume`][]\n            Base function. See for details and examples.\n\n        \"\"\"\n        if inplace:\n            x = self\n        else:\n            x = self.copy()\n\n        intersection.in_volume(x, v, inplace=True,\n                               prevent_fragments=prevent_fragments,\n                               mode=mode)\n\n        # Clear temporary attributes\n        # x._clear_temp_attr()\n\n        if not inplace:\n            return x\n        return None\n\n    def to_swc(self,\n               filename: Optional[str] = None,\n               **kwargs) -&gt; None:\n        \"\"\"Generate SWC file from this neuron.\n\n        Parameters\n        ----------\n        filename :      str | None, optional\n                        If `None`, will use \"neuron_{id}.swc\".\n        kwargs\n                        Additional arguments passed to [`navis.write_swc`][].\n\n        Returns\n        -------\n        Nothing\n\n        See Also\n        --------\n        [`navis.write_swc`][]\n                See this function for further details.\n\n        \"\"\"\n        return io.write_swc(self, filename, **kwargs)  # type: ignore  # double import of \"io\"\n\n    def reload(self,\n               inplace: bool = False,\n               ) -&gt; Optional['TreeNeuron']:\n        \"\"\"Reload neuron. Must have filepath as `.origin` as attribute.\n\n        Returns\n        -------\n        TreeNeuron\n                If `inplace=False`.\n\n        \"\"\"\n        if not hasattr(self, 'origin'):\n            raise AttributeError('To reload TreeNeuron must have `.origin` '\n                                 'attribute')\n\n        if self.origin in ('DataFrame', 'string'):\n            raise ValueError('Unable to reload TreeNeuron: it appears to have '\n                             'been created from string or DataFrame.')\n\n        kwargs = {}\n        if hasattr(self, 'soma_label'):\n            kwargs['soma_label'] = self.soma_label\n        if hasattr(self, 'connector_labels'):\n            kwargs['connector_labels'] = self.connector_labels\n\n        x = io.read_swc(self.origin, **kwargs)\n\n        if inplace:\n            self.__dict__.update(x.__dict__)\n            self._clear_temp_attr()\n        else:\n            # This makes sure that we keep any additional data stored after\n            # this neuron has been loaded\n            x2 = self.copy()\n            x2.__dict__.update(x.__dict__)\n            x2._clear_temp_attr()\n            return x\n\n    def snap(self, locs, to='nodes'):\n        \"\"\"Snap xyz location(s) to closest node or synapse.\n\n        Parameters\n        ----------\n        locs :      (N, 3) array | (3, ) array\n                    Either single or multiple XYZ locations.\n        to :        \"nodes\" | \"connectors\"\n                    Whether to snap to nodes or connectors.\n\n        Returns\n        -------\n        id :        int | list of int\n                    ID(s) of the closest node/connector.\n        dist :      float | list of float\n                    Distance(s) to the closest node/connector.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import navis\n        &gt;&gt;&gt; n = navis.example_neurons(1)\n        &gt;&gt;&gt; id, dist = n.snap([0, 0, 0])\n        &gt;&gt;&gt; id\n        1124\n\n        \"\"\"\n        locs = np.asarray(locs).astype(np.float64)\n\n        is_single = (locs.ndim == 1 and len(locs) == 3)\n        is_multi = (locs.ndim == 2 and locs.shape[1] == 3)\n        if not is_single and not is_multi:\n            raise ValueError('Expected a single (x, y, z) location or a '\n                             '(N, 3) array of multiple locations')\n\n        if to not in ['nodes', 'connectors']:\n            raise ValueError('`to` must be \"nodes\" or \"connectors\", '\n                             f'got {to}')\n\n        # Generate tree\n        tree = graph.neuron2KDTree(self, data=to)\n\n        # Find the closest node\n        dist, ix = tree.query(locs)\n\n        if to == 'nodes':\n            id = self.nodes.node_id.values[ix]\n        else:\n            id = self.connectors.connector_id.values[ix]\n\n        return id, dist\n</code></pre>"},{"location":"reference/navis/#navis.TreeNeuron.adjacency_matrix","title":"<code>adjacency_matrix</code>  <code>property</code>","text":"<p>Adjacency matrix of the skeleton.</p>"},{"location":"reference/navis/#navis.TreeNeuron.bbox","title":"<code>bbox: np.ndarray</code>  <code>property</code>","text":"<p>Bounding box (includes connectors).</p>"},{"location":"reference/navis/#navis.TreeNeuron.branch_points","title":"<code>branch_points</code>  <code>property</code>","text":"<p>Branch node table.</p>"},{"location":"reference/navis/#navis.TreeNeuron.cable_length","title":"<code>cable_length: Union[int, float]</code>  <code>property</code>","text":"<p>Cable length.</p>"},{"location":"reference/navis/#navis.TreeNeuron.connectors","title":"<code>connectors: pd.DataFrame</code>  <code>property</code> <code>writable</code>","text":"<p>Connector table. If none, will return <code>None</code>.</p>"},{"location":"reference/navis/#navis.TreeNeuron.cycles","title":"<code>cycles: Optional[List[int]]</code>  <code>property</code>","text":"<p>Cycles in neuron (if any).</p> See also <p>networkx.find_cycles()             Function used to find cycles.</p>"},{"location":"reference/navis/#navis.TreeNeuron.edge_coords","title":"<code>edge_coords: np.ndarray</code>  <code>property</code>","text":"<p>Coordinates of edges between nodes.</p> See Also <p>edges         Same but with node IDs instead of x/y/z coordinates.</p>"},{"location":"reference/navis/#navis.TreeNeuron.edges","title":"<code>edges: np.ndarray</code>  <code>property</code>","text":"<p>Edges between nodes.</p> See Also <p>edge_coords         Same but with x/y/z coordinates instead of node IDs.</p>"},{"location":"reference/navis/#navis.TreeNeuron.ends","title":"<code>ends</code>  <code>property</code>","text":"<p>End node table (same as leafs).</p>"},{"location":"reference/navis/#navis.TreeNeuron.geodesic_matrix","title":"<code>geodesic_matrix</code>  <code>property</code>","text":"<p>Matrix with geodesic (along-the-arbor) distance between nodes.</p>"},{"location":"reference/navis/#navis.TreeNeuron.graph","title":"<code>graph: nx.DiGraph</code>  <code>property</code>","text":"<p>Networkx Graph representation of this neuron.</p>"},{"location":"reference/navis/#navis.TreeNeuron.igraph","title":"<code>igraph: igraph.Graph</code>  <code>property</code>","text":"<p>iGraph representation of this neuron.</p>"},{"location":"reference/navis/#navis.TreeNeuron.is_tree","title":"<code>is_tree: bool</code>  <code>property</code>","text":"<p>Whether neuron is a tree.</p> <p>Also returns True if neuron consists of multiple separate trees!</p> See also <p>networkx.is_forest()             Function used to test whether neuron is a tree. :attr:<code>TreeNeuron.cycles</code>             If your neuron is not a tree, this will help you identify             cycles.</p>"},{"location":"reference/navis/#navis.TreeNeuron.leafs","title":"<code>leafs: pd.DataFrame</code>  <code>property</code>","text":"<p>Leaf node table.</p>"},{"location":"reference/navis/#navis.TreeNeuron.n_branches","title":"<code>n_branches: Optional[int]</code>  <code>property</code>","text":"<p>Number of branch points.</p>"},{"location":"reference/navis/#navis.TreeNeuron.n_leafs","title":"<code>n_leafs: Optional[int]</code>  <code>property</code>","text":"<p>Number of leaf nodes.</p>"},{"location":"reference/navis/#navis.TreeNeuron.n_skeletons","title":"<code>n_skeletons: int</code>  <code>property</code>","text":"<p>Number of seperate skeletons in this neuron.</p>"},{"location":"reference/navis/#navis.TreeNeuron.n_trees","title":"<code>n_trees: int</code>  <code>property</code>","text":"<p>Count number of connected trees in this neuron.</p>"},{"location":"reference/navis/#navis.TreeNeuron.nodes","title":"<code>nodes: pd.DataFrame</code>  <code>property</code> <code>writable</code>","text":"<p>Node table.</p>"},{"location":"reference/navis/#navis.TreeNeuron.root","title":"<code>root: Sequence</code>  <code>property</code> <code>writable</code>","text":"<p>Root node(s).</p>"},{"location":"reference/navis/#navis.TreeNeuron.sampling_resolution","title":"<code>sampling_resolution: float</code>  <code>property</code>","text":"<p>Average cable length between child -&gt; parent nodes.</p>"},{"location":"reference/navis/#navis.TreeNeuron.segments","title":"<code>segments: List[list]</code>  <code>property</code>","text":"<p>Neuron broken down into linear segments (see also <code>.small_segments</code>).</p>"},{"location":"reference/navis/#navis.TreeNeuron.simple","title":"<code>simple: TreeNeuron</code>  <code>property</code>","text":"<p>Simplified representation consisting only of root, branch points and leafs.</p>"},{"location":"reference/navis/#navis.TreeNeuron.small_segments","title":"<code>small_segments: List[list]</code>  <code>property</code>","text":"<p>Neuron broken down into small linear segments (see also <code>.segments</code>).</p>"},{"location":"reference/navis/#navis.TreeNeuron.soma","title":"<code>soma: Optional[Union[str, int]]</code>  <code>property</code> <code>writable</code>","text":"<p>Search for soma and return node ID(s).</p> <p><code>None</code> if no soma. You can assign either a function that accepts a TreeNeuron as input or a fix value. The default is <code>navis.find_soma</code>.</p>"},{"location":"reference/navis/#navis.TreeNeuron.soma_pos","title":"<code>soma_pos: Optional[Sequence]</code>  <code>property</code> <code>writable</code>","text":"<p>Search for soma and return its position.</p> <p>Returns <code>None</code> if no soma. You can also use this to assign a soma by position in which case it will snap to the closest node.</p>"},{"location":"reference/navis/#navis.TreeNeuron.subtrees","title":"<code>subtrees: List[List[int]]</code>  <code>property</code>","text":"<p>List of subtrees. Sorted by size as sets of node IDs.</p>"},{"location":"reference/navis/#navis.TreeNeuron.surface_area","title":"<code>surface_area: float</code>  <code>property</code>","text":"<p>Radius-based lateral surface area.</p>"},{"location":"reference/navis/#navis.TreeNeuron.type","title":"<code>type: str</code>  <code>property</code>","text":"<p>Neuron type.</p>"},{"location":"reference/navis/#navis.TreeNeuron.vertices","title":"<code>vertices: np.ndarray</code>  <code>property</code>","text":"<p>Vertices of the skeleton.</p>"},{"location":"reference/navis/#navis.TreeNeuron.volume","title":"<code>volume: float</code>  <code>property</code>","text":"<p>Radius-based volume.</p>"},{"location":"reference/navis/#navis.TreeNeuron.__init__","title":"<code>__init__</code>","text":"<p>Initialize Skeleton Neuron.</p> Source code in <code>navis/core/skeleton.py</code> <pre><code>def __init__(self,\n             x: Union[pd.DataFrame,\n                      BufferedIOBase,\n                      str,\n                      'TreeNeuron',\n                      nx.DiGraph],\n             units: Union[pint.Unit, str] = None,\n             **metadata\n             ):\n    \"\"\"Initialize Skeleton Neuron.\"\"\"\n    super().__init__()\n\n    # Lock neuron during construction\n    self._lock = 1\n\n    if isinstance(x, pd.DataFrame):\n        self.nodes = x\n    elif isinstance(x, pd.Series):\n        if not hasattr(x, 'nodes'):\n            raise ValueError('pandas.Series must have `nodes` entry.')\n        elif not isinstance(x.nodes, pd.DataFrame):\n            raise TypeError(f'Nodes must be pandas DataFrame, got \"{type(x.nodes)}\"')\n        self.nodes = x.nodes\n        metadata.update(x.to_dict())\n    elif isinstance(x, nx.Graph):\n        self.nodes = graph.nx2neuron(x).nodes\n    elif isinstance(x, BufferedIOBase) or isinstance(x, str):\n        x = io.read_swc(x)  # type: ignore\n        self.__dict__.update(x.__dict__)\n    elif isinstance(x, sk.Skeleton):\n        self.nodes = x.swc.copy()\n        self.vertex_map = x.mesh_map\n    elif isinstance(x, TreeNeuron):\n        self.__dict__.update(x.copy().__dict__)\n        # Try to copy every attribute\n        for at in self.__dict__:\n            try:\n                setattr(self, at, copy.copy(getattr(self, at)))\n            except BaseException:\n                logger.warning(f'Unable to deep-copy attribute \"{at}\"')\n    elif isinstance(x, tuple):\n        # Tuple of vertices and edges\n        if len(x) != 2:\n            raise ValueError('Tuple must have 2 elements: vertices and edges.')\n        self.nodes = graph.edges2neuron(edges=x[1], vertices=x[0]).nodes\n    elif isinstance(x, type(None)):\n        # This is a essentially an empty neuron\n        pass\n    else:\n        raise utils.ConstructionError(f'Unable to construct TreeNeuron from \"{type(x)}\"')\n\n    for k, v in metadata.items():\n        try:\n            setattr(self, k, v)\n        except AttributeError:\n            raise AttributeError(f\"Unable to set neuron's `{k}` attribute.\")\n\n    self.units = units\n    self._current_md5 = self.core_md5\n\n    self._lock = 0\n</code></pre>"},{"location":"reference/navis/#navis.TreeNeuron.cell_body_fiber","title":"<code>cell_body_fiber</code>","text":"<p>Prune neuron to its cell body fiber.</p> PARAMETER DESCRIPTION <code>reroot_soma</code> <pre><code>            If True, will reroot to soma.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>True</code> </p> <code>inplace</code> <pre><code>            If True, operation will be performed on itself.\n            If False, operation is performed on copy which is\n            then returned.\n</code></pre> <p> TYPE: <code>          bool</code> DEFAULT: <code>False</code> </p> See Also <p><code>navis.cell_body_fiber</code>     This is the base function. See for details and examples.</p> Source code in <code>navis/core/skeleton.py</code> <pre><code>def cell_body_fiber(self,\n                    reroot_soma: bool = True,\n                    inplace: bool = False,\n                    ) -&gt; Optional['TreeNeuron']:\n    \"\"\"Prune neuron to its cell body fiber.\n\n    Parameters\n    ----------\n    reroot_soma :       bool, optional\n                        If True, will reroot to soma.\n    inplace :           bool, optional\n                        If True, operation will be performed on itself.\n                        If False, operation is performed on copy which is\n                        then returned.\n\n    See Also\n    --------\n    [`navis.cell_body_fiber`][]\n        This is the base function. See for details and examples.\n\n    \"\"\"\n    if inplace:\n        x = self\n    else:\n        x = self.copy()\n\n    morpho.cell_body_fiber(x, inplace=True, reroot_soma=reroot_soma)\n\n    # Clear temporary attributes\n    x._clear_temp_attr()\n\n    if not inplace:\n        return x\n    return None\n</code></pre>"},{"location":"reference/navis/#navis.TreeNeuron.copy","title":"<code>copy</code>","text":"<p>Return a copy of the neuron.</p> PARAMETER DESCRIPTION <code>deepcopy</code> <pre><code>    If False, `.graph` (NetworkX DiGraph) will be returned\n    as view - changes to nodes/edges can progagate back!\n    `.igraph` (iGraph) - if available - will always be\n    deepcopied.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>TreeNeuron</code> Source code in <code>navis/core/skeleton.py</code> <pre><code>def copy(self, deepcopy: bool = False) -&gt; 'TreeNeuron':\n    \"\"\"Return a copy of the neuron.\n\n    Parameters\n    ----------\n    deepcopy :  bool, optional\n                If False, `.graph` (NetworkX DiGraph) will be returned\n                as view - changes to nodes/edges can progagate back!\n                `.igraph` (iGraph) - if available - will always be\n                deepcopied.\n\n    Returns\n    -------\n    TreeNeuron\n\n    \"\"\"\n    no_copy = ['_lock']\n    # Generate new empty neuron\n    x = self.__class__(None)\n    # Populate with this neuron's data\n    x.__dict__.update({k: copy.copy(v) for k, v in self.__dict__.items() if k not in no_copy})\n\n    # Copy graphs only if neuron is not stale\n    if not self.is_stale:\n        if '_graph_nx' in self.__dict__:\n            x._graph_nx = self._graph_nx.copy(as_view=deepcopy is not True)\n        if '_igraph' in self.__dict__:\n            if self._igraph is not None:\n                # This is pretty cheap, so we will always make a deep copy\n                x._igraph = self._igraph.copy()\n    else:\n        x._clear_temp_attr()\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.TreeNeuron.downsample","title":"<code>downsample</code>","text":"<pre><code>downsample\n</code></pre><pre><code>downsample\n</code></pre> <p>Downsample the neuron by given factor.</p> PARAMETER DESCRIPTION <code>factor</code> <pre><code>                Factor by which to downsample the neurons.\n                Default = 5.\n</code></pre> <p> TYPE: <code>               int</code> DEFAULT: <code>5</code> </p> <code>inplace</code> <pre><code>                If True, operation will be performed on\n                itself. If False, operation is performed on\n                copy which is then returned.\n</code></pre> <p> TYPE: <code>              bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <pre><code>                Additional arguments passed to\n                [`navis.downsample_neuron`][].\n</code></pre> <p> DEFAULT: <code>{}</code> </p> See Also <p><code>navis.downsample_neuron</code>     Base function. See for details and examples.</p> Source code in <code>navis/core/skeleton.py</code> <pre><code>def downsample(self, factor=5, inplace=False, **kwargs):\n    \"\"\"Downsample the neuron by given factor.\n\n    Parameters\n    ----------\n    factor :                int, optional\n                            Factor by which to downsample the neurons.\n                            Default = 5.\n    inplace :               bool, optional\n                            If True, operation will be performed on\n                            itself. If False, operation is performed on\n                            copy which is then returned.\n    **kwargs\n                            Additional arguments passed to\n                            [`navis.downsample_neuron`][].\n\n    See Also\n    --------\n    [`navis.downsample_neuron`][]\n        Base function. See for details and examples.\n\n    \"\"\"\n    if inplace:\n        x = self\n    else:\n        x = self.copy(deepcopy=False)\n\n    sampling.downsample_neuron(x, factor, inplace=True, **kwargs)\n\n    # Delete outdated attributes\n    x._clear_temp_attr()\n\n    if not inplace:\n        return x\n    return None\n</code></pre>"},{"location":"reference/navis/#navis.TreeNeuron.get_graph_nx","title":"<code>get_graph_nx</code>","text":"<p>Calculate and return networkX representation of neuron.</p> <p>Once calculated stored as <code>.graph</code>. Call function again to update graph.</p> See Also <p><code>navis.neuron2nx</code></p> Source code in <code>navis/core/skeleton.py</code> <pre><code>def get_graph_nx(self) -&gt; nx.DiGraph:\n    \"\"\"Calculate and return networkX representation of neuron.\n\n    Once calculated stored as `.graph`. Call function again to update\n    graph.\n\n    See Also\n    --------\n    [`navis.neuron2nx`][]\n\n    \"\"\"\n    self._graph_nx = graph.neuron2nx(self)\n    return self._graph_nx\n</code></pre>"},{"location":"reference/navis/#navis.TreeNeuron.get_igraph","title":"<code>get_igraph</code>","text":"<p>Calculate and return iGraph representation of neuron.</p> <p>Once calculated stored as <code>.igraph</code>. Call function again to update iGraph.</p> Important <p>Returns <code>None</code> if igraph is not installed!</p> See Also <p><code>navis.neuron2igraph</code></p> Source code in <code>navis/core/skeleton.py</code> <pre><code>def get_igraph(self) -&gt; 'igraph.Graph':  # type: ignore\n    \"\"\"Calculate and return iGraph representation of neuron.\n\n    Once calculated stored as `.igraph`. Call function again to update\n    iGraph.\n\n    Important\n    ---------\n    Returns `None` if igraph is not installed!\n\n    See Also\n    --------\n    [`navis.neuron2igraph`][]\n\n    \"\"\"\n    self._igraph = graph.neuron2igraph(self, raise_not_installed=False)\n    return self._igraph\n</code></pre>"},{"location":"reference/navis/#navis.TreeNeuron.prune_at_depth","title":"<code>prune_at_depth</code>","text":"<p>Prune all neurites past a given distance from a source.</p> PARAMETER DESCRIPTION <code>x</code> <p> TYPE: <code>            TreeNeuron | NeuronList</code> </p> <code>depth</code> <pre><code>        Distance from source at which to start pruning.\n</code></pre> <p> TYPE: <code>        int | float</code> </p> <code>source</code> <pre><code>        Source node for depth calculation. If `None`, will use\n        root. If `x` is a list of neurons then must provide a\n        source for each neuron.\n</code></pre> <p> TYPE: <code>       int</code> DEFAULT: <code>None</code> </p> <code>inplace</code> <pre><code>        If False, pruning is performed on copy of original neuron\n        which is then returned.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>TreeNeuron / List</code> <p>Pruned neuron(s).</p> See Also <p><code>navis.prune_at_depth</code>     This is the base function. See for details and examples.</p> Source code in <code>navis/core/skeleton.py</code> <pre><code>def prune_at_depth(self,\n                   depth: Union[float, int],\n                   source: Optional[int] = None,\n                   inplace: bool = False\n                   ) -&gt; Optional['TreeNeuron']:\n    \"\"\"Prune all neurites past a given distance from a source.\n\n    Parameters\n    ----------\n    x :             TreeNeuron | NeuronList\n    depth :         int | float\n                    Distance from source at which to start pruning.\n    source :        int, optional\n                    Source node for depth calculation. If `None`, will use\n                    root. If `x` is a list of neurons then must provide a\n                    source for each neuron.\n    inplace :       bool, optional\n                    If False, pruning is performed on copy of original neuron\n                    which is then returned.\n\n    Returns\n    -------\n    TreeNeuron/List\n                    Pruned neuron(s).\n\n    See Also\n    --------\n    [`navis.prune_at_depth`][]\n        This is the base function. See for details and examples.\n\n    \"\"\"\n    if inplace:\n        x = self\n    else:\n        x = self.copy()\n\n    morpho.prune_at_depth(x, depth=depth, source=source, inplace=True)\n\n    if not inplace:\n        return x\n    return None\n</code></pre>"},{"location":"reference/navis/#navis.TreeNeuron.prune_by_longest_neurite","title":"<code>prune_by_longest_neurite</code>","text":"<p>Prune neuron down to the longest neurite.</p> PARAMETER DESCRIPTION <code>n</code> <pre><code>            Number of longest neurites to preserve.\n</code></pre> <p> TYPE: <code>                int</code> DEFAULT: <code>1</code> </p> <code>reroot_soma</code> <pre><code>            If True, will reroot to soma before pruning.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> <code>inplace</code> <pre><code>            If True, operation will be performed on itself.\n            If False, operation is performed on copy which is\n            then returned.\n</code></pre> <p> TYPE: <code>          bool</code> DEFAULT: <code>False</code> </p> See Also <p><code>navis.longest_neurite</code>     This is the base function. See for details and examples.</p> Source code in <code>navis/core/skeleton.py</code> <pre><code>def prune_by_longest_neurite(self,\n                             n: int = 1,\n                             reroot_soma: bool = False,\n                             inplace: bool = False,\n                             ) -&gt; Optional['TreeNeuron']:\n    \"\"\"Prune neuron down to the longest neurite.\n\n    Parameters\n    ----------\n    n :                 int, optional\n                        Number of longest neurites to preserve.\n    reroot_soma :       bool, optional\n                        If True, will reroot to soma before pruning.\n    inplace :           bool, optional\n                        If True, operation will be performed on itself.\n                        If False, operation is performed on copy which is\n                        then returned.\n\n    See Also\n    --------\n    [`navis.longest_neurite`][]\n        This is the base function. See for details and examples.\n\n    \"\"\"\n    if inplace:\n        x = self\n    else:\n        x = self.copy()\n\n    graph.longest_neurite(\n        x, n, inplace=True, reroot_soma=reroot_soma)\n\n    # Clear temporary attributes\n    x._clear_temp_attr()\n\n    if not inplace:\n        return x\n    return None\n</code></pre>"},{"location":"reference/navis/#navis.TreeNeuron.prune_by_strahler","title":"<code>prune_by_strahler</code>","text":"<p>Prune neuron based on Strahler order.</p> <p>Will reroot neuron to soma if possible.</p> PARAMETER DESCRIPTION <code>to_prune</code> <pre><code>    Strahler indices to prune. For example:\n\n    1. `to_prune=1` removes all leaf branches\n    2. `to_prune=[1, 2]` removes SI 1 and 2\n    3. `to_prune=range(1, 4)` removes SI 1, 2 and 3\n    4. `to_prune=slice(1, -1)` removes everything but the\n       highest SI\n    5. `to_prune=slice(-1, None)` removes only the highest\n       SI\n</code></pre> <p> TYPE: <code> int | list | range | slice</code> </p> <code>inplace</code> <pre><code>    If True, operation will be performed on itself. If False,\n    operation is performed on copy which is then returned.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> See Also <p><code>navis.prune_by_strahler</code>     This is the base function. See for details and examples.</p> Source code in <code>navis/core/skeleton.py</code> <pre><code>def prune_by_strahler(self,\n                      to_prune: Union[int, List[int], slice],\n                      inplace: bool = False) -&gt; Optional['TreeNeuron']:\n    \"\"\"Prune neuron based on [Strahler order](https://en.wikipedia.org/wiki/Strahler_number).\n\n    Will reroot neuron to soma if possible.\n\n    Parameters\n    ----------\n    to_prune :  int | list | range | slice\n                Strahler indices to prune. For example:\n\n                1. `to_prune=1` removes all leaf branches\n                2. `to_prune=[1, 2]` removes SI 1 and 2\n                3. `to_prune=range(1, 4)` removes SI 1, 2 and 3\n                4. `to_prune=slice(1, -1)` removes everything but the\n                   highest SI\n                5. `to_prune=slice(-1, None)` removes only the highest\n                   SI\n\n    inplace :   bool, optional\n                If True, operation will be performed on itself. If False,\n                operation is performed on copy which is then returned.\n\n    See Also\n    --------\n    [`navis.prune_by_strahler`][]\n        This is the base function. See for details and examples.\n\n    \"\"\"\n    if inplace:\n        x = self\n    else:\n        x = self.copy()\n\n    morpho.prune_by_strahler(\n        x, to_prune=to_prune, reroot_soma=True, inplace=True)\n\n    # No need to call this as morpho.prune_by_strahler does this already\n    # self._clear_temp_attr()\n\n    if not inplace:\n        return x\n    return None\n</code></pre>"},{"location":"reference/navis/#navis.TreeNeuron.prune_by_volume","title":"<code>prune_by_volume</code>","text":"<p>Prune neuron by intersection with given volume(s).</p> PARAMETER DESCRIPTION <code>v</code> <pre><code>            Volume(s) to check for intersection\n</code></pre> <p> TYPE: <code>                str | navis.Volume | list of either</code> </p> <code>mode</code> <pre><code>            If 'IN', parts of the neuron inside the volume are\n            kept.\n</code></pre> <p> TYPE: <code>             'IN' | 'OUT'</code> DEFAULT: <code>'IN'</code> </p> <code>prevent_fragments</code> <pre><code>            If True, will add nodes to `subset` required to\n            keep neuron from fragmenting.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>inplace</code> <pre><code>            If True, operation will be performed on itself. If\n            False, operation is performed on copy which is then\n            returned.\n</code></pre> <p> TYPE: <code>          bool</code> DEFAULT: <code>False</code> </p> See Also <p><code>navis.in_volume</code>     Base function. See for details and examples.</p> Source code in <code>navis/core/skeleton.py</code> <pre><code>def prune_by_volume(self,\n                    v: Union[core.Volume,\n                             List[core.Volume],\n                             Dict[str, core.Volume]],\n                    mode: Union[Literal['IN'], Literal['OUT']] = 'IN',\n                    prevent_fragments: bool = False,\n                    inplace: bool = False\n                    ) -&gt; Optional['TreeNeuron']:\n    \"\"\"Prune neuron by intersection with given volume(s).\n\n    Parameters\n    ----------\n    v :                 str | navis.Volume | list of either\n                        Volume(s) to check for intersection\n    mode :              'IN' | 'OUT', optional\n                        If 'IN', parts of the neuron inside the volume are\n                        kept.\n    prevent_fragments : bool, optional\n                        If True, will add nodes to `subset` required to\n                        keep neuron from fragmenting.\n    inplace :           bool, optional\n                        If True, operation will be performed on itself. If\n                        False, operation is performed on copy which is then\n                        returned.\n\n    See Also\n    --------\n    [`navis.in_volume`][]\n        Base function. See for details and examples.\n\n    \"\"\"\n    if inplace:\n        x = self\n    else:\n        x = self.copy()\n\n    intersection.in_volume(x, v, inplace=True,\n                           prevent_fragments=prevent_fragments,\n                           mode=mode)\n\n    # Clear temporary attributes\n    # x._clear_temp_attr()\n\n    if not inplace:\n        return x\n    return None\n</code></pre>"},{"location":"reference/navis/#navis.TreeNeuron.prune_distal_to","title":"<code>prune_distal_to</code>","text":"<p>Cut off nodes distal to given nodes.</p> PARAMETER DESCRIPTION <code>node</code> <pre><code>    Provide either node ID(s) or a unique tag(s)\n</code></pre> <p> TYPE: <code>     node ID | node tag</code> </p> <code>inplace</code> <pre><code>    If True, operation will be performed on itself. If False,\n    operation is performed on copy which is then returned.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> See Also <p><code>navis.cut_skeleton</code>     Base function. See for details and examples.</p> Source code in <code>navis/core/skeleton.py</code> <pre><code>def prune_distal_to(self,\n                    node: Union[str, int],\n                    inplace: bool = False) -&gt; Optional['TreeNeuron']:\n    \"\"\"Cut off nodes distal to given nodes.\n\n    Parameters\n    ----------\n    node :      node ID | node tag\n                Provide either node ID(s) or a unique tag(s)\n    inplace :   bool, optional\n                If True, operation will be performed on itself. If False,\n                operation is performed on copy which is then returned.\n\n    See Also\n    --------\n    [`navis.cut_skeleton`][]\n        Base function. See for details and examples.\n\n    \"\"\"\n    if inplace:\n        x = self\n    else:\n        x = self.copy(deepcopy=False)\n\n    node = utils.make_iterable(node, force_type=None)\n\n    for n in node:\n        prox = graph.cut_skeleton(x, n, ret='proximal')[0]\n        # Reinitialise with proximal data\n        x.__init__(prox)  # type: ignore  # Cannot access \"__init__\" directly\n        # Remove potential \"left over\" attributes (happens if we use a copy)\n        x._clear_temp_attr()\n\n    if not inplace:\n        return x\n    return None\n</code></pre>"},{"location":"reference/navis/#navis.TreeNeuron.prune_proximal_to","title":"<code>prune_proximal_to</code>","text":"<p>Remove nodes proximal to given node. Reroots neuron to cut node.</p> PARAMETER DESCRIPTION <code>node</code> <pre><code>    Provide either a node ID or a (unique) tag\n</code></pre> <p> TYPE: <code>     node_id | node tag</code> </p> <code>inplace</code> <pre><code>    If True, operation will be performed on itself. If False,\n    operation is performed on copy which is then returned.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> See Also <p><code>navis.cut_skeleton</code>     Base function. See for details and examples.</p> Source code in <code>navis/core/skeleton.py</code> <pre><code>def prune_proximal_to(self,\n                      node: Union[str, int],\n                      inplace: bool = False) -&gt; Optional['TreeNeuron']:\n    \"\"\"Remove nodes proximal to given node. Reroots neuron to cut node.\n\n    Parameters\n    ----------\n    node :      node_id | node tag\n                Provide either a node ID or a (unique) tag\n    inplace :   bool, optional\n                If True, operation will be performed on itself. If False,\n                operation is performed on copy which is then returned.\n\n    See Also\n    --------\n    [`navis.cut_skeleton`][]\n        Base function. See for details and examples.\n\n    \"\"\"\n    if inplace:\n        x = self\n    else:\n        x = self.copy(deepcopy=False)\n\n    node = utils.make_iterable(node, force_type=None)\n\n    for n in node:\n        dist = graph.cut_skeleton(x, n, ret='distal')[0]\n        # Reinitialise with distal data\n        x.__init__(dist)  # type: ignore  # Cannot access \"__init__\" directly\n        # Remove potential \"left over\" attributes (happens if we use a copy)\n        x._clear_temp_attr()\n\n    # Clear temporary attributes is done by cut_skeleton\n    # x._clear_temp_attr()\n\n    if not inplace:\n        return x\n    return None\n</code></pre>"},{"location":"reference/navis/#navis.TreeNeuron.prune_twigs","title":"<code>prune_twigs</code>","text":"<p>Prune terminal twigs under a given size.</p> PARAMETER DESCRIPTION <code>size</code> <pre><code>        Twigs shorter than this will be pruned.\n</code></pre> <p> TYPE: <code>         int | float</code> </p> <code>inplace</code> <pre><code>        If False, pruning is performed on copy of original neuron\n        which is then returned.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> <code>recursive</code> <pre><code>        If `int` will undergo that many rounds of recursive\n        pruning. Use `float(\"inf\")` to prune until no more\n        twigs under the given size are left.\n</code></pre> <p> TYPE: <code>    int | bool | \"inf\"</code> DEFAULT: <code>False</code> </p> See Also <p><code>navis.prune_twigs</code>     This is the base function. See for details and examples.</p> Source code in <code>navis/core/skeleton.py</code> <pre><code>def prune_twigs(self,\n                size: float,\n                inplace: bool = False,\n                recursive: Union[int, bool, float] = False\n                ) -&gt; Optional['TreeNeuron']:\n    \"\"\"Prune terminal twigs under a given size.\n\n    Parameters\n    ----------\n    size :          int | float\n                    Twigs shorter than this will be pruned.\n    inplace :       bool, optional\n                    If False, pruning is performed on copy of original neuron\n                    which is then returned.\n    recursive :     int | bool | \"inf\", optional\n                    If `int` will undergo that many rounds of recursive\n                    pruning. Use `float(\"inf\")` to prune until no more\n                    twigs under the given size are left.\n\n    See Also\n    --------\n    [`navis.prune_twigs`][]\n        This is the base function. See for details and examples.\n\n    \"\"\"\n    if inplace:\n        x = self\n    else:\n        x = self.copy()\n\n    morpho.prune_twigs(x, size=size, inplace=True)\n\n    if not inplace:\n        return x\n    return None\n</code></pre>"},{"location":"reference/navis/#navis.TreeNeuron.reload","title":"<code>reload</code>","text":"<p>Reload neuron. Must have filepath as <code>.origin</code> as attribute.</p> RETURNS DESCRIPTION <code>TreeNeuron</code> <p>If <code>inplace=False</code>.</p> Source code in <code>navis/core/skeleton.py</code> <pre><code>def reload(self,\n           inplace: bool = False,\n           ) -&gt; Optional['TreeNeuron']:\n    \"\"\"Reload neuron. Must have filepath as `.origin` as attribute.\n\n    Returns\n    -------\n    TreeNeuron\n            If `inplace=False`.\n\n    \"\"\"\n    if not hasattr(self, 'origin'):\n        raise AttributeError('To reload TreeNeuron must have `.origin` '\n                             'attribute')\n\n    if self.origin in ('DataFrame', 'string'):\n        raise ValueError('Unable to reload TreeNeuron: it appears to have '\n                         'been created from string or DataFrame.')\n\n    kwargs = {}\n    if hasattr(self, 'soma_label'):\n        kwargs['soma_label'] = self.soma_label\n    if hasattr(self, 'connector_labels'):\n        kwargs['connector_labels'] = self.connector_labels\n\n    x = io.read_swc(self.origin, **kwargs)\n\n    if inplace:\n        self.__dict__.update(x.__dict__)\n        self._clear_temp_attr()\n    else:\n        # This makes sure that we keep any additional data stored after\n        # this neuron has been loaded\n        x2 = self.copy()\n        x2.__dict__.update(x.__dict__)\n        x2._clear_temp_attr()\n        return x\n</code></pre>"},{"location":"reference/navis/#navis.TreeNeuron.reroot","title":"<code>reroot</code>","text":"<p>Reroot neuron to given node ID or node tag.</p> PARAMETER DESCRIPTION <code>new_root</code> <pre><code>    Either node ID or node tag.\n</code></pre> <p> TYPE: <code> int | str</code> </p> <code>inplace</code> <pre><code>    If True, operation will be performed on itself. If False,\n    operation is performed on copy which is then returned.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> See Also <p><code>navis.reroot_skeleton</code>     Base function. See for details and examples.</p> Source code in <code>navis/core/skeleton.py</code> <pre><code>def reroot(self,\n           new_root: Union[int, str],\n           inplace: bool = False) -&gt; Optional['TreeNeuron']:\n    \"\"\"Reroot neuron to given node ID or node tag.\n\n    Parameters\n    ----------\n    new_root :  int | str\n                Either node ID or node tag.\n    inplace :   bool, optional\n                If True, operation will be performed on itself. If False,\n                operation is performed on copy which is then returned.\n\n    See Also\n    --------\n    [`navis.reroot_skeleton`][]\n        Base function. See for details and examples.\n\n    \"\"\"\n    if inplace:\n        x = self\n    else:\n        x = self.copy(deepcopy=False)\n\n    graph.reroot_skeleton(x, new_root, inplace=True)\n\n    # Clear temporary attributes is done by morpho.reroot_skeleton()\n    # x._clear_temp_attr()\n\n    if not inplace:\n        return x\n    return None\n</code></pre>"},{"location":"reference/navis/#navis.TreeNeuron.resample","title":"<code>resample</code>","text":"<pre><code>resample\n</code></pre><pre><code>resample\n</code></pre> <p>Resample neuron to given resolution.</p> PARAMETER DESCRIPTION <code>resample_to</code> <pre><code>                Resolution to which to resample the neuron.\n</code></pre> <p> TYPE: <code>          int</code> </p> <code>inplace</code> <pre><code>                If True, operation will be performed on\n                itself. If False, operation is performed on\n                copy which is then returned.\n</code></pre> <p> TYPE: <code>              bool</code> DEFAULT: <code>False</code> </p> See Also <p><code>navis.resample_skeleton</code>     Base function. See for details and examples.</p> Source code in <code>navis/core/skeleton.py</code> <pre><code>def resample(self, resample_to, inplace=False):\n    \"\"\"Resample neuron to given resolution.\n\n    Parameters\n    ----------\n    resample_to :           int\n                            Resolution to which to resample the neuron.\n    inplace :               bool, optional\n                            If True, operation will be performed on\n                            itself. If False, operation is performed on\n                            copy which is then returned.\n\n    See Also\n    --------\n    [`navis.resample_skeleton`][]\n        Base function. See for details and examples.\n\n    \"\"\"\n    if inplace:\n        x = self\n    else:\n        x = self.copy(deepcopy=False)\n\n    sampling.resample_skeleton(x, resample_to, inplace=True)\n\n    # No need to call this as base function does this for us\n    # x._clear_temp_attr()\n\n    if not inplace:\n        return x\n    return None\n</code></pre>"},{"location":"reference/navis/#navis.TreeNeuron.snap","title":"<code>snap</code>","text":"<p>Snap xyz location(s) to closest node or synapse.</p> PARAMETER DESCRIPTION <code>locs</code> <pre><code>    Either single or multiple XYZ locations.\n</code></pre> <p> TYPE: <code>     (N, 3) array | (3, ) array</code> </p> <code>to</code> <pre><code>    Whether to snap to nodes or connectors.\n</code></pre> <p> TYPE: <code>       \"nodes\" | \"connectors\"</code> DEFAULT: <code>'nodes'</code> </p> RETURNS DESCRIPTION <code>id</code> <p>ID(s) of the closest node/connector.</p> <p> TYPE: <code>int | list of int</code> </p> <code>dist</code> <p>Distance(s) to the closest node/connector.</p> <p> TYPE: <code>float | list of float</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; id, dist = n.snap([0, 0, 0])\n&gt;&gt;&gt; id\n1124\n</code></pre> Source code in <code>navis/core/skeleton.py</code> <pre><code>def snap(self, locs, to='nodes'):\n    \"\"\"Snap xyz location(s) to closest node or synapse.\n\n    Parameters\n    ----------\n    locs :      (N, 3) array | (3, ) array\n                Either single or multiple XYZ locations.\n    to :        \"nodes\" | \"connectors\"\n                Whether to snap to nodes or connectors.\n\n    Returns\n    -------\n    id :        int | list of int\n                ID(s) of the closest node/connector.\n    dist :      float | list of float\n                Distance(s) to the closest node/connector.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; id, dist = n.snap([0, 0, 0])\n    &gt;&gt;&gt; id\n    1124\n\n    \"\"\"\n    locs = np.asarray(locs).astype(np.float64)\n\n    is_single = (locs.ndim == 1 and len(locs) == 3)\n    is_multi = (locs.ndim == 2 and locs.shape[1] == 3)\n    if not is_single and not is_multi:\n        raise ValueError('Expected a single (x, y, z) location or a '\n                         '(N, 3) array of multiple locations')\n\n    if to not in ['nodes', 'connectors']:\n        raise ValueError('`to` must be \"nodes\" or \"connectors\", '\n                         f'got {to}')\n\n    # Generate tree\n    tree = graph.neuron2KDTree(self, data=to)\n\n    # Find the closest node\n    dist, ix = tree.query(locs)\n\n    if to == 'nodes':\n        id = self.nodes.node_id.values[ix]\n    else:\n        id = self.connectors.connector_id.values[ix]\n\n    return id, dist\n</code></pre>"},{"location":"reference/navis/#navis.TreeNeuron.to_swc","title":"<code>to_swc</code>","text":"<p>Generate SWC file from this neuron.</p> PARAMETER DESCRIPTION <code>filename</code> <pre><code>        If `None`, will use \"neuron_{id}.swc\".\n</code></pre> <p> TYPE: <code>     str | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <pre><code>        Additional arguments passed to [`navis.write_swc`][].\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Nothing</code> See Also <p><code>navis.write_swc</code>         See this function for further details.</p> Source code in <code>navis/core/skeleton.py</code> <pre><code>def to_swc(self,\n           filename: Optional[str] = None,\n           **kwargs) -&gt; None:\n    \"\"\"Generate SWC file from this neuron.\n\n    Parameters\n    ----------\n    filename :      str | None, optional\n                    If `None`, will use \"neuron_{id}.swc\".\n    kwargs\n                    Additional arguments passed to [`navis.write_swc`][].\n\n    Returns\n    -------\n    Nothing\n\n    See Also\n    --------\n    [`navis.write_swc`][]\n            See this function for further details.\n\n    \"\"\"\n    return io.write_swc(self, filename, **kwargs)  # type: ignore  # double import of \"io\"\n</code></pre>"},{"location":"reference/navis/#navis.Viewer","title":"<code>navis.Viewer</code>","text":"<p>Vispy 3D viewer.</p> PARAMETER DESCRIPTION <code>picking</code> <pre><code>    If `True`, allow selecting neurons by shift-clicking on\n    neurons and placing a 3D cursor via control-click (for OSX:\n    command-click).\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>= False</code> </p> <code>**kwargs</code> <pre><code>  Keyword arguments passed to `vispy.scene.SceneCanvas`.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> ATTRIBUTE DESCRIPTION <code>picking</code> <p>Set to <code>True</code> to allow picking via shift-clicking.</p> <p> TYPE: <code>(bool,)</code> </p> <code>selected</code> <p>List of currently selected neurons. Can also be used to set the selection.</p> <p> TYPE: <code>np.array</code> </p> <code>show_legend</code> <p>Set to <code>True</code> or press <code>L</code> to show legend. This may impact performance.</p> <p> TYPE: <code>bool</code> </p> <code>legend_font_size</code> <p>Font size for legend.</p> <p> TYPE: <code>int</code> </p> <p>Examples:</p> <p>This viewer is what <code>navis.plot3d</code> uses when <code>backend='vispy'</code>. Instead of <code>navis.plot3d</code> we can interact with the viewer directly:</p> <pre><code>&gt;&gt;&gt; # Open a 3D viewer\n&gt;&gt;&gt; import navis\n&gt;&gt;&gt; v = navis.Viewer()\n&gt;&gt;&gt; # Close the 3D viewer\n&gt;&gt;&gt; v.close()\n</code></pre> <p>You can change the background color from the start or on-the-go:</p> <pre><code>&gt;&gt;&gt; # Set background to green\n&gt;&gt;&gt; v = navis.Viewer(bgcolor='green')\n&gt;&gt;&gt; # Set background back to white\n&gt;&gt;&gt; v.set_bgcolor((1, 1, 1))\n&gt;&gt;&gt; # Alternative to v.close():\n&gt;&gt;&gt; navis.close3d()\n</code></pre> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>class Viewer:\n    \"\"\"Vispy 3D viewer.\n\n    Parameters\n    ----------\n    picking :   bool, default = False\n                If `True`, allow selecting neurons by shift-clicking on\n                neurons and placing a 3D cursor via control-click (for OSX:\n                command-click).\n    **kwargs\n              Keyword arguments passed to `vispy.scene.SceneCanvas`.\n\n    Attributes\n    ----------\n    picking :       bool,\n                    Set to `True` to allow picking via shift-clicking.\n    selected :      np.array\n                    List of currently selected neurons. Can also be used to\n                    set the selection.\n    show_legend :   bool\n                    Set to `True` or press `L` to show legend. This may\n                    impact performance.\n    legend_font_size : int\n                    Font size for legend.\n\n    Examples\n    --------\n    This viewer is what [`navis.plot3d`][] uses when `backend='vispy'`.\n    Instead of [`navis.plot3d`][] we can interact with the viewer directly:\n\n    &gt;&gt;&gt; # Open a 3D viewer\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; v = navis.Viewer()\n    &gt;&gt;&gt; # Close the 3D viewer\n    &gt;&gt;&gt; v.close()\n\n    You can change the background color from the start or on-the-go:\n\n    &gt;&gt;&gt; # Set background to green\n    &gt;&gt;&gt; v = navis.Viewer(bgcolor='green')\n    &gt;&gt;&gt; # Set background back to white\n    &gt;&gt;&gt; v.set_bgcolor((1, 1, 1))\n    &gt;&gt;&gt; # Alternative to v.close():\n    &gt;&gt;&gt; navis.close3d()\n\n    \"\"\"\n\n    def __init__(self, picking=False, **kwargs):\n        if not scene:\n            raise ModuleNotFoundError(\n                '`navis.Viewer` requires the `vispy` package to '\n                'be installed:\\n  pip3 install vispy'\n                )\n        # Update some defaults as necessary\n        defaults = dict(keys=None,\n                        show=True,\n                        title='Vispy Viewer',\n                        bgcolor='black')\n        defaults.update(kwargs)\n\n        # If we're runningg in headless mode (primarily for tests on CI) we will\n        # simply not initialize the vispy objects. Not ideal but it turns\n        # out to be very annoying to correctly setup on Github Actions.\n        if getattr(config, 'headless', False):\n            return\n\n        # Set border rim -&gt; this depends on how the framework (e.g. QT5)\n        # renders the window\n        self._rim_bot = 15\n        self._rim_top = 20\n        self._rim_left = 10\n        self._rim_right = 10\n\n        # Generate canvas\n        self.canvas = scene.SceneCanvas(**defaults)\n\n        \"\"\"\n        from PyQt5.QtWidgets import QPushButton\n\n        # Create canvas\n        button = QPushButton('PyQt5 button', self.canvas.native)\n        button.move(10, 10)\n        self.canvas.show()\n        \"\"\"\n\n        # Add and setup 3d view\n        self.view3d = self.canvas.central_widget.add_view()\n        self.camera3d = scene.ArcballCamera()\n        self.view3d.camera = self.camera3d\n\n        # Add permanent overlays\n        self.overlay = self._draw_overlay()\n\n        self.canvas.unfreeze()\n        self.canvas._overlay = self.overlay\n        self.canvas._view3d = self.view3d\n        self.canvas._wrapper = self\n        self.canvas.freeze()\n\n        # Add picking functionality\n        if picking:\n            self.picking = True\n        else:\n            self.picking = False\n\n        # Set cursor_pos to None\n        self.cursor_pos = None\n\n        # Add keyboard shortcuts\n        self.canvas.connect(on_key_press)\n\n        # Add resize control to keep overlay in position\n        self.canvas.connect(on_resize)\n\n        # Legend settings\n        self.__show_legend = False\n        self.__selected = np.array([], dtype='object')\n        self._cycle_index = -1\n        self.__legend_font_size = 7\n\n        # Color to use when selecting neurons\n        self.highlight_color = (1, .9, .6)\n\n        # Keep track of initial camera position\n        self._camera_default = self.view3d.camera.get_state()\n\n        # Cycle mode can be 'hide' or 'alpha'\n        self._cycle_mode = 'alpha'\n\n        # Cursors\n        self._cursor = None\n        self._picking_radius = 20\n\n        # Other stuff\n        self._show_bounds = False\n        self._show_axes = False\n\n    def _draw_overlay(self):\n        overlay = scene.widgets.ViewBox(parent=self.canvas.scene)\n        self.view3d.add_widget(overlay)\n\n        \"\"\"\n        # Legend title\n        t = scene.visuals.Text('Legend', pos=(10,10),\n                                  anchor_x='left', name='permanent',\n                                  parent=overlay,\n                                  color=(0,0,0), font_size=9)\n        \"\"\"\n\n        # Text color depends on background color\n        v = self.canvas.bgcolor.hsv[2]\n        text_color = colorsys.hsv_to_rgb(0, 0, 1 - v)\n\n        # Keyboard shortcuts\n        self._key_shortcuts = {'O': 'toggle overlay',\n                               'L': 'toggle legend',\n                               'P': 'toggle picking',\n                               'Q/W': 'cycle neurons',\n                               'U': 'unhide all',\n                               'B': 'bounding box',\n                               'F': 'show/hide FPS',\n                               '1': 'XY',\n                               '2': 'XZ',\n                               '3': 'YZ'}\n\n        shorts_text = 'SHORTCUTS: ' + ' | '.join([f\"&lt;{k}&gt; {v}\" for k, v in self._key_shortcuts.items()])\n        self._shortcuts = scene.visuals.Text(shorts_text,\n                                             pos=(self._rim_left,\n                                                  overlay.size[1] - self._rim_bot),\n                                             anchor_x='left',\n                                             anchor_y='bottom',\n                                             name='permanent',\n                                             method='gpu',\n                                             parent=overlay,\n                                             color=text_color,\n                                             font_size=6)\n\n        # FPS (hidden at start)\n        self._fps_text = scene.visuals.Text('FPS',\n                                            pos=(overlay.size[0] / 2,\n                                                 self._rim_top),\n                                            anchor_x='center',\n                                            anchor_y='top',\n                                            name='permanent',\n                                            method='gpu',\n                                            parent=overlay,\n                                            color=(0, 0, 0), font_size=6)\n        self._fps_text.visible = False\n\n        # Picking shortcuts (hidden at start)\n        self._picking_shortcuts = {'LMB @legend': 'show/hide neuron',\n                                   'SHIFT+LMB @neuron': 'select neuron',\n                                   'D': 'deselect all',\n                                   'H': 'hide selected',\n                                   'C': 'url to cursor'}\n        # Add platform-specific modifiers\n        if platform.system() == 'darwin':\n            self._picking_shortcuts['CMD+LMB'] = 'set cursor'\n        else:\n            self._picking_shortcuts['CTRL+LMB'] = 'set cursor'\n\n        shorts_text = 'PICKING: ' + ' | '.join(['&lt;{k}&gt; {v}' for k, v in self._picking_shortcuts.items()])\n        self._picking_text = scene.visuals.Text(shorts_text,\n                                                pos=(self._rim_left,\n                                                     overlay.size[1] - self._rim_bot - 10),\n                                                anchor_x='left',\n                                                anchor_y='bottom',\n                                                name='permanent',\n                                                method='gpu',\n                                                parent=overlay,\n                                                color=text_color,\n                                                font_size=6)\n        self._picking_text.visible = False\n\n        # Text box in top right to display arbitrary data\n        self._data_text = scene.visuals.Text('',\n                                             pos=(overlay.size[0] - self._rim_right,\n                                                  self._rim_top),\n                                             anchor_x='right',\n                                             anchor_y='top',\n                                             name='permanent',\n                                             method='gpu',\n                                             parent=overlay,\n                                             color=text_color,\n                                             font_size=6)\n\n        return overlay\n\n    @property\n    def size(self):\n        \"\"\"Size of canvas.\"\"\"\n        return self.canvas.size\n\n    @size.setter\n    def size(self, size):\n        self.canvas.size = size\n\n    @property\n    def show_legend(self):\n        \"\"\"Set to `True` to hide neuron legend.\"\"\"\n        return self.__show_legend\n\n    @show_legend.setter\n    def show_legend(self, v):\n        if not isinstance(v, bool):\n            raise TypeError(f'Need boolean, got \"{type(v)}\"')\n\n        if v != self.show_legend:\n            self.__show_legend = v\n            # Make sure changes take effect\n            self.update_legend()\n\n    @property\n    def legend_font_size(self):\n        \"\"\"Change legend's font size.\"\"\"\n        return self.__legend_font_size\n\n    @legend_font_size.setter\n    def legend_font_size(self, val):\n        self.__legend_font_size = val\n        if self.show_legend:\n            self.update_legend()\n\n    @property\n    def picking(self):\n        \"\"\"Set to `True` to allow picking.\"\"\"\n        return self.__picking\n\n    def toggle_picking(self):\n        \"\"\"Toggle picking and overlay text.\"\"\"\n        if self.picking:\n            self.picking = False\n            self._picking_text.visible = False\n        else:\n            self.picking = True\n            self._picking_text.visible = True\n\n    @picking.setter\n    def picking(self, v):\n        if not isinstance(v, bool):\n            raise TypeError(f'Need bool, got {type(v)}')\n\n        self.__picking = v\n\n        if self.picking:\n            self.canvas.connect(on_mouse_press)\n        else:\n            self.canvas.events.mouse_press.disconnect(on_mouse_press)\n\n    def _render_fb(self, crop=None):\n        \"\"\"Render framebuffer.\"\"\"\n        if not crop:\n            crop = (0, 0,\n                    self.canvas.size[0] * self.canvas.pixel_scale,\n                    self.canvas.size[1] * self.canvas.pixel_scale)\n\n        # We have to temporarily deactivate the overlay and view3d\n        # otherwise we won't be able to see what's on the 3D or might\n        # see holes in the framebuffer\n        self.view3d.interactive = False\n        self.overlay.interactive = False\n        p = self.canvas._render_picking(crop=crop)\n        self.view3d.interactive = True\n        self.overlay.interactive = True\n        return p\n\n    @property\n    def visible(self):\n        \"\"\"List IDs of currently visible neurons.\"\"\"\n        neurons = self.neurons  # grab this only once to speed things up\n        return [s for s in neurons if neurons[s][0].visible]\n\n    @property\n    def invisible(self):\n        \"\"\"List IDs of currently visible neurons.\"\"\"\n        neurons = self.neurons  # grab this only once to speed things up\n        return [s for s in neurons if not neurons[s][0].visible]\n\n    @property\n    def pinned(self):\n        \"\"\"List IDs of currently pinned neurons.\"\"\"\n        neurons = self.neurons  # grab this only once to speed things up\n        return [s for s in neurons if getattr(neurons[s][0], 'pinned', False)]\n\n    @property\n    def selected(self):\n        \"\"\"Return IDs of or set selected neurons.\"\"\"\n        return self.__selected\n\n    @selected.setter\n    def selected(self, val):\n        n = np.asarray(val).astype('object')\n\n        neurons = self.neurons  # grab once to speed things up\n        logger.debug(f'{len(n)} neurons selected ({len(self.selected)} previously)')\n        # First un-highlight neurons no more selected\n        for s in [s for s in self.__selected if s not in set(n)]:\n            for v in neurons[s]:\n                if isinstance(v, scene.visuals.Mesh):\n                    v.color = v._stored_color\n                else:\n                    v.set_data(color=v._stored_color)\n\n        # Highlight new additions\n        for s in n:\n            if s not in self.__selected:\n                for v in neurons[s]:\n                    # Keep track of old colour\n                    v.unfreeze()\n                    v._stored_color = v.color\n                    v.freeze()\n                    if isinstance(v, scene.visuals.Mesh):\n                        v.color = self.highlight_color\n                    else:\n                        v.set_data(color=self.highlight_color)\n\n        self.__selected = n\n\n        # Update legend\n        if self.show_legend:\n            self.update_legend()\n\n        # Update data text\n        # Currently only the development version of vispy supports escape\n        # character (e.g. \\n)\n        t = '| '.join([f'{neurons[s][0]._name} - #{s}' for s in self.__selected])\n        self._data_text.text = t\n\n    @property\n    def visuals(self):\n        \"\"\"List of all 3D visuals on this canvas.\"\"\"\n        return [v for v in self.view3d.children[0].children if isinstance(v, scene.visuals.VisualNode)]\n\n    @property\n    def bounds(self):\n        \"\"\"Bounds of all currently visuals (visible and invisible).\"\"\"\n        bounds = []\n        for vis in self.visuals:\n            # Skip the bounding box itself\n            if getattr(vis, '_object_type', '') == 'boundingbox':\n                continue\n\n            try:\n                bounds.append(vis._bounds)\n            except BaseException:\n                pass\n\n        if not bounds:\n            return None\n\n        bounds = np.dstack(bounds)\n\n        mn = bounds[:, 0, :].min(axis=1)\n        mx = bounds[:, 1, :].max(axis=1)\n\n        return np.vstack((mn, mx)).T\n\n    @property\n    def _object_ids(self):\n        \"\"\"All object IDs on this canvas in order of addition.\"\"\"\n        obj_ids = [getattr(v, '_object_id') for v in self.visuals]\n        return sorted(set(obj_ids), key=lambda x: obj_ids.index(x))\n\n    @property\n    def objects(self):\n        \"\"\"Ordered dictionary {uuid-&gt;[visuals]} of all objects in order of addition.\"\"\"\n        objects = OrderedDict()\n        for ob in self._object_ids:\n            objects[ob] = [v for v in self.visuals if getattr(v, '_object_id') == ob]\n\n        return objects\n\n    @property\n    def neurons(self):\n        \"\"\"Return visible and invisible neuron visuals currently on the canvas.\n\n        Returns\n        -------\n        OrderedDict\n                    `{id: [neurites, soma]}`\n\n        \"\"\"\n        # Collect neuron objects (neurites + somata)\n        visuals = self.visuals  # Get this only once to speed things up\n        neuron_obj = [c for c in visuals if 'neuron' in getattr(c,\n                                                                '_object_type',\n                                                                '')]\n\n        # Collect IDs\n        neuron_ids = set([ob._id for ob in neuron_obj])\n\n        # Collect somata and neurites by ID\n        coll = OrderedDict()\n        for ob in neuron_ids:\n            coll[ob] = [v for v in visuals if getattr(v, '_id') == ob]\n        return coll\n\n    @property\n    def _neuron_obj(self):\n        \"\"\"Return neurons by their object id.\"\"\"\n        # Collect neuron objects\n        neuron_obj = [c for c in self.visuals if 'neuron' in getattr(\n            c, '_object_type', '')]\n\n        # Collect skeleton IDs\n        obj_ids = set([ob._object_id for ob in neuron_obj])\n\n        # Map visuals to unique skids\n        return {s: [ob for ob in neuron_obj if ob._object_id == s] for s in obj_ids}\n\n    def clear_legend(self):\n        \"\"\"Clear legend.\"\"\"\n        # Clear legend except for title\n        for l in [l for l in self.overlay.children if isinstance(l, scene.visuals.Text) and l.name != 'permanent']:\n            l.parent = None\n\n    def clear(self):\n        \"\"\"Clear canvas.\"\"\"\n        # Skip if running in headless mode\n        if getattr(config, 'headless', False):\n            return\n\n        for v in self.visuals:\n            v.parent = None\n\n        # `remove_bounds` set this to False but\n        # here we want the current setting to persist\n        show_bounds = self.show_bounds\n\n        self.remove_bounds()\n        self.clear_legend()\n\n        self.show_bounds = show_bounds\n\n    def remove(self, to_remove):\n        \"\"\"Remove given neurons/visuals from canvas.\"\"\"\n        to_remove = utils.make_iterable(to_remove)\n\n        neurons = self.neurons  # grab this only once to speed things up\n        for vis in to_remove:\n            if isinstance(vis, scene.visuals.VisualNode):\n                vis.parent = None\n            else:\n                uuids = utils.eval_id(to_remove)\n                for u in uuids:\n                    for v in neurons.get(u, []):\n                        v.parent = None\n\n        if self.show_bounds:\n            self.update_bounds()\n\n    def pop(self, N=1):\n        \"\"\"Remove the most recently added N visuals.\"\"\"\n        for vis in list(self.objects.values())[-N:]:\n            self.remove(vis)\n\n    @property\n    def show_bounds(self):\n        \"\"\"Set to `True` to show bounding box.\"\"\"\n        return self._show_bounds\n\n    def toggle_bounds(self):\n        \"\"\"Toggle bounding box.\"\"\"\n        self.show_bounds = not self.show_bounds\n\n    @show_bounds.setter\n    def show_bounds(self, v):\n        if not isinstance(v, bool):\n            raise TypeError(f'Need bool, got {type(v)}')\n\n        self._show_bounds = v\n\n        if self.show_bounds:\n            self.update_bounds()\n        else:\n            self.remove_bounds()\n\n    def remove_bounds(self):\n        \"\"\"Remove bounding box visual.\"\"\"\n        self._show_bounds = False\n        for v in self.visuals:\n            if getattr(v, '_object_type', '') == 'boundingbox':\n                self.remove(v)\n\n    @block_canvas\n    def update_bounds(self, color='w', width=1):\n        \"\"\"Update bounding box visual.\"\"\"\n        # Remove any existing visual\n        self.remove_bounds()\n\n        bounds = self.bounds\n        self._show_bounds = True\n\n        # Skip if no visual on canvas\n        if isinstance(bounds, type(None)):\n            return\n\n        # Create box visual\n        dims = bounds[:, 1] - bounds[:, 0]\n        center = bounds.mean(axis=1)\n        box = tm.primitives.Box(extents=dims).apply_scale(1.1)\n\n        # Recenter vertices\n        vertices = np.array(box.vertices) + center\n        connect = np.array([[0, 1], [0, 2], [0, 4],\n                            [1, 3], [1, 5],\n                            [2, 3], [2, 6],\n                            [3, 7],\n                            [4, 5], [4, 6],\n                            [5, 7],\n                            [6, 7]])\n\n        box = scene.visuals.Line(pos=vertices,\n                                 color=mcl.to_rgb(color),\n                                 # Can only be used with method 'agg'\n                                 width=width,\n                                 connect=connect,\n                                 antialias=True,\n                                 name='BoundingBox',\n                                 method='gl')\n\n        # Add custom attributes\n        box.unfreeze()\n        box._object_type = 'boundingbox'\n        box._object_id = uuid.uuid4()\n        box.freeze()\n\n        self.view3d.add(box)\n\n    @block_canvas\n    def update_legend(self):\n        \"\"\"Update legend.\"\"\"\n        # Get existing labels\n        labels = {l._object_id: l for l in self.overlay.children if getattr(l, '_object_id', None)}\n\n        # If legend is not meant to be shown, make sure everything is hidden and return\n        if not self.show_legend:\n            for v in labels.values():\n                if v.visible:\n                    v.visible = False\n            return\n        else:\n            for v in labels.values():\n                if not v.visible:\n                    v.visible = True\n\n        # Labels to be removed\n        neuron_obj = self._neuron_obj  # grab only once to speed things up\n        to_remove = [s for s in labels if s not in neuron_obj]\n        for s in to_remove:\n            labels[s].parent = None\n\n        # Generate new labels\n        to_add = [s for s in neuron_obj if s not in labels]\n        for s in to_add:\n            # Fallback is name or in lieu of that the object's type\n            lbl = getattr(neuron_obj[s][0], '_name',\n                          str(type(neuron_obj[s][0])))\n            # See if we find a \"label\" property\n            if hasattr(neuron_obj[s][0], '_object'):\n                if hasattr(neuron_obj[s][0]._object, 'label'):\n                    lbl = neuron_obj[s][0]._object.label\n\n            txt = scene.visuals.Text(lbl,\n                                     anchor_x='left',\n                                     anchor_y='top',\n                                     parent=self.overlay,\n                                     method='gpu',\n                                     font_size=self.legend_font_size)\n            txt.interactive = True\n            txt.unfreeze()\n            txt._object_id = s\n            txt._id = neuron_obj[s][0]._id\n            txt.freeze()\n\n        # Position and color labels\n        labels = {l._object_id: l for l in self.overlay.children if getattr(\n            l, '_object_id', None)}\n        for i, s in enumerate(sorted(neuron_obj)):\n            if neuron_obj[s][0].visible:\n                color = neuron_obj[s][0].color\n            else:\n                color = (.3, .3, .3)\n\n            offset = 10 * (self.legend_font_size / 7)\n\n            labels[s].pos = (10, offset * (i + 1))\n            labels[s].color = color\n            labels[s].font_size = self.legend_font_size\n\n    def toggle_overlay(self):\n        \"\"\"Toggle legend on and off.\"\"\"\n        self.overlay.visible = self.overlay.visible is False\n\n    def center_camera(self):\n        \"\"\"Center camera on visuals.\"\"\"\n        visuals = self.visuals  # Get this only once to speed things up\n        if not visuals:\n            return\n\n        xbounds = np.array([v.bounds(0) for v in visuals]).flatten()\n        ybounds = np.array([v.bounds(1) for v in visuals]).flatten()\n        zbounds = np.array([v.bounds(2) for v in visuals]).flatten()\n\n        self.camera3d.set_range((xbounds.min(), xbounds.max()),\n                                (ybounds.min(), ybounds.max()),\n                                (zbounds.min(), zbounds.max()))\n\n    def add(self, x, center=True, clear=False, combine=False, **kwargs):\n        \"\"\"Add objects to canvas.\n\n        Parameters\n        ----------\n        x :         Neuron/List | Dotprops | Volumes | Points | vispy Visuals\n                    Object(s) to add to the canvas.\n        center :    bool, optional\n                    If True, re-center camera to all objects on canvas.\n        clear :     bool, optional\n                    If True, clear canvas before adding new objects.\n        combine :   bool, optional\n                    If True, will try combining similar objects into a single\n                    visual. This reduces the number of shader programs and\n                    can greatly increase the frame rate. Downside: objects can\n                    no longer be individually manipulated.\n        **kwargs\n                    Keyword arguments passed when generating visuals. See\n                    [`navis.plot3d`][] for options.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        from .visuals import neuron2vispy, volume2vispy, points2vispy, combine_visuals\n        from ..settings import VispySettings\n\n        settings = VispySettings().update_settings(**kwargs)\n\n        (neurons, volumes, points, visuals) = utils.parse_objects(x)\n\n        if neurons:\n            visuals += neuron2vispy(neurons, settings)\n        if volumes:\n            visuals += volume2vispy(volumes, settings)\n        if points:\n            visuals += points2vispy(points, **settings.scatter_kws)\n\n        if not visuals:\n            raise ValueError('No visuals created.')\n\n        if clear:\n            self.clear()\n\n        if combine:\n            visuals = combine_visuals(visuals, settings.name)\n\n        # If we're runningg in headless mode (primarily for tests on CI) we will\n        # simply not add the objects. Not ideal but it turns out to be very\n        # annoying to correctly setup on Github Actions.\n        if getattr(config, 'headless', False):\n            return\n\n        for v in visuals:\n            # Give visuals an _object_id if they don't already have one\n            if not hasattr(v, '_object_id'):\n                v.unfreeze()\n                v._object_id = uuid.uuid4()\n                v.freeze()\n            self.view3d.add(v)\n\n        if center:\n            self.center_camera()\n\n        if self.show_legend:\n            self.update_legend()\n\n        if self.show_bounds:\n            self.update_bounds()\n\n    def show(self):\n        \"\"\"Show viewer.\"\"\"\n        # This is for e.g. headless testing\n        if getattr(config, 'headless', False):\n            logger.info(\"Viewer widget not shown - navis running in headless mode. \")\n            return\n\n        self.canvas.show()\n\n        # To actually show the widget, we need to return the canvas\n        if utils.is_jupyter():\n            from IPython.display import display\n            display(self.canvas)\n\n    def close(self):\n        \"\"\"Close viewer.\"\"\"\n        # Skip if this is headless mode\n        if getattr(config, 'headless', False):\n            return\n\n        # Clear first to free all visuals\n        self.clear()\n        if self == getattr(config, 'primary_viewer', None):\n            del config.primary_viewer\n        self.canvas.close()\n\n    def hide_neurons(self, n):\n        \"\"\"Hide given neuron(s).\"\"\"\n        ids = utils.eval_id(n)\n\n        neurons = self.neurons   # grab once to speed things up\n        for s in ids:\n            for v in neurons[s]:\n                if getattr(v, 'pinned', False):\n                    continue\n                if v.visible:\n                    v.visible = False\n\n        self.update_legend()\n\n    def hide_selected(self):\n        \"\"\"Hide currently selected neuron(s).\"\"\"\n        self.hide_neurons(self.selected)\n\n    def unhide_neurons(self, n=None, check_alpha=False):\n        \"\"\"Unhide given neuron(s).\n\n        Use `n` to unhide specific neurons.\n\n        \"\"\"\n        neurons = self.neurons  # grab once to speed things up\n        if not isinstance(n, type(None)):\n            ids = utils.eval_id(n)\n        else:\n            ids = list(neurons.keys())\n\n        for s in ids:\n            for v in neurons[s]:\n                if getattr(v, 'pinned', False):\n                    continue\n                if not v.visible:\n                    v.visible = True\n            if check_alpha:\n                # Make sure color has an alpha channel\n                c = to_rgba(neurons[s][0].color)\n                # Make sure alpha is 1\n                if c.ndim == 1 and c[3] != 1:\n                    c[3] = 1\n                    self.set_colors({s: c})\n                elif c.ndim == 2 and np.any(c[:, 3] != 1):\n                    c[:, 3] = 1\n                    self.set_colors({s: c})\n\n        self.update_legend()\n\n    def pin_neurons(self, n):\n        \"\"\"Pin given neuron(s).\n\n        Changes to the color or visibility of pinned neurons are silently\n        ignored. You can use this to keep specific neurons visible while\n        cycling through the rest - useful for comparisons.\n\n        \"\"\"\n        ids = utils.eval_id(n)\n\n        neurons = self.neurons  # grab only once to speed things up\n\n        for s in ids:\n            for v in neurons[s]:\n                v.unfreeze()\n                v.pinned = True\n                v.freeze()\n\n    def unpin_neurons(self, n=None):\n        \"\"\"Unpin given neuron(s).\n\n        Use `n` to unhide specific neurons.\n\n        \"\"\"\n        neurons = self.neurons  # grab once to speed things up\n        if not isinstance(n, type(None)):\n            ids = utils.eval_id(n)\n        else:\n            ids = list(neurons.keys())\n\n        for s in ids:\n            for v in neurons[s]:\n                v.unfreeze()\n                v.pinned = False\n                v.freeze()\n\n    def toggle_neurons(self, n):\n        \"\"\"Toggle neuron(s) visibility.\"\"\"\n        n = utils.make_iterable(n)\n\n        if False not in [isinstance(u, uuid.UUID) for u in n]:\n            obj = self._neuron_obj\n        else:\n            n = utils.eval_id(n)\n            obj = self.neurons\n\n        for s in n:\n            for v in obj[s]:\n                v.visible = v.visible is False\n\n        self.update_legend()\n\n    def toggle_select(self, n):\n        \"\"\"Toggle selected of given neuron.\"\"\"\n        skids = utils.eval_id(n)\n\n        neurons = self.neurons  # grab once to speed things up\n\n        for s in skids:\n            if self.selected != s:\n                self.selected = s\n                for v in neurons[s]:\n                    self._selected_color = v.color\n                    v.set_data(color=self.highlight_color)\n            else:\n                self.selected = None\n                for v in neurons[s]:\n                    v.set_data(color=self._selected_color)\n\n        self.update_legend()\n\n    @block_all\n    def set_colors(self, c, include_connectors=False):\n        \"\"\"Set neuron color.\n\n        Parameters\n        ----------\n        c :      tuple | dict\n                 RGB color(s) to apply. Values must be 0-1. Accepted:\n                   1. Tuple of single color. Applied to all visible neurons.\n                   2. Dictionary mapping skeleton IDs to colors.\n\n        \"\"\"\n        neurons = self.neurons  # grab once to speed things up\n        if isinstance(c, (tuple, list, np.ndarray, str)):\n            cmap = {s: c for s in neurons}\n        elif isinstance(c, dict):\n            cmap = c\n        else:\n            raise TypeError(f'Unable to use colors of type \"{type(c)}\"')\n\n        for n in neurons:\n            if n in cmap:\n                for v in neurons[n]:\n                    if getattr(v, 'pinned', False):\n                        continue\n                    if v._neuron_part == 'connectors' and not include_connectors:\n                        continue\n                    new_c = mcl.to_rgba(cmap[n])\n                    if isinstance(v, scene.visuals.Mesh):\n                        v.color = new_c\n                    else:\n                        v.set_data(color=mcl.to_rgba(cmap[n]))\n\n        if self.show_legend:\n            self.update_legend()\n\n    @block_all\n    def set_alpha(self, a, include_connectors=True):\n        \"\"\"Set neuron color alphas.\n\n        Parameters\n        ----------\n        a :      tuple | dict\n                 Alpha value(s) to apply. Values must be 0-1. Accepted:\n                   1. Tuple of single alpha. Applied to all visible neurons.\n                   2. Dictionary mapping skeleton IDs to alpha.\n\n        \"\"\"\n        neurons = self.neurons  # grab once to speed things up\n        if isinstance(a, (tuple, list, np.ndarray, str)):\n            amap = {s: a for s in neurons}\n        elif isinstance(a, dict):\n            amap = a\n        else:\n            raise TypeError(f'Unable to use colors of type \"{type(a)}\"')\n\n        for n in neurons:\n            if n in amap:\n                for v in neurons[n]:\n                    if getattr(v, 'pinned', False):\n                        continue\n                    if v._neuron_part == 'connectors' and not include_connectors:\n                        continue\n                    try:\n                        this_c = v.color.rgba\n                    except BaseException:\n                        this_c = v.color\n\n                    this_c = np.asarray(this_c)\n\n                    # For arrays of colors\n                    if this_c.ndim == 2:\n                        # If no alpha channel yet, add one\n                        if this_c.shape[1] == 3:\n                            this_c = np.insert(this_c,\n                                               3,\n                                               np.ones(this_c.shape[0]),\n                                               axis=1)\n\n                        # If already the correct alpha value\n                        if np.all(this_c[:, 3] == amap[n]):\n                            continue\n                        else:\n                            this_c[:, 3] = amap[n]\n                    else:\n                        if len(this_c) == 4 and this_c[3] == amap[n]:\n                            continue\n                        else:\n                            this_c = tuple([this_c[0], this_c[1], this_c[2], amap[n]])\n\n                    if isinstance(v, scene.visuals.Mesh):\n                        v.color = this_c\n                    else:\n                        v.set_data(color=this_c)\n\n        if self.show_legend:\n            self.update_legend()\n\n    def colorize(self, palette='hls', include_connectors=False):\n        \"\"\"Colorize neurons using a seaborn color palette.\"\"\"\n        neurons = self.neurons  # grab once to speed things up\n        colors = sns.color_palette(palette, len(neurons))\n        cmap = {s: colors[i] for i, s in enumerate(neurons)}\n\n        self.set_colors(cmap, include_connectors=include_connectors)\n\n    def set_bgcolor(self, c):\n        \"\"\"Set background color.\"\"\"\n        if getattr(config, 'headless', False):\n            return\n        self.canvas.bgcolor = c\n\n    def _cycle_neurons(self, increment):\n        \"\"\"Cycle through neurons.\"\"\"\n        self._cycle_index += increment\n\n        # If mode is 'hide' cycle over all neurons\n        neurons = self.neurons  # grab once to speed things up\n        if self._cycle_mode == 'hide':\n            to_cycle = neurons\n        # If mode is 'alpha' ignore all hidden neurons\n        elif self._cycle_mode == 'alpha':\n            # Make sure to keep the order\n            to_cycle = OrderedDict()\n            for s in self.visible:\n                to_cycle[s] = neurons[s]\n        else:\n            raise ValueError(f'Unknown cycle mode \"{self._cycle_mode}\".')\n\n        if self._cycle_index &lt; 0:\n            self._cycle_index = len(to_cycle) - 1\n        elif self._cycle_index &gt; len(to_cycle) - 1:\n            self._cycle_index = 0\n\n        to_hide = [n for i, n in enumerate(to_cycle) if i != self._cycle_index]\n        to_show = [list(to_cycle.keys())[self._cycle_index]]\n\n        # Depending on background color, we have to use different alphas\n        v = self.canvas.bgcolor.hsv[2]\n        out_alpha = .05 + .2 * v\n\n        if self._cycle_mode == 'hide':\n            self.hide_neurons(to_hide)\n            self.unhide_neurons(to_show)\n        elif self._cycle_mode == 'alpha':\n            # Get current colors\n            new_amap = {}\n            for n in to_cycle:\n                this_c = np.asarray(to_cycle[n][0].color)\n\n                if this_c.ndim == 2:\n                    if this_c.shape[1] == 4:\n                        this_a = this_c[0, 3]\n                    else:\n                        this_a = 1\n                else:\n                    if this_c.shape[0] == 4:\n                        this_a = this_c[3]\n                    else:\n                        this_a = 1\n\n                # If neuron needs to be hidden, add to cmap\n                if n in to_hide and this_a != out_alpha:\n                    new_amap[n] = out_alpha\n                elif n in to_show and this_a != 1:\n                    new_amap[n] = 1\n            self.set_alpha(new_amap)\n        else:\n            raise ValueError(f'Unknown cycle mode: \"{self._cycle_mode}\". Use '\n                             '\"hide\" or \"alpha\"!')\n\n        self.active_neuron = to_show\n\n        # Generate names\n        names = []\n        for u in to_show:\n            n = getattr(neurons[u][0], \"name\", \"NA\")\n            if not isinstance(u, uuid.UUID):\n                n += f' ({u})'\n            names.append(n)\n\n        self._data_text.text = f'{\"|\".join(names)}' \\\n                               f' [{self._cycle_index + 1}/{len(neurons)}]'\n\n    def _draw_fps(self, fps):\n        \"\"\"Callback for `canvas.measure_fps`.\"\"\"\n        self._fps_text.text = f'{fps:.2f} FPS'\n\n    def _toggle_fps(self):\n        \"\"\"Switch FPS measurement on and off.\"\"\"\n        if not self._fps_text.visible:\n            self.canvas.measure_fps(1, self._draw_fps)\n            self._fps_text.visible = True\n        else:\n            self.canvas.measure_fps(1, None)\n            self._fps_text.visible = False\n\n    def _snap_cursor(self, pos, visual, open_browser=False):\n        \"\"\"Snap cursor to clostest vertex of visual.\"\"\"\n        if not getattr(self, '_cursor', None):\n            self._cursor = scene.visuals.Arrow(pos=np.array([(0, 0, 0), (1000, 0, 0)]),\n                                               color=(1, 0, 0, 1),\n                                               arrow_color=(1, 0, 0, 1),\n                                               arrow_size=10,\n                                               arrows=np.array([[800, 0, 0, 1000, 0, 0]]))\n\n        if not self._cursor.parent:\n            self.add(self._cursor, center=False)\n\n        # Get vertices for this visual\n        if isinstance(visual, scene.visuals.Line):\n            verts = visual.pos\n        elif isinstance(visual, scene.visuals.Mesh):\n            verts = visual.mesh_data.get_vertices()\n\n        # Map vertices to canvas\n        tr = visual.get_transform(map_to='canvas')\n        co_on_canvas = tr.map(verts)[:, [0, 1]]\n\n        # Find the closest vertex to this mouse click pos\n        tree = scipy.spatial.cKDTree(co_on_canvas)\n        dist, ix = tree.query(pos)\n\n        # Map canvas pos back to world coordinates\n        self.cursor_pos = np.array(verts[ix])\n        self.cursor_active_skeleton = getattr(visual, '_id', None)\n\n        # Generate arrow coords\n        vec_to_center = np.array(self.camera3d.center) - self.cursor_pos\n        norm_to_center = vec_to_center / np.sqrt(np.sum(vec_to_center**2))\n        start = self.cursor_pos - (norm_to_center * 10000)\n        arrows = np.array([np.append(self.cursor_pos - (norm_to_center * 200),\n                                     self.cursor_pos - (norm_to_center * 100))])\n\n        self._cursor.set_data(pos=np.array([start, self.cursor_pos]),\n                              arrows=arrows)\n\n        logger.debug(f'World coordinates: {self.cursor_pos}')\n\n    def screenshot(self, filename='screenshot.png', pixel_scale=2,\n                   alpha=True, hide_overlay=True):\n        \"\"\"Save a screenshot of this viewer.\n\n        Parameters\n        ----------\n        filename :      str, optional\n                        Filename to save to.\n        pixel_scale :   int, optional\n                        Factor by which to scale canvas. Determines image\n                        dimensions.\n        alpha :         bool, optional\n                        If True, will export transparent background.\n        hide_overlay :  bool, optional\n                        If True, will hide overlay for screenshot.\n\n        \"\"\"\n\n        m = self._screenshot(pixel_scale=pixel_scale,\n                             alpha=alpha,\n                             hide_overlay=hide_overlay)\n\n        im = png.from_array(m, mode='RGBA')\n        im.save(filename)\n\n    def _screenshot(self, pixel_scale=2, alpha=True, hide_overlay=True):\n        \"\"\"Return image array for screenshot.\"\"\"\n        if alpha:\n            bgcolor = list(self.canvas.bgcolor.rgb) + [0]\n        else:\n            bgcolor = list(self.canvas.bgcolor.rgb)\n\n        # region = (0, 0, self.canvas.size[0], self.canvas.size[1])\n        size = tuple(np.array(self.canvas.size) * pixel_scale)\n\n        if hide_overlay:\n            prev_state = self.overlay.visible\n            self.overlay.visible = False\n\n        try:\n            m = self.canvas.render(size=size, bgcolor=bgcolor)\n        except BaseException:\n            raise\n        finally:\n            if hide_overlay:\n                self.overlay.visible = prev_state\n\n        return m\n\n    def visuals_at(self, pos):\n        \"\"\"List visuals at given canvas position.\"\"\"\n        # There appears to be some odd y offset - perhaps because of the\n        # window's top bar? On OSX this is about 15px\n        pos = (pos[0], pos[1] - 15)\n\n        # Map mouse pos to framebuffer\n        tr = self.canvas.transforms.get_transform(map_from='canvas',\n                                                  map_to='framebuffer')\n        pos = tr.map(pos)\n\n        # Render framebuffer in picking mode\n        p = self._render_fb(crop=(pos[0] - self._picking_radius / 2,\n                                  pos[1] - self._picking_radius / 2,\n                                  self._picking_radius,\n                                  self._picking_radius))\n\n        logger.debug('Picking framebuffer:')\n        logger.debug(p)\n\n        # List visuals in order from distance to center\n        ids = []\n        seen = set()\n        center = (np.array(p.shape) / 2).astype(int)\n        for i in range(self._picking_radius * self.canvas.pixel_scale):\n            subr = p[center[0] - i: center[0] + i + 1,\n                     center[1] - i: center[1] + i + 1]\n            subr_ids = set(list(np.unique(subr)))\n            ids.extend(list(subr_ids - seen))\n            seen |= subr_ids\n        visuals = [scene.visuals.VisualNode._visual_ids.get(x, None) for x in ids]\n\n        return [v for v in visuals if v is not None]\n\n    def set_view(self, view):\n        \"\"\"(Re-)set camera position.\n\n        Parameters\n        ----------\n        view :      XY | XZ | YZ\n\n        \"\"\"\n        if isinstance(view, Quaternion):\n            q = view\n        elif view == 'XY':\n            q = Quaternion(w=0.707, x=0.707, y=0, z=0)\n        elif view == 'XZ':\n            q = Quaternion(w=1, x=0, y=0, z=0)\n        elif view == 'YZ':\n            q = Quaternion(w=.5, x=0.5, y=0.5, z=-.5)\n        else:\n            raise TypeError(f'Unable to set view from {type(view)}')\n\n        self.camera3d._quaternion = q\n        # This is necessary to force a redraw\n        self.camera3d.set_range()\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.bounds","title":"<code>bounds</code>  <code>property</code>","text":"<p>Bounds of all currently visuals (visible and invisible).</p>"},{"location":"reference/navis/#navis.Viewer.canvas","title":"<code>canvas = scene.SceneCanvas(**defaults)</code>  <code>instance-attribute</code>","text":"<p>from PyQt5.QtWidgets import QPushButton</p>"},{"location":"reference/navis/#navis.Viewer.canvas--create-canvas","title":"Create canvas","text":"<p>button = QPushButton('PyQt5 button', self.canvas.native) button.move(10, 10) self.canvas.show()</p>"},{"location":"reference/navis/#navis.Viewer.invisible","title":"<code>invisible</code>  <code>property</code>","text":"<p>List IDs of currently visible neurons.</p>"},{"location":"reference/navis/#navis.Viewer.legend_font_size","title":"<code>legend_font_size</code>  <code>property</code> <code>writable</code>","text":"<p>Change legend's font size.</p>"},{"location":"reference/navis/#navis.Viewer.neurons","title":"<code>neurons</code>  <code>property</code>","text":"<p>Return visible and invisible neuron visuals currently on the canvas.</p> RETURNS DESCRIPTION <code>OrderedDict</code> <p><code>{id: [neurites, soma]}</code></p>"},{"location":"reference/navis/#navis.Viewer.objects","title":"<code>objects</code>  <code>property</code>","text":"<p>Ordered dictionary {uuid-&gt;[visuals]} of all objects in order of addition.</p>"},{"location":"reference/navis/#navis.Viewer.picking","title":"<code>picking</code>  <code>property</code> <code>writable</code>","text":"<p>Set to <code>True</code> to allow picking.</p>"},{"location":"reference/navis/#navis.Viewer.pinned","title":"<code>pinned</code>  <code>property</code>","text":"<p>List IDs of currently pinned neurons.</p>"},{"location":"reference/navis/#navis.Viewer.selected","title":"<code>selected</code>  <code>property</code> <code>writable</code>","text":"<p>Return IDs of or set selected neurons.</p>"},{"location":"reference/navis/#navis.Viewer.show_bounds","title":"<code>show_bounds</code>  <code>property</code> <code>writable</code>","text":"<p>Set to <code>True</code> to show bounding box.</p>"},{"location":"reference/navis/#navis.Viewer.show_legend","title":"<code>show_legend</code>  <code>property</code> <code>writable</code>","text":"<p>Set to <code>True</code> to hide neuron legend.</p>"},{"location":"reference/navis/#navis.Viewer.size","title":"<code>size</code>  <code>property</code> <code>writable</code>","text":"<p>Size of canvas.</p>"},{"location":"reference/navis/#navis.Viewer.visible","title":"<code>visible</code>  <code>property</code>","text":"<p>List IDs of currently visible neurons.</p>"},{"location":"reference/navis/#navis.Viewer.visuals","title":"<code>visuals</code>  <code>property</code>","text":"<p>List of all 3D visuals on this canvas.</p>"},{"location":"reference/navis/#navis.Viewer.add","title":"<code>add</code>","text":"<p>Add objects to canvas.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Object(s) to add to the canvas.\n</code></pre> <p> TYPE: <code>        Neuron/List | Dotprops | Volumes | Points | vispy Visuals</code> </p> <code>center</code> <pre><code>    If True, re-center camera to all objects on canvas.\n</code></pre> <p> TYPE: <code>   bool</code> DEFAULT: <code>True</code> </p> <code>clear</code> <pre><code>    If True, clear canvas before adding new objects.\n</code></pre> <p> TYPE: <code>    bool</code> DEFAULT: <code>False</code> </p> <code>combine</code> <pre><code>    If True, will try combining similar objects into a single\n    visual. This reduces the number of shader programs and\n    can greatly increase the frame rate. Downside: objects can\n    no longer be individually manipulated.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <pre><code>    Keyword arguments passed when generating visuals. See\n    [`navis.plot3d`][] for options.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>None</code> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def add(self, x, center=True, clear=False, combine=False, **kwargs):\n    \"\"\"Add objects to canvas.\n\n    Parameters\n    ----------\n    x :         Neuron/List | Dotprops | Volumes | Points | vispy Visuals\n                Object(s) to add to the canvas.\n    center :    bool, optional\n                If True, re-center camera to all objects on canvas.\n    clear :     bool, optional\n                If True, clear canvas before adding new objects.\n    combine :   bool, optional\n                If True, will try combining similar objects into a single\n                visual. This reduces the number of shader programs and\n                can greatly increase the frame rate. Downside: objects can\n                no longer be individually manipulated.\n    **kwargs\n                Keyword arguments passed when generating visuals. See\n                [`navis.plot3d`][] for options.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    from .visuals import neuron2vispy, volume2vispy, points2vispy, combine_visuals\n    from ..settings import VispySettings\n\n    settings = VispySettings().update_settings(**kwargs)\n\n    (neurons, volumes, points, visuals) = utils.parse_objects(x)\n\n    if neurons:\n        visuals += neuron2vispy(neurons, settings)\n    if volumes:\n        visuals += volume2vispy(volumes, settings)\n    if points:\n        visuals += points2vispy(points, **settings.scatter_kws)\n\n    if not visuals:\n        raise ValueError('No visuals created.')\n\n    if clear:\n        self.clear()\n\n    if combine:\n        visuals = combine_visuals(visuals, settings.name)\n\n    # If we're runningg in headless mode (primarily for tests on CI) we will\n    # simply not add the objects. Not ideal but it turns out to be very\n    # annoying to correctly setup on Github Actions.\n    if getattr(config, 'headless', False):\n        return\n\n    for v in visuals:\n        # Give visuals an _object_id if they don't already have one\n        if not hasattr(v, '_object_id'):\n            v.unfreeze()\n            v._object_id = uuid.uuid4()\n            v.freeze()\n        self.view3d.add(v)\n\n    if center:\n        self.center_camera()\n\n    if self.show_legend:\n        self.update_legend()\n\n    if self.show_bounds:\n        self.update_bounds()\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.center_camera","title":"<code>center_camera</code>","text":"<p>Center camera on visuals.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def center_camera(self):\n    \"\"\"Center camera on visuals.\"\"\"\n    visuals = self.visuals  # Get this only once to speed things up\n    if not visuals:\n        return\n\n    xbounds = np.array([v.bounds(0) for v in visuals]).flatten()\n    ybounds = np.array([v.bounds(1) for v in visuals]).flatten()\n    zbounds = np.array([v.bounds(2) for v in visuals]).flatten()\n\n    self.camera3d.set_range((xbounds.min(), xbounds.max()),\n                            (ybounds.min(), ybounds.max()),\n                            (zbounds.min(), zbounds.max()))\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.clear","title":"<code>clear</code>","text":"<p>Clear canvas.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def clear(self):\n    \"\"\"Clear canvas.\"\"\"\n    # Skip if running in headless mode\n    if getattr(config, 'headless', False):\n        return\n\n    for v in self.visuals:\n        v.parent = None\n\n    # `remove_bounds` set this to False but\n    # here we want the current setting to persist\n    show_bounds = self.show_bounds\n\n    self.remove_bounds()\n    self.clear_legend()\n\n    self.show_bounds = show_bounds\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.clear_legend","title":"<code>clear_legend</code>","text":"<p>Clear legend.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def clear_legend(self):\n    \"\"\"Clear legend.\"\"\"\n    # Clear legend except for title\n    for l in [l for l in self.overlay.children if isinstance(l, scene.visuals.Text) and l.name != 'permanent']:\n        l.parent = None\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.close","title":"<code>close</code>","text":"<p>Close viewer.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def close(self):\n    \"\"\"Close viewer.\"\"\"\n    # Skip if this is headless mode\n    if getattr(config, 'headless', False):\n        return\n\n    # Clear first to free all visuals\n    self.clear()\n    if self == getattr(config, 'primary_viewer', None):\n        del config.primary_viewer\n    self.canvas.close()\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.colorize","title":"<code>colorize</code>","text":"<p>Colorize neurons using a seaborn color palette.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def colorize(self, palette='hls', include_connectors=False):\n    \"\"\"Colorize neurons using a seaborn color palette.\"\"\"\n    neurons = self.neurons  # grab once to speed things up\n    colors = sns.color_palette(palette, len(neurons))\n    cmap = {s: colors[i] for i, s in enumerate(neurons)}\n\n    self.set_colors(cmap, include_connectors=include_connectors)\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.hide_neurons","title":"<code>hide_neurons</code>","text":"<p>Hide given neuron(s).</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def hide_neurons(self, n):\n    \"\"\"Hide given neuron(s).\"\"\"\n    ids = utils.eval_id(n)\n\n    neurons = self.neurons   # grab once to speed things up\n    for s in ids:\n        for v in neurons[s]:\n            if getattr(v, 'pinned', False):\n                continue\n            if v.visible:\n                v.visible = False\n\n    self.update_legend()\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.hide_selected","title":"<code>hide_selected</code>","text":"<p>Hide currently selected neuron(s).</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def hide_selected(self):\n    \"\"\"Hide currently selected neuron(s).\"\"\"\n    self.hide_neurons(self.selected)\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.pin_neurons","title":"<code>pin_neurons</code>","text":"<p>Pin given neuron(s).</p> <p>Changes to the color or visibility of pinned neurons are silently ignored. You can use this to keep specific neurons visible while cycling through the rest - useful for comparisons.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def pin_neurons(self, n):\n    \"\"\"Pin given neuron(s).\n\n    Changes to the color or visibility of pinned neurons are silently\n    ignored. You can use this to keep specific neurons visible while\n    cycling through the rest - useful for comparisons.\n\n    \"\"\"\n    ids = utils.eval_id(n)\n\n    neurons = self.neurons  # grab only once to speed things up\n\n    for s in ids:\n        for v in neurons[s]:\n            v.unfreeze()\n            v.pinned = True\n            v.freeze()\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.pop","title":"<code>pop</code>","text":"<p>Remove the most recently added N visuals.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def pop(self, N=1):\n    \"\"\"Remove the most recently added N visuals.\"\"\"\n    for vis in list(self.objects.values())[-N:]:\n        self.remove(vis)\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.remove","title":"<code>remove</code>","text":"<p>Remove given neurons/visuals from canvas.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def remove(self, to_remove):\n    \"\"\"Remove given neurons/visuals from canvas.\"\"\"\n    to_remove = utils.make_iterable(to_remove)\n\n    neurons = self.neurons  # grab this only once to speed things up\n    for vis in to_remove:\n        if isinstance(vis, scene.visuals.VisualNode):\n            vis.parent = None\n        else:\n            uuids = utils.eval_id(to_remove)\n            for u in uuids:\n                for v in neurons.get(u, []):\n                    v.parent = None\n\n    if self.show_bounds:\n        self.update_bounds()\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.remove_bounds","title":"<code>remove_bounds</code>","text":"<p>Remove bounding box visual.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def remove_bounds(self):\n    \"\"\"Remove bounding box visual.\"\"\"\n    self._show_bounds = False\n    for v in self.visuals:\n        if getattr(v, '_object_type', '') == 'boundingbox':\n            self.remove(v)\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.screenshot","title":"<code>screenshot</code>","text":"<p>Save a screenshot of this viewer.</p> PARAMETER DESCRIPTION <code>filename</code> <pre><code>        Filename to save to.\n</code></pre> <p> TYPE: <code>     str</code> DEFAULT: <code>'screenshot.png'</code> </p> <code>pixel_scale</code> <pre><code>        Factor by which to scale canvas. Determines image\n        dimensions.\n</code></pre> <p> TYPE: <code>  int</code> DEFAULT: <code>2</code> </p> <code>alpha</code> <pre><code>        If True, will export transparent background.\n</code></pre> <p> TYPE: <code>        bool</code> DEFAULT: <code>True</code> </p> <code>hide_overlay</code> <pre><code>        If True, will hide overlay for screenshot.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>True</code> </p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def screenshot(self, filename='screenshot.png', pixel_scale=2,\n               alpha=True, hide_overlay=True):\n    \"\"\"Save a screenshot of this viewer.\n\n    Parameters\n    ----------\n    filename :      str, optional\n                    Filename to save to.\n    pixel_scale :   int, optional\n                    Factor by which to scale canvas. Determines image\n                    dimensions.\n    alpha :         bool, optional\n                    If True, will export transparent background.\n    hide_overlay :  bool, optional\n                    If True, will hide overlay for screenshot.\n\n    \"\"\"\n\n    m = self._screenshot(pixel_scale=pixel_scale,\n                         alpha=alpha,\n                         hide_overlay=hide_overlay)\n\n    im = png.from_array(m, mode='RGBA')\n    im.save(filename)\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.set_alpha","title":"<code>set_alpha</code>","text":"<p>Set neuron color alphas.</p> PARAMETER DESCRIPTION <code>a</code> <pre><code> Alpha value(s) to apply. Values must be 0-1. Accepted:\n   1. Tuple of single alpha. Applied to all visible neurons.\n   2. Dictionary mapping skeleton IDs to alpha.\n</code></pre> <p> TYPE: <code>     tuple | dict</code> </p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>@block_all\ndef set_alpha(self, a, include_connectors=True):\n    \"\"\"Set neuron color alphas.\n\n    Parameters\n    ----------\n    a :      tuple | dict\n             Alpha value(s) to apply. Values must be 0-1. Accepted:\n               1. Tuple of single alpha. Applied to all visible neurons.\n               2. Dictionary mapping skeleton IDs to alpha.\n\n    \"\"\"\n    neurons = self.neurons  # grab once to speed things up\n    if isinstance(a, (tuple, list, np.ndarray, str)):\n        amap = {s: a for s in neurons}\n    elif isinstance(a, dict):\n        amap = a\n    else:\n        raise TypeError(f'Unable to use colors of type \"{type(a)}\"')\n\n    for n in neurons:\n        if n in amap:\n            for v in neurons[n]:\n                if getattr(v, 'pinned', False):\n                    continue\n                if v._neuron_part == 'connectors' and not include_connectors:\n                    continue\n                try:\n                    this_c = v.color.rgba\n                except BaseException:\n                    this_c = v.color\n\n                this_c = np.asarray(this_c)\n\n                # For arrays of colors\n                if this_c.ndim == 2:\n                    # If no alpha channel yet, add one\n                    if this_c.shape[1] == 3:\n                        this_c = np.insert(this_c,\n                                           3,\n                                           np.ones(this_c.shape[0]),\n                                           axis=1)\n\n                    # If already the correct alpha value\n                    if np.all(this_c[:, 3] == amap[n]):\n                        continue\n                    else:\n                        this_c[:, 3] = amap[n]\n                else:\n                    if len(this_c) == 4 and this_c[3] == amap[n]:\n                        continue\n                    else:\n                        this_c = tuple([this_c[0], this_c[1], this_c[2], amap[n]])\n\n                if isinstance(v, scene.visuals.Mesh):\n                    v.color = this_c\n                else:\n                    v.set_data(color=this_c)\n\n    if self.show_legend:\n        self.update_legend()\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.set_bgcolor","title":"<code>set_bgcolor</code>","text":"<p>Set background color.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def set_bgcolor(self, c):\n    \"\"\"Set background color.\"\"\"\n    if getattr(config, 'headless', False):\n        return\n    self.canvas.bgcolor = c\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.set_colors","title":"<code>set_colors</code>","text":"<p>Set neuron color.</p> PARAMETER DESCRIPTION <code>c</code> <pre><code> RGB color(s) to apply. Values must be 0-1. Accepted:\n   1. Tuple of single color. Applied to all visible neurons.\n   2. Dictionary mapping skeleton IDs to colors.\n</code></pre> <p> TYPE: <code>     tuple | dict</code> </p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>@block_all\ndef set_colors(self, c, include_connectors=False):\n    \"\"\"Set neuron color.\n\n    Parameters\n    ----------\n    c :      tuple | dict\n             RGB color(s) to apply. Values must be 0-1. Accepted:\n               1. Tuple of single color. Applied to all visible neurons.\n               2. Dictionary mapping skeleton IDs to colors.\n\n    \"\"\"\n    neurons = self.neurons  # grab once to speed things up\n    if isinstance(c, (tuple, list, np.ndarray, str)):\n        cmap = {s: c for s in neurons}\n    elif isinstance(c, dict):\n        cmap = c\n    else:\n        raise TypeError(f'Unable to use colors of type \"{type(c)}\"')\n\n    for n in neurons:\n        if n in cmap:\n            for v in neurons[n]:\n                if getattr(v, 'pinned', False):\n                    continue\n                if v._neuron_part == 'connectors' and not include_connectors:\n                    continue\n                new_c = mcl.to_rgba(cmap[n])\n                if isinstance(v, scene.visuals.Mesh):\n                    v.color = new_c\n                else:\n                    v.set_data(color=mcl.to_rgba(cmap[n]))\n\n    if self.show_legend:\n        self.update_legend()\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.set_view","title":"<code>set_view</code>","text":"<p>(Re-)set camera position.</p> PARAMETER DESCRIPTION <code>view</code> <p> TYPE: <code>     XY | XZ | YZ</code> </p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def set_view(self, view):\n    \"\"\"(Re-)set camera position.\n\n    Parameters\n    ----------\n    view :      XY | XZ | YZ\n\n    \"\"\"\n    if isinstance(view, Quaternion):\n        q = view\n    elif view == 'XY':\n        q = Quaternion(w=0.707, x=0.707, y=0, z=0)\n    elif view == 'XZ':\n        q = Quaternion(w=1, x=0, y=0, z=0)\n    elif view == 'YZ':\n        q = Quaternion(w=.5, x=0.5, y=0.5, z=-.5)\n    else:\n        raise TypeError(f'Unable to set view from {type(view)}')\n\n    self.camera3d._quaternion = q\n    # This is necessary to force a redraw\n    self.camera3d.set_range()\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.show","title":"<code>show</code>","text":"<p>Show viewer.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def show(self):\n    \"\"\"Show viewer.\"\"\"\n    # This is for e.g. headless testing\n    if getattr(config, 'headless', False):\n        logger.info(\"Viewer widget not shown - navis running in headless mode. \")\n        return\n\n    self.canvas.show()\n\n    # To actually show the widget, we need to return the canvas\n    if utils.is_jupyter():\n        from IPython.display import display\n        display(self.canvas)\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.toggle_bounds","title":"<code>toggle_bounds</code>","text":"<p>Toggle bounding box.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def toggle_bounds(self):\n    \"\"\"Toggle bounding box.\"\"\"\n    self.show_bounds = not self.show_bounds\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.toggle_neurons","title":"<code>toggle_neurons</code>","text":"<p>Toggle neuron(s) visibility.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def toggle_neurons(self, n):\n    \"\"\"Toggle neuron(s) visibility.\"\"\"\n    n = utils.make_iterable(n)\n\n    if False not in [isinstance(u, uuid.UUID) for u in n]:\n        obj = self._neuron_obj\n    else:\n        n = utils.eval_id(n)\n        obj = self.neurons\n\n    for s in n:\n        for v in obj[s]:\n            v.visible = v.visible is False\n\n    self.update_legend()\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.toggle_overlay","title":"<code>toggle_overlay</code>","text":"<p>Toggle legend on and off.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def toggle_overlay(self):\n    \"\"\"Toggle legend on and off.\"\"\"\n    self.overlay.visible = self.overlay.visible is False\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.toggle_picking","title":"<code>toggle_picking</code>","text":"<p>Toggle picking and overlay text.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def toggle_picking(self):\n    \"\"\"Toggle picking and overlay text.\"\"\"\n    if self.picking:\n        self.picking = False\n        self._picking_text.visible = False\n    else:\n        self.picking = True\n        self._picking_text.visible = True\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.toggle_select","title":"<code>toggle_select</code>","text":"<p>Toggle selected of given neuron.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def toggle_select(self, n):\n    \"\"\"Toggle selected of given neuron.\"\"\"\n    skids = utils.eval_id(n)\n\n    neurons = self.neurons  # grab once to speed things up\n\n    for s in skids:\n        if self.selected != s:\n            self.selected = s\n            for v in neurons[s]:\n                self._selected_color = v.color\n                v.set_data(color=self.highlight_color)\n        else:\n            self.selected = None\n            for v in neurons[s]:\n                v.set_data(color=self._selected_color)\n\n    self.update_legend()\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.unhide_neurons","title":"<code>unhide_neurons</code>","text":"<p>Unhide given neuron(s).</p> <p>Use <code>n</code> to unhide specific neurons.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def unhide_neurons(self, n=None, check_alpha=False):\n    \"\"\"Unhide given neuron(s).\n\n    Use `n` to unhide specific neurons.\n\n    \"\"\"\n    neurons = self.neurons  # grab once to speed things up\n    if not isinstance(n, type(None)):\n        ids = utils.eval_id(n)\n    else:\n        ids = list(neurons.keys())\n\n    for s in ids:\n        for v in neurons[s]:\n            if getattr(v, 'pinned', False):\n                continue\n            if not v.visible:\n                v.visible = True\n        if check_alpha:\n            # Make sure color has an alpha channel\n            c = to_rgba(neurons[s][0].color)\n            # Make sure alpha is 1\n            if c.ndim == 1 and c[3] != 1:\n                c[3] = 1\n                self.set_colors({s: c})\n            elif c.ndim == 2 and np.any(c[:, 3] != 1):\n                c[:, 3] = 1\n                self.set_colors({s: c})\n\n    self.update_legend()\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.unpin_neurons","title":"<code>unpin_neurons</code>","text":"<p>Unpin given neuron(s).</p> <p>Use <code>n</code> to unhide specific neurons.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def unpin_neurons(self, n=None):\n    \"\"\"Unpin given neuron(s).\n\n    Use `n` to unhide specific neurons.\n\n    \"\"\"\n    neurons = self.neurons  # grab once to speed things up\n    if not isinstance(n, type(None)):\n        ids = utils.eval_id(n)\n    else:\n        ids = list(neurons.keys())\n\n    for s in ids:\n        for v in neurons[s]:\n            v.unfreeze()\n            v.pinned = False\n            v.freeze()\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.update_bounds","title":"<code>update_bounds</code>","text":"<p>Update bounding box visual.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>@block_canvas\ndef update_bounds(self, color='w', width=1):\n    \"\"\"Update bounding box visual.\"\"\"\n    # Remove any existing visual\n    self.remove_bounds()\n\n    bounds = self.bounds\n    self._show_bounds = True\n\n    # Skip if no visual on canvas\n    if isinstance(bounds, type(None)):\n        return\n\n    # Create box visual\n    dims = bounds[:, 1] - bounds[:, 0]\n    center = bounds.mean(axis=1)\n    box = tm.primitives.Box(extents=dims).apply_scale(1.1)\n\n    # Recenter vertices\n    vertices = np.array(box.vertices) + center\n    connect = np.array([[0, 1], [0, 2], [0, 4],\n                        [1, 3], [1, 5],\n                        [2, 3], [2, 6],\n                        [3, 7],\n                        [4, 5], [4, 6],\n                        [5, 7],\n                        [6, 7]])\n\n    box = scene.visuals.Line(pos=vertices,\n                             color=mcl.to_rgb(color),\n                             # Can only be used with method 'agg'\n                             width=width,\n                             connect=connect,\n                             antialias=True,\n                             name='BoundingBox',\n                             method='gl')\n\n    # Add custom attributes\n    box.unfreeze()\n    box._object_type = 'boundingbox'\n    box._object_id = uuid.uuid4()\n    box.freeze()\n\n    self.view3d.add(box)\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.update_legend","title":"<code>update_legend</code>","text":"<p>Update legend.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>@block_canvas\ndef update_legend(self):\n    \"\"\"Update legend.\"\"\"\n    # Get existing labels\n    labels = {l._object_id: l for l in self.overlay.children if getattr(l, '_object_id', None)}\n\n    # If legend is not meant to be shown, make sure everything is hidden and return\n    if not self.show_legend:\n        for v in labels.values():\n            if v.visible:\n                v.visible = False\n        return\n    else:\n        for v in labels.values():\n            if not v.visible:\n                v.visible = True\n\n    # Labels to be removed\n    neuron_obj = self._neuron_obj  # grab only once to speed things up\n    to_remove = [s for s in labels if s not in neuron_obj]\n    for s in to_remove:\n        labels[s].parent = None\n\n    # Generate new labels\n    to_add = [s for s in neuron_obj if s not in labels]\n    for s in to_add:\n        # Fallback is name or in lieu of that the object's type\n        lbl = getattr(neuron_obj[s][0], '_name',\n                      str(type(neuron_obj[s][0])))\n        # See if we find a \"label\" property\n        if hasattr(neuron_obj[s][0], '_object'):\n            if hasattr(neuron_obj[s][0]._object, 'label'):\n                lbl = neuron_obj[s][0]._object.label\n\n        txt = scene.visuals.Text(lbl,\n                                 anchor_x='left',\n                                 anchor_y='top',\n                                 parent=self.overlay,\n                                 method='gpu',\n                                 font_size=self.legend_font_size)\n        txt.interactive = True\n        txt.unfreeze()\n        txt._object_id = s\n        txt._id = neuron_obj[s][0]._id\n        txt.freeze()\n\n    # Position and color labels\n    labels = {l._object_id: l for l in self.overlay.children if getattr(\n        l, '_object_id', None)}\n    for i, s in enumerate(sorted(neuron_obj)):\n        if neuron_obj[s][0].visible:\n            color = neuron_obj[s][0].color\n        else:\n            color = (.3, .3, .3)\n\n        offset = 10 * (self.legend_font_size / 7)\n\n        labels[s].pos = (10, offset * (i + 1))\n        labels[s].color = color\n        labels[s].font_size = self.legend_font_size\n</code></pre>"},{"location":"reference/navis/#navis.Viewer.visuals_at","title":"<code>visuals_at</code>","text":"<p>List visuals at given canvas position.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def visuals_at(self, pos):\n    \"\"\"List visuals at given canvas position.\"\"\"\n    # There appears to be some odd y offset - perhaps because of the\n    # window's top bar? On OSX this is about 15px\n    pos = (pos[0], pos[1] - 15)\n\n    # Map mouse pos to framebuffer\n    tr = self.canvas.transforms.get_transform(map_from='canvas',\n                                              map_to='framebuffer')\n    pos = tr.map(pos)\n\n    # Render framebuffer in picking mode\n    p = self._render_fb(crop=(pos[0] - self._picking_radius / 2,\n                              pos[1] - self._picking_radius / 2,\n                              self._picking_radius,\n                              self._picking_radius))\n\n    logger.debug('Picking framebuffer:')\n    logger.debug(p)\n\n    # List visuals in order from distance to center\n    ids = []\n    seen = set()\n    center = (np.array(p.shape) / 2).astype(int)\n    for i in range(self._picking_radius * self.canvas.pixel_scale):\n        subr = p[center[0] - i: center[0] + i + 1,\n                 center[1] - i: center[1] + i + 1]\n        subr_ids = set(list(np.unique(subr)))\n        ids.extend(list(subr_ids - seen))\n        seen |= subr_ids\n    visuals = [scene.visuals.VisualNode._visual_ids.get(x, None) for x in ids]\n\n    return [v for v in visuals if v is not None]\n</code></pre>"},{"location":"reference/navis/#navis.Volume","title":"<code>navis.Volume</code>","text":"<p>Mesh consisting of vertices and faces.</p> <p>Subclass of <code>trimesh.Trimesh</code> with a few additional methods.</p> PARAMETER DESCRIPTION <code>vertices</code> <pre><code>    `(N, 3)` vertices coordinates or an object that has\n    `.vertices` and `.faces` attributes in which case `faces`\n    parameter will be ignored.\n</code></pre> <p> TYPE: <code> list | array | mesh-like</code> </p> <code>faces</code> <pre><code>    `(M, 3)` array of indexed triangle faces.\n</code></pre> <p> TYPE: <code>    list | array</code> DEFAULT: <code>None</code> </p> <code>name</code> <pre><code>    A name for the volume.\n</code></pre> <p> TYPE: <code>     str</code> DEFAULT: <code>None</code> </p> <code>color</code> <pre><code>    RGB(A) color.\n</code></pre> <p> TYPE: <code>    tuple</code> DEFAULT: <code>(0.85, 0.85, 0.85, 0.2)</code> </p> <code>id</code> <pre><code>    If not provided, neuron will be assigned a random UUID as `.id`.\n</code></pre> <p> TYPE: <code>       int</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <pre><code>    Keyword arguments passed through to `trimesh.Trimesh`\n</code></pre> <p> DEFAULT: <code>{}</code> </p> See Also <p><code>navis.example_volume</code>             Loads example volume(s).</p> Source code in <code>navis/core/volumes.py</code> <pre><code>class Volume(UnitObject, trimesh.Trimesh):\n    \"\"\"Mesh consisting of vertices and faces.\n\n    Subclass of `trimesh.Trimesh` with a few additional methods.\n\n    Parameters\n    ----------\n    vertices :  list | array | mesh-like\n                `(N, 3)` vertices coordinates or an object that has\n                `.vertices` and `.faces` attributes in which case `faces`\n                parameter will be ignored.\n    faces :     list | array\n                `(M, 3)` array of indexed triangle faces.\n    name :      str, optional\n                A name for the volume.\n    color :     tuple, optional\n                RGB(A) color.\n    id :        int, optional\n                If not provided, neuron will be assigned a random UUID as `.id`.\n    **kwargs\n                Keyword arguments passed through to `trimesh.Trimesh`\n\n    See Also\n    --------\n    [`navis.example_volume`][]\n                Loads example volume(s).\n\n    \"\"\"\n\n    def __init__(\n        self,\n        vertices: Union[list, np.ndarray],\n        faces: Union[list, np.ndarray] = None,\n        name: Optional[str] = None,\n        color: Union[str, Sequence[Union[int, float]]] = (0.85, 0.85, 0.85, 0.2),\n        id: Optional[int] = None,\n        units: Optional[str] = None,\n        **kwargs,\n    ):\n        if hasattr(vertices, \"vertices\") and hasattr(vertices, \"faces\"):\n            vertices, faces = vertices.vertices, vertices.faces\n\n        super().__init__(vertices=vertices, faces=faces, **kwargs)\n\n        self.name: Optional[str] = name\n        self.color: Union[str, Sequence[Union[int, float]]] = color\n        self.id: Optional[int] = id if id else uuid.uuid4()\n        self.units = units\n\n        # This is very hackish but we want to make sure that parent methods of\n        # Trimesh return a navis.Volume instead of a trimesh.Trimesh\n        for f in dir(trimesh.Trimesh):\n            # Don't mess with magic/private methods\n            if f.startswith(\"_\"):\n                continue\n            # Skip properties\n            if not callable(getattr(trimesh.Trimesh, f)):\n                continue\n            setattr(self, f, _force_volume(getattr(self, f)))\n\n    @property\n    def name(self):\n        \"\"\"Name of this volume.\"\"\"\n        return self.metadata.get(\"name\")\n\n    @name.setter\n    def name(self, value):\n        self.metadata[\"name\"] = value\n\n    @property\n    def color(self):\n        \"\"\"Color used for plotting.\"\"\"\n        return self.metadata.get(\"color\")\n\n    @color.setter\n    def color(self, value):\n        self.metadata[\"color\"] = value\n\n    @property\n    def id(self):\n        \"\"\"ID of this volume.\"\"\"\n        return self.metadata.get(\"id\")\n\n    @id.setter\n    def id(self, value):\n        self.metadata[\"id\"] = value\n\n    @classmethod\n    def from_csv(\n        cls,\n        vertices: str,\n        faces: str,\n        name: Optional[str] = None,\n        color: Union[str, Sequence[Union[int, float]]] = (0.85, 0.85, 0.85, 0.2),\n        volume_id: Optional[int] = None,\n        **kwargs,\n    ) -&gt; \"Volume\":\n        \"\"\"Load volume from csv files containing vertices and faces.\n\n        Parameters\n        ----------\n        vertices :      filepath | file-like\n                        CSV file containing vertices.\n        faces :         filepath | file-like\n                        CSV file containing faces.\n        **kwargs\n                        Keyword arguments passed to `csv.reader`.\n\n        Returns\n        -------\n        navis.Volume\n\n        \"\"\"\n        if not os.path.isfile(vertices) or not os.path.isfile(faces):\n            raise ValueError(\"File(s) not found.\")\n\n        with open(vertices, \"r\") as f:\n            reader = csv.reader(f, **kwargs)\n            vertices = np.array([r for r in reader]).astype(float)\n\n        with open(faces, \"r\") as f:\n            reader = csv.reader(f, **kwargs)\n            faces = np.array([r for r in reader]).astype(int)\n\n        return cls(\n            faces=faces, vertices=vertices, name=name, color=color, volume_id=volume_id\n        )\n\n    def to_csv(self, filename: str, **kwargs) -&gt; None:\n        \"\"\"Save volume as two separated csv files containing vertices and faces.\n\n        Parameters\n        ----------\n        filename :      str\n                        Filename to use. Will get a `_vertices.csv` and\n                        `_faces.csv` suffix.\n        **kwargs\n                        Keyword arguments passed to `csv.reader`.\n\n        \"\"\"\n        for data, suffix in zip(\n            [self.faces, self.vertices], [\"_faces.csv\", \"_vertices.csv\"]\n        ):\n            with open(filename + suffix, \"w\") as csvfile:\n                writer = csv.writer(csvfile)\n                writer.writerows(data)\n\n    @classmethod\n    def from_json(\n        cls, filename: str, import_kwargs: Dict = {}, **init_kwargs\n    ) -&gt; \"Volume\":\n        \"\"\"Load volume from json file containing vertices and faces.\n\n        Parameters\n        ----------\n        filename\n        import_kwargs\n                        Keyword arguments passed to `json.load`.\n        **init_kwargs\n                    Keyword arguments passed to navis.Volume upon\n                    initialization.\n\n        Returns\n        -------\n        navis.Volume\n\n        \"\"\"\n        if not os.path.isfile(filename):\n            raise ValueError(\"File not found.\")\n\n        with open(filename, \"r\") as f:\n            data = json.load(f, **import_kwargs)\n\n        return cls(faces=data[\"faces\"], vertices=data[\"vertices\"], **init_kwargs)\n\n    @classmethod\n    def from_object(cls, obj: Any, **init_kwargs) -&gt; \"Volume\":\n        \"\"\"Load volume from generic object that has `.vertices` and\n        `.faces` attributes.\n\n        Parameters\n        ----------\n        obj\n        **init_kwargs\n                    Keyword arguments passed to navis.Volume upon\n                    initialization.\n\n        Returns\n        -------\n        navis.Volume\n\n        \"\"\"\n        if not hasattr(obj, \"vertices\") or not hasattr(obj, \"faces\"):\n            raise ValueError(\"Object must have faces and vertices attributes.\")\n\n        return cls(faces=obj.faces, vertices=obj.vertices, **init_kwargs)\n\n    @classmethod\n    def from_file(\n        cls, filename: str, import_kwargs: Dict = {}, **init_kwargs\n    ) -&gt; \"Volume\":\n        \"\"\"Load volume from file.\n\n        Parameters\n        ----------\n        filename :      str\n                        File to load from.\n        import_kwargs\n                        Keyword arguments passed to importer:\n                          - `json.load` for JSON file\n                          - `trimesh.load_mesh` for OBJ and STL files\n        **init_kwargs\n                    Keyword arguments passed to navis.Volume upon\n                    initialization.\n\n        Returns\n        -------\n        navis.Volume\n\n        \"\"\"\n        if not os.path.isfile(filename):\n            raise ValueError(\"File not found.\")\n\n        f, ext = os.path.splitext(filename)\n\n        if ext == \".json\":\n            return cls.from_json(\n                filename=filename, import_kwargs=import_kwargs, **init_kwargs\n            )\n\n        try:\n            import trimesh\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                \"Unable to import: trimesh missing - please \"\n                'install: \"pip install trimesh\"'\n            )\n        except BaseException:\n            raise\n\n        tm = trimesh.load_mesh(filename, **import_kwargs)\n\n        return cls.from_object(tm, **init_kwargs)\n\n    def to_json(self, filename: str) -&gt; None:\n        \"\"\"Save volume as json file.\n\n        Parameters\n        ----------\n        filename :      str\n                        Filename to use.\n\n        \"\"\"\n        with open(filename, \"w\") as f:\n            json.dump(\n                {\"vertices\": self.vertices.tolist(), \"faces\": self.faces.tolist()}, f\n            )\n\n    @classmethod\n    def combine(\n        cls,\n        x: Sequence[\"Volume\"],\n        name: str = \"comb_vol\",\n        color: Union[str, Sequence[Union[int, float]]] = (0.85, 0.85, 0.85, 0.2),\n    ) -&gt; \"Volume\":\n        \"\"\"Merge multiple volumes into a single object.\n\n        Parameters\n        ----------\n        x :     list or dict of Volumes\n        name :  str, optional\n                Name of the combined volume.\n        color : tuple | str, optional\n                Color of the combined volume.\n\n        Returns\n        -------\n        [`navis.Volume`][]\n\n        \"\"\"\n        if isinstance(x, Volume):\n            return x\n\n        if isinstance(x, dict):\n            x = list(x.values())\n\n        if not utils.is_iterable(x):\n            x = [x]  # type: ignore\n\n        if False in [isinstance(v, Volume) for v in x]:\n            raise TypeError(\"Input must be list of volumes\")\n\n        vertices: np.ndarray = np.empty((0, 3))\n        faces: List[List[int]] = []\n\n        # Reindex faces\n        for vol in x:\n            offs = len(vertices)\n            vertices = np.append(vertices, vol.vertices, axis=0)\n            faces += [[f[0] + offs, f[1] + offs, f[2] + offs] for f in vol.faces]\n\n        return cls(vertices=vertices, faces=faces, name=name, color=color)\n\n    @property\n    def bbox(self) -&gt; np.ndarray:\n        \"\"\"Bounding box of this volume.\"\"\"\n        return self.bounds\n\n    @property\n    def verts(self) -&gt; np.ndarray:\n        \"\"\"Legacy access to `.vertices`.\"\"\"\n        return self.vertices\n\n    @verts.setter\n    def verts(self, v):\n        self.vertices = v\n\n    @property\n    def center(self) -&gt; np.ndarray:\n        \"\"\"Center of mass.\"\"\"\n        return np.mean(self.vertices, axis=0)\n\n    def __getstate__(self):\n        \"\"\"Get state (used e.g. for pickling).\"\"\"\n        return {k: v for k, v in self.__dict__.items() if not callable(v)}\n\n    def __setstate__(self, d):\n        \"\"\"Update state (used e.g. for pickling).\"\"\"\n        self.__dict__.update(d)\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __repr__(self):\n        \"\"\"Return quick summary of the current geometry.\n\n        Avoids computing properties.\n\n        \"\"\"\n        elements = []\n        if hasattr(self, \"name\"):\n            # for Trimesh\n            elements.append(f\"name={self.name}\")\n        if hasattr(self, \"id\") and not isinstance(self.id, uuid.UUID):\n            # for Trimesh\n            elements.append(f\"id={self.id}\")\n        elements.append(f\"units={self.units}\")\n        if hasattr(self, \"color\"):\n            # for Trimesh\n            elements.append(f\"color={self.color}\")\n        if hasattr(self, \"vertices\"):\n            # for Trimesh and PointCloud\n            elements.append(f\"vertices.shape={self.vertices.shape}\")\n        if hasattr(self, \"faces\"):\n            # for Trimesh\n            elements.append(f\"faces.shape={self.faces.shape}\")\n        return f'&lt;navis.Volume({\", \".join(elements)})&gt;'\n\n    def __truediv__(self, other):\n        \"\"\"Implement division for vertices.\"\"\"\n        if isinstance(other, numbers.Number) or utils.is_iterable(other):\n            n = self.copy()\n            _ = np.divide(n.vertices, other, out=n.vertices, casting=\"unsafe\")\n            return n\n        return NotImplemented\n\n    def __mul__(self, other):\n        \"\"\"Implement multiplication for vertices.\"\"\"\n        if isinstance(other, numbers.Number) or utils.is_iterable(other):\n            n = self.copy()\n            _ = np.multiply(n.vertices, other, out=n.vertices, casting=\"unsafe\")\n            return n\n        return NotImplemented\n\n    def __add__(self, other):\n        \"\"\"Implement addition for vertices.\"\"\"\n        if isinstance(other, numbers.Number) or utils.is_iterable(other):\n            n = self.copy()\n            _ = np.add(n.vertices, other, out=n.vertices, casting=\"unsafe\")\n            return n\n        return NotImplemented\n\n    def __sub__(self, other):\n        \"\"\"Implement subtraction for vertices.\"\"\"\n        if isinstance(other, numbers.Number) or utils.is_iterable(other):\n            n = self.copy()\n            _ = np.subtract(n.vertices, other, out=n.vertices, casting=\"unsafe\")\n            return n\n        return NotImplemented\n\n    def resize(\n        self,\n        x: Union[float, int],\n        method: Union[\n            Literal[\"center\"],\n            Literal[\"centroid\"],\n            Literal[\"normals\"],\n            Literal[\"origin\"],\n        ] = \"center\",\n        inplace: bool = False,\n    ) -&gt; Optional[\"Volume\"]:\n        \"\"\"Resize volume.\n\n        Parameters\n        ----------\n        x :         int | float\n                    Resizing factor. For methods \"center\", \"centroid\" and\n                    \"origin\" this is the fraction of original size (e.g.\n                    `.5` for half size). For method \"normals\", this is\n                    is the absolute displacement (e.g. `-1000` to shrink\n                    volume by that many units)!\n        method :    \"center\" | \"centroid\" | \"normals\" | \"origin\"\n                    Point in space to use for resizing.\n\n                    .. list-table::\n                        :widths: 15 75\n                        :header-rows: 1\n\n                        * - method\n                          - explanation\n                        * - center\n                          - average of all vertices\n                        * - centroid\n                          - average of the triangle centroids weighted by the\n                            area of each triangle.\n                        * - origin\n                          - resizes relative to origin of coordinate system\n                            (0, 0, 0)\n                        * - normals\n                          - resize using face normals. Note that this method\n                            uses absolute displacement for parameter `x`.\n\n        inplace :   bool, optional\n                    If False, will return resized copy.\n\n        Returns\n        -------\n        [`navis.Volume`][]\n                    Resized copy of original volume. Only if `inplace=False`.\n        None\n                    If `inplace=True`.\n\n        \"\"\"\n        assert isinstance(method, str)\n\n        method = method.lower()\n\n        perm_methods = [\"center\", \"origin\", \"normals\", \"centroid\"]\n        if method not in perm_methods:\n            raise ValueError(\n                f'Unknown method \"{method}\". Allowed '\n                f'methods: {\", \".join(perm_methods)}'\n            )\n\n        if not inplace:\n            v = self.copy()\n        else:\n            v = self\n\n        if method == \"normals\":\n            v.vertices = v.vertices + (v.vertex_normals * x)\n        else:\n            # Get the center\n            if method == \"center\":\n                cn = np.mean(v.vertices, axis=0)\n            elif method == \"centroid\":\n                cn = v.centroid\n            elif method == \"origin\":\n                cn = np.array([0, 0, 0])\n\n            # Get vector from center to each vertex\n            vec = v.vertices - cn\n\n            # Multiply vector by resize factor\n            vec *= x\n\n            # Recalculate vertex positions\n            v.vertices = vec + cn\n\n        # Make sure to reset any pyoctree data on this volume\n        if hasattr(v, \"pyoctree\"):\n            delattr(v, \"pyoctree\")\n\n        if not inplace:\n            return v\n\n    def plot3d(self, **kwargs):\n        \"\"\"Plot volume using [`navis.plot3d`][].\n\n        Parameters\n        ----------\n        **kwargs\n                Keyword arguments. Will be passed to [`navis.plot3d`][].\n                See `help(navis.plot3d)` for a list of keywords.\n\n        See Also\n        --------\n        [`navis.plot3d`][]\n                    Function called to generate 3d plot.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import navis\n        &gt;&gt;&gt; vol = navis.example_volume('LH')\n        &gt;&gt;&gt; v = vol.plot3d(color = (255, 0, 0))\n\n        \"\"\"\n        from .. import plotting\n\n        if \"color\" in kwargs:\n            self.color = kwargs[\"color\"]\n\n        return plotting.plot3d(self, **kwargs)\n\n    def show(self, **kwargs):\n        \"\"\"See `.plot3d`.\"\"\"\n        # This is mostly to override trimesh.Trimesh method\n        return self.plot3d(**kwargs)\n\n    def _outlines_3d(self, view=\"xy\", **kwargs):\n        \"\"\"Generate 3d outlines along a given view (see `.to_2d()`).\n\n        Parameters\n        ----------\n        **kwargs\n                    Keyword arguments passed to [`navis.Volume.to_2d`][].\n\n        Returns\n        -------\n        list\n                    Coordinates of 2d circumference.\n                    e.g. `[(x1, y1, z1), (x2, y2, z2), (x3, y3, z3), ...]`\n                    Third dimension is averaged.\n\n        \"\"\"\n        co2d = np.array(self.to_2d(view=view, **kwargs))\n\n        if view in [\"xy\", \"yx\"]:\n            third = np.repeat(self.center[2], co2d.shape[0])\n        elif view in [\"xz\", \"zx\"]:\n            third = np.repeat(self.center[1], co2d.shape[0])\n        elif view in [\"yz\", \"zy\"]:\n            third = np.repeat(self.center[0], co2d.shape[0])\n\n        return np.append(co2d, third.reshape(co2d.shape[0], 1), axis=1)\n\n    def to_2d(\n        self, alpha: float = 0.00017, view: tuple = (\"x\", \"y\"), invert_y: bool = False\n    ) -&gt; Sequence[Union[float, int]]:\n        \"\"\"Compute the 2d alpha shape (concave hull) this volume.\n\n        Uses Scipy Delaunay and shapely.\n\n        Parameters\n        ----------\n        alpha:      float, optional\n                    Alpha value to influence the gooeyness of the border.\n                    Smaller numbers don't fall inward as much as larger\n                    numbers. Too large, and you lose everything!\n        view :      tuple\n                    Determines axis.\n\n        Returns\n        -------\n        list\n                    Coordinates of 2d circumference\n                    e.g. `[(x1, y1), (x2, y2), (x3, y3), ...]`\n\n        \"\"\"\n\n        def add_edge(edges, edge_points, coords, i, j):\n            \"\"\"Add line between the i-th and j-th points.\"\"\"\n            if (i, j) in edges or (j, i) in edges:\n                # already added\n                return\n            edges.add((i, j))\n            edge_points.append(coords[[i, j]])\n\n        accepted_views = [\"x\", \"z\", \"y\", \"-x\", \"-z\", \"-y\"]\n\n        for ax in view:\n            if ax not in accepted_views:\n                raise ValueError(f'Unable to parse \"{ax}\" view')\n\n        try:\n            from shapely.ops import unary_union, polygonize  # type: ignore\n            import shapely.geometry as geometry  # type: ignore\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\"This function needs the shapely&gt;=1.8.0\")\n\n        coords: np.ndarray\n\n        map = {\"x\": 0, \"y\": 1, \"z\": 2}\n\n        x_ix = map[view[0].replace(\"-\", \"\").replace(\"+\", \"\")]\n        y_ix = map[view[1].replace(\"-\", \"\").replace(\"+\", \"\")]\n\n        coords = self.vertices[:, [x_ix, y_ix]]\n\n        tri = scipy.spatial.Delaunay(coords)\n        edges: set = set()\n        edge_points: list = []\n        # loop over triangles:\n        # ia, ib, ic = indices of corner points of the triangle\n        # Note that \"vertices\" property was renamed to \"simplices\"\n        for ia, ib, ic in getattr(tri, \"simplices\", getattr(tri, \"vertices\", [])):\n            pa: np.ndarray = coords[ia]  # type: ignore\n            pb: np.ndarray = coords[ib]  # type: ignore\n            pc: np.ndarray = coords[ic]  # type: ignore\n            # Lengths of sides of triangle\n            a = math.sqrt((pa[0] - pb[0]) ** 2 + (pa[1] - pb[1]) ** 2)  # type: ignore\n            b = math.sqrt((pb[0] - pc[0]) ** 2 + (pb[1] - pc[1]) ** 2)  # type: ignore\n            c = math.sqrt((pc[0] - pa[0]) ** 2 + (pc[1] - pa[1]) ** 2)  # type: ignore\n            # Semiperimeter of triangle\n            s = (a + b + c) / 2.0\n            # Area of triangle by Heron's formula\n            area = math.sqrt(s * (s - a) * (s - b) * (s - c))\n            circum_r = a * b * c / (4.0 * area)\n            # Here's the radius filter.\n            if circum_r &lt; 1.0 / alpha:\n                add_edge(edges, edge_points, coords, ia, ib)\n                add_edge(edges, edge_points, coords, ib, ic)\n                add_edge(edges, edge_points, coords, ic, ia)\n\n        m = geometry.MultiLineString(edge_points)\n        triangles = list(polygonize(m))\n        concave_hull = unary_union(triangles)\n\n        # Try with current settings, if this is not successful, try again\n        # with lower alpha\n        try:\n            return list(concave_hull.exterior.coords)\n        except AttributeError:\n            return self.to_2d(alpha=alpha / 10, view=view, invert_y=invert_y)\n        except BaseException:\n            raise\n\n    def validate(self):\n        \"\"\"Use trimesh to try and fix issues (holes/normals).\"\"\"\n        if not self.is_volume:\n            logger.info(\"Mesh not valid, attempting to fix\")\n            self.fill_holes()\n            self.fix_normals()\n            if not self.is_volume:\n                raise utils.VolumeError(\n                    \"Mesh is not a volume \"\n                    \"(e.g. not watertight, incorrect \"\n                    \"winding) and could not be fixed.\"\n                )\n</code></pre>"},{"location":"reference/navis/#navis.Volume.bbox","title":"<code>bbox: np.ndarray</code>  <code>property</code>","text":"<p>Bounding box of this volume.</p>"},{"location":"reference/navis/#navis.Volume.center","title":"<code>center: np.ndarray</code>  <code>property</code>","text":"<p>Center of mass.</p>"},{"location":"reference/navis/#navis.Volume.color","title":"<code>color</code>  <code>property</code> <code>writable</code>","text":"<p>Color used for plotting.</p>"},{"location":"reference/navis/#navis.Volume.id","title":"<code>id</code>  <code>property</code> <code>writable</code>","text":"<p>ID of this volume.</p>"},{"location":"reference/navis/#navis.Volume.name","title":"<code>name</code>  <code>property</code> <code>writable</code>","text":"<p>Name of this volume.</p>"},{"location":"reference/navis/#navis.Volume.verts","title":"<code>verts: np.ndarray</code>  <code>property</code> <code>writable</code>","text":"<p>Legacy access to <code>.vertices</code>.</p>"},{"location":"reference/navis/#navis.Volume.combine","title":"<code>combine</code>  <code>classmethod</code>","text":"<p>Merge multiple volumes into a single object.</p> PARAMETER DESCRIPTION <code>x</code> <p> TYPE: <code>    list or dict of Volumes</code> </p> <code>name</code> <pre><code>Name of the combined volume.\n</code></pre> <p> TYPE: <code> str</code> DEFAULT: <code>'comb_vol'</code> </p> <code>color</code> <pre><code>Color of the combined volume.\n</code></pre> <p> TYPE: <code>tuple | str</code> DEFAULT: <code>(0.85, 0.85, 0.85, 0.2)</code> </p> RETURNS DESCRIPTION <code>[`navis.Volume`][]</code> Source code in <code>navis/core/volumes.py</code> <pre><code>@classmethod\ndef combine(\n    cls,\n    x: Sequence[\"Volume\"],\n    name: str = \"comb_vol\",\n    color: Union[str, Sequence[Union[int, float]]] = (0.85, 0.85, 0.85, 0.2),\n) -&gt; \"Volume\":\n    \"\"\"Merge multiple volumes into a single object.\n\n    Parameters\n    ----------\n    x :     list or dict of Volumes\n    name :  str, optional\n            Name of the combined volume.\n    color : tuple | str, optional\n            Color of the combined volume.\n\n    Returns\n    -------\n    [`navis.Volume`][]\n\n    \"\"\"\n    if isinstance(x, Volume):\n        return x\n\n    if isinstance(x, dict):\n        x = list(x.values())\n\n    if not utils.is_iterable(x):\n        x = [x]  # type: ignore\n\n    if False in [isinstance(v, Volume) for v in x]:\n        raise TypeError(\"Input must be list of volumes\")\n\n    vertices: np.ndarray = np.empty((0, 3))\n    faces: List[List[int]] = []\n\n    # Reindex faces\n    for vol in x:\n        offs = len(vertices)\n        vertices = np.append(vertices, vol.vertices, axis=0)\n        faces += [[f[0] + offs, f[1] + offs, f[2] + offs] for f in vol.faces]\n\n    return cls(vertices=vertices, faces=faces, name=name, color=color)\n</code></pre>"},{"location":"reference/navis/#navis.Volume.from_csv","title":"<code>from_csv</code>  <code>classmethod</code>","text":"<p>Load volume from csv files containing vertices and faces.</p> PARAMETER DESCRIPTION <code>vertices</code> <pre><code>        CSV file containing vertices.\n</code></pre> <p> TYPE: <code>     filepath | file-like</code> </p> <code>faces</code> <pre><code>        CSV file containing faces.\n</code></pre> <p> TYPE: <code>        filepath | file-like</code> </p> <code>**kwargs</code> <pre><code>        Keyword arguments passed to `csv.reader`.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>navis.Volume</code> Source code in <code>navis/core/volumes.py</code> <pre><code>@classmethod\ndef from_csv(\n    cls,\n    vertices: str,\n    faces: str,\n    name: Optional[str] = None,\n    color: Union[str, Sequence[Union[int, float]]] = (0.85, 0.85, 0.85, 0.2),\n    volume_id: Optional[int] = None,\n    **kwargs,\n) -&gt; \"Volume\":\n    \"\"\"Load volume from csv files containing vertices and faces.\n\n    Parameters\n    ----------\n    vertices :      filepath | file-like\n                    CSV file containing vertices.\n    faces :         filepath | file-like\n                    CSV file containing faces.\n    **kwargs\n                    Keyword arguments passed to `csv.reader`.\n\n    Returns\n    -------\n    navis.Volume\n\n    \"\"\"\n    if not os.path.isfile(vertices) or not os.path.isfile(faces):\n        raise ValueError(\"File(s) not found.\")\n\n    with open(vertices, \"r\") as f:\n        reader = csv.reader(f, **kwargs)\n        vertices = np.array([r for r in reader]).astype(float)\n\n    with open(faces, \"r\") as f:\n        reader = csv.reader(f, **kwargs)\n        faces = np.array([r for r in reader]).astype(int)\n\n    return cls(\n        faces=faces, vertices=vertices, name=name, color=color, volume_id=volume_id\n    )\n</code></pre>"},{"location":"reference/navis/#navis.Volume.from_file","title":"<code>from_file</code>  <code>classmethod</code>","text":"<p>Load volume from file.</p> PARAMETER DESCRIPTION <code>filename</code> <pre><code>        File to load from.\n</code></pre> <p> TYPE: <code>     str</code> </p> <code>import_kwargs</code> <pre><code>        Keyword arguments passed to importer:\n          - `json.load` for JSON file\n          - `trimesh.load_mesh` for OBJ and STL files\n</code></pre> <p> TYPE: <code>Dict</code> DEFAULT: <code>{}</code> </p> <code>**init_kwargs</code> <pre><code>    Keyword arguments passed to navis.Volume upon\n    initialization.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>navis.Volume</code> Source code in <code>navis/core/volumes.py</code> <pre><code>@classmethod\ndef from_file(\n    cls, filename: str, import_kwargs: Dict = {}, **init_kwargs\n) -&gt; \"Volume\":\n    \"\"\"Load volume from file.\n\n    Parameters\n    ----------\n    filename :      str\n                    File to load from.\n    import_kwargs\n                    Keyword arguments passed to importer:\n                      - `json.load` for JSON file\n                      - `trimesh.load_mesh` for OBJ and STL files\n    **init_kwargs\n                Keyword arguments passed to navis.Volume upon\n                initialization.\n\n    Returns\n    -------\n    navis.Volume\n\n    \"\"\"\n    if not os.path.isfile(filename):\n        raise ValueError(\"File not found.\")\n\n    f, ext = os.path.splitext(filename)\n\n    if ext == \".json\":\n        return cls.from_json(\n            filename=filename, import_kwargs=import_kwargs, **init_kwargs\n        )\n\n    try:\n        import trimesh\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"Unable to import: trimesh missing - please \"\n            'install: \"pip install trimesh\"'\n        )\n    except BaseException:\n        raise\n\n    tm = trimesh.load_mesh(filename, **import_kwargs)\n\n    return cls.from_object(tm, **init_kwargs)\n</code></pre>"},{"location":"reference/navis/#navis.Volume.from_json","title":"<code>from_json</code>  <code>classmethod</code>","text":"<p>Load volume from json file containing vertices and faces.</p> PARAMETER DESCRIPTION <code>filename</code> <p> TYPE: <code>str</code> </p> <code>import_kwargs</code> <pre><code>        Keyword arguments passed to `json.load`.\n</code></pre> <p> TYPE: <code>Dict</code> DEFAULT: <code>{}</code> </p> <code>**init_kwargs</code> <pre><code>    Keyword arguments passed to navis.Volume upon\n    initialization.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>navis.Volume</code> Source code in <code>navis/core/volumes.py</code> <pre><code>@classmethod\ndef from_json(\n    cls, filename: str, import_kwargs: Dict = {}, **init_kwargs\n) -&gt; \"Volume\":\n    \"\"\"Load volume from json file containing vertices and faces.\n\n    Parameters\n    ----------\n    filename\n    import_kwargs\n                    Keyword arguments passed to `json.load`.\n    **init_kwargs\n                Keyword arguments passed to navis.Volume upon\n                initialization.\n\n    Returns\n    -------\n    navis.Volume\n\n    \"\"\"\n    if not os.path.isfile(filename):\n        raise ValueError(\"File not found.\")\n\n    with open(filename, \"r\") as f:\n        data = json.load(f, **import_kwargs)\n\n    return cls(faces=data[\"faces\"], vertices=data[\"vertices\"], **init_kwargs)\n</code></pre>"},{"location":"reference/navis/#navis.Volume.from_object","title":"<code>from_object</code>  <code>classmethod</code>","text":"<p>Load volume from generic object that has <code>.vertices</code> and <code>.faces</code> attributes.</p> PARAMETER DESCRIPTION <code>obj</code> <p> TYPE: <code>Any</code> </p> <code>**init_kwargs</code> <pre><code>    Keyword arguments passed to navis.Volume upon\n    initialization.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>navis.Volume</code> Source code in <code>navis/core/volumes.py</code> <pre><code>@classmethod\ndef from_object(cls, obj: Any, **init_kwargs) -&gt; \"Volume\":\n    \"\"\"Load volume from generic object that has `.vertices` and\n    `.faces` attributes.\n\n    Parameters\n    ----------\n    obj\n    **init_kwargs\n                Keyword arguments passed to navis.Volume upon\n                initialization.\n\n    Returns\n    -------\n    navis.Volume\n\n    \"\"\"\n    if not hasattr(obj, \"vertices\") or not hasattr(obj, \"faces\"):\n        raise ValueError(\"Object must have faces and vertices attributes.\")\n\n    return cls(faces=obj.faces, vertices=obj.vertices, **init_kwargs)\n</code></pre>"},{"location":"reference/navis/#navis.Volume.plot3d","title":"<code>plot3d</code>","text":"<p>Plot volume using <code>navis.plot3d</code>.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <pre><code>Keyword arguments. Will be passed to [`navis.plot3d`][].\nSee `help(navis.plot3d)` for a list of keywords.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> See Also <p><code>navis.plot3d</code>             Function called to generate 3d plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; vol = navis.example_volume('LH')\n&gt;&gt;&gt; v = vol.plot3d(color = (255, 0, 0))\n</code></pre> Source code in <code>navis/core/volumes.py</code> <pre><code>def plot3d(self, **kwargs):\n    \"\"\"Plot volume using [`navis.plot3d`][].\n\n    Parameters\n    ----------\n    **kwargs\n            Keyword arguments. Will be passed to [`navis.plot3d`][].\n            See `help(navis.plot3d)` for a list of keywords.\n\n    See Also\n    --------\n    [`navis.plot3d`][]\n                Function called to generate 3d plot.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; vol = navis.example_volume('LH')\n    &gt;&gt;&gt; v = vol.plot3d(color = (255, 0, 0))\n\n    \"\"\"\n    from .. import plotting\n\n    if \"color\" in kwargs:\n        self.color = kwargs[\"color\"]\n\n    return plotting.plot3d(self, **kwargs)\n</code></pre>"},{"location":"reference/navis/#navis.Volume.resize","title":"<code>resize</code>","text":"<p>Resize volume.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Resizing factor. For methods \"center\", \"centroid\" and\n    \"origin\" this is the fraction of original size (e.g.\n    `.5` for half size). For method \"normals\", this is\n    is the absolute displacement (e.g. `-1000` to shrink\n    volume by that many units)!\n</code></pre> <p> TYPE: <code>        int | float</code> </p> <code>method</code> <pre><code>    Point in space to use for resizing.\n\n    .. list-table::\n        :widths: 15 75\n        :header-rows: 1\n\n        * - method\n          - explanation\n        * - center\n          - average of all vertices\n        * - centroid\n          - average of the triangle centroids weighted by the\n            area of each triangle.\n        * - origin\n          - resizes relative to origin of coordinate system\n            (0, 0, 0)\n        * - normals\n          - resize using face normals. Note that this method\n            uses absolute displacement for parameter `x`.\n</code></pre> <p> TYPE: <code>   \"center\" | \"centroid\" | \"normals\" | \"origin\"</code> DEFAULT: <code>'center'</code> </p> <code>inplace</code> <pre><code>    If False, will return resized copy.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>[`navis.Volume`][]</code> <p>Resized copy of original volume. Only if <code>inplace=False</code>.</p> <code>None</code> <p>If <code>inplace=True</code>.</p> Source code in <code>navis/core/volumes.py</code> <pre><code>def resize(\n    self,\n    x: Union[float, int],\n    method: Union[\n        Literal[\"center\"],\n        Literal[\"centroid\"],\n        Literal[\"normals\"],\n        Literal[\"origin\"],\n    ] = \"center\",\n    inplace: bool = False,\n) -&gt; Optional[\"Volume\"]:\n    \"\"\"Resize volume.\n\n    Parameters\n    ----------\n    x :         int | float\n                Resizing factor. For methods \"center\", \"centroid\" and\n                \"origin\" this is the fraction of original size (e.g.\n                `.5` for half size). For method \"normals\", this is\n                is the absolute displacement (e.g. `-1000` to shrink\n                volume by that many units)!\n    method :    \"center\" | \"centroid\" | \"normals\" | \"origin\"\n                Point in space to use for resizing.\n\n                .. list-table::\n                    :widths: 15 75\n                    :header-rows: 1\n\n                    * - method\n                      - explanation\n                    * - center\n                      - average of all vertices\n                    * - centroid\n                      - average of the triangle centroids weighted by the\n                        area of each triangle.\n                    * - origin\n                      - resizes relative to origin of coordinate system\n                        (0, 0, 0)\n                    * - normals\n                      - resize using face normals. Note that this method\n                        uses absolute displacement for parameter `x`.\n\n    inplace :   bool, optional\n                If False, will return resized copy.\n\n    Returns\n    -------\n    [`navis.Volume`][]\n                Resized copy of original volume. Only if `inplace=False`.\n    None\n                If `inplace=True`.\n\n    \"\"\"\n    assert isinstance(method, str)\n\n    method = method.lower()\n\n    perm_methods = [\"center\", \"origin\", \"normals\", \"centroid\"]\n    if method not in perm_methods:\n        raise ValueError(\n            f'Unknown method \"{method}\". Allowed '\n            f'methods: {\", \".join(perm_methods)}'\n        )\n\n    if not inplace:\n        v = self.copy()\n    else:\n        v = self\n\n    if method == \"normals\":\n        v.vertices = v.vertices + (v.vertex_normals * x)\n    else:\n        # Get the center\n        if method == \"center\":\n            cn = np.mean(v.vertices, axis=0)\n        elif method == \"centroid\":\n            cn = v.centroid\n        elif method == \"origin\":\n            cn = np.array([0, 0, 0])\n\n        # Get vector from center to each vertex\n        vec = v.vertices - cn\n\n        # Multiply vector by resize factor\n        vec *= x\n\n        # Recalculate vertex positions\n        v.vertices = vec + cn\n\n    # Make sure to reset any pyoctree data on this volume\n    if hasattr(v, \"pyoctree\"):\n        delattr(v, \"pyoctree\")\n\n    if not inplace:\n        return v\n</code></pre>"},{"location":"reference/navis/#navis.Volume.show","title":"<code>show</code>","text":"<p>See <code>.plot3d</code>.</p> Source code in <code>navis/core/volumes.py</code> <pre><code>def show(self, **kwargs):\n    \"\"\"See `.plot3d`.\"\"\"\n    # This is mostly to override trimesh.Trimesh method\n    return self.plot3d(**kwargs)\n</code></pre>"},{"location":"reference/navis/#navis.Volume.to_2d","title":"<code>to_2d</code>","text":"<p>Compute the 2d alpha shape (concave hull) this volume.</p> <p>Uses Scipy Delaunay and shapely.</p> PARAMETER DESCRIPTION <code>alpha</code> <pre><code>    Alpha value to influence the gooeyness of the border.\n    Smaller numbers don't fall inward as much as larger\n    numbers. Too large, and you lose everything!\n</code></pre> <p> TYPE: <code>float</code> DEFAULT: <code>0.00017</code> </p> <code>view</code> <pre><code>    Determines axis.\n</code></pre> <p> TYPE: <code>     tuple</code> DEFAULT: <code>('x', 'y')</code> </p> RETURNS DESCRIPTION <code>list</code> <p>Coordinates of 2d circumference e.g. <code>[(x1, y1), (x2, y2), (x3, y3), ...]</code></p> Source code in <code>navis/core/volumes.py</code> <pre><code>def to_2d(\n    self, alpha: float = 0.00017, view: tuple = (\"x\", \"y\"), invert_y: bool = False\n) -&gt; Sequence[Union[float, int]]:\n    \"\"\"Compute the 2d alpha shape (concave hull) this volume.\n\n    Uses Scipy Delaunay and shapely.\n\n    Parameters\n    ----------\n    alpha:      float, optional\n                Alpha value to influence the gooeyness of the border.\n                Smaller numbers don't fall inward as much as larger\n                numbers. Too large, and you lose everything!\n    view :      tuple\n                Determines axis.\n\n    Returns\n    -------\n    list\n                Coordinates of 2d circumference\n                e.g. `[(x1, y1), (x2, y2), (x3, y3), ...]`\n\n    \"\"\"\n\n    def add_edge(edges, edge_points, coords, i, j):\n        \"\"\"Add line between the i-th and j-th points.\"\"\"\n        if (i, j) in edges or (j, i) in edges:\n            # already added\n            return\n        edges.add((i, j))\n        edge_points.append(coords[[i, j]])\n\n    accepted_views = [\"x\", \"z\", \"y\", \"-x\", \"-z\", \"-y\"]\n\n    for ax in view:\n        if ax not in accepted_views:\n            raise ValueError(f'Unable to parse \"{ax}\" view')\n\n    try:\n        from shapely.ops import unary_union, polygonize  # type: ignore\n        import shapely.geometry as geometry  # type: ignore\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\"This function needs the shapely&gt;=1.8.0\")\n\n    coords: np.ndarray\n\n    map = {\"x\": 0, \"y\": 1, \"z\": 2}\n\n    x_ix = map[view[0].replace(\"-\", \"\").replace(\"+\", \"\")]\n    y_ix = map[view[1].replace(\"-\", \"\").replace(\"+\", \"\")]\n\n    coords = self.vertices[:, [x_ix, y_ix]]\n\n    tri = scipy.spatial.Delaunay(coords)\n    edges: set = set()\n    edge_points: list = []\n    # loop over triangles:\n    # ia, ib, ic = indices of corner points of the triangle\n    # Note that \"vertices\" property was renamed to \"simplices\"\n    for ia, ib, ic in getattr(tri, \"simplices\", getattr(tri, \"vertices\", [])):\n        pa: np.ndarray = coords[ia]  # type: ignore\n        pb: np.ndarray = coords[ib]  # type: ignore\n        pc: np.ndarray = coords[ic]  # type: ignore\n        # Lengths of sides of triangle\n        a = math.sqrt((pa[0] - pb[0]) ** 2 + (pa[1] - pb[1]) ** 2)  # type: ignore\n        b = math.sqrt((pb[0] - pc[0]) ** 2 + (pb[1] - pc[1]) ** 2)  # type: ignore\n        c = math.sqrt((pc[0] - pa[0]) ** 2 + (pc[1] - pa[1]) ** 2)  # type: ignore\n        # Semiperimeter of triangle\n        s = (a + b + c) / 2.0\n        # Area of triangle by Heron's formula\n        area = math.sqrt(s * (s - a) * (s - b) * (s - c))\n        circum_r = a * b * c / (4.0 * area)\n        # Here's the radius filter.\n        if circum_r &lt; 1.0 / alpha:\n            add_edge(edges, edge_points, coords, ia, ib)\n            add_edge(edges, edge_points, coords, ib, ic)\n            add_edge(edges, edge_points, coords, ic, ia)\n\n    m = geometry.MultiLineString(edge_points)\n    triangles = list(polygonize(m))\n    concave_hull = unary_union(triangles)\n\n    # Try with current settings, if this is not successful, try again\n    # with lower alpha\n    try:\n        return list(concave_hull.exterior.coords)\n    except AttributeError:\n        return self.to_2d(alpha=alpha / 10, view=view, invert_y=invert_y)\n    except BaseException:\n        raise\n</code></pre>"},{"location":"reference/navis/#navis.Volume.to_csv","title":"<code>to_csv</code>","text":"<p>Save volume as two separated csv files containing vertices and faces.</p> PARAMETER DESCRIPTION <code>filename</code> <pre><code>        Filename to use. Will get a `_vertices.csv` and\n        `_faces.csv` suffix.\n</code></pre> <p> TYPE: <code>     str</code> </p> <code>**kwargs</code> <pre><code>        Keyword arguments passed to `csv.reader`.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> Source code in <code>navis/core/volumes.py</code> <pre><code>def to_csv(self, filename: str, **kwargs) -&gt; None:\n    \"\"\"Save volume as two separated csv files containing vertices and faces.\n\n    Parameters\n    ----------\n    filename :      str\n                    Filename to use. Will get a `_vertices.csv` and\n                    `_faces.csv` suffix.\n    **kwargs\n                    Keyword arguments passed to `csv.reader`.\n\n    \"\"\"\n    for data, suffix in zip(\n        [self.faces, self.vertices], [\"_faces.csv\", \"_vertices.csv\"]\n    ):\n        with open(filename + suffix, \"w\") as csvfile:\n            writer = csv.writer(csvfile)\n            writer.writerows(data)\n</code></pre>"},{"location":"reference/navis/#navis.Volume.to_json","title":"<code>to_json</code>","text":"<p>Save volume as json file.</p> PARAMETER DESCRIPTION <code>filename</code> <pre><code>        Filename to use.\n</code></pre> <p> TYPE: <code>     str</code> </p> Source code in <code>navis/core/volumes.py</code> <pre><code>def to_json(self, filename: str) -&gt; None:\n    \"\"\"Save volume as json file.\n\n    Parameters\n    ----------\n    filename :      str\n                    Filename to use.\n\n    \"\"\"\n    with open(filename, \"w\") as f:\n        json.dump(\n            {\"vertices\": self.vertices.tolist(), \"faces\": self.faces.tolist()}, f\n        )\n</code></pre>"},{"location":"reference/navis/#navis.Volume.validate","title":"<code>validate</code>","text":"<p>Use trimesh to try and fix issues (holes/normals).</p> Source code in <code>navis/core/volumes.py</code> <pre><code>def validate(self):\n    \"\"\"Use trimesh to try and fix issues (holes/normals).\"\"\"\n    if not self.is_volume:\n        logger.info(\"Mesh not valid, attempting to fix\")\n        self.fill_holes()\n        self.fix_normals()\n        if not self.is_volume:\n            raise utils.VolumeError(\n                \"Mesh is not a volume \"\n                \"(e.g. not watertight, incorrect \"\n                \"winding) and could not be fixed.\"\n            )\n</code></pre>"},{"location":"reference/navis/#navis.VoxelNeuron","title":"<code>navis.VoxelNeuron</code>","text":"<p>Neuron represented as voxels.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Data to construct neuron from:\n         - a 2D (N, 3) array of voxel positions (x, y, z)\n         - a 2D (N, 4) array of voxel positions + values (x, y, z, value)\n         - a 3D (N, M, J) array representing the voxel grid\n</code></pre> <p> TYPE: <code>Union[np.ndarray]</code> </p> <code>offset</code> <pre><code>        An (optional) offset in voxels. This is useful to keep the\n        voxel grid small while still maintaining correct positioning\n        e.g. for plotting.\n</code></pre> <p> TYPE: <code>       (3, ) array</code> DEFAULT: <code>None</code> </p> <code>cache</code> <pre><code>        Whether to cache different representations (i.e. grid\n        and voxels) of the data. Set to False to save some memory.\n</code></pre> <p> TYPE: <code>        bool</code> DEFAULT: <code>True</code> </p> <code>units</code> <pre><code>        Units (scales) for voxels. Defaults to `1` (dimensionless).\n        Strings must be parsable by pint: e.g. \"nm\", \"um\",\n        \"micrometer\" or \"8 nanometers\".\n</code></pre> <p> TYPE: <code>        str | pint.Units | pint.Quantity</code> DEFAULT: <code>None</code> </p> <code>**metadata</code> <pre><code>        Any additional data to attach to neuron.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> Source code in <code>navis/core/voxel.py</code> <pre><code>class VoxelNeuron(BaseNeuron):\n    \"\"\"Neuron represented as voxels.\n\n    Parameters\n    ----------\n    x\n                    Data to construct neuron from:\n                     - a 2D (N, 3) array of voxel positions (x, y, z)\n                     - a 2D (N, 4) array of voxel positions + values (x, y, z, value)\n                     - a 3D (N, M, J) array representing the voxel grid\n\n    offset :        (3, ) array, optional\n                    An (optional) offset in voxels. This is useful to keep the\n                    voxel grid small while still maintaining correct positioning\n                    e.g. for plotting.\n    cache :         bool\n                    Whether to cache different representations (i.e. grid\n                    and voxels) of the data. Set to False to save some memory.\n    units :         str | pint.Units | pint.Quantity\n                    Units (scales) for voxels. Defaults to `1` (dimensionless).\n                    Strings must be parsable by pint: e.g. \"nm\", \"um\",\n                    \"micrometer\" or \"8 nanometers\".\n    **metadata\n                    Any additional data to attach to neuron.\n\n    \"\"\"\n\n    connectors: Optional[pd.DataFrame]\n\n    #: (N, 3) array of x/y/z voxels locations\n    voxels: np.ndarray\n    #: (N, ) array of values for each voxel\n    values: np.ndarray\n    # (N, M, K) voxel grid\n    grid: np.ndarray\n    # shape of voxel grid\n    shape: tuple\n\n    soma: Optional[Union[list, np.ndarray]]\n\n    #: Attributes used for neuron summary\n    SUMMARY_PROPS = ['type', 'name', 'units', 'shape', 'dtype']\n\n    #: Attributes to be used when comparing two neurons.\n    EQ_ATTRIBUTES = ['name', 'shape', 'dtype']\n\n    #: Temporary attributes that need clearing when neuron data changes\n    TEMP_ATTR = ['_memory_usage', '_shape', '_voxels', '_grid']\n\n    #: Core data table(s) used to calculate hash\n    CORE_DATA = ['_data']\n\n    def __init__(self,\n                 x: Union[np.ndarray],\n                 offset: Optional[np.ndarray] = None,\n                 cache: bool = True,\n                 units: Union[pint.Unit, str] = None,\n                 **metadata\n                 ):\n        \"\"\"Initialize Voxel Neuron.\"\"\"\n        super().__init__()\n\n        if not isinstance(x, (np.ndarray, type(None))):\n            raise utils.ConstructionError(f'Unable to construct VoxelNeuron from \"{type(x)}\".')\n\n        if isinstance(x, np.ndarray):\n            if x.ndim == 2 and x.shape[1] in [3, 4]:\n                # Contiguous arrays are required for hashing and we save a lot\n                # of time by doing this once up-front\n                self._data = np.ascontiguousarray(x)\n            elif x.ndim == 3:\n                self._data = np.ascontiguousarray(x)\n            else:\n                raise utils.ConstructionError(f'Unable to construct VoxelNeuron from {x.shape} array.')\n\n        for k, v in metadata.items():\n            try:\n                setattr(self, k, v)\n            except AttributeError:\n                raise AttributeError(f\"Unable to set neuron's `{k}` attribute.\")\n\n        self.cache = cache\n        self.units = units\n        self.offset = offset\n\n    def __getstate__(self):\n        \"\"\"Get state (used e.g. for pickling).\"\"\"\n        state = {k: v for k, v in self.__dict__.items() if not callable(v)}\n\n        SKIP = []\n        for s in SKIP:\n            if s in state:\n                _ = state.pop(s)\n\n        return state\n\n    def __setstate__(self, d):\n        \"\"\"Update state (used e.g. for pickling).\"\"\"\n        self.__dict__.update(d)\n\n    def __truediv__(self, other, copy=True):\n        \"\"\"Implement division for coordinates (units, connectors, offset).\"\"\"\n        if isinstance(other, numbers.Number) or utils.is_iterable(other):\n            # If a number, consider this an offset for coordinates\n            n = self.copy() if copy else self\n\n            # Convert units\n            # Note: .to_compact() throws a RuntimeWarning and returns unchanged\n            # values  when `units` is a iterable\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                n.units = (n.units / other).to_compact()\n\n            n.offset = n.offset / other\n            if n.has_connectors:\n                n.connectors.loc[:, ['x', 'y', 'z']] /= other\n\n            self._clear_temp_attr()\n\n            return n\n        return NotImplemented\n\n    def __mul__(self, other, copy=True):\n        \"\"\"Implement multiplication for coordinates (units, connectors, offset).\"\"\"\n        if isinstance(other, numbers.Number) or utils.is_iterable(other):\n            # If a number, consider this an offset for coordinates\n            n = self.copy() if copy else self\n\n            # Convert units\n            # Note: .to_compact() throws a RuntimeWarning and returns unchanged\n            # values  when `units` is a iterable\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                n.units = (n.units * other).to_compact()\n\n            n.offset = n.offset * other\n            if n.has_connectors:\n                n.connectors.loc[:, ['x', 'y', 'z']] *= other\n\n            self._clear_temp_attr()\n\n            return n\n        return NotImplemented\n\n    def __add__(self, other, copy=True):\n        \"\"\"Implement addition for coordinates (offset, connectors).\"\"\"\n        if isinstance(other, numbers.Number) or utils.is_iterable(other):\n            # If a number, consider this an offset for coordinates\n            n = self.copy() if copy else self\n\n            n.offset = n.offset + other\n            if n.has_connectors:\n                n.connectors.loc[:, ['x', 'y', 'z']] += other\n\n            self._clear_temp_attr()\n\n            return n\n        return NotImplemented\n\n    def __sub__(self, other, copy=True):\n        \"\"\"Implement subtraction for coordinates (offset, connectors).\"\"\"\n        if isinstance(other, numbers.Number) or utils.is_iterable(other):\n            # If a number, consider this an offset for coordinates\n            n = self.copy() if copy else self\n\n            n.offset = n.offset - other\n            if n.has_connectors:\n                n.connectors.loc[:, ['x', 'y', 'z']] -= other\n\n            self._clear_temp_attr()\n\n            return n\n        return NotImplemented\n\n    @property\n    def _base_data_type(self) -&gt; str:\n        \"\"\"Type of data (grid or voxels) underlying this neuron.\"\"\"\n        if self._data.ndim == 3:\n            return 'grid'\n        else:\n            return 'voxels'\n\n    @property\n    def dtype(self) -&gt; type:\n        \"\"\"Data type of voxel values.\"\"\"\n        return self._data.dtype\n\n    @property\n    def bbox(self) -&gt; np.ndarray:\n        \"\"\"Bounding box (includes connectors) in units.\"\"\"\n        mn = self.offset\n        if self._base_data_type == 'voxels':\n            mx = np.max(self.voxels, axis=0) * self.units.magnitude + self.offset\n        else:\n            mx = np.array(self.grid.shape) * self.units.magnitude + self.offset\n\n        if self.has_connectors:\n            cn_mn = np.min(self.connectors[['x', 'y', 'z']].values, axis=0)\n            cn_mx = np.max(self.connectors[['x', 'y', 'z']].values, axis=0)\n\n            mn = np.min(np.vstack((mn, cn_mn)), axis=0)\n            mx = np.max(np.vstack((mx, cn_mx)), axis=0)\n\n        return np.vstack((mn, mx)).T\n\n    @property\n    @add_units(compact=True, power=3)\n    def volume(self) -&gt; float:\n        \"\"\"Volume of neuron.\"\"\"\n        # Get volume of a single voxel\n        voxel_volume = self.units_xyz[0] * self.units_xyz[2] * self.units_xyz[2]\n        return (self.nnz * voxel_volume).to_compact()\n\n    @property\n    @temp_property\n    def voxels(self):\n        \"\"\"Voxels making up the neuron.\"\"\"\n        if self._base_data_type == 'voxels':\n            return self._data[:, :3]\n\n        if hasattr(self, '_voxels'):\n            return self._voxels\n\n        voxels = np.dstack(np.where(self._data))[0]\n        if self.cache:\n            self._voxels = voxels\n        return voxels\n\n    @voxels.setter\n    def voxels(self, voxels):\n        if not isinstance(voxels, np.ndarray):\n            raise TypeError(f'Voxels must be numpy array, got \"{type(voxels)}\"')\n        if voxels.ndim != 2 or voxels.shape[1] != 3:\n            raise ValueError('Voxels must be (N, 3) array')\n        if 'float' in str(voxels.dtype):\n            voxels = voxels.astype(np.int64)\n        self._data = voxels\n        self._clear_temp_attr()\n\n    @property\n    @temp_property\n    def grid(self):\n        \"\"\"Voxel grid representation.\"\"\"\n        if self._base_data_type == 'grid':\n            return self._data\n\n        if hasattr(self, '_grid'):\n            return self._grid\n\n        grid = np.zeros(self.shape, dtype=self.values.dtype)\n        grid[self._data[:, 0],\n             self._data[:, 1],\n             self._data[:, 2]] = self.values\n\n        if self.cache:\n            self._grid = grid\n        return grid\n\n    @grid.setter\n    def grid(self, grid):\n        if not isinstance(grid, np.ndarray):\n            raise TypeError(f'Grid must be numpy array, got \"{type(grid)}\"')\n        if grid.ndim != 3:\n            raise ValueError('Grid must be 3D array')\n        self._data = grid\n        self._clear_temp_attr()\n\n    @property\n    @temp_property\n    def values(self):\n        \"\"\"Values for each voxel (can be None).\"\"\"\n        if self._base_data_type == 'grid':\n            values = self._data.flatten()\n            return values[values &gt; 0]\n        else:\n            if not isinstance(getattr(self, '_values', None), type(None)):\n                return self._values\n            else:\n                return np.ones(self._data.shape[0])\n\n    @values.setter\n    def values(self, values):\n        if self._base_data_type == 'grid':\n            raise ValueError('Unable to set values for VoxelNeurons that were '\n                             'initialized with a grid')\n\n        if isinstance(values, type(None)):\n            if hasattr(self, '_values'):\n                delattr(self, '_values')\n            return\n\n        if not isinstance(values, np.ndarray):\n            raise TypeError(f'Values must be numpy array, got \"{type(values)}\"')\n        elif values.ndim != 1 or values.shape[0] != self.voxels.shape[0]:\n            raise ValueError('Voxels must be (N, ) array of the same length as voxels')\n\n        self._values = values\n        self._clear_temp_attr()\n\n    @property\n    def offset(self) -&gt; np.ndarray:\n        \"\"\"Offset (in voxels).\"\"\"\n        return self._offset\n\n    @offset.setter\n    def offset(self, offset):\n        if isinstance(offset, type(None)):\n            self._offset = np.array((0, 0, 0))\n        else:\n            offset = np.asarray(offset)\n            if offset.ndim != 1 or offset.shape[0] != 3:\n                raise ValueError('Offset must be (3, ) array of x/y/z coordinates.')\n            self._offset = offset\n\n        self._clear_temp_attr()\n\n    @property\n    @temp_property\n    def shape(self):\n        \"\"\"Shape of voxel grid.\"\"\"\n        if not hasattr(self, '_shape'):\n            if self._base_data_type == 'voxels':\n                self._shape = tuple(self.voxels.max(axis=0) + 1)\n            else:\n                self._shape = self._data.shape\n        return self._shape\n\n    @property\n    def type(self) -&gt; str:\n        \"\"\"Neuron type.\"\"\"\n        return 'navis.VoxelNeuron'\n\n    @property\n    def density(self) -&gt; float:\n        \"\"\"Fraction of filled voxels.\"\"\"\n        return self.nnz / np.product(self.shape)\n\n    @property\n    def nnz(self) -&gt; int:\n        \"\"\"Number of non-zero voxels.\"\"\"\n        return self.count_nonzero()\n\n    def count_nonzero(self) -&gt; int:\n        \"\"\"Count non-zero voxels.\"\"\"\n        if self._base_data_type == \"grid\":\n            return np.count_nonzero(self.grid)\n        elif self._base_data_type == \"voxels\":\n            return np.count_nonzero(self.values)\n\n        raise TypeError(f\"Unexpected data type: {self._base_data_type}\")\n\n    def copy(self) -&gt; \"VoxelNeuron\":\n        \"\"\"Return a copy of the neuron.\"\"\"\n        no_copy = [\"_lock\"]\n\n        # Generate new neuron\n        x = self.__class__(None)\n        # Override with this neuron's data\n        x.__dict__.update(\n            {k: copy.copy(v) for k, v in self.__dict__.items() if k not in no_copy}\n        )\n\n        return x\n\n    def flip(self, axis: str, inplace: bool = False) -&gt; Optional[\"VoxelNeuron\"]:\n        \"\"\"Flip the volume along the specified axis.\n\n        Parameters\n        ----------\n        axis :       \"x\" | \"y\" | \"z\"\n                    Axis to flip along.\n        inplace :   bool, optional\n                    If False, will return flipped copy.\n\n        Returns\n        -------\n        [`navis.VoxelNeuron`][]\n                    Flipped copy of original neuron. Only if `inplace=False`.\n\n        \"\"\"\n        assert axis in (\"x\", \"y\", \"z\"), (\n            f'Unknown axis \"{axis}\". Allowed axes: \"x\", \"y\", \"z\"'\n        )\n\n        x = self\n        if not inplace:\n            x = x.copy()\n\n        # Flip voxels\n        if self._base_data_type == \"voxels\":\n            if axis == \"x\":\n                x._data[:, 0] = x.shape[0] - 1 - x._data[:, 0]\n            elif axis == \"y\":\n                x._data[:, 1] = x.shape[1] - 1 - x._data[:, 1]\n            elif axis == \"z\":\n                x._data[:, 2] = x.shape[2] - 1 - x._data[:, 2]\n        else:\n            x._data = np.flip(x._data, axis=\"xyz\".index(axis))\n\n        # Flip offset\n        if axis == \"x\":\n            x.offset[0] = x.shape[0] - 1 - x.offset[0]\n        elif axis == \"y\":\n            x.offset[1] = x.shape[1] - 1 - x.offset[1]\n        elif axis == \"z\":\n            x.offset[2] = x.shape[2] - 1 - x.offset[2]\n\n        # Flip connectors\n        if x.has_connectors:\n            if axis == \"x\":\n                x.connectors.loc[:, \"x\"] = x.shape[0] - 1 - x.connectors[\"x\"]\n            elif axis == \"y\":\n                x.connectors.loc[:, \"y\"] = x.shape[1] - 1 - x.connectors[\"y\"]\n            elif axis == \"z\":\n                x.connectors.loc[:, \"z\"] = x.shape[2] - 1 - x.connectors[\"z\"]\n\n        x._clear_temp_attr()\n\n        if not inplace:\n            return x\n\n    def strip(self, inplace=False) -&gt; \"VoxelNeuron\":\n        \"\"\"Strip empty voxels (leading/trailing planes of zeros).\"\"\"\n        x = self\n        if not inplace:\n            x = x.copy()\n\n        # Get offset until first filled voxel\n        voxels = x.voxels\n        mn = voxels.min(axis=0)\n        x.offset = np.array(x.offset) + mn * x.units_xyz.magnitude\n\n        # Drop empty planes\n        if x._base_data_type == 'voxels':\n            x._data = voxels - mn\n        else:\n            mx = voxels.max(axis=0)\n            x._data = x._data[mn[0]: mx[0] + 1,\n                              mn[1]: mx[1] + 1,\n                              mn[2]: mx[2] + 1]\n\n        if not inplace:\n            return x\n\n    def threshold(self, threshold, inplace=False) -&gt; 'VoxelNeuron':\n        \"\"\"Drop below-threshold voxels.\"\"\"\n        x = self\n        if not inplace:\n            x = x.copy()\n\n        if x._base_data_type == 'grid':\n            x._data[x._data &lt; threshold] = 0\n        else:\n            x._data = x._data[x.values &gt;= threshold]\n\n        x._clear_temp_attr()\n\n        if not inplace:\n            return x\n\n    def normalize(self, inplace=False) -&gt; \"VoxelNeuron\":\n        \"\"\"Normalize voxel values.\n\n        For float data, this scales values to the [0-1] range.\n        For integer data, this scales values to the [0-max]\n        range depending on the datatype.\n        \"\"\"\n        mx = np.iinfo(self.dtype).max if np.issubdtype(self.dtype, np.integer) else 1.0\n\n        x = self\n        if not inplace:\n            x = x.copy()\n\n        if x._base_data_type == \"grid\":\n            x._data = (x._data / x._data.max()) * mx\n        else:\n            x._data = (x._data / x.values.max()) * mx\n\n        x._clear_temp_attr()\n\n        if not inplace:\n            return x\n\n    def min(self) -&gt; Union[int, float]:\n        \"\"\"Minimum value (excludes zeros).\"\"\"\n        return self.values.min()\n\n    def max(self) -&gt; Union[int, float]:\n        \"\"\"Maximum value (excludes zeros).\"\"\"\n        return self.values.max()\n</code></pre>"},{"location":"reference/navis/#navis.VoxelNeuron.bbox","title":"<code>bbox: np.ndarray</code>  <code>property</code>","text":"<p>Bounding box (includes connectors) in units.</p>"},{"location":"reference/navis/#navis.VoxelNeuron.density","title":"<code>density: float</code>  <code>property</code>","text":"<p>Fraction of filled voxels.</p>"},{"location":"reference/navis/#navis.VoxelNeuron.dtype","title":"<code>dtype: type</code>  <code>property</code>","text":"<p>Data type of voxel values.</p>"},{"location":"reference/navis/#navis.VoxelNeuron.grid","title":"<code>grid</code>  <code>property</code> <code>writable</code>","text":"<p>Voxel grid representation.</p>"},{"location":"reference/navis/#navis.VoxelNeuron.nnz","title":"<code>nnz: int</code>  <code>property</code>","text":"<p>Number of non-zero voxels.</p>"},{"location":"reference/navis/#navis.VoxelNeuron.offset","title":"<code>offset: np.ndarray</code>  <code>property</code> <code>writable</code>","text":"<p>Offset (in voxels).</p>"},{"location":"reference/navis/#navis.VoxelNeuron.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>Shape of voxel grid.</p>"},{"location":"reference/navis/#navis.VoxelNeuron.type","title":"<code>type: str</code>  <code>property</code>","text":"<p>Neuron type.</p>"},{"location":"reference/navis/#navis.VoxelNeuron.values","title":"<code>values</code>  <code>property</code> <code>writable</code>","text":"<p>Values for each voxel (can be None).</p>"},{"location":"reference/navis/#navis.VoxelNeuron.volume","title":"<code>volume: float</code>  <code>property</code>","text":"<p>Volume of neuron.</p>"},{"location":"reference/navis/#navis.VoxelNeuron.voxels","title":"<code>voxels</code>  <code>property</code> <code>writable</code>","text":"<p>Voxels making up the neuron.</p>"},{"location":"reference/navis/#navis.VoxelNeuron.__init__","title":"<code>__init__</code>","text":"<p>Initialize Voxel Neuron.</p> Source code in <code>navis/core/voxel.py</code> <pre><code>def __init__(self,\n             x: Union[np.ndarray],\n             offset: Optional[np.ndarray] = None,\n             cache: bool = True,\n             units: Union[pint.Unit, str] = None,\n             **metadata\n             ):\n    \"\"\"Initialize Voxel Neuron.\"\"\"\n    super().__init__()\n\n    if not isinstance(x, (np.ndarray, type(None))):\n        raise utils.ConstructionError(f'Unable to construct VoxelNeuron from \"{type(x)}\".')\n\n    if isinstance(x, np.ndarray):\n        if x.ndim == 2 and x.shape[1] in [3, 4]:\n            # Contiguous arrays are required for hashing and we save a lot\n            # of time by doing this once up-front\n            self._data = np.ascontiguousarray(x)\n        elif x.ndim == 3:\n            self._data = np.ascontiguousarray(x)\n        else:\n            raise utils.ConstructionError(f'Unable to construct VoxelNeuron from {x.shape} array.')\n\n    for k, v in metadata.items():\n        try:\n            setattr(self, k, v)\n        except AttributeError:\n            raise AttributeError(f\"Unable to set neuron's `{k}` attribute.\")\n\n    self.cache = cache\n    self.units = units\n    self.offset = offset\n</code></pre>"},{"location":"reference/navis/#navis.VoxelNeuron.copy","title":"<code>copy</code>","text":"<p>Return a copy of the neuron.</p> Source code in <code>navis/core/voxel.py</code> <pre><code>def copy(self) -&gt; \"VoxelNeuron\":\n    \"\"\"Return a copy of the neuron.\"\"\"\n    no_copy = [\"_lock\"]\n\n    # Generate new neuron\n    x = self.__class__(None)\n    # Override with this neuron's data\n    x.__dict__.update(\n        {k: copy.copy(v) for k, v in self.__dict__.items() if k not in no_copy}\n    )\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.VoxelNeuron.count_nonzero","title":"<code>count_nonzero</code>","text":"<p>Count non-zero voxels.</p> Source code in <code>navis/core/voxel.py</code> <pre><code>def count_nonzero(self) -&gt; int:\n    \"\"\"Count non-zero voxels.\"\"\"\n    if self._base_data_type == \"grid\":\n        return np.count_nonzero(self.grid)\n    elif self._base_data_type == \"voxels\":\n        return np.count_nonzero(self.values)\n\n    raise TypeError(f\"Unexpected data type: {self._base_data_type}\")\n</code></pre>"},{"location":"reference/navis/#navis.VoxelNeuron.flip","title":"<code>flip</code>","text":"<p>Flip the volume along the specified axis.</p> PARAMETER DESCRIPTION <code>axis</code> <pre><code>    Axis to flip along.\n</code></pre> <p> TYPE: <code>      \"x\" | \"y\" | \"z\"</code> </p> <code>inplace</code> <pre><code>    If False, will return flipped copy.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>[`navis.VoxelNeuron`][]</code> <p>Flipped copy of original neuron. Only if <code>inplace=False</code>.</p> Source code in <code>navis/core/voxel.py</code> <pre><code>def flip(self, axis: str, inplace: bool = False) -&gt; Optional[\"VoxelNeuron\"]:\n    \"\"\"Flip the volume along the specified axis.\n\n    Parameters\n    ----------\n    axis :       \"x\" | \"y\" | \"z\"\n                Axis to flip along.\n    inplace :   bool, optional\n                If False, will return flipped copy.\n\n    Returns\n    -------\n    [`navis.VoxelNeuron`][]\n                Flipped copy of original neuron. Only if `inplace=False`.\n\n    \"\"\"\n    assert axis in (\"x\", \"y\", \"z\"), (\n        f'Unknown axis \"{axis}\". Allowed axes: \"x\", \"y\", \"z\"'\n    )\n\n    x = self\n    if not inplace:\n        x = x.copy()\n\n    # Flip voxels\n    if self._base_data_type == \"voxels\":\n        if axis == \"x\":\n            x._data[:, 0] = x.shape[0] - 1 - x._data[:, 0]\n        elif axis == \"y\":\n            x._data[:, 1] = x.shape[1] - 1 - x._data[:, 1]\n        elif axis == \"z\":\n            x._data[:, 2] = x.shape[2] - 1 - x._data[:, 2]\n    else:\n        x._data = np.flip(x._data, axis=\"xyz\".index(axis))\n\n    # Flip offset\n    if axis == \"x\":\n        x.offset[0] = x.shape[0] - 1 - x.offset[0]\n    elif axis == \"y\":\n        x.offset[1] = x.shape[1] - 1 - x.offset[1]\n    elif axis == \"z\":\n        x.offset[2] = x.shape[2] - 1 - x.offset[2]\n\n    # Flip connectors\n    if x.has_connectors:\n        if axis == \"x\":\n            x.connectors.loc[:, \"x\"] = x.shape[0] - 1 - x.connectors[\"x\"]\n        elif axis == \"y\":\n            x.connectors.loc[:, \"y\"] = x.shape[1] - 1 - x.connectors[\"y\"]\n        elif axis == \"z\":\n            x.connectors.loc[:, \"z\"] = x.shape[2] - 1 - x.connectors[\"z\"]\n\n    x._clear_temp_attr()\n\n    if not inplace:\n        return x\n</code></pre>"},{"location":"reference/navis/#navis.VoxelNeuron.max","title":"<code>max</code>","text":"<p>Maximum value (excludes zeros).</p> Source code in <code>navis/core/voxel.py</code> <pre><code>def max(self) -&gt; Union[int, float]:\n    \"\"\"Maximum value (excludes zeros).\"\"\"\n    return self.values.max()\n</code></pre>"},{"location":"reference/navis/#navis.VoxelNeuron.min","title":"<code>min</code>","text":"<p>Minimum value (excludes zeros).</p> Source code in <code>navis/core/voxel.py</code> <pre><code>def min(self) -&gt; Union[int, float]:\n    \"\"\"Minimum value (excludes zeros).\"\"\"\n    return self.values.min()\n</code></pre>"},{"location":"reference/navis/#navis.VoxelNeuron.normalize","title":"<code>normalize</code>","text":"<p>Normalize voxel values.</p> <p>For float data, this scales values to the [0-1] range. For integer data, this scales values to the [0-max] range depending on the datatype.</p> Source code in <code>navis/core/voxel.py</code> <pre><code>def normalize(self, inplace=False) -&gt; \"VoxelNeuron\":\n    \"\"\"Normalize voxel values.\n\n    For float data, this scales values to the [0-1] range.\n    For integer data, this scales values to the [0-max]\n    range depending on the datatype.\n    \"\"\"\n    mx = np.iinfo(self.dtype).max if np.issubdtype(self.dtype, np.integer) else 1.0\n\n    x = self\n    if not inplace:\n        x = x.copy()\n\n    if x._base_data_type == \"grid\":\n        x._data = (x._data / x._data.max()) * mx\n    else:\n        x._data = (x._data / x.values.max()) * mx\n\n    x._clear_temp_attr()\n\n    if not inplace:\n        return x\n</code></pre>"},{"location":"reference/navis/#navis.VoxelNeuron.strip","title":"<code>strip</code>","text":"<p>Strip empty voxels (leading/trailing planes of zeros).</p> Source code in <code>navis/core/voxel.py</code> <pre><code>def strip(self, inplace=False) -&gt; \"VoxelNeuron\":\n    \"\"\"Strip empty voxels (leading/trailing planes of zeros).\"\"\"\n    x = self\n    if not inplace:\n        x = x.copy()\n\n    # Get offset until first filled voxel\n    voxels = x.voxels\n    mn = voxels.min(axis=0)\n    x.offset = np.array(x.offset) + mn * x.units_xyz.magnitude\n\n    # Drop empty planes\n    if x._base_data_type == 'voxels':\n        x._data = voxels - mn\n    else:\n        mx = voxels.max(axis=0)\n        x._data = x._data[mn[0]: mx[0] + 1,\n                          mn[1]: mx[1] + 1,\n                          mn[2]: mx[2] + 1]\n\n    if not inplace:\n        return x\n</code></pre>"},{"location":"reference/navis/#navis.VoxelNeuron.threshold","title":"<code>threshold</code>","text":"<p>Drop below-threshold voxels.</p> Source code in <code>navis/core/voxel.py</code> <pre><code>def threshold(self, threshold, inplace=False) -&gt; 'VoxelNeuron':\n    \"\"\"Drop below-threshold voxels.\"\"\"\n    x = self\n    if not inplace:\n        x = x.copy()\n\n    if x._base_data_type == 'grid':\n        x._data[x._data &lt; threshold] = 0\n    else:\n        x._data = x._data[x.values &gt;= threshold]\n\n    x._clear_temp_attr()\n\n    if not inplace:\n        return x\n</code></pre>"},{"location":"reference/navis/#navis.arbor_segregation_index","title":"<code>navis.arbor_segregation_index</code>","text":"<p>Per arbor seggregation index (SI).</p> <p>The segregation index (SI) as established by Schneider-Mizell et al. (eLife, 2016) is a measure for how polarized a neuron is. SI of 1 indicates total segregation of inputs and outputs into dendrites and axon, respectively. SI of 0 indicates homogeneous distribution. Here, we apply this to each arbour within a neuron by asking \"If we were to cut a neuron at this node, what would the SI of the two resulting fragments be?\"</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Neuron(s) to calculate segregation indices for. Must have\n    connectors!\n</code></pre> <p> TYPE: <code>        TreeNeuron | MeshNeuron | NeuronList</code> </p> RETURNS DESCRIPTION <code>neuron</code> <p>Adds \"segregation_index\" as column in the node table (for TreeNeurons) or as <code>.segregation_index</code> property (for MeshNeurons).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; n.reroot(n.soma, inplace=True)\n&gt;&gt;&gt; _ = navis.arbor_segregation_index(n)\n&gt;&gt;&gt; n.nodes.segregation_index.max().round(3)\n0.277\n</code></pre> See Also <p><code>navis.segregation_index</code>         Calculate segregation score (polarity) between two fragments of         a neuron. <code>navis.synapse_flow_centrality</code>         Calculate synapse flow centrality after Schneider-Mizell et al. <code>navis.bending_flow</code>         Variation on the Schneider-Mizell et al. synapse flow. <code>navis.split_axon_dendrite</code>         Split the neuron into axon, dendrite and primary neurite.</p> Source code in <code>navis/morpho/mmetrics.py</code> <pre><code>@utils.map_neuronlist(desc=\"Calc. seg.\", allow_parallel=True)\n@utils.meshneuron_skeleton(method=\"node_properties\", node_props=[\"segregation_index\"])\ndef arbor_segregation_index(x: \"core.NeuronObject\") -&gt; \"core.NeuronObject\":\n    \"\"\"Per arbor seggregation index (SI).\n\n    The segregation index (SI) as established by Schneider-Mizell et al. (eLife,\n    2016) is a measure for how polarized a neuron is. SI of 1 indicates total\n    segregation of inputs and outputs into dendrites and axon, respectively.\n    SI of 0 indicates homogeneous distribution. Here, we apply this to\n    each arbour within a neuron by asking \"If we were to cut a neuron at this\n    node, what would the SI of the two resulting fragments be?\"\n\n    Parameters\n    ----------\n    x :         TreeNeuron | MeshNeuron | NeuronList\n                Neuron(s) to calculate segregation indices for. Must have\n                connectors!\n\n    Returns\n    -------\n    neuron\n                Adds \"segregation_index\" as column in the node table (for\n                TreeNeurons) or as `.segregation_index` property\n                (for MeshNeurons).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; n.reroot(n.soma, inplace=True)\n    &gt;&gt;&gt; _ = navis.arbor_segregation_index(n)\n    &gt;&gt;&gt; n.nodes.segregation_index.max().round(3)\n    0.277\n\n    See Also\n    --------\n    [`navis.segregation_index`][]\n            Calculate segregation score (polarity) between two fragments of\n            a neuron.\n    [`navis.synapse_flow_centrality`][]\n            Calculate synapse flow centrality after Schneider-Mizell et al.\n    [`navis.bending_flow`][]\n            Variation on the Schneider-Mizell et al. synapse flow.\n    [`navis.split_axon_dendrite`][]\n            Split the neuron into axon, dendrite and primary neurite.\n\n    \"\"\"\n    if not isinstance(x, core.TreeNeuron):\n        raise ValueError(f'Expected TreeNeuron(s), got \"{type(x)}\"')\n\n    if not x.has_connectors:\n        raise ValueError(\"Neuron must have connectors.\")\n\n    # Figure out how connector types are labeled\n    cn_types = x.connectors.type.unique()\n    if all(np.isin([\"pre\", \"post\"], cn_types)):\n        pre, post = \"pre\", \"post\"\n    elif all(np.isin([0, 1], cn_types)):\n        pre, post = 0, 1\n    else:\n        raise ValueError(f\"Unable to parse connector types for neuron {x.id}\")\n\n    # Get list of nodes with pre/postsynapses\n    pre_node_ids = x.connectors[x.connectors.type == pre].node_id.values\n    post_node_ids = x.connectors[x.connectors.type == post].node_id.values\n\n    # Get list of points to calculate SI for:\n    # branches points and their children plus nodes with connectors\n    is_bp = x.nodes[\"type\"].isin([\"branch\", \"root\"])\n    is_bp_child = x.nodes.parent_id.isin(x.nodes.loc[is_bp, \"node_id\"].values)\n    is_cn = x.nodes.node_id.isin(x.connectors.node_id)\n    calc_node_ids = x.nodes[is_bp | is_bp_child | is_cn].node_id.values\n\n    # We will be processing a super downsampled version of the neuron to speed\n    # up calculations\n    current_level = logger.level\n    logger.setLevel(\"ERROR\")\n    y = x.downsample(factor=float(\"inf\"), preserve_nodes=calc_node_ids, inplace=False)\n    logger.setLevel(current_level)\n\n    # Get number of pre/postsynapses distal to each branch's childs\n    # Note that we're using geodesic matrix here because it is much more\n    # efficient than for `distal_to` for larger queries/neurons\n    dists = graph.geodesic_matrix(\n        y, from_=np.append(pre_node_ids, post_node_ids), directed=True, weight=None\n    )\n    distal = dists[calc_node_ids] &lt; np.inf\n\n    # Since nodes can have multiple pre-/postsynapses but they show up only\n    # once in distal, we have to reindex to reflect the correct number of synapes\n    distal_pre = distal.loc[pre_node_ids]\n    distal_post = distal.loc[post_node_ids]\n\n    # Sum up columns: now each row represents the number of pre/postsynapses\n    # distal to that node\n    distal_pre_sum = distal_pre.sum(axis=0)\n    distal_post_sum = distal_post.sum(axis=0)\n\n    # Now go over all branch points and check flow between branches\n    # (centrifugal) vs flow from branches to root (centripetal)\n    SI = {}\n    total_pre = pre_node_ids.shape[0]\n    total_post = post_node_ids.shape[0]\n    for n in calc_node_ids:\n        # Get the SI if we were to cut at this point\n        post = distal_post_sum[n]\n        pre = distal_pre_sum[n]\n        n_syn = [\n            {\"presynapses\": pre, \"postsynapses\": post},\n            {\"presynapses\": total_pre - pre, \"postsynapses\": total_post - post},\n        ]\n        SI[n] = segregation_index(n_syn)\n\n    # At this point there are only segregation indices for branch points and\n    # their childs. Let's complete that mapping by adding SI for the nodes\n    # between branch points.\n    for s in x.small_segments:\n        # Segments' orientation goes from distal -&gt; proximal\n        # Each segment will have at least its last (branch point) and\n        # second last (branch point's child) node mapped\n\n        # Drop first (distal) node if it is not a leaf\n        if s[0] in SI:\n            s = s[1:]\n\n        # If shorter than 3 nodes all nodes should already have an SI\n        if len(s) &lt;= 2:\n            continue\n\n        # Update remaining nodes with the SI of the first child\n        this_SI = SI[s[-2]]\n        SI.update({n: this_SI for n in s[:-2]})\n\n    # Add segregation index to node table\n    x.nodes[\"segregation_index\"] = x.nodes.node_id.map(SI)\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.average_skeletons","title":"<code>navis.average_skeletons</code>","text":"<p>Compute an average from a list of skeletons.</p> <p>This is a very simple implementation which may give odd results if used on complex neurons. Works fine on e.g. backbones or tracts.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Neurons to be averaged.\n</code></pre> <p> TYPE: <code>            NeuronList</code> </p> <code>limit</code> <pre><code>        Max distance for nearest neighbour search. If the neurons\n        have `.units` set, you can also pass a string such as e.g.\n        \"2 microns\".\n</code></pre> <p> TYPE: <code>        int | str</code> DEFAULT: <code>10</code> </p> <code>base_neuron</code> <pre><code>        Neuron to use as template for averaging. If not provided,\n        the first neuron in the list is used as template!\n</code></pre> <p> TYPE: <code>  neuron id | TreeNeuron</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>TreeNeuron</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Get a bunch of neurons\n&gt;&gt;&gt; import navis\n&gt;&gt;&gt; da2 = navis.example_neurons()\n&gt;&gt;&gt; # Prune down to longest neurite\n&gt;&gt;&gt; for n in da2:\n...     if n.has_soma:\n...         n.reroot(n.soma, inplace=True)\n&gt;&gt;&gt; da2_pr = da2.prune_by_longest_neurite(inplace=False)\n&gt;&gt;&gt; # Make average\n&gt;&gt;&gt; da2_avg = navis.average_skeletons(da2_pr, limit=10e3)\n&gt;&gt;&gt; # Plot\n&gt;&gt;&gt; da2.plot3d()\n&gt;&gt;&gt; da2_avg.plot3d()\n</code></pre> Source code in <code>navis/morpho/manipulation.py</code> <pre><code>def average_skeletons(\n    x: \"core.NeuronList\",\n    limit: Union[int, str] = 10,\n    base_neuron: Optional[Union[int, \"core.TreeNeuron\"]] = None,\n) -&gt; \"core.TreeNeuron\":\n    \"\"\"Compute an average from a list of skeletons.\n\n    This is a very simple implementation which may give odd results if used\n    on complex neurons. Works fine on e.g. backbones or tracts.\n\n    Parameters\n    ----------\n    x :             NeuronList\n                    Neurons to be averaged.\n    limit :         int | str\n                    Max distance for nearest neighbour search. If the neurons\n                    have `.units` set, you can also pass a string such as e.g.\n                    \"2 microns\".\n    base_neuron :   neuron id | TreeNeuron, optional\n                    Neuron to use as template for averaging. If not provided,\n                    the first neuron in the list is used as template!\n\n    Returns\n    -------\n    TreeNeuron\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Get a bunch of neurons\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; da2 = navis.example_neurons()\n    &gt;&gt;&gt; # Prune down to longest neurite\n    &gt;&gt;&gt; for n in da2:\n    ...     if n.has_soma:\n    ...         n.reroot(n.soma, inplace=True)\n    &gt;&gt;&gt; da2_pr = da2.prune_by_longest_neurite(inplace=False)\n    &gt;&gt;&gt; # Make average\n    &gt;&gt;&gt; da2_avg = navis.average_skeletons(da2_pr, limit=10e3)\n    &gt;&gt;&gt; # Plot\n    &gt;&gt;&gt; da2.plot3d() # doctest: +SKIP\n    &gt;&gt;&gt; da2_avg.plot3d() # doctest: +SKIP\n\n    \"\"\"\n    if not isinstance(x, core.NeuronList):\n        raise TypeError(f'Need NeuronList, got \"{type(x)}\"')\n\n    if len(x) &lt; 2:\n        raise ValueError(\"Need at least 2 neurons to average!\")\n\n    # Map limit into unit space, if applicable\n    limit = x[0].map_units(limit, on_error=\"raise\")\n\n    # Generate KDTrees for each neuron\n    for n in x:\n        n.tree = graph.neuron2KDTree(n, tree_type=\"c\", data=\"nodes\")  # type: ignore  # TreeNeuron has no tree\n\n    # Set base for average: we will use this neurons nodes to query\n    # the KDTrees\n    if isinstance(base_neuron, core.TreeNeuron):\n        bn = base_neuron.copy()\n    elif isinstance(base_neuron, int):\n        bn = x[base_neuron].copy()\n    elif isinstance(base_neuron, type(None)):\n        bn = x[0].copy()\n    else:\n        raise ValueError(\n            f'Unable to interpret base_neuron of type \"{type(base_neuron)}\"'\n        )\n\n    base_nodes = bn.nodes[[\"x\", \"y\", \"z\"]].values\n    other_neurons = x[[n != bn for n in x]]\n\n    # Make sure these stay 2-dimensional arrays -&gt; will add a colum for each\n    # \"other\" neuron\n    base_x = base_nodes[:, 0:1]\n    base_y = base_nodes[:, 1:2]\n    base_z = base_nodes[:, 2:3]\n\n    # For each \"other\" neuron, collect nearest neighbour coordinates\n    for n in other_neurons:\n        nn_dist, nn_ix = n.tree.query(base_nodes, k=1, distance_upper_bound=limit)\n\n        # Translate indices into coordinates\n        # First, make empty array\n        this_coords = np.zeros((len(nn_dist), 3))\n        # Set coords without a nearest neighbour within distances to \"None\"\n        this_coords[nn_dist == float(\"inf\")] = None\n        # Fill in coords of nearest neighbours\n        this_coords[nn_dist != float(\"inf\")] = n.tree.data[\n            nn_ix[nn_dist != float(\"inf\")]\n        ]\n        # Add coords to base coords\n        base_x = np.append(base_x, this_coords[:, 0:1], axis=1)\n        base_y = np.append(base_y, this_coords[:, 1:2], axis=1)\n        base_z = np.append(base_z, this_coords[:, 2:3], axis=1)\n\n    # Calculate means\n    mean_x = np.mean(base_x, axis=1)\n    mean_y = np.mean(base_y, axis=1)\n    mean_z = np.mean(base_z, axis=1)\n\n    # If any of the base coords has NO nearest neighbour within limit\n    # whatsoever, the average of that row will be \"NaN\" -&gt; in this case we\n    # will fall back to the base coordinate\n    mean_x[np.isnan(mean_x)] = base_nodes[np.isnan(mean_x), 0]\n    mean_y[np.isnan(mean_y)] = base_nodes[np.isnan(mean_y), 1]\n    mean_z[np.isnan(mean_z)] = base_nodes[np.isnan(mean_z), 2]\n\n    # Change coordinates accordingly\n    bn.nodes[\"x\"] = mean_x\n    bn.nodes[\"y\"] = mean_y\n    bn.nodes[\"z\"] = mean_z\n\n    return bn\n</code></pre>"},{"location":"reference/navis/#navis.bending_flow","title":"<code>navis.bending_flow</code>","text":"<p>Calculate synapse \"bending\" flow.</p> <p>This is a variation of the algorithm for calculating synapse flow from Schneider-Mizell et al. (eLife, 2016).</p> <p>The way this implementation works is by iterating over each branch point and counting the number of pre-&gt;post synapse paths that \"flow\" from one child branch to the other(s).</p> Notes <p>This is algorithm appears to be more reliable than synapse flow centrality for identifying the main branch point for neurons that have incompletely annotated synapses.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Neuron(s) to calculate bending flow for. Must have connectors!\n</code></pre> <p> TYPE: <code>        TreeNeuron | MeshNeuron | NeuronList</code> </p> RETURNS DESCRIPTION <code>neuron</code> <p>Adds \"bending_flow\" as column in the node table (for TreeNeurons) or as <code>.bending_flow</code> property (for MeshNeurons).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; n.reroot(n.soma, inplace=True)\n&gt;&gt;&gt; _ = navis.bending_flow(n)\n&gt;&gt;&gt; n.nodes.bending_flow.max()\n785645\n</code></pre> See Also <p><code>navis.synapse_flow_centrality</code>         Calculate synapse flow centrality after Schneider-Mizell et al. <code>navis.segregation_index</code>         Calculate segregation score (polarity). <code>navis.arbor_segregation_index</code>         Calculate the a by-arbor segregation index. <code>navis.split_axon_dendrite</code>         Split the neuron into axon, dendrite and primary neurite.</p> Source code in <code>navis/morpho/mmetrics.py</code> <pre><code>@utils.map_neuronlist(desc=\"Calc. flow\", allow_parallel=True)\n@utils.meshneuron_skeleton(\n    method=\"node_properties\",\n    include_connectors=True,\n    heal=True,\n    node_props=[\"bending_flow\"],\n)\ndef bending_flow(x: \"core.NeuronObject\") -&gt; \"core.NeuronObject\":\n    \"\"\"Calculate synapse \"bending\" flow.\n\n    This is a variation of the algorithm for calculating synapse flow from\n    Schneider-Mizell et al. (eLife, 2016).\n\n    The way this implementation works is by iterating over each branch point\n    and counting the number of pre-&gt;post synapse paths that \"flow\" from one\n    child branch to the other(s).\n\n    Notes\n    -----\n    This is algorithm appears to be more reliable than synapse flow\n    centrality for identifying the main branch point for neurons that have\n    incompletely annotated synapses.\n\n    Parameters\n    ----------\n    x :         TreeNeuron | MeshNeuron | NeuronList\n                Neuron(s) to calculate bending flow for. Must have connectors!\n\n    Returns\n    -------\n    neuron\n                Adds \"bending_flow\" as column in the node table (for\n                TreeNeurons) or as `.bending_flow` property\n                (for MeshNeurons).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; n.reroot(n.soma, inplace=True)\n    &gt;&gt;&gt; _ = navis.bending_flow(n)\n    &gt;&gt;&gt; n.nodes.bending_flow.max()\n    785645\n\n    See Also\n    --------\n    [`navis.synapse_flow_centrality`][]\n            Calculate synapse flow centrality after Schneider-Mizell et al.\n    [`navis.segregation_index`][]\n            Calculate segregation score (polarity).\n    [`navis.arbor_segregation_index`][]\n            Calculate the a by-arbor segregation index.\n    [`navis.split_axon_dendrite`][]\n            Split the neuron into axon, dendrite and primary neurite.\n\n    \"\"\"\n    if not isinstance(x, core.TreeNeuron):\n        raise ValueError(f'Expected TreeNeuron(s), got \"{type(x)}\"')\n\n    if not x.has_connectors:\n        raise ValueError(\"Neuron must have connectors.\")\n\n    if np.any(x.soma) and not np.all(np.isin(x.soma, x.root)):\n        logger.warning(f\"Neuron {x.id} is not rooted to its soma!\")\n\n    # We will be processing a super downsampled version of the neuron to speed\n    # up calculations\n    current_level = logger.level\n    logger.setLevel(\"ERROR\")\n    y = x.downsample(factor=float(\"inf\"), preserve_nodes=\"connectors\", inplace=False)\n    logger.setLevel(current_level)\n\n    # Figure out how connector types are labeled\n    cn_types = y.connectors.type.unique()\n    if all(np.isin([\"pre\", \"post\"], cn_types)):\n        pre, post = \"pre\", \"post\"\n    elif all(np.isin([0, 1], cn_types)):\n        pre, post = 0, 1\n    else:\n        raise ValueError(f\"Unable to parse connector types for neuron {y.id}\")\n\n    # Get list of nodes with pre/postsynapses\n    pre_node_ids = y.connectors[y.connectors.type == pre].node_id.values\n    post_node_ids = y.connectors[y.connectors.type == post].node_id.values\n\n    # Get list of branch_points\n    bp_node_ids = y.nodes[y.nodes.type == \"branch\"].node_id.values.tolist()\n    # Add root if it is also a branch point\n    for root in y.root:\n        if y.graph.degree(root) &gt; 1:\n            bp_node_ids += [root]\n\n    # Get a list of childs of each branch point\n    bp_childs = {t: [e[0] for e in y.graph.in_edges(t)] for t in bp_node_ids}\n    childs = [tn for l in bp_childs.values() for tn in l]\n\n    # Get number of pre/postsynapses distal to each branch's childs\n    # Note that we're using geodesic matrix here because it is much more\n    # efficient than for `distal_to` for larger queries/neurons\n    dists = graph.geodesic_matrix(\n        y, from_=np.append(pre_node_ids, post_node_ids), directed=True, weight=None\n    )\n    distal = dists[childs] &lt; np.inf\n\n    # Since nodes can have multiple pre-/postsynapses but they show up only\n    # once in distal, we have to reindex to reflect the correct\n    # number of synapes\n    distal_pre = distal.loc[pre_node_ids]\n    distal_post = distal.loc[post_node_ids]\n\n    # Sum up columns: now each row represents the number of pre/postsynapses\n    # distal to that node\n    distal_pre_sum = distal_pre.sum(axis=0)\n    distal_post_sum = distal_post.sum(axis=0)\n\n    # Now go over all branch points and check flow between branches\n    # (centrifugal) vs flow from branches to root (centripetal)\n    flow = {bp: 0 for bp in bp_childs}\n    for bp in bp_childs:\n        # We will use left/right to label the different branches here\n        # (even if there is more than two)\n        for left, right in itertools.permutations(bp_childs[bp], r=2):\n            flow[bp] += distal_post_sum.loc[left] * distal_pre_sum.loc[right]\n\n    # At this point there are only flows for the childs of branch points.\n    # Let's complete that mapping by adding flow for the nodes\n    # between branch points.\n    for s in x.small_segments:\n        # Segments' orientation goes from distal -&gt; proximal\n        # Drop first (distal) node if it is not a leaf\n        if s[0] in flow:\n            s = s[1:]\n\n        # Update remaining nodes with the flow of the first child\n        this_flow = flow.get(s[-1], 0)\n        flow.update({n: this_flow for n in s})\n\n    # Set flow centrality to None for all nodes\n    x.nodes[\"bending_flow\"] = x.nodes.node_id.map(flow)\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.betweeness_centrality","title":"<code>navis.betweeness_centrality</code>","text":"<p>Calculate vertex/node betweenness.</p> <p>Betweenness is (roughly) defined by the number of shortest paths going through a vertex or an edge.</p> PARAMETER DESCRIPTION <code>x</code> <p> TYPE: <code>            TreeNeuron | MeshNeuron | NeuronList</code> </p> <code>from_</code> <pre><code>        If provided will only consider paths from given nodes to\n        root(s):\n          - `leafs` will only use paths from leafs to the root\n          - `branch_points` will only use paths from branch points\n            to the root\n          - `from_` can also be a list/array of node IDs\n        Only implemented for `directed=True`!\n</code></pre> <p> TYPE: <code>        \"leafs\" | \"branch_points\" | iterable</code> DEFAULT: <code>None</code> </p> <code>directed</code> <pre><code>        Whether to use the directed or undirected graph.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>neuron</code> <p>Adds \"betweenness\" as column in the node table (for TreeNeurons) or as <code>.betweenness</code> property (for MeshNeurons).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(2, kind='skeleton')\n&gt;&gt;&gt; n.reroot(n.soma, inplace=True)\n&gt;&gt;&gt; _ = navis.betweeness_centrality(n)\n&gt;&gt;&gt; n[0].nodes.betweenness.max()\n436866\n&gt;&gt;&gt; m = navis.example_neurons(1, kind='mesh')\n&gt;&gt;&gt; _ = navis.betweeness_centrality(m)\n&gt;&gt;&gt; m.betweenness.max()\n59637\n</code></pre> Source code in <code>navis/morpho/mmetrics.py</code> <pre><code>@utils.map_neuronlist(desc=\"Calc. betweeness\", allow_parallel=True)\n@utils.meshneuron_skeleton(\n    method=\"node_properties\", reroot_soma=True, node_props=[\"betweenness\"]\n)\ndef betweeness_centrality(\n    x: \"core.NeuronObject\",\n    from_: Optional[Union[Literal[\"leafs\"], Literal[\"branch_points\"], Sequence]] = None,\n    directed: bool = True,\n) -&gt; \"core.NeuronObject\":\n    \"\"\"Calculate vertex/node betweenness.\n\n    Betweenness is (roughly) defined by the number of shortest paths going\n    through a vertex or an edge.\n\n    Parameters\n    ----------\n    x :             TreeNeuron | MeshNeuron | NeuronList\n    from_ :         \"leafs\" | \"branch_points\" | iterable, optional\n                    If provided will only consider paths from given nodes to\n                    root(s):\n                      - `leafs` will only use paths from leafs to the root\n                      - `branch_points` will only use paths from branch points\n                        to the root\n                      - `from_` can also be a list/array of node IDs\n                    Only implemented for `directed=True`!\n    directed :      bool\n                    Whether to use the directed or undirected graph.\n\n    Returns\n    -------\n    neuron\n                Adds \"betweenness\" as column in the node table (for\n                TreeNeurons) or as `.betweenness` property\n                (for MeshNeurons).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(2, kind='skeleton')\n    &gt;&gt;&gt; n.reroot(n.soma, inplace=True)\n    &gt;&gt;&gt; _ = navis.betweeness_centrality(n)\n    &gt;&gt;&gt; n[0].nodes.betweenness.max()\n    436866\n    &gt;&gt;&gt; m = navis.example_neurons(1, kind='mesh')\n    &gt;&gt;&gt; _ = navis.betweeness_centrality(m)\n    &gt;&gt;&gt; m.betweenness.max()\n    59637\n\n    \"\"\"\n    utils.eval_param(x, name=\"x\", allowed_types=(core.TreeNeuron,))\n\n    if isinstance(from_, str):\n        utils.eval_param(from_, name=\"from_\", allowed_values=(\"leafs\", \"branch_points\"))\n    else:\n        utils.eval_param(\n            from_,\n            name=\"from_\",\n            allowed_types=(type(None), np.ndarray, list, tuple, set),\n        )\n\n    G = x.igraph\n    if isinstance(from_, type(None)):\n        bc = dict(\n            zip(G.vs.get_attribute_values(\"node_id\"), G.betweenness(directed=directed))\n        )\n    else:\n        if not directed:\n            raise ValueError(\"`from_!=None` only implemented for `directed=True`\")\n        paths = []\n\n        if from_ == \"leafs\":\n            sources = G.vs.select(_indegree=0)\n        elif from_ == \"branch_points\":\n            sources = G.vs.select(_indegree_ge=2)\n        else:\n            sources = G.vs.select(node_id_in=from_)\n\n        roots = G.vs.select(_outdegree=0)\n        for r in roots:\n            paths += G.get_shortest_paths(r, to=sources, mode=\"in\")\n        # Drop too short paths\n        paths = [p for p in paths if len(p) &gt; 2]\n        flat_ix = [i for p in paths for i in p[:-1]]\n        ix, counts = np.unique(flat_ix, return_counts=True)\n        ids = [G.vs[i][\"node_id\"] for i in ix]\n        bc = {i: 0 for i in x.nodes.node_id.values}\n        bc.update(dict(zip(ids, counts)))\n\n    x.nodes[\"betweenness\"] = x.nodes.node_id.map(bc).astype(int)\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.break_fragments","title":"<code>navis.break_fragments</code>","text":"<p>Break neuron into its connected components.</p> <p>Neurons can consists of several disconnected fragments. This function turns these fragments into separate neurons.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Fragmented neuron.\n</code></pre> <p> TYPE: <code>            TreeNeuron | MeshNeuron</code> </p> <code>labels_only</code> <pre><code>        If True, will only label each node/vertex by which\n        fragment it belongs to. For TreeNeurons, this adds a\n        `\"fragment\"` column and for MeshNeurons, it adds a\n        `.fragments` property.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> <code>min_size</code> <pre><code>        Fragments smaller than this (# of nodes/vertices) will be\n        dropped. Ignored if `labels_only=True`.\n</code></pre> <p> TYPE: <code>     int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>NeuronList</code> See Also <p><code>navis.heal_skeleton</code>             Use to heal fragmentation instead of breaking it up.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; # Artifically disconnect parts of the neuron\n&gt;&gt;&gt; n.nodes.loc[100, 'parent_id'] = -1\n&gt;&gt;&gt; # Break into fragments\n&gt;&gt;&gt; frags = navis.break_fragments(n)\n&gt;&gt;&gt; len(frags)\n2\n</code></pre> Source code in <code>navis/morpho/manipulation.py</code> <pre><code>def break_fragments(\n    x: Union[\"core.TreeNeuron\", \"core.MeshNeuron\"],\n    labels_only: bool = False,\n    min_size: Optional[int] = None,\n) -&gt; \"core.NeuronList\":\n    \"\"\"Break neuron into its connected components.\n\n    Neurons can consists of several disconnected fragments. This function\n    turns these fragments into separate neurons.\n\n    Parameters\n    ----------\n    x :             TreeNeuron | MeshNeuron\n                    Fragmented neuron.\n    labels_only :   bool\n                    If True, will only label each node/vertex by which\n                    fragment it belongs to. For TreeNeurons, this adds a\n                    `\"fragment\"` column and for MeshNeurons, it adds a\n                    `.fragments` property.\n    min_size :      int, optional\n                    Fragments smaller than this (# of nodes/vertices) will be\n                    dropped. Ignored if `labels_only=True`.\n\n    Returns\n    -------\n    NeuronList\n\n    See Also\n    --------\n    [`navis.heal_skeleton`][]\n                Use to heal fragmentation instead of breaking it up.\n\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; # Artifically disconnect parts of the neuron\n    &gt;&gt;&gt; n.nodes.loc[100, 'parent_id'] = -1\n    &gt;&gt;&gt; # Break into fragments\n    &gt;&gt;&gt; frags = navis.break_fragments(n)\n    &gt;&gt;&gt; len(frags)\n    2\n\n    \"\"\"\n    if isinstance(x, core.NeuronList) and len(x) == 1:\n        x = x[0]\n\n    if not isinstance(x, (core.TreeNeuron, core.MeshNeuron)):\n        raise TypeError(f'Expected Tree- or MeshNeuron, got \"{type(x)}\"')\n\n    # Get connected components\n    comp = graph._connected_components(x)\n    # Sort so that the first component is the largest\n    comp = sorted(comp, key=len, reverse=True)\n\n    if labels_only:\n        cc_id = {n: i for i, cc in enumerate(comp) for n in cc}\n        if isinstance(x, core.TreeNeuron):\n            x.nodes[\"fragment\"] = x.nodes.node_id.map(cc_id).astype(str)\n        elif isinstance(x, core.MeshNeuron):\n            x.fragments = np.array([cc_id[i] for i in range(x.n_vertices)]).astype(str)\n        return x\n\n    if min_size:\n        comp = [cc for cc in comp if len(cc) &gt;= min_size]\n\n    return core.NeuronList(\n        [\n            subset.subset_neuron(x, list(ss), inplace=False)\n            for ss in config.tqdm(\n                comp, desc=\"Breaking\", disable=config.pbar_hide, leave=config.pbar_leave\n            )\n        ]\n    )\n</code></pre>"},{"location":"reference/navis/#navis.cable_overlap","title":"<code>navis.cable_overlap</code>","text":"<p>Calculate the amount of cable of neuron A within distance of neuron B.</p> PARAMETER DESCRIPTION <code>a</code> <pre><code>    Neuron(s) for which to compute cable within distance. It is\n    highly recommended to resample neurons to guarantee an even\n    sampling rate.\n</code></pre> <p> TYPE: <code>NeuronObject</code> </p> <code>dist</code> <pre><code>    Maximum distance. If the neurons have their `.units` set, you\n    can also provides this as a string such as \"2 microns\".\n</code></pre> <p> TYPE: <code>     int | float</code> DEFAULT: <code>2</code> </p> <code>method</code> <pre><code>    Method by which to calculate the overlapping cable between\n    two cables:\n\n      Assuming that neurons A and B have 300 and 150 um of cable\n      within given distances, respectively:\n\n        1. 'min' returns 150\n        2. 'max' returns 300\n        3. 'mean' returns 225\n        4. 'forward' returns 300 (i.e. A-&gt;B)\n        5. 'reverse' returns 150 (i.e. B-&gt;A)\n</code></pre> <p> TYPE: <code>   'min' | 'max' | 'mean' | 'forward' | 'reverse'</code> DEFAULT: <code>'min'</code> </p> RETURNS DESCRIPTION <code>pandas.DataFrame</code> <p>Matrix in which neurons A are rows, neurons B are columns. Cable within distance is given in the neuron's native units: </p><pre><code>            neuronD  neuronE   neuronF  ...\nneuronA         5        1         0\nneuronB        10       20         5\nneuronC         4        3        15\n...\n</code></pre><p></p> See Also <p><code>navis.resample_skeleton</code>             Use to resample neurons before calculating overlap.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(4)\n&gt;&gt;&gt; # Cable overlap is given in the neurons' units\n&gt;&gt;&gt; # Converting the example neurons from 8x8x8 voxel space into microns\n&gt;&gt;&gt; # make the results easier to interpret\n&gt;&gt;&gt; nl = nl.convert_units('um')\n&gt;&gt;&gt; # Resample to half a micron\n&gt;&gt;&gt; nl_res = nl.resample('.5 micron', inplace=False)\n&gt;&gt;&gt; # Get overlapping cable within 2 microns\n&gt;&gt;&gt; ol = navis.cable_overlap(nl_res[:2], nl_res[2:], dist='2 microns')\n</code></pre> Source code in <code>navis/connectivity/predict.py</code> <pre><code>def cable_overlap(a: NeuronObject,\n                  b: NeuronObject,\n                  dist: Union[float, str] = 2,\n                  method: Union[Literal['min'], Literal['max'], Literal['mean'],\n                                Literal['foward'], Literal['reverse']] = 'min'\n                  ) -&gt; pd.DataFrame:\n    \"\"\"Calculate the amount of cable of neuron A within distance of neuron B.\n\n    Parameters\n    ----------\n    a,b :       TreeNeuron | NeuronList\n                Neuron(s) for which to compute cable within distance. It is\n                highly recommended to resample neurons to guarantee an even\n                sampling rate.\n    dist :      int | float, optional\n                Maximum distance. If the neurons have their `.units` set, you\n                can also provides this as a string such as \"2 microns\".\n    method :    'min' | 'max' | 'mean' | 'forward' | 'reverse'\n                Method by which to calculate the overlapping cable between\n                two cables:\n\n                  Assuming that neurons A and B have 300 and 150 um of cable\n                  within given distances, respectively:\n\n                    1. 'min' returns 150\n                    2. 'max' returns 300\n                    3. 'mean' returns 225\n                    4. 'forward' returns 300 (i.e. A-&gt;B)\n                    5. 'reverse' returns 150 (i.e. B-&gt;A)\n\n    Returns\n    -------\n    pandas.DataFrame\n            Matrix in which neurons A are rows, neurons B are columns. Cable\n            within distance is given in the neuron's native units:\n            ```\n                        neuronD  neuronE   neuronF  ...\n            neuronA         5        1         0\n            neuronB        10       20         5\n            neuronC         4        3        15\n            ...\n            ```\n\n    See Also\n    --------\n    [`navis.resample_skeleton`][]\n                Use to resample neurons before calculating overlap.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(4)\n    &gt;&gt;&gt; # Cable overlap is given in the neurons' units\n    &gt;&gt;&gt; # Converting the example neurons from 8x8x8 voxel space into microns\n    &gt;&gt;&gt; # make the results easier to interpret\n    &gt;&gt;&gt; nl = nl.convert_units('um')\n    &gt;&gt;&gt; # Resample to half a micron\n    &gt;&gt;&gt; nl_res = nl.resample('.5 micron', inplace=False)\n    &gt;&gt;&gt; # Get overlapping cable within 2 microns\n    &gt;&gt;&gt; ol = navis.cable_overlap(nl_res[:2], nl_res[2:], dist='2 microns')\n\n    \"\"\"\n    if not isinstance(a, (TreeNeuron, NeuronList)) \\\n       or not isinstance(b, (TreeNeuron, NeuronList)):\n        raise TypeError(f'Expected `TreeNeurons`, got \"{type(a)}\" and \"{type(b)}\"')\n\n    if not isinstance(a, NeuronList):\n        a = NeuronList(a)\n\n    if not isinstance(b, NeuronList):\n        b = NeuronList(b)\n\n    # Make sure neurons have the same units\n    # Do not use np.unique here because unit_str can be `None`\n    units = set(np.append(a._unit_str, b._unit_str))\n    units = np.array(list(units)).astype(str)\n    if len(units) &gt; 1:\n        logger.warning('Neurons appear to have different units: '\n                       f'{\", \".join(units)}. If that is the case, cable '\n                       'matrix overlap results will be garbage.')\n\n    allowed_methods = ['min', 'max', 'mean', 'forward', 'reverse']\n    if method not in allowed_methods:\n        raise ValueError(f'Unknown method \"{method}\". Allowed methods: '\n                         f'\"{\", \".join(allowed_methods)}\"')\n\n    dist = a[0].map_units(dist, on_error='raise')\n\n    matrix = pd.DataFrame(np.zeros((a.shape[0], b.shape[0])),\n                          index=a.id, columns=b.id)\n\n    # Compute required props\n    treesA = []\n    lengthsA = []\n    for nA in a:\n        points, vect, length = graph.neuron2tangents(nA)\n        treesA.append(scipy.spatial.cKDTree(points))\n        lengthsA.append(length)\n\n    treesB = []\n    lengthsB = []\n    for nB in b:\n        points, vect, length = graph.neuron2tangents(nB)\n        treesB.append(scipy.spatial.cKDTree(points))\n        lengthsB.append(length)\n\n    with config.tqdm(total=len(a), desc='Calc. overlap',\n                     disable=config.pbar_hide,\n                     leave=config.pbar_leave) as pbar:\n        for i, nA in enumerate(a):\n            # Get cKDTree for nA\n            tA = treesA[i]\n\n            for k, nB in enumerate(b):\n                # Get cKDTree for nB\n                tB = treesB[k]\n\n                # Query nB -&gt; nA\n                distA, ixA = tA.query(tB.data,\n                                      k=1,\n                                      distance_upper_bound=dist,\n                                      workers=-1\n                                      )\n                # Query nA -&gt; nB\n                distB, ixB = tB.query(tA.data,\n                                      k=1,\n                                      distance_upper_bound=dist,\n                                      workers=-1\n                                      )\n\n                nA_lengths = lengthsA[i][ixA[distA != float('inf')]]\n                nB_lengths = lengthsB[k][ixB[distB != float('inf')]]\n\n                if method == 'mean':\n                    overlap = (nA_lengths.sum() + nB_lengths.sum()) / 2\n                elif method == 'max':\n                    overlap = max(nA_lengths.sum(), nB_lengths.sum())\n                elif method == 'min':\n                    overlap = min(nA_lengths.sum(), nB_lengths.sum())\n                elif method == 'forward':\n                    overlap = nA_lengths.sum()\n                elif method == 'reverse':\n                    overlap = nB_lengths.sum()\n\n                matrix.iloc[i, k] = overlap\n\n            pbar.update(1)\n\n    return matrix\n</code></pre>"},{"location":"reference/navis/#navis.cell_body_fiber","title":"<code>navis.cell_body_fiber</code>","text":"<p>Prune neuron to its cell body fiber.</p> <p>Here, \"cell body fiber\" (CBF) refers to the tract connecting the soma to the backbone in unipolar neuron (common in e.g. insects). This function works best for typical neurons with clean skeletons.</p> PARAMETER DESCRIPTION <code>x</code> <p> TYPE: <code>            TreeNeuron | MeshNeuron | NeuronList</code> </p> <code>method</code> <pre><code>        The method to use:\n          - \"longest_neurite\" assumes that the main branch point\n            is where the two largest branches converge\n          - \"betweenness\" uses centrality to determine the point\n            which most shortest paths traverse\n</code></pre> <p> TYPE: <code>       \"longest_neurite\" | \"betweenness\"</code> DEFAULT: <code>'betweenness'</code> </p> <code>reroot_soma</code> <pre><code>        If True (recommended) and neuron has a soma, it will be\n        rerooted to its soma.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>True</code> </p> <code>heal</code> <pre><code>        If True (recommended), will heal fragmented neurons.\n        Fragmented neurons are not guaranteed to have correct CBFs.\n</code></pre> <p> TYPE: <code>         bool</code> DEFAULT: <code>True</code> </p> <code>threshold</code> <pre><code>        For method \"betweenness\" only: threshold at which to cut the\n        cell body fiber. Lower thresholds produce longer CBFs.\n</code></pre> <p> TYPE: <code>    float [0-1]</code> DEFAULT: <code>0.95</code> </p> <code>inverse</code> <pre><code>        If True, will instead *remove* the cell body fiber.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> <code>inplace</code> <pre><code>        If False, pruning is performed on copy of original neuron\n        which is then returned.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>TreeNeuron / List</code> <p>Pruned neuron(s). Neurons without branches (i.e. w/ a single long segment) will be returned unaltered.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; cbf = navis.cell_body_fiber(n, inplace=False)\n&gt;&gt;&gt; # Neuron now has only a single segment from the soma to the main fork\n&gt;&gt;&gt; len(cbf.segments)\n1\n</code></pre> See Also <p><code>navis.find_main_branchpoint</code>                 Find the main branch point.</p> <p><code>navis.betweeness_centrality</code>                 Calculate the per-node betweeness centrality. This is used                 under the hood for <code>method='betweeness'</code>.</p> Source code in <code>navis/morpho/manipulation.py</code> <pre><code>@utils.map_neuronlist(desc=\"Pruning\", allow_parallel=True)\n@utils.meshneuron_skeleton(method=\"subset\")\ndef cell_body_fiber(\n    x: NeuronObject,\n    method: Union[Literal[\"longest_neurite\"], Literal[\"betweenness\"]] = \"betweenness\",\n    reroot_soma: bool = True,\n    heal: bool = True,\n    threshold: float = 0.95,\n    inverse: bool = False,\n    inplace: bool = False,\n):\n    \"\"\"Prune neuron to its cell body fiber.\n\n    Here, \"cell body fiber\" (CBF) refers to the tract connecting the soma to the\n    backbone in unipolar neuron (common in e.g. insects). This function works\n    best for typical neurons with clean skeletons.\n\n    Parameters\n    ----------\n    x :             TreeNeuron | MeshNeuron | NeuronList\n    method :        \"longest_neurite\" | \"betweenness\"\n                    The method to use:\n                      - \"longest_neurite\" assumes that the main branch point\n                        is where the two largest branches converge\n                      - \"betweenness\" uses centrality to determine the point\n                        which most shortest paths traverse\n    reroot_soma :   bool\n                    If True (recommended) and neuron has a soma, it will be\n                    rerooted to its soma.\n    heal :          bool\n                    If True (recommended), will heal fragmented neurons.\n                    Fragmented neurons are not guaranteed to have correct CBFs.\n    threshold :     float [0-1]\n                    For method \"betweenness\" only: threshold at which to cut the\n                    cell body fiber. Lower thresholds produce longer CBFs.\n    inverse :       bool\n                    If True, will instead *remove* the cell body fiber.\n    inplace :       bool, optional\n                    If False, pruning is performed on copy of original neuron\n                    which is then returned.\n\n    Returns\n    -------\n    TreeNeuron/List\n                    Pruned neuron(s). Neurons without branches (i.e. w/ a single\n                    long segment) will be returned unaltered.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; cbf = navis.cell_body_fiber(n, inplace=False)\n    &gt;&gt;&gt; # Neuron now has only a single segment from the soma to the main fork\n    &gt;&gt;&gt; len(cbf.segments)\n    1\n\n    See Also\n    --------\n    [`navis.find_main_branchpoint`][]\n                    Find the main branch point.\n\n    [`navis.betweeness_centrality`][]\n                    Calculate the per-node betweeness centrality. This is used\n                    under the hood for `method='betweeness'`.\n\n    \"\"\"\n    utils.eval_param(\n        method, \"method\", allowed_values=(\"longest_neurite\", \"betweenness\")\n    )\n\n    # The decorator makes sure that at this point we have single neurons\n    if not isinstance(x, core.TreeNeuron):\n        raise TypeError(f\"Expected TreeNeuron(s), got {type(x)}\")\n\n    if not inplace:\n        x = x.copy()\n\n    if x.n_trees &gt; 1 and heal:\n        _ = heal_skeleton(x, method=\"LEAFS\", inplace=True)\n\n    # If no branches, just return the neuron\n    if \"branch\" not in x.nodes.type.values:\n        return x\n\n    if reroot_soma and not isinstance(x.soma, type(None)):\n        x.reroot(x.soma, inplace=True)\n\n    # Find main branch point\n    cut = graph.find_main_branchpoint(\n        x, method=method, threshold=threshold, reroot_soma=False\n    )\n\n    # Find the path to root (and account for multiple roots)\n    for r in x.root:\n        try:\n            path = nx.shortest_path(x.graph, target=r, source=cut)\n            break\n        except nx.NetworkXNoPath:\n            continue\n        except BaseException:\n            raise\n\n    if not inverse:\n        keep = path\n    else:\n        keep = x.nodes.node_id.values[~np.isin(x.nodes.node_id, path)]\n\n    _ = subset.subset_neuron(x, keep, inplace=True)\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.clear3d","title":"<code>navis.clear3d</code>","text":"<p>Clear viewer 3D canvas.</p> Source code in <code>navis/plotting/vispy/vputils.py</code> <pre><code>def clear3d():\n    \"\"\"Clear viewer 3D canvas.\"\"\"\n    viewer = get_viewer()\n\n    if viewer:\n        viewer.clear()\n</code></pre>"},{"location":"reference/navis/#navis.close3d","title":"<code>navis.close3d</code>","text":"<p>Close existing 3D viewer (wipes memory).</p> Source code in <code>navis/plotting/vispy/vputils.py</code> <pre><code>def close3d():\n    \"\"\"Close existing 3D viewer (wipes memory).\"\"\"\n    try:\n        viewer = get_viewer()\n        viewer.close()\n        globals().pop('viewer')\n        del viewer\n    except BaseException:\n        pass\n</code></pre>"},{"location":"reference/navis/#navis.combine_neurons","title":"<code>navis.combine_neurons</code>","text":"<p>Combine multiple neurons into one.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            Neurons to combine. Must all be of the same type. Does\n            not yet work with VoxelNeurons. The combined neuron will\n            inherit its name, id, units, etc. from the first neuron\n            in `x`.\n</code></pre> <p> TYPE: <code>                NeuronList | Neuron/List</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Neuron</code> <p>Combined neuron.</p> See Also <p><code>navis.stitch_skeletons</code>                     Stitches multiple skeletons together to create one                     continuous neuron.</p> <p>Examples:</p> <p>Combine skeletons:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(3)\n&gt;&gt;&gt; comb = navis.combine_neurons(nl)\n</code></pre> <p>Combine meshes:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(3, kind='mesh')\n&gt;&gt;&gt; comb = navis.combine_neurons(nl)\n</code></pre> <p>Combine dotprops:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(3)\n&gt;&gt;&gt; dp = navis.make_dotprops(nl)\n&gt;&gt;&gt; comb = navis.combine_neurons(dp)\n</code></pre> Source code in <code>navis/morpho/manipulation.py</code> <pre><code>def combine_neurons(\n    *x: Union[Sequence[NeuronObject], \"core.NeuronList\"],\n) -&gt; \"core.NeuronObject\":\n    \"\"\"Combine multiple neurons into one.\n\n    Parameters\n    ----------\n    x :                 NeuronList | Neuron/List\n                        Neurons to combine. Must all be of the same type. Does\n                        not yet work with VoxelNeurons. The combined neuron will\n                        inherit its name, id, units, etc. from the first neuron\n                        in `x`.\n\n    Returns\n    -------\n    Neuron\n                        Combined neuron.\n\n    See Also\n    --------\n    [`navis.stitch_skeletons`][]\n                        Stitches multiple skeletons together to create one\n                        continuous neuron.\n\n    Examples\n    --------\n    Combine skeletons:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(3)\n    &gt;&gt;&gt; comb = navis.combine_neurons(nl)\n\n    Combine meshes:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(3, kind='mesh')\n    &gt;&gt;&gt; comb = navis.combine_neurons(nl)\n\n    Combine dotprops:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(3)\n    &gt;&gt;&gt; dp = navis.make_dotprops(nl)\n    &gt;&gt;&gt; comb = navis.combine_neurons(dp)\n\n    \"\"\"\n    # Compile list of individual neurons\n    nl = utils.unpack_neurons(x)\n    nl = core.NeuronList(nl)\n\n    # Check that neurons are all of the same type\n    if len(nl.types) &gt; 1:\n        raise TypeError(\"Unable to combine neurons of different types\")\n\n    if isinstance(nl[0], core.TreeNeuron):\n        x = stitch_skeletons(*nl, method=\"NONE\", master=\"FIRST\")\n    elif isinstance(nl[0], core.MeshNeuron):\n        x = nl[0].copy()\n        comb = tm.util.concatenate([n.trimesh for n in nl])\n        x._vertices = comb.vertices\n        x._faces = comb.faces\n\n        if any(nl.has_connectors):\n            x._connectors = pd.concat(\n                [n.connectors for n in nl],  # type: ignore  # no stubs for concat\n                ignore_index=True,\n            )\n    elif isinstance(nl[0], core.Dotprops):\n        x = nl[0].copy()\n        x._points = np.vstack(nl._points)\n\n        x._vect = np.vstack(nl.vect)\n\n        if not any([isinstance(n._alpha, type(None)) for n in nl]):\n            x._alpha = np.hstack(nl.alpha)\n\n        if any(nl.has_connectors):\n            x._connectors = pd.concat(\n                [n.connectors for n in nl],  # type: ignore  # no stubs for concat\n                ignore_index=True,\n            )\n    elif isinstance(nl[0], core.VoxelNeuron):\n        raise TypeError(\"Combining VoxelNeuron not (yet) supported\")\n    else:\n        raise TypeError(f\"Unable to combine {type(nl[0])}\")\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.connectivity_similarity","title":"<code>navis.connectivity_similarity</code>","text":"<p>Calculate connectivity similarity.</p> <p>This functions offers a selection of metrics to compare connectivity:</p> <ul> <li>cosine: Cosine similarity (see here)</li> <li>rank_index: Normalized difference in rank of synaptic partners.</li> <li>matching_index: Number of shared partners divided by total number of partners.</li> <li> <p>matching_index_synapses: Number of shared synapses (i.e. number of connections from/onto the same partners)   divided by total number of synapses.</p> <p>Attention</p> <p>This metric is tricky when there is a disparity of total number of connections between neuron A and B. For example, consider 100/200 and 1/50 shared/total synapse: 101/250 results in a fairly high matching index of 0.404.</p> </li> <li> <p>matching_index_weighted_synapses: Similar to matching_index_synapses but slightly less prone to above   mentioned error as it uses the percentage of shared synapses:</p> \\[ S = \\frac{\\mathrm{NeuronA}_{\\mathrm{sharedSynapses}}}{\\mathrm{NeuronA}_{\\mathrm{totalSynapses}}} \\times \\frac{\\mathrm{NeuronB}_{\\mathrm{sharedSynapses}}}{\\mathrm{NeuronB}_{\\mathrm{totalSynapses}}} \\] </li> <li> <p>vertex: Matching index that rewards shared and punishes non-shared partners. Based on   Jarrell et al., 2012:    $$    f(x,y) = min(x,y) - C1 \\times max(x,y) \\times \\exp(-C2 * min(x,y))    $$    The final score is the sum of \\(f(x,y)\\) over all edges x, y between neurons A+B and their partners. C1 determines    how negatively a case where one edge is much stronger than another is punished. C2 determines the point where the    similarity switches from negative to positive. C1 and C2 default to 0.5 and 1, respectively, but can be changed    by passing them in a dictionary as <code>**kwargs</code>.</p> </li> <li>vertex_normalized: This is vertex similarity normalized by the lowest (hypothetical total dissimilarity)   and highest (all edge weights the same) achievable score.</li> </ul> PARAMETER DESCRIPTION <code>adjacency</code> <pre><code>            (N, M) observation vector with M observations for N\n            neurons - e.g. an adjacency matrix. Will calculate\n            similarity for all rows using the columns as observations.\n</code></pre> <p> TYPE: <code>        pandas DataFrame | numpy array</code> </p> <code>metric</code> <pre><code>            Metric used to compare connectivity. See notes for\n            detailed explanation.\n</code></pre> <p> TYPE: <code>           'cosine' | 'rank_index'| 'matching_index' | 'matching_index_synapses' | 'matching_index_weighted_synapses' | 'vertex' | 'vertex_normalized'</code> DEFAULT: <code>'vertex_normalized'</code> </p> <code>threshold</code> <pre><code>            Connections weaker than this will be set to zero.\n</code></pre> <p> TYPE: <code>        int</code> DEFAULT: <code>None</code> </p> <code>n_cores</code> <pre><code>            Number of parallel processes to use. Defaults to half\n            the available cores.\n</code></pre> <p> TYPE: <code>          int</code> DEFAULT: <code>max(1, os.cpu_count() // 2)</code> </p> <code>**kwargs</code> <pre><code>            Additional keyword arguments to pass to the metric function.\n            See notes above for details.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Pandas DataFrame with similarity scores. Neurons without any connectivity will show up with <code>np.nan</code> for scores.</p> Source code in <code>navis/connectivity/similarity.py</code> <pre><code>def connectivity_similarity(adjacency: Union[pd.DataFrame, np.ndarray],\n                            metric: Union[Literal['matching_index'],\n                                          Literal['matching_index_synapses'],\n                                          Literal['matching_index_weighted_synapses'],\n                                          Literal['vertex'],\n                                          Literal['vertex_normalized'],\n                                          Literal['cosine'],\n                                          ] = 'vertex_normalized',\n                            threshold: Optional[int] = None,\n                            n_cores: int = max(1, os.cpu_count() // 2),\n                            **kwargs) -&gt; pd.DataFrame:\n    r\"\"\"Calculate connectivity similarity.\n\n    This functions offers a selection of metrics to compare connectivity:\n\n    - **cosine**: Cosine similarity (see [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html))\n    - **rank_index**: Normalized difference in rank of synaptic partners.\n    - **matching_index**: Number of shared partners divided by total number of partners.\n    - **matching_index_synapses**: Number of shared synapses (i.e. number of connections from/onto the same partners)\n      divided by total number of synapses.\n\n        !!! info \"Attention\"\n            This metric is tricky when there is a disparity of total number of connections between neuron A and B.\n            For example, consider 100/200 and 1/50 shared/total synapse: 101/250 results in a fairly high matching\n            index of 0.404.\n\n    - **matching_index_weighted_synapses**: Similar to *matching_index_synapses* but slightly less prone to above\n      mentioned error as it uses the percentage of shared synapses:\n\n        $$\n        S = \\frac{\\mathrm{NeuronA}_{\\mathrm{sharedSynapses}}}{\\mathrm{NeuronA}_{\\mathrm{totalSynapses}}} \\times \\frac{\\mathrm{NeuronB}_{\\mathrm{sharedSynapses}}}{\\mathrm{NeuronB}_{\\mathrm{totalSynapses}}}\n        $$\n\n    - **vertex**: Matching index that rewards shared and punishes non-shared partners. Based on\n      [Jarrell et al., 2012](http://science.sciencemag.org/content/337/6093/437):\n       $$\n       f(x,y) = min(x,y) - C1 \\times max(x,y) \\times \\exp(-C2 * min(x,y))\n       $$\n       The final score is the sum of $f(x,y)$ over all edges x, y between neurons A+B and their partners. C1 determines\n       how negatively a case where one edge is much stronger than another is punished. C2 determines the point where the\n       similarity switches from negative to positive. C1 and C2 default to 0.5 and 1, respectively, but can be changed\n       by passing them in a dictionary as `**kwargs`.\n    - **vertex_normalized**: This is *vertex* similarity normalized by the lowest (hypothetical total dissimilarity)\n      and highest (all edge weights the same) achievable score.\n\n    Parameters\n    ----------\n    adjacency :         pandas DataFrame | numpy array\n                        (N, M) observation vector with M observations for N\n                        neurons - e.g. an adjacency matrix. Will calculate\n                        similarity for all rows using the columns as observations.\n    metric :            'cosine' | 'rank_index'| 'matching_index' | 'matching_index_synapses' | 'matching_index_weighted_synapses' | 'vertex' | 'vertex_normalized'\n                        Metric used to compare connectivity. See notes for\n                        detailed explanation.\n    threshold :         int, optional\n                        Connections weaker than this will be set to zero.\n    n_cores :           int\n                        Number of parallel processes to use. Defaults to half\n                        the available cores.\n    **kwargs\n                        Additional keyword arguments to pass to the metric function.\n                        See notes above for details.\n\n    Returns\n    -------\n    DataFrame\n                        Pandas DataFrame with similarity scores. Neurons without\n                        any connectivity will show up with `np.nan` for scores.\n\n    \"\"\"\n    FUNC_MAP = {'rank_index': _calc_rank_index,\n                'matching_index': _calc_matching_index,\n                'matching_index_synapses': _calc_matching_index_synapses,\n                'matching_index_weighted_synapses': partial(_calc_matching_index_synapses, weighted=True),\n                'vertex': _calc_vertex_similarity,\n                'vertex_normalized': partial(_calc_vertex_similarity, normalize=True),\n                'cosine': _calc_cosine_similarity\n                }\n\n    if not isinstance(metric, str) or metric.lower() not in FUNC_MAP:\n        raise ValueError(f'\"metric\" must be either: {\", \".join(FUNC_MAP.keys())}')\n\n    score_func = FUNC_MAP[metric.lower()]\n\n    if isinstance(adjacency, np.ndarray):\n        adjacency = pd.DataFrame(adjacency)\n    elif not isinstance(adjacency, pd.DataFrame):\n        raise TypeError(f'Expected DataFrame, got \"{type(adjacency)}\"')\n\n    if threshold:\n        # Do not manipulate original\n        adjacency = adjacency.copy()\n        adjacency[adjacency &lt; threshold] = 0\n\n    # Skip expensive checks if no empty vectors\n    if (adjacency.max(axis=1) == 0).any():\n        kwargs['validate'] = True\n    else:\n        kwargs['validate'] = False\n\n    # Prepare combinations matching scores\n    comb = combinations_generator(score_func, adjacency, **kwargs)\n\n    # Note that while we are mapping from a generator (`comb`), the pool will\n    # unfortunately not evaluate this lazily. This is a \"bug\" in the standard\n    # library that might get fixed at some point.\n    if n_cores &gt; 1:\n        with ProcessPoolExecutor(max_workers=n_cores) as e:\n            futures = e.map(_distributor, comb, chunksize=50000)\n\n            matching_indices = [n for n in config.tqdm(futures,\n                                                       total=adjacency.shape[0]**2,\n                                                       desc='Calc. similarity',\n                                                       disable=config.pbar_hide,\n                                                       leave=config.pbar_leave)]\n    else:\n        matching_indices = []\n        for c in config.tqdm(comb,\n                             total=adjacency.shape[0]**2,\n                             desc='Calc. similarity',\n                             disable=config.pbar_hide,\n                             leave=config.pbar_leave):\n            matching_indices.append(_distributor(c))\n\n    # Create empty scores matrix\n    neurons = adjacency.index.values\n    matching_scores = pd.DataFrame(np.zeros((len(neurons), len(neurons))),\n                                   index=neurons, columns=neurons)\n    # Populate scores matrix\n    comb_id = product(neurons, neurons)\n    for i, v in enumerate(comb_id):\n        matching_scores.at[v[0], v[1]] = matching_indices[i]\n\n    return matching_scores\n</code></pre>"},{"location":"reference/navis/#navis.connectivity_sparseness","title":"<code>navis.connectivity_sparseness</code>","text":"<p>Calculate sparseness.</p> <p>Sparseness comes in three flavors:</p> <p>Lifetime kurtosis (LTK) quantifies the widths of tuning curves (according to Muench &amp; Galizia, 2016):</p> \\[ S = \\Bigg\\{ \\frac{1}{N} \\sum^N_{i=1} \\Big[ \\frac{r_i - \\overline{r}}{\\sigma_r} \\Big] ^4  \\Bigg\\} - 3 \\] <p>where \\(N\\) is the number of observations, \\(r_i\\) the value of observation \\(i\\), and \\(\\overline{r}\\) and \\(\\sigma_r\\) the mean and the standard deviation of the observations' values, respectively. LTK is assuming a normal, or at least symmetric distribution.</p> <p>Lifetime sparseness (LTS) quantifies selectivity (Bhandawat et al., 2007):</p> \\[ S = \\frac{1}{1-1/N} \\Bigg[1- \\frac{\\big(\\sum^N_{j=1} r_j / N\\big)^2}{\\sum^N_{j=1} r_j^2 / N} \\Bigg] \\] <p>where \\(N\\) is the number of observations, and \\(r_j\\) is the value of an observation.</p> <p>Activity ratio describes distributions with heavy tails (Rolls and Tovee, 1995).</p> Notes <p><code>NaN</code> values will be ignored. You can use that to e.g. ignore zero values in a large connectivity matrix by changing these values to <code>NaN</code> before passing it to <code>navis.sparseness</code>.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    (N, M) dataset with N (rows) observations for M (columns)\n    neurons. One-dimensional data will be converted to two\n    dimensions (N rows, 1 column).\n</code></pre> <p> TYPE: <code>        DataFrame | array-like</code> </p> <code>which</code> <pre><code>    Determines whether lifetime sparseness (LTS) or lifetime\n    kurtosis (LTK) is returned.\n</code></pre> <p> TYPE: <code>    \"LTS\" | \"LTK\" | \"activity_ratio\"</code> DEFAULT: <code>'LTS'</code> </p> RETURNS DESCRIPTION <code>sparseness</code> <p><code>pandas.Series</code> if input was pandas DataFrame, else <code>numpy.array</code>.</p> <p>Examples:</p> <p>Calculate sparseness of olfactory inputs to group of neurons:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; # Get ORN response matrix from DoOR database\n&gt;&gt;&gt; url = 'https://raw.githubusercontent.com/ropensci/DoOR.data/master/data/door_response_matrix.csv'\n&gt;&gt;&gt; adj = pd.read_csv(url, delimiter=';')\n&gt;&gt;&gt; # Calculate lifetime sparseness\n&gt;&gt;&gt; S = navis.connectivity_sparseness(adj, which='LTS')\n&gt;&gt;&gt; # Plot distribution\n&gt;&gt;&gt; ax = S.plot.hist(bins=np.arange(0, 1, .1))\n&gt;&gt;&gt; _ = ax.set_xlabel('LTS')\n&gt;&gt;&gt; plt.show()\n</code></pre> Source code in <code>navis/connectivity/cnmetrics.py</code> <pre><code>def connectivity_sparseness(x: Union[pd.DataFrame, np.ndarray],\n                            which: Union[Literal['LTS'],\n                                         Literal['LTK'],\n                                         Literal['activity_ratio']] = 'LTS') -&gt; Union[pd.Series, np.ndarray]:\n    r\"\"\"Calculate sparseness.\n\n    Sparseness comes in three flavors:\n\n    **Lifetime kurtosis (LTK)** quantifies the widths of tuning curves\n    (according to Muench &amp; Galizia, 2016):\n\n    $$\n    S = \\Bigg\\{ \\frac{1}{N} \\sum^N_{i=1} \\Big[ \\frac{r_i - \\overline{r}}{\\sigma_r} \\Big] ^4  \\Bigg\\} - 3\n    $$\n\n    where $N$ is the number of observations, $r_i$ the value of\n    observation $i$, and $\\overline{r}$ and $\\sigma_r$ the mean and\n    the standard deviation of the observations' values, respectively.\n    LTK is assuming a normal, or at least symmetric distribution.\n\n    **Lifetime sparseness (LTS)** quantifies selectivity\n    (Bhandawat et al., 2007):\n\n    $$\n    S = \\frac{1}{1-1/N} \\Bigg[1- \\frac{\\big(\\sum^N_{j=1} r_j / N\\big)^2}{\\sum^N_{j=1} r_j^2 / N} \\Bigg]\n    $$\n\n    where $N$ is the number of observations, and $r_j$ is the\n    value of an observation.\n\n    **Activity ratio** describes distributions with heavy tails (Rolls and\n    Tovee, 1995).\n\n\n    Notes\n    -----\n    `NaN` values will be ignored. You can use that to e.g. ignore zero\n    values in a large connectivity matrix by changing these values to `NaN`\n    before passing it to `navis.sparseness`.\n\n\n    Parameters\n    ----------\n    x :         DataFrame | array-like\n                (N, M) dataset with N (rows) observations for M (columns)\n                neurons. One-dimensional data will be converted to two\n                dimensions (N rows, 1 column).\n    which :     \"LTS\" | \"LTK\" | \"activity_ratio\"\n                Determines whether lifetime sparseness (LTS) or lifetime\n                kurtosis (LTK) is returned.\n\n    Returns\n    -------\n    sparseness\n                `pandas.Series` if input was pandas DataFrame, else\n                `numpy.array`.\n\n    Examples\n    --------\n    Calculate sparseness of olfactory inputs to group of neurons:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; # Get ORN response matrix from DoOR database\n    &gt;&gt;&gt; url = 'https://raw.githubusercontent.com/ropensci/DoOR.data/master/data/door_response_matrix.csv'\n    &gt;&gt;&gt; adj = pd.read_csv(url, delimiter=';')\n    &gt;&gt;&gt; # Calculate lifetime sparseness\n    &gt;&gt;&gt; S = navis.connectivity_sparseness(adj, which='LTS')\n    &gt;&gt;&gt; # Plot distribution\n    &gt;&gt;&gt; ax = S.plot.hist(bins=np.arange(0, 1, .1))\n    &gt;&gt;&gt; _ = ax.set_xlabel('LTS')\n    &gt;&gt;&gt; plt.show()                                              # doctest: +SKIP\n\n    \"\"\"\n    if not isinstance(x, (pd.DataFrame, np.ndarray)):\n        x = np.array(x)\n\n    # Make sure we are working with 2 dimensional data\n    if isinstance(x, np.ndarray) and x.ndim == 1:\n        x = x.reshape(x.shape[0], 1)\n\n    N = np.sum(~np.isnan(x), axis=0)\n\n    if which == 'LTK':\n        return np.nansum(((x - np.nanmean(x, axis=0)) / np.nanstd(x, axis=0)) ** 4, axis=0) / N - 3\n    elif which == 'LTS':\n        return 1 / (1 - (1 / N)) * (1 - np.nansum(x / N, axis=0) ** 2 / np.nansum(x**2 / N, axis=0))\n    elif which == 'activity_ratio':\n        a = (np.nansum(x, axis=0) / N) ** 2 / (np.nansum(x, axis=0) ** 2 / N)\n        return 1 - a\n    else:\n        raise ValueError('Parameter \"which\" must be either \"LTS\", \"LTK\" or '\n                         '\"activity_ratio\"')\n</code></pre>"},{"location":"reference/navis/#navis.cut_skeleton","title":"<code>navis.cut_skeleton</code>","text":"<p>Split skeleton at given point and returns two new neurons.</p> <p>Split is performed between cut node and its parent node. The cut node itself will still be present in both resulting neurons.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>   Must be a single skeleton.\n</code></pre> <p> TYPE: <code>       TreeNeuron | NeuronList</code> </p> <code>where</code> <pre><code>   Node ID(s) or tag(s) of the node(s) to cut. The edge that is\n   cut is the one between this node and its parent. So cut node\n   must not be a root node! Multiple cuts are performed in the\n   order of `cut_node`. Fragments are ordered distal -&gt; proximal.\n</code></pre> <p> TYPE: <code>   int | str | list</code> </p> <code>ret</code> <pre><code>   Define which parts of the neuron to return. Use this to speed\n   up processing when you need only parts of the neuron.\n</code></pre> <p> TYPE: <code>     'proximal' | 'distal' | 'both'</code> DEFAULT: <code>'both'</code> </p> RETURNS DESCRIPTION <code>split</code> <p>Fragments of the input neuron after cutting sorted such that distal parts come before proximal parts. For example, with a single cut you can expect to return a NeuronList containing two neurons: the first contains the part distal and the second the part proximal to the cut node.</p> <p>The distal-&gt;proximal order of fragments is tried to be maintained for multiple cuts but this is not guaranteed.</p> <p> TYPE: <code>NeuronList</code> </p> <p>Examples:</p> <p>Cut skeleton at a (somewhat random) branch point</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; bp = n.nodes[n.nodes.type=='branch'].node_id.values\n&gt;&gt;&gt; dist, prox = navis.cut_skeleton(n, bp[0])\n</code></pre> <p>Make cuts at multiple branch points</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; bp = n.nodes[n.nodes.type=='branch'].node_id.values\n&gt;&gt;&gt; splits = navis.cut_skeleton(n, bp[:10])\n</code></pre> See Also <p><code>navis.TreeNeuron.prune_distal_to</code> <code>navis.TreeNeuron.prune_proximal_to</code> <code>TreeNeuron/List</code> shorthands to this function. <code>navis.subset_neuron</code>         Returns a neuron consisting of a subset of its nodes.</p> Source code in <code>navis/graph/graph_utils.py</code> <pre><code>def cut_skeleton(\n    x: \"core.NeuronObject\",\n    where: Union[int, str, List[Union[int, str]]],\n    ret: Union[Literal[\"both\"], Literal[\"proximal\"], Literal[\"distal\"]] = \"both\",\n) -&gt; \"core.NeuronList\":\n    \"\"\"Split skeleton at given point and returns two new neurons.\n\n    Split is performed between cut node and its parent node. The cut node itself\n    will still be present in both resulting neurons.\n\n    Parameters\n    ----------\n    x :        TreeNeuron | NeuronList\n               Must be a single skeleton.\n    where :    int | str | list\n               Node ID(s) or tag(s) of the node(s) to cut. The edge that is\n               cut is the one between this node and its parent. So cut node\n               must not be a root node! Multiple cuts are performed in the\n               order of `cut_node`. Fragments are ordered distal -&gt; proximal.\n    ret :      'proximal' | 'distal' | 'both', optional\n               Define which parts of the neuron to return. Use this to speed\n               up processing when you need only parts of the neuron.\n\n    Returns\n    -------\n    split :    NeuronList\n               Fragments of the input neuron after cutting sorted such that\n               distal parts come before proximal parts. For example, with a\n               single cut you can expect to return a NeuronList containing two\n               neurons: the first contains the part distal and the second the\n               part proximal to the cut node.\n\n               The distal-&gt;proximal order of fragments is tried to be maintained\n               for multiple cuts but this is not guaranteed.\n\n    Examples\n    --------\n    Cut skeleton at a (somewhat random) branch point\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; bp = n.nodes[n.nodes.type=='branch'].node_id.values\n    &gt;&gt;&gt; dist, prox = navis.cut_skeleton(n, bp[0])\n\n    Make cuts at multiple branch points\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; bp = n.nodes[n.nodes.type=='branch'].node_id.values\n    &gt;&gt;&gt; splits = navis.cut_skeleton(n, bp[:10])\n\n    See Also\n    --------\n    [`navis.TreeNeuron.prune_distal_to`][]\n    [`navis.TreeNeuron.prune_proximal_to`][]\n            `TreeNeuron/List` shorthands to this function.\n    [`navis.subset_neuron`][]\n            Returns a neuron consisting of a subset of its nodes.\n\n    \"\"\"\n    utils.eval_param(ret, name=\"ret\", allowed_values=(\"proximal\", \"distal\", \"both\"))\n\n    if isinstance(x, core.NeuronList):\n        if len(x) == 1:\n            x = x[0]\n        else:\n            raise Exception(f\"Expected a single TreeNeuron, got {len(x)}\")\n\n    if not isinstance(x, core.TreeNeuron):\n        raise TypeError(f'Expected a single TreeNeuron, got \"{type(x)}\"')\n\n    if x.n_trees != 1:\n        raise ValueError(\n            f\"Unable to cut: neuron {x.id} consists of multiple \"\n            \"disconnected trees. Use navis.heal_skeleton()\"\n            \" to fix.\"\n        )\n\n    # At this point x is TreeNeuron\n    x: core.TreeNeuron\n\n    # Turn cut node into iterable\n    if not utils.is_iterable(where):\n        where = [where]\n\n    # Process cut nodes (i.e. if tag)\n    cn_ids: List[int] = []\n    for cn in where:\n        # If cut_node is a tag (rather than an ID), try finding that node\n        if isinstance(cn, str):\n            if x.tags is None:\n                raise ValueError(f\"Neuron {x.id} has no tags\")\n            if cn not in x.tags:\n                raise ValueError(\n                    f\"#{x.id}: Found no node with tag {cn}\" \" - please double check!\"\n                )\n            cn_ids += x.tags[cn]\n        elif cn not in x.nodes.node_id.values:\n            raise ValueError(f'No node with ID \"{cn}\" found.')\n        elif cn in x.root:\n            raise ValueError(f'Unable to cut at node \"{cn}\" - node is root')\n        else:\n            cn_ids.append(cn)\n\n    # Remove duplicates while retaining order - set() would mess that up\n    seen: Set[int] = set()\n    cn_ids = [cn for cn in cn_ids if not (cn in seen or seen.add(cn))]\n\n    # Warn if not all returned\n    if len(cn_ids) &gt; 1 and ret != \"both\":\n        logger.warning('Multiple cuts should use `ret = \"both\"`.')\n\n    # Go over all cut_nodes -&gt; order matters!\n    res = [x]\n    for cn in cn_ids:\n        # First, find out in which neuron the cut node is\n        to_cut = [n for n in res if cn in n.nodes.node_id.values][0]\n        to_cut_ix = res.index(to_cut)\n\n        # Remove this neuron from results (will be cut into two)\n        res.remove(to_cut)\n\n        # Cut neuron\n        if x.igraph and config.use_igraph:\n            cut = _cut_igraph(to_cut, cn, ret)\n        else:\n            cut = _cut_networkx(to_cut, cn, ret)\n\n        # If ret != 'both', we will get only a single neuron - therefore\n        # make sure cut is iterable\n        cut = utils.make_iterable(cut)\n\n        # Add results back to results at same index, proximal first\n        for c in cut[::-1]:\n            res.insert(to_cut_ix, c)\n\n    return core.NeuronList(res)\n</code></pre>"},{"location":"reference/navis/#navis.despike_skeleton","title":"<code>navis.despike_skeleton</code>","text":"<p>Remove spikes in skeleton (e.g. from jumps in image data).</p> <p>For each node A, the Euclidean distance to its next successor (parent) B and that node's successor C (i.e A-&gt;B-&gt;C) is computed. If \\(\\frac{dist(A,B)}{dist(A,C)}&gt;sigma\\), node B is considered a spike and realigned between A and C.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            Neuron(s) to be processed.\n</code></pre> <p> TYPE: <code>                TreeNeuron | NeuronList</code> </p> <code>sigma</code> <pre><code>            Threshold for spike detection. Smaller sigma = more\n            aggressive spike detection.\n</code></pre> <p> TYPE: <code>            float | int</code> DEFAULT: <code>5</code> </p> <code>max_spike_length</code> <pre><code>            Determines how long (# of nodes) a spike can be.\n</code></pre> <p> TYPE: <code> int</code> DEFAULT: <code>1</code> </p> <code>inplace</code> <pre><code>            If False, a copy of the neuron is returned.\n</code></pre> <p> TYPE: <code>          bool</code> DEFAULT: <code>False</code> </p> <code>reverse</code> <pre><code>            If True, will **also** walk the segments from proximal\n            to distal. Use this to catch spikes on e.g. terminal\n            nodes.\n</code></pre> <p> TYPE: <code>          bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>TreeNeuron / List</code> <p>Despiked neuron(s). Only if <code>inplace=False</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; despiked = navis.despike_skeleton(n)\n</code></pre> Source code in <code>navis/morpho/manipulation.py</code> <pre><code>@utils.map_neuronlist(desc=\"Despiking\", allow_parallel=True)\ndef despike_skeleton(\n    x: NeuronObject,\n    sigma: int = 5,\n    max_spike_length: int = 1,\n    inplace: bool = False,\n    reverse: bool = False,\n) -&gt; Optional[NeuronObject]:\n    r\"\"\"Remove spikes in skeleton (e.g. from jumps in image data).\n\n    For each node A, the Euclidean distance to its next successor (parent)\n    B and that node's successor C (i.e A-&gt;B-&gt;C) is computed. If\n    $\\frac{dist(A,B)}{dist(A,C)}&gt;sigma$, node B is considered a spike\n    and realigned between A and C.\n\n    Parameters\n    ----------\n    x :                 TreeNeuron | NeuronList\n                        Neuron(s) to be processed.\n    sigma :             float | int, optional\n                        Threshold for spike detection. Smaller sigma = more\n                        aggressive spike detection.\n    max_spike_length :  int, optional\n                        Determines how long (# of nodes) a spike can be.\n    inplace :           bool, optional\n                        If False, a copy of the neuron is returned.\n    reverse :           bool, optional\n                        If True, will **also** walk the segments from proximal\n                        to distal. Use this to catch spikes on e.g. terminal\n                        nodes.\n\n    Returns\n    -------\n    TreeNeuron/List\n                Despiked neuron(s). Only if `inplace=False`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; despiked = navis.despike_skeleton(n)\n\n    \"\"\"\n    # TODO:\n    # - flattening all segments first before Spike detection should speed up\n    #   quite a lot\n    # -&gt; as intermediate step: assign all new positions at once\n\n    # The decorator makes sure that we have single neurons at this point\n    if not isinstance(x, core.TreeNeuron):\n        raise TypeError(f\"Can only process TreeNeurons, not {type(x)}\")\n\n    if not inplace:\n        x = x.copy()\n\n    # Index nodes table by node ID\n    this_nodes = x.nodes.set_index(\"node_id\", inplace=False)\n\n    segs_to_walk = x.segments\n\n    if reverse:\n        segs_to_walk += x.segments[::-1]\n\n    # For each spike length do -&gt; do this in reverse to correct the long\n    # spikes first\n    for l in list(range(1, max_spike_length + 1))[::-1]:\n        # Go over all segments\n        for seg in segs_to_walk:\n            # Get nodes A, B and C of this segment\n            this_A = this_nodes.loc[seg[: -l - 1]]\n            this_B = this_nodes.loc[seg[l:-1]]\n            this_C = this_nodes.loc[seg[l + 1 :]]\n\n            # Get coordinates\n            A = this_A[[\"x\", \"y\", \"z\"]].values\n            B = this_B[[\"x\", \"y\", \"z\"]].values\n            C = this_C[[\"x\", \"y\", \"z\"]].values\n\n            # Calculate euclidean distances A-&gt;B and A-&gt;C\n            dist_AB = np.linalg.norm(A - B, axis=1)\n            dist_AC = np.linalg.norm(A - C, axis=1)\n\n            # Get the spikes\n            spikes_ix = np.where(\n                np.divide(dist_AB, dist_AC, where=dist_AC != 0) &gt; sigma\n            )[0]\n            spikes = this_B.iloc[spikes_ix]\n\n            if not spikes.empty:\n                # Interpolate new position(s) between A and C\n                new_positions = A[spikes_ix] + (C[spikes_ix] - A[spikes_ix]) / 2\n\n                this_nodes.loc[spikes.index, [\"x\", \"y\", \"z\"]] = new_positions\n\n    # Reassign node table\n    x.nodes = this_nodes.reset_index(drop=False, inplace=False)\n\n    # The weights in the graph have changed, we need to update that\n    x._clear_temp_attr(exclude=[\"segments\", \"small_segments\", \"classify_nodes\"])\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.dist_between","title":"<code>navis.dist_between</code>","text":"<p>Get the geodesic distance between nodes in nanometers.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        If NeuronList must contain only a single neuron.\n</code></pre> <p> TYPE: <code>            TreeNeuron | MeshNeuron | NeuronList</code> </p> <code>a</code> <pre><code>        Node IDs (for TreeNeurons) or vertex indices (MeshNeurons)\n        to check the distance between.\n</code></pre> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>int</code> <p>distance in nm</p> See Also <p><code>navis.distal_to</code>     Check if a node A is distal to node B. <code>navis.geodesic_matrix</code>     Get all-by-all geodesic distance matrix. <code>navis.segment_length</code>     Much faster if you have a linear segment and know all node IDs.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; d = navis.dist_between(n,\n...                        n.nodes.node_id.values[0],\n...                        n.nodes.node_id.values[1])\n</code></pre> Source code in <code>navis/graph/graph_utils.py</code> <pre><code>@utils.lock_neuron\ndef dist_between(x: \"core.NeuronObject\", a: int, b: int) -&gt; float:\n    \"\"\"Get the geodesic distance between nodes in nanometers.\n\n    Parameters\n    ----------\n    x :             TreeNeuron | MeshNeuron | NeuronList\n                    If NeuronList must contain only a single neuron.\n    a,b :           int\n                    Node IDs (for TreeNeurons) or vertex indices (MeshNeurons)\n                    to check the distance between.\n\n    Returns\n    -------\n    int\n                    distance in nm\n\n    See Also\n    --------\n    [`navis.distal_to`][]\n        Check if a node A is distal to node B.\n    [`navis.geodesic_matrix`][]\n        Get all-by-all geodesic distance matrix.\n    [`navis.segment_length`][]\n        Much faster if you have a linear segment and know all node IDs.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; d = navis.dist_between(n,\n    ...                        n.nodes.node_id.values[0],\n    ...                        n.nodes.node_id.values[1])\n\n    \"\"\"\n    if isinstance(x, core.NeuronList):\n        if len(x) == 1:\n            x = x[0]\n        else:\n            raise ValueError(f\"Need a single TreeNeuron, got {len(x)}\")\n\n    if isinstance(x, (core.TreeNeuron, core.MeshNeuron)):\n        G: Union[\n            \"igraph.Graph\",  # noqa\n            \"nx.DiGraph\",\n        ] = x.igraph if (x.igraph and config.use_igraph) else x.graph\n    elif isinstance(x, nx.DiGraph):\n        G = x\n    elif \"igraph\" in str(type(x.igraph)):\n        # We can't use isinstance here because igraph library might not be installed\n        G = x\n    else:\n        raise ValueError(f\"Unable to process data of type {type(x)}\")\n\n    if (\n        (utils.is_iterable(a) and len(a) &gt; 1)  # type: ignore  # this is just a check\n        or (utils.is_iterable(b) and len(b) &gt; 1)\n    ):  # type: ignore  # this is just a check\n        raise ValueError(\n            \"Can only process single nodes/vertices. Use \"\n            \"navis.geodesic_matrix instead.\"\n        )\n\n    a = utils.make_non_iterable(a)\n    b = utils.make_non_iterable(b)\n\n    try:\n        _ = int(a)\n        _ = int(b)\n    except BaseException:\n        raise ValueError(\"a, b need to be node IDs or vertex indices!\")\n\n    # If we're working with network X DiGraph\n    if isinstance(G, nx.DiGraph):\n        return int(\n            nx.algorithms.shortest_path_length(\n                G.to_undirected(as_view=True), a, b, weight=\"weight\"\n            )\n        )\n    else:\n        if isinstance(x, core.TreeNeuron):\n            a = G.vs.find(node_id=a)\n            b = G.vs.find(node_id=b)\n\n        # If not, we're assuming g is an iGraph object\n        return G.distances(a, b, weights=\"weight\", mode=\"ALL\")[0][0]\n</code></pre>"},{"location":"reference/navis/#navis.dist_to_root","title":"<code>navis.dist_to_root</code>","text":"<p>Calculate distance to root for each node.</p> PARAMETER DESCRIPTION <code>x</code> <p> TYPE: <code>                TreeNeuron</code> </p> <code>weight</code> <pre><code>            Use \"weight\" if you want geodesic distance and `None`\n            if you want node count.\n</code></pre> <p> TYPE: <code>           str</code> DEFAULT: <code>None</code> </p> <code>igraph_indices</code> <pre><code>            Whether to return igraph node indices instead of node\n            IDs. This is mainly used for internal functions.\n</code></pre> <p> TYPE: <code>   bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>dist</code> <p>Dictionary with root distances.</p> <p> TYPE: <code>dict</code> </p> <p>Examples:</p> <p>For doctest only</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; seg = navis.graph.dist_to_root(n)\n</code></pre> See Also <p><code>navis.geodesic_matrix</code>                     For distances between all points.</p> Source code in <code>navis/graph/graph_utils.py</code> <pre><code>@utils.lock_neuron\ndef dist_to_root(\n    x: \"core.TreeNeuron\", weight=None, igraph_indices: bool = False\n) -&gt; dict:\n    \"\"\"Calculate distance to root for each node.\n\n    Parameters\n    ----------\n    x :                 TreeNeuron\n    weight :            str, optional\n                        Use \"weight\" if you want geodesic distance and `None`\n                        if you want node count.\n    igraph_indices :    bool\n                        Whether to return igraph node indices instead of node\n                        IDs. This is mainly used for internal functions.\n\n    Returns\n    -------\n    dist :              dict\n                        Dictionary with root distances.\n\n    Examples\n    --------\n    For doctest only\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; seg = navis.graph.dist_to_root(n)\n\n    See Also\n    --------\n    [`navis.geodesic_matrix`][]\n                        For distances between all points.\n\n    \"\"\"\n    if not isinstance(x, core.TreeNeuron):\n        raise TypeError(f\"Expected TreeNeuron, got {type(x)}\")\n\n    dist = {}\n    for root in x.root:\n        dist.update(nx.shortest_path_length(x.graph, target=root, weight=weight))\n\n    # Map node ID to vertex index for igraph\n    if igraph_indices:\n        if not x.igraph:\n            raise ValueError(\"Neuron does not have an igraph representation.\")\n        id2ix = dict(zip(x.igraph.vs[\"node_id\"], range(len(x.igraph.vs))))\n        dist = {id2ix[k]: v for k, v in dist.items()}\n\n    return dist\n</code></pre>"},{"location":"reference/navis/#navis.distal_to","title":"<code>navis.distal_to</code>","text":"<pre><code>distal_to\n</code></pre><pre><code>distal_to\n</code></pre><pre><code>distal_to\n</code></pre> <p>Check if nodes A are distal to nodes B.</p> Important <p>Please note that if node A is not distal to node B, this does not automatically mean it is proximal instead: if nodes are on different branches, they are neither distal nor proximal to one another! To test for this case run a-&gt;b and b-&gt;a - if both return <code>False</code>, nodes are on different branches.</p> <p>Also: if a and b are the same node, this function will return <code>True</code>!</p> PARAMETER DESCRIPTION <code>x</code> <p> TYPE: <code>    TreeNeuron</code> </p> <code>a</code> <pre><code>If no node IDs are provided, will consider all node. Note that for\nlarge sets of nodes it might be more efficient to use\n[`navis.geodesic_matrix`][] (see examples).\n</code></pre> <p> TYPE: <code>Optional[Union[str, int, List[Union[str, int]]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>If <code>a</code> and <code>b</code> are single node IDs respectively.</p> <code>pd.DataFrame</code> <p>If <code>a</code> and/or <code>b</code> are lists of node IDs. Columns and rows (index) represent node IDs. Neurons <code>a</code> are rows, neurons <code>b</code> are columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; # Get a neuron\n&gt;&gt;&gt; x = navis.example_neurons(1)\n&gt;&gt;&gt; # Get a random node\n&gt;&gt;&gt; n = x.nodes.iloc[100].node_id\n&gt;&gt;&gt; # Check all nodes if they are distal or proximal to that node\n&gt;&gt;&gt; df = navis.distal_to(x, n)\n&gt;&gt;&gt; # Get the IDs of the nodes that are distal\n&gt;&gt;&gt; dist = df.loc[n, df.loc[n]].index.values\n&gt;&gt;&gt; len(dist)\n101\n</code></pre> <p>For large neurons and/or large sets of <code>a</code>/<code>b</code> it can be much faster to use <code>geodesic_matrix</code> instead:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; x = navis.example_neurons(1)\n&gt;&gt;&gt; # Get an all-by-all distal_to\n&gt;&gt;&gt; df = navis.geodesic_matrix(x, weight=None, directed=True) &lt; np.inf\n&gt;&gt;&gt; # Get distal_to for specific nodes\n&gt;&gt;&gt; df = navis.geodesic_matrix(x, weight=None, directed=True) &lt; np.inf\n&gt;&gt;&gt; # Get distal_to for specific nodes\n&gt;&gt;&gt; a, b = x.nodes.node_id.values[:100], x.nodes.node_id.values[-100:]\n&gt;&gt;&gt; dist = navis.geodesic_matrix(x, weight=None, directed=True, from_=a)\n&gt;&gt;&gt; distal_to = dist[b] &lt; np.inf\n</code></pre> See Also <p><code>navis.geodesic_matrix</code>         Depending on your neuron and how many nodes you're asking for,         this function can be considerably faster! See examples.</p> Source code in <code>navis/graph/graph_utils.py</code> <pre><code>@utils.lock_neuron\ndef distal_to(\n    x: \"core.TreeNeuron\",\n    a: Optional[Union[str, int, List[Union[str, int]]]] = None,\n    b: Optional[Union[str, int, List[Union[str, int]]]] = None,\n) -&gt; Union[bool, pd.DataFrame]:\n    \"\"\"Check if nodes A are distal to nodes B.\n\n    Important\n    ---------\n    Please note that if node A is not distal to node B, this does **not**\n    automatically mean it is proximal instead: if nodes are on different\n    branches, they are neither distal nor proximal to one another! To test\n    for this case run a-&gt;b and b-&gt;a - if both return `False`, nodes are on\n    different branches.\n\n    Also: if a and b are the same node, this function will return `True`!\n\n    Parameters\n    ----------\n    x :     TreeNeuron\n    a,b :   single node ID | list of node IDs | None, optional\n            If no node IDs are provided, will consider all node. Note that for\n            large sets of nodes it might be more efficient to use\n            [`navis.geodesic_matrix`][] (see examples).\n\n    Returns\n    -------\n    bool\n            If `a` and `b` are single node IDs respectively.\n    pd.DataFrame\n            If `a` and/or `b` are lists of node IDs. Columns and rows\n            (index) represent node IDs. Neurons `a` are rows, neurons\n            `b` are columns.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; # Get a neuron\n    &gt;&gt;&gt; x = navis.example_neurons(1)\n    &gt;&gt;&gt; # Get a random node\n    &gt;&gt;&gt; n = x.nodes.iloc[100].node_id\n    &gt;&gt;&gt; # Check all nodes if they are distal or proximal to that node\n    &gt;&gt;&gt; df = navis.distal_to(x, n)\n    &gt;&gt;&gt; # Get the IDs of the nodes that are distal\n    &gt;&gt;&gt; dist = df.loc[n, df.loc[n]].index.values\n    &gt;&gt;&gt; len(dist)\n    101\n\n    For large neurons and/or large sets of `a`/`b` it can be much faster to use\n    `geodesic_matrix` instead:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; x = navis.example_neurons(1)\n    &gt;&gt;&gt; # Get an all-by-all distal_to\n    &gt;&gt;&gt; df = navis.geodesic_matrix(x, weight=None, directed=True) &lt; np.inf\n    &gt;&gt;&gt; # Get distal_to for specific nodes\n    &gt;&gt;&gt; df = navis.geodesic_matrix(x, weight=None, directed=True) &lt; np.inf\n    &gt;&gt;&gt; # Get distal_to for specific nodes\n    &gt;&gt;&gt; a, b = x.nodes.node_id.values[:100], x.nodes.node_id.values[-100:]\n    &gt;&gt;&gt; dist = navis.geodesic_matrix(x, weight=None, directed=True, from_=a)\n    &gt;&gt;&gt; distal_to = dist[b] &lt; np.inf\n\n    See Also\n    --------\n    [`navis.geodesic_matrix`][]\n            Depending on your neuron and how many nodes you're asking for,\n            this function can be considerably faster! See examples.\n\n    \"\"\"\n    if isinstance(x, core.NeuronList) and len(x) == 1:\n        x = x[0]\n\n    if not isinstance(x, core.TreeNeuron):\n        raise ValueError(f\"Please pass a single TreeNeuron, got {type(x)}\")\n\n    # At this point x is TreeNeuron\n    x: core.TreeNeuron\n\n    if not isinstance(a, type(None)):\n        tnA = utils.make_iterable(a)\n        # Make sure we're dealing with integers\n        tnA = np.unique(tnA).astype(int)\n    else:\n        tnA = x.nodes.node_id.values\n\n    if not isinstance(b, type(None)):\n        tnB = utils.make_iterable(b)\n        # Make sure we're dealing with integers\n        tnB = np.unique(tnB).astype(int)\n    else:\n        tnB = x.nodes.node_id.values\n\n    if x.igraph and config.use_igraph:\n        # Map node ID to index\n        id2ix = {n: v for v, n in zip(x.igraph.vs.indices, x.igraph.vs[\"node_id\"])}\n\n        # Convert node IDs to indices\n        tnA = [id2ix[n] for n in tnA]  # type: ignore\n        tnB = [id2ix[n] for n in tnB]  # type: ignore\n\n        # Get path lengths\n        le = x.igraph.distances(tnA, tnB, mode=\"OUT\")\n\n        # Converting to numpy array first is ~2X as fast\n        le = np.asarray(le)\n\n        # Convert to True/False\n        le = le != float(\"inf\")\n\n        df = pd.DataFrame(\n            le, index=x.igraph.vs[tnA][\"node_id\"], columns=x.igraph.vs[tnB][\"node_id\"]\n        )\n    else:\n        # Generate empty DataFrame\n        df = pd.DataFrame(\n            np.zeros((len(tnA), len(tnB)), dtype=bool), columns=tnB, index=tnA\n        )\n\n        # Iterate over all targets\n        # Grab graph once to avoid overhead from stale checks\n        g = x.graph\n        for nB in config.tqdm(\n            tnB,\n            desc=\"Querying paths\",\n            disable=(len(tnB) &lt; 1000) | config.pbar_hide,\n            leave=config.pbar_leave,\n        ):\n            # Get all paths TO this target. This function returns a dictionary:\n            # { source1 : path_length, source2 : path_length, ... } containing\n            # all nodes distal to this node.\n            paths = nx.shortest_path_length(g, source=None, target=nB)\n            # Check if sources are among our targets\n            df[nB] = [nA in paths for nA in tnA]\n\n    if df.shape == (1, 1):\n        return df.values[0][0]\n    else:\n        # Return boolean\n        return df\n</code></pre>"},{"location":"reference/navis/#navis.downsample_neuron","title":"<code>navis.downsample_neuron</code>","text":"<p>Downsample neuron(s) by a given factor.</p> <p>For skeletons: preserves root, leafs, branchpoints by default. Preservation of nodes with synapses can be toggled - see <code>preserve_nodes</code> parameter. Use <code>downsampling_factor=float('inf')</code> to get a skeleton consisting only of root, branch and end points.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>                Neuron(s) to downsample. Note that for MeshNeurons\n                we use the first available backend.\n</code></pre> <p> TYPE: <code>                    single neuron | NeuronList</code> </p> <code>downsampling_factor</code> <pre><code>                Factor by which downsample. For TreeNeuron, Dotprops\n                and MeshNeurons this reduces the node, point\n                and face count, respectively. For VoxelNeurons it\n                reduces the dimensions by given factor.\n</code></pre> <p> TYPE: <code>  int | float('inf')</code> </p> <code>preserve_nodes</code> <pre><code>                Can be either list of node IDs to exclude from\n                downsampling or a string to a DataFrame attached\n                to the neuron (e.g. \"connectors\"). DataFrame must\n                have `node_id` column. Only relevant for\n                TreeNeurons.\n</code></pre> <p> TYPE: <code>       str | list</code> DEFAULT: <code>None</code> </p> <code>inplace</code> <pre><code>                If True, will modify original neuron. If False, we\n                will operate and return o a copy.\n</code></pre> <p> TYPE: <code>              bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>TreeNeuron / Dotprops / VoxelNeurons / NeuronList</code> <p>Same datatype as input.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; n_ds = navis.downsample_neuron(n,\n...                                downsampling_factor=5,\n...                                inplace=False)\n&gt;&gt;&gt; n.n_nodes &gt; n_ds.n_nodes\nTrue\n</code></pre> See Also <p><code>navis.resample_skeleton</code>                          This function resamples a neuron to given                          resolution. This will change node IDs! <code>navis.simplify_mesh</code>                          This is the function used for <code>MeshNeurons</code>. Use                          directly for more control of the simplification.</p> Source code in <code>navis/sampling/downsampling.py</code> <pre><code>@utils.map_neuronlist(desc=\"Downsampling\", allow_parallel=True)\ndef downsample_neuron(\n    x: \"core.NeuronObject\",\n    downsampling_factor: Union[int, float],\n    inplace: bool = False,\n    preserve_nodes: Optional[List[int]] = None,\n) -&gt; Optional[\"core.NeuronObject\"]:\n    \"\"\"Downsample neuron(s) by a given factor.\n\n    For skeletons: preserves root, leafs, branchpoints by default. Preservation\n    of nodes with synapses can be toggled - see `preserve_nodes` parameter.\n    Use `downsampling_factor=float('inf')` to get a skeleton consisting only\n    of root, branch and end points.\n\n    Parameters\n    ----------\n    x :                     single neuron | NeuronList\n                            Neuron(s) to downsample. Note that for MeshNeurons\n                            we use the first available backend.\n    downsampling_factor :   int | float('inf')\n                            Factor by which downsample. For TreeNeuron, Dotprops\n                            and MeshNeurons this reduces the node, point\n                            and face count, respectively. For VoxelNeurons it\n                            reduces the dimensions by given factor.\n    preserve_nodes :        str | list, optional\n                            Can be either list of node IDs to exclude from\n                            downsampling or a string to a DataFrame attached\n                            to the neuron (e.g. \"connectors\"). DataFrame must\n                            have `node_id` column. Only relevant for\n                            TreeNeurons.\n    inplace :               bool, optional\n                            If True, will modify original neuron. If False, we\n                            will operate and return o a copy.\n\n    Returns\n    -------\n    TreeNeuron/Dotprops/VoxelNeurons/NeuronList\n                            Same datatype as input.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; n_ds = navis.downsample_neuron(n,\n    ...                                downsampling_factor=5,\n    ...                                inplace=False)\n    &gt;&gt;&gt; n.n_nodes &gt; n_ds.n_nodes\n    True\n\n    See Also\n    --------\n    [`navis.resample_skeleton`][]\n                             This function resamples a neuron to given\n                             resolution. This will change node IDs!\n    [`navis.simplify_mesh`][]\n                             This is the function used for `MeshNeurons`. Use\n                             directly for more control of the simplification.\n\n    \"\"\"\n    if downsampling_factor &lt;= 1:\n        raise ValueError('Downsampling factor must be greater than 1.')\n\n    if not inplace:\n        x = x.copy()\n\n    if isinstance(x, core.TreeNeuron):\n        _ = _downsample_treeneuron(x,\n                                   downsampling_factor=downsampling_factor,\n                                   preserve_nodes=preserve_nodes)\n    elif isinstance(x, core.Dotprops):\n        _ = _downsample_dotprops(x,\n                                 downsampling_factor=downsampling_factor)\n    elif isinstance(x, core.VoxelNeuron):\n        _ = _downsample_voxels(x,\n                               downsampling_factor=downsampling_factor)\n    elif isinstance(x, core.MeshNeuron):\n        _ = meshes.simplify_mesh(x,\n                                 F=1/downsampling_factor,\n                                 inplace=True)\n    else:\n        raise TypeError(f'Unable to downsample data of type \"{type(x)}\"')\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.drop_fluff","title":"<code>navis.drop_fluff</code>","text":"<p>Remove small disconnected pieces of \"fluff\".</p> <p>By default, this function will remove all but the largest connected component from the neuron. You can change that behavior using the <code>keep_size</code> and <code>n_largest</code> parameters. Connectors (if present) will be remapped to the closest surviving vertex/node.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    The neuron(s) to remove fluff from.\n</code></pre> <p> TYPE: <code>        TreeNeuron | MeshNeuron | Dotprops | NeuronList</code> </p> <code>keep_size</code> <pre><code>    Use this to set a size (in number of nodes/vertices) for small\n    bits to keep. If `keep_size` &lt; 1 it will be intepreted as\n    fraction of total nodes/vertices/points.\n</code></pre> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>n_largest</code> <pre><code>    If set, will keep the `n_largest` connected components. Note:\n    if provided, `keep_size` will be applied first!\n</code></pre> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>epsilon</code> <pre><code>    For Dotprops: distance at which to consider two points to be\n    connected. If `None`, will use the default value of 5 times\n    the average node distance (`x.sampling_resolution`).\n</code></pre> <p> TYPE: <code>  float</code> DEFAULT: <code>None</code> </p> <code>inplace</code> <pre><code>    If False, pruning is performed on copy of original neuron\n    which is then returned.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Neuron / List</code> <p>Neuron(s) without fluff.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; m = navis.example_neurons(1, kind='mesh')\n&gt;&gt;&gt; m.n_vertices\n6309\n&gt;&gt;&gt; # Remove all but the largest connected component\n&gt;&gt;&gt; top = navis.drop_fluff(m)\n&gt;&gt;&gt; top.n_vertices\n5951\n&gt;&gt;&gt; # Keep the ten largest connected components\n&gt;&gt;&gt; two = navis.drop_fluff(m, n_largest=10)\n&gt;&gt;&gt; two.n_vertices\n6069\n&gt;&gt;&gt; # Keep all fragments with at least 100 vertices\n&gt;&gt;&gt; clean = navis.drop_fluff(m, keep_size=100)\n&gt;&gt;&gt; clean.n_vertices\n5951\n&gt;&gt;&gt; # Keep the two largest fragments with at least 50 vertices each\n&gt;&gt;&gt; # (for this neuron the result is just the largest fragment)\n&gt;&gt;&gt; clean2 = navis.drop_fluff(m, keep_size=50, n_largest=2)\n&gt;&gt;&gt; clean2.n_vertices\n6037\n</code></pre> Source code in <code>navis/morpho/manipulation.py</code> <pre><code>@utils.map_neuronlist(desc=\"Removing fluff\", allow_parallel=True)\ndef drop_fluff(\n    x: Union[\"core.TreeNeuron\", \"core.MeshNeuron\", \"core.NeuronList\"],\n    keep_size: Optional[float] = None,\n    n_largest: Optional[int] = None,\n    epsilon: Optional[float] = None,\n    inplace: bool = False,\n):\n    \"\"\"Remove small disconnected pieces of \"fluff\".\n\n    By default, this function will remove all but the largest connected\n    component from the neuron. You can change that behavior using the\n    `keep_size` and `n_largest` parameters. Connectors (if present) will\n    be remapped to the closest surviving vertex/node.\n\n    Parameters\n    ----------\n    x :         TreeNeuron | MeshNeuron | Dotprops | NeuronList\n                The neuron(s) to remove fluff from.\n    keep_size : float, optional\n                Use this to set a size (in number of nodes/vertices) for small\n                bits to keep. If `keep_size` &lt; 1 it will be intepreted as\n                fraction of total nodes/vertices/points.\n    n_largest : int, optional\n                If set, will keep the `n_largest` connected components. Note:\n                if provided, `keep_size` will be applied first!\n    epsilon :   float, optional\n                For Dotprops: distance at which to consider two points to be\n                connected. If `None`, will use the default value of 5 times\n                the average node distance (`x.sampling_resolution`).\n    inplace :   bool, optional\n                If False, pruning is performed on copy of original neuron\n                which is then returned.\n\n    Returns\n    -------\n    Neuron/List\n                Neuron(s) without fluff.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; m = navis.example_neurons(1, kind='mesh')\n    &gt;&gt;&gt; m.n_vertices\n    6309\n    &gt;&gt;&gt; # Remove all but the largest connected component\n    &gt;&gt;&gt; top = navis.drop_fluff(m)\n    &gt;&gt;&gt; top.n_vertices\n    5951\n    &gt;&gt;&gt; # Keep the ten largest connected components\n    &gt;&gt;&gt; two = navis.drop_fluff(m, n_largest=10)\n    &gt;&gt;&gt; two.n_vertices\n    6069\n    &gt;&gt;&gt; # Keep all fragments with at least 100 vertices\n    &gt;&gt;&gt; clean = navis.drop_fluff(m, keep_size=100)\n    &gt;&gt;&gt; clean.n_vertices\n    5951\n    &gt;&gt;&gt; # Keep the two largest fragments with at least 50 vertices each\n    &gt;&gt;&gt; # (for this neuron the result is just the largest fragment)\n    &gt;&gt;&gt; clean2 = navis.drop_fluff(m, keep_size=50, n_largest=2)\n    &gt;&gt;&gt; clean2.n_vertices\n    6037\n\n    \"\"\"\n    utils.eval_param(x, name=\"x\", allowed_types=(core.TreeNeuron, core.MeshNeuron, core.Dotprops))\n\n    if isinstance(x, (core.MeshNeuron, core.TreeNeuron)):\n        G = x.graph\n        # Skeleton graphs are directed\n        if G.is_directed():\n            G = G.to_undirected()\n    elif isinstance(x, core.Dotprops):\n        G = graph.neuron2nx(x, epsilon=epsilon)\n\n    cc = sorted(nx.connected_components(G), key=lambda x: len(x), reverse=True)\n\n    # Translate keep_size to number of nodes\n    if keep_size and keep_size &lt; 1:\n        keep_size = len(G.nodes) * keep_size\n\n    if keep_size:\n        cc = [c for c in cc if len(c) &gt;= keep_size]\n        if not n_largest:\n            keep = [i for c in cc for i in c]\n        else:\n            keep = [i for c in cc[:n_largest] for i in c]\n    elif n_largest:\n        keep = [i for c in cc[:n_largest] for i in c]\n    else:\n        keep = cc[0]\n\n    # Subset neuron\n    x = subset.subset_neuron(x, subset=keep, inplace=inplace, keep_disc_cn=True)\n\n    # See if we need to/can re-attach any connectors\n    if x.has_connectors:\n        id_col = [c for c in ('node_id', 'vertex_id', 'point_id') if c in x.connectors.columns]\n        if id_col:\n            id_col = id_col[0]\n            disc = ~x.connectors[id_col].isin(x.graph.nodes).values\n            if any(disc):\n                xyz = x.connectors.loc[disc, [\"x\", \"y\", \"z\"]].values\n                x.connectors.loc[disc, id_col] = x.snap(xyz)[0]\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.edges2neuron","title":"<code>navis.edges2neuron</code>","text":"<p>Create TreeNeuron from edges and (optional) vertex coordinates.</p> PARAMETER DESCRIPTION <code>edges</code> <pre><code>        Edges between vertices.\n</code></pre> <p> TYPE: <code>        (N, 2) array</code> </p> <code>vertices</code> <pre><code>        Vertex positions. If not provided, will position\n        all vertices at (0, 0, 0).\n</code></pre> <p> TYPE: <code>     (N, 3) array</code> DEFAULT: <code>None</code> </p> <code>validate</code> <pre><code>        If True (default) will fix issues with cycles\n        and edges orientation. Only skip this if\n        you are absolutely sure your data are good.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> <code>**kwargs</code> <pre><code>        Additional keyword arguments are passed to\n        initialization of the TreeNeuron.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>TreeNeuron</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; verts = np.random.rand(5, 3)\n&gt;&gt;&gt; edges = np.array([(0, 1), (1, 2), (2, 3), (2, 4)])\n&gt;&gt;&gt; sk = navis.edges2neuron(edges, vertices=verts)\n</code></pre> Source code in <code>navis/graph/converters.py</code> <pre><code>def edges2neuron(edges, vertices=None, validate=True, **kwargs):\n    \"\"\"Create TreeNeuron from edges and (optional) vertex coordinates.\n\n    Parameters\n    ----------\n    edges :         (N, 2) array\n                    Edges between vertices.\n    vertices :      (N, 3) array, optional\n                    Vertex positions. If not provided, will position\n                    all vertices at (0, 0, 0).\n    validate :      bool\n                    If True (default) will fix issues with cycles\n                    and edges orientation. Only skip this if\n                    you are absolutely sure your data are good.\n    **kwargs\n                    Additional keyword arguments are passed to\n                    initialization of the TreeNeuron.\n\n    Returns\n    -------\n    TreeNeuron\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; verts = np.random.rand(5, 3)\n    &gt;&gt;&gt; edges = np.array([(0, 1), (1, 2), (2, 3), (2, 4)])\n    &gt;&gt;&gt; sk = navis.edges2neuron(edges, vertices=verts)\n\n    \"\"\"\n    # Make sure we're dealing with arrays\n    edges = np.asarray(edges)\n\n    if vertices is not None:\n        vertices = np.asarray(vertices)\n    else:\n        vertices = np.zeros((edges.max() + 1, 3))\n\n    if vertices.ndim != 2 or vertices.shape[1] != 3:\n        raise ValueError(\n            f\"Expected `vertices` to be of shape (N, 3), got {vertices.shape}\"\n        )\n    if edges.ndim != 2 or edges.shape[1] != 2:\n        raise ValueError(\n            f\"Expected `edges` to be of shape (N, 2), got {edges.shape}\"\n        )\n\n    if edges.max() &gt; (len(vertices)-1):\n        raise IndexError(\"vertex index out of range\")\n\n    G = nx.Graph()\n    G.add_nodes_from(np.arange(len(vertices)))\n    G.add_edges_from(edges)\n\n    # Note: at this point we could just pass the graph to nx2neuron\n    # But because we know it came from from vertices and edges, we\n    # can skip certain checks and make the process a bit faster\n\n    if validate:\n        if not nx.is_forest(G):\n            while True:\n                try:\n                    # Find cycle\n                    cycle = nx.find_cycle(G)\n                except nx.exception.NetworkXNoCycle:\n                    break\n                except BaseException:\n                    raise\n\n                # Sort by degree\n                cycle = sorted(cycle, key=lambda x: G.degree[x[0]])\n\n                # Remove the edge with the lowest degree\n                G.remove_edge(cycle[0][0], cycle[0][1])\n\n        parents = {}\n        for cc in nx.connected_components(G):\n            # If this is a disconnected node\n            if len(cc) == 1:\n                parents[cc.pop()] = -1\n                continue\n\n            sg = nx.subgraph(G, cc)\n            # Pick a random root\n            r = cc.pop()\n            # Generate parent-&gt;child dictionary\n            this = nx.predecessor(sg, r)\n\n            # Update overall parent dictionary\n            # (note that we assign -1 as root's parent)\n            parents.update({k: v[0] if v else -1 for k, v in this.items()})\n\n    nodes = pd.DataFrame(vertices, columns=['x', 'y', 'z'])\n    nodes.insert(0, 'node_id', nodes.index)\n    nodes.insert(1, 'parent_id', nodes.index.map(parents))\n\n    return core.TreeNeuron(nodes, **kwargs)\n</code></pre>"},{"location":"reference/navis/#navis.example_neurons","title":"<code>navis.example_neurons</code>","text":"<p>Load example neuron(s).</p> <p>These example neurons are skeletons and meshes of the same olfactory projection neurons from the DA1 glomerulus which have been automatically segmented in the Janelia hemibrain data set [1]. See also <code>https://neuprint.janelia.org</code>.</p> <p>Coordinates are in voxels which equal 8 x 8 x 8 nanometers.</p> PARAMETER DESCRIPTION <code>n</code> <pre><code>    Number of neurons to return. If None, will return all available\n    example neurons. Can never return more than the maximum number\n    of available example neurons.\n</code></pre> <p> TYPE: <code>        int | None</code> DEFAULT: <code>None</code> </p> <code>kind</code> <pre><code>    What kind of neurons to return.\n</code></pre> <p> TYPE: <code>     \"skeleton\" | \"mesh\" | \"mix\"</code> DEFAULT: <code>'skeleton'</code> </p> <code>synapses</code> <pre><code>    If True, will also load synapses.\n</code></pre> <p> TYPE: <code> bool,</code> DEFAULT: <code>True</code> </p> <code>source</code> <pre><code>    Only relevant for skeletons. Skeletons can be generated from SWC\n    files or GML graphs (this is really only used for testing).\n</code></pre> <p> TYPE: <code>   'swc' | 'gml'</code> DEFAULT: <code>'swc'</code> </p> RETURNS DESCRIPTION <code>TreeNeuron</code> <p>If <code>n=1</code> and <code>kind='skeleton'</code>.</p> <code>MeshNeuron</code> <p>If <code>n=1</code> and <code>kind='mesh'</code>.</p> <code>NeuronList</code> <p>List of the above neuron types if <code>n&gt;1</code>.</p> References <p>[1] Louis K. Scheffer et al., bioRxiv. 2020. doi: https://doi.org/10.1101/2020.04.07.030213 A Connectome and Analysis of the Adult Drosophila Central Brain.</p> <p>Examples:</p> <p>Load a single neuron</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(n=1)\n</code></pre> <p>Load all example neurons</p> <pre><code>&gt;&gt;&gt; nl = navis.example_neurons()\n</code></pre> Source code in <code>navis/data/load_data.py</code> <pre><code>def example_neurons(n: Optional[int] = None,\n                    kind:  Union[Literal['mesh'],\n                                 Literal['skeleton'],\n                                 Literal['mix']] = 'skeleton',\n                    synapses: bool = True,\n                    source: Union[Literal['swc'],\n                                  Literal['gml']] = 'swc',\n                    ) -&gt; NeuronObject:\n    \"\"\"Load example neuron(s).\n\n    These example neurons are skeletons and meshes of the same olfactory\n    projection neurons from the DA1 glomerulus which have been automatically\n    segmented in the Janelia hemibrain data set [1]. See also\n    `https://neuprint.janelia.org`.\n\n    Coordinates are in voxels which equal 8 x 8 x 8 nanometers.\n\n    Parameters\n    ----------\n    n :         int | None, optional\n                Number of neurons to return. If None, will return all available\n                example neurons. Can never return more than the maximum number\n                of available example neurons.\n    kind :      \"skeleton\" | \"mesh\" | \"mix\"\n                What kind of neurons to return.\n    synapses :  bool,\n                If True, will also load synapses.\n    source :    'swc' | 'gml', optional\n                Only relevant for skeletons. Skeletons can be generated from SWC\n                files or GML graphs (this is really only used for testing).\n\n    Returns\n    -------\n    TreeNeuron\n                If `n=1` and `kind='skeleton'`.\n    MeshNeuron\n                If `n=1` and `kind='mesh'`.\n    NeuronList\n                List of the above neuron types if `n&gt;1`.\n\n    References\n    ----------\n    [1] Louis K. Scheffer et al., bioRxiv. 2020. doi: https://doi.org/10.1101/2020.04.07.030213\n    A Connectome and Analysis of the Adult Drosophila Central Brain.\n\n    Examples\n    --------\n    Load a single neuron\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(n=1)\n\n    Load all example neurons\n\n    &gt;&gt;&gt; nl = navis.example_neurons()\n\n    \"\"\"\n    if kind not in ['skeleton', 'mesh', 'mix']:\n        raise ValueError(f'Unknown value for `kind`: \"{kind}\"')\n\n    if isinstance(n, type(None)):\n        if kind == 'mix':\n            n = len(swc) + len(obj)\n        else:\n            n = len(swc)\n    elif not isinstance(n, int):\n        raise TypeError(f'Expected int or None, got \"{type(n)}\"')\n\n    if isinstance(n, int) and n &lt; 1:\n        raise ValueError(\"Unable to return less than 1 neuron.\")\n\n    if kind == 'mix':\n        n_mesh = round(n/2)\n        n_skel = n - n_mesh\n    else:\n        n_mesh = n_skel = n\n\n    nl = []\n    if kind in ['skeleton', 'mix']:\n        if source == 'gml':\n            graphs = [nx.read_gml(os.path.join(gml_path, g)) for g in gml[:n_skel]]\n            nl += [nx2neuron(g,\n                             units='8 nm',\n                             id=int(f.split('.')[0])) for f, g in zip(gml, graphs)]\n        elif source == 'swc':\n            nl += [read_swc(os.path.join(swc_path, f),\n                            units='8 nm',\n                            id=int(f.split('.')[0])) for f in swc[:n_skel]]\n        else:\n            raise ValueError(f'Source must be \"swc\" or \"gml\", not \"{source}\"')\n\n    if kind in ['mesh', 'mix']:\n        files = [os.path.join(obj_path, f) for f in obj[:n_mesh]]\n        nl += [MeshNeuron(fp,\n                          units='8 nm',\n                          name=f.split('.')[0],\n                          id=int(f.split('.')[0])) for f, fp in zip(obj, files)]\n        for n in nl:\n            n.soma_pos = SOMA_POS[n.id]\n\n    if synapses:\n        for n in nl:\n            n.connectors = pd.read_csv(os.path.join(syn_path, f'{n.id}.csv'))\n\n            if isinstance(n, MeshNeuron):\n                n._connectors.drop('node_id', axis=1, inplace=True)\n\n    with open(os.path.join(fp, 'meta.json'), 'r') as f:\n        meta = json.load(f)\n\n    for n in nl:\n        n.name = meta[str(n.id)]['instance']\n\n    if len(nl) == 1:\n        return nl[0]\n    return NeuronList(nl)\n</code></pre>"},{"location":"reference/navis/#navis.example_volume","title":"<code>navis.example_volume</code>","text":"<p>Load an example volume.</p> <p>Volumes are in hemibrain space which means coordinates are in voxels at 8 x 8 x 8 nanometers/voxel.</p> PARAMETER DESCRIPTION <code>name</code> <pre><code>    Name of available volume. Currently available::\n\n      \"LH\" = lateral horn in hemibrain space\n      \"neuropil\" = neuropil in hemibrain space\n</code></pre> <p> TYPE: <code>     str</code> </p> RETURNS DESCRIPTION <code>navis.Volume</code> <p>Examples:</p> <p>Load LH volume</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; lh = navis.example_volume('LH')\n</code></pre> Source code in <code>navis/data/load_data.py</code> <pre><code>def example_volume(name: str) -&gt; Volume:\n    \"\"\"Load an example volume.\n\n    Volumes are in hemibrain space which means coordinates are in voxels\n    at 8 x 8 x 8 nanometers/voxel.\n\n    Parameters\n    ----------\n    name :      str\n                Name of available volume. Currently available::\n\n                  \"LH\" = lateral horn in hemibrain space\n                  \"neuropil\" = neuropil in hemibrain space\n\n    Returns\n    -------\n    navis.Volume\n\n    Examples\n    --------\n    Load LH volume\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; lh = navis.example_volume('LH')\n\n    \"\"\"\n    if not isinstance(name, str):\n        raise TypeError(f'Expected string, got \"{type(name)}\"')\n\n    # Force lower case\n    name = name.lower()\n\n    # Make sure extension is correct\n    if not name.endswith(\".obj\"):\n        name += \".obj\"\n\n    if name not in vols:\n        raise ValueError(\n            f'No volume named \"{name}\". Available volumes: {\",\".join(vols)}'\n        )\n\n    vol = Volume.from_file(\n        os.path.join(vols_path, name), name=name.split(\".\")[0], units=\"nm\"\n    )\n\n    return vol\n</code></pre>"},{"location":"reference/navis/#navis.find_main_branchpoint","title":"<code>navis.find_main_branchpoint</code>","text":"<p>Find main branch point of unipolar (e.g. insect) neurons.</p> <p>Note that this might produce garbage if the neuron is fragmented.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        May contain multiple neurons.\n</code></pre> <p> TYPE: <code>            TreeNeuron | NeuronList</code> </p> <code>method</code> <pre><code>        The method to use:\n          - \"longest_neurite\" assumes that the main branch point\n            is where the two largest branches converge\n          - \"betweenness\" uses centrality to determine the point\n            which most shortest paths traverse\n</code></pre> <p> TYPE: <code>       \"longest_neurite\" | \"centrality\"</code> DEFAULT: <code>'betweenness'</code> </p> <code>threshold</code> <pre><code>        Sets the cutoff for method \"betweenness\". Decrease threshold\n        to be more inclusive (useful if the cell body fiber has\n        little bristles), increase to be more stringent (i.e. when\n        the skeleton is very clean).\n</code></pre> <p> TYPE: <code>    float [0-1]</code> DEFAULT: <code>0.95</code> </p> <code>reroot_soma</code> <pre><code>        If True, neuron will be rerooted to soma.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>branch_point</code> <p>Node ID or list of node IDs of the main branch point(s).</p> <p> TYPE: <code>int | list of int</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; navis.find_main_branchpoint(n, reroot_soma=True)\n110\n&gt;&gt;&gt; # Cut neuron into axon, dendrites and primary neurite tract:\n&gt;&gt;&gt; # for this we need to cut twice - once at the main branch point\n&gt;&gt;&gt; # and once at one of its childs\n&gt;&gt;&gt; child = n.nodes[n.nodes.parent_id == 2066].node_id.values[0]\n&gt;&gt;&gt; split = navis.cut_skeleton(n, [2066, child])\n&gt;&gt;&gt; split\n&lt;class 'navis.core.neuronlist.NeuronList'&gt; of 3 neurons\n          type  n_nodes  n_connectors  n_branches  n_leafs   cable_length    soma\n0  TreeNeuron     2572             0         170      176  475078.177926    None\n1  TreeNeuron      139             0           1        3   89983.511392  [3490]\n2  TreeNeuron     3656             0          63       66  648285.745750    None\n</code></pre> Source code in <code>navis/graph/graph_utils.py</code> <pre><code>@utils.map_neuronlist(desc=\"Searching\", allow_parallel=True)\n@utils.meshneuron_skeleton(method=\"node_to_vertex\")\ndef find_main_branchpoint(\n    x: \"core.NeuronObject\",\n    method: Union[Literal[\"longest_neurite\"], Literal[\"betweenness\"]] = \"betweenness\",\n    threshold: float = 0.95,\n    reroot_soma: bool = False,\n) -&gt; Union[int, List[int]]:\n    \"\"\"Find main branch point of unipolar (e.g. insect) neurons.\n\n    Note that this might produce garbage if the neuron is fragmented.\n\n    Parameters\n    ----------\n    x :             TreeNeuron | NeuronList\n                    May contain multiple neurons.\n    method :        \"longest_neurite\" | \"centrality\"\n                    The method to use:\n                      - \"longest_neurite\" assumes that the main branch point\n                        is where the two largest branches converge\n                      - \"betweenness\" uses centrality to determine the point\n                        which most shortest paths traverse\n    threshold :     float [0-1]\n                    Sets the cutoff for method \"betweenness\". Decrease threshold\n                    to be more inclusive (useful if the cell body fiber has\n                    little bristles), increase to be more stringent (i.e. when\n                    the skeleton is very clean).\n    reroot_soma :   bool, optional\n                    If True, neuron will be rerooted to soma.\n\n    Returns\n    -------\n    branch_point :  int | list of int\n                    Node ID or list of node IDs of the main branch point(s).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; navis.find_main_branchpoint(n, reroot_soma=True)\n    110\n    &gt;&gt;&gt; # Cut neuron into axon, dendrites and primary neurite tract:\n    &gt;&gt;&gt; # for this we need to cut twice - once at the main branch point\n    &gt;&gt;&gt; # and once at one of its childs\n    &gt;&gt;&gt; child = n.nodes[n.nodes.parent_id == 2066].node_id.values[0]\n    &gt;&gt;&gt; split = navis.cut_skeleton(n, [2066, child])\n    &gt;&gt;&gt; split                                                   # doctest: +SKIP\n    &lt;class 'navis.core.neuronlist.NeuronList'&gt; of 3 neurons\n              type  n_nodes  n_connectors  n_branches  n_leafs   cable_length    soma\n    0  TreeNeuron     2572             0         170      176  475078.177926    None\n    1  TreeNeuron      139             0           1        3   89983.511392  [3490]\n    2  TreeNeuron     3656             0          63       66  648285.745750    None\n\n    \"\"\"\n    utils.eval_param(\n        method, name=\"method\", allowed_values=(\"longest_neurite\", \"betweenness\")\n    )\n\n    if not isinstance(x, core.TreeNeuron):\n        raise TypeError(f'Expected TreeNeuron(s), got \"{type(x)}\"')\n\n    # At this point x is TreeNeuron\n    x: core.TreeNeuron\n\n    # If no branches\n    if x.nodes[x.nodes.type == \"branch\"].empty:\n        raise ValueError(\"Neuron has no branch points.\")\n\n    if reroot_soma and not isinstance(x.soma, type(None)):\n        x = x.reroot(x.soma, inplace=False)\n\n    if method == \"longest_neurite\":\n        G = x.graph\n\n        # First, find longest path\n        longest = nx.dag_longest_path(G, weight=\"weight\")\n\n        # Remove longest path\n        # (use subgraph to avoid editing original or copying raph)\n        keep = ~np.isin(G.nodes, longest)\n        G = G.subgraph(np.array(G.nodes)[keep])\n\n        # Find second longest path\n        sc_longest = nx.dag_longest_path(G, weight=\"weight\")\n\n        # Parent of the last node in sc_longest is the common branch point\n        bp = list(x.graph.successors(sc_longest[-1]))[0]\n    else:\n        # Get betweenness for each node\n        x = morpho.betweeness_centrality(x, directed=True, from_=\"branch_points\")\n        # Get branch points with highest centrality\n        high_between = (\n            x.branch_points.betweenness &gt;= x.branch_points.betweenness.max() * threshold\n        )\n        candidates = x.branch_points[high_between]\n\n        # If only one nodes just go with it\n        if candidates.shape[0] == 1:\n            bp = candidates.node_id.values[0]\n        else:\n            # If multiple points get the farthest one from the root\n            root_dists = dist_to_root(x)\n            bp = sorted(candidates.node_id.values, key=lambda x: root_dists[x])[-1]\n\n    # This makes sure we get the same data type as in the node table\n    # -&gt; Network X seems to sometimes convert integers to floats\n    return x.nodes.node_id.dtype.type(bp)\n</code></pre>"},{"location":"reference/navis/#navis.find_soma","title":"<code>navis.find_soma</code>","text":"<p>Try finding a neuron's soma.</p> <p>Will use the <code>.soma_detection_radius</code> and <code>.soma_detection_label</code> attribute of a neuron to search for the soma in the node table.</p> <p>If attributes don't exists, will fallback to defaults: <code>None</code> and <code>1</code>, respectively.</p> PARAMETER DESCRIPTION <code>x</code> <p> TYPE: <code>        Neuron</code> </p> RETURNS DESCRIPTION <code>Node ID(s) of potential somata.</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; navis.find_soma(n)\narray([4177], dtype=int32)\n</code></pre> Source code in <code>navis/morpho/analyze.py</code> <pre><code>def find_soma(x: 'core.TreeNeuron') -&gt; Sequence[int]:\n    \"\"\"Try finding a neuron's soma.\n\n    Will use the `.soma_detection_radius` and `.soma_detection_label`\n    attribute of a neuron to search for the soma in the node table.\n\n    If attributes don't exists, will fallback to defaults: `None` and\n    `1`, respectively.\n\n    Parameters\n    ----------\n    x :         Neuron\n\n    Returns\n    -------\n    Node ID(s) of potential somata.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; navis.find_soma(n)\n    array([4177], dtype=int32)\n\n    \"\"\"\n    if not isinstance(x, core.TreeNeuron):\n        raise TypeError(f'Input must be TreeNeuron, not \"{type(x)}\"')\n\n    soma_radius = getattr(x, 'soma_detection_radius', None)\n    soma_label = getattr(x, 'soma_detection_label', None)\n\n    check_labels = not isinstance(soma_label, type(None)) and 'label' in x.nodes.columns\n    check_radius = not isinstance(soma_radius, type(None))\n\n    # If no label or radius is given, return empty array\n    if not check_labels and not check_radius:\n        return np.array([], dtype=x.nodes.node_id.values.dtype)\n\n    # Note to self: I've optimised the s**t out of this function\n    # The reason reason why we're using a mask and this somewhat\n    # convoluted logic is to avoid having to subset the node table\n    # because that's really slow.\n\n    # Start with a mask that includes all nodes\n    mask = np.ones(len(x.nodes), dtype=bool)\n\n    if check_radius:\n        # When checking for radii, we use an empty mask and fill it\n        # with nodes that have a large enough radius\n        mask[:] = False\n\n        # Drop nodes that don't have a radius\n        radii = x.nodes.radius.values\n        has_radius = ~np.isnan(radii)\n\n        # Filter further to nodes that have a large enough radius\n        if has_radius.any():\n            if isinstance(soma_radius, pint.Quantity):\n                if isinstance(x.units, (pint.Quantity, pint.Unit)) and \\\n                   not x.units.dimensionless and \\\n                   not isinstance(x.units._magnitude, np.ndarray) \\\n                   and x.units != soma_radius:  # only convert if units are different\n                    # Do NOT remove the .values here -&gt; otherwise conversion to units won't work\n                    is_large = radii * x.units &gt;= soma_radius\n                else:\n                    # If neurons has no units or if units are non-isotropic,\n                    # assume they are the same as the soma radius\n                    is_large = radii &gt;= soma_radius._magnitude\n            else:\n                is_large = radii &gt;= soma_radius\n\n            # Mark nodes that have a large enough radius\n            mask[is_large] = True\n\n    # See if we (also) need to check for a specific label\n    if check_labels:\n        # Important: we need to use np.asarray here because the `label` column\n        # can be categorical in which case a `soma_nodes.label.astype(str)` might\n        # throw annoying runtime warnings\n        soma_node_ids = x.nodes.node_id.values[mask]\n        soma_node_labels = np.asarray(x.nodes.label.values[mask]).astype(str)\n\n        return soma_node_ids[soma_node_labels == str(soma_label)]\n    # If no labels to check we can return the mask directly\n    else:\n        return x.nodes.node_id.values[mask]\n</code></pre>"},{"location":"reference/navis/#navis.fix_mesh","title":"<code>navis.fix_mesh</code>","text":"<p>Try to fix some common problems with mesh.</p> <ol> <li>Remove infinite values</li> <li>Merge duplicate vertices</li> <li>Remove duplicate and degenerate faces</li> <li>Fix normals</li> <li>Remove unreference vertices</li> <li>Remove disconnected fragments (Optional)</li> <li>Fill holes (Optional)</li> </ol> PARAMETER DESCRIPTION <code>mesh</code> <p> TYPE: <code>             trimesh.Trimesh | navis.MeshNeuron</code> </p> <code>fill_holes</code> <pre><code>            If True will try to fix holes in the mesh.\n</code></pre> <p> TYPE: <code>       bool</code> DEFAULT: <code>False</code> </p> <code>remove_fragments</code> <pre><code>            If a number is given, will iterate over the mesh's\n            connected components and remove those consisting of less\n            than the given number of vertices. For example,\n            `remove_fragments=5` will drop parts of the mesh\n            that consist of five or less connected vertices.\n</code></pre> <p> TYPE: <code> False | int</code> DEFAULT: <code>False</code> </p> <code>inplace</code> <pre><code>            If True, will perform fixes on the input mesh. If False,\n            will make a copy and leave the original untouched.\n</code></pre> <p> TYPE: <code>          bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>fixed object :      trimesh.Trimesh or navis.MeshNeuron</code> Source code in <code>navis/meshes/mesh_utils.py</code> <pre><code>def fix_mesh(mesh: Union[tm.Trimesh, 'core.MeshNeuron'],\n             fill_holes: bool = False,\n             remove_fragments: bool = False,\n             inplace: bool = False):\n    \"\"\"Try to fix some common problems with mesh.\n\n     1. Remove infinite values\n     2. Merge duplicate vertices\n     3. Remove duplicate and degenerate faces\n     4. Fix normals\n     5. Remove unreference vertices\n     6. Remove disconnected fragments (Optional)\n     7. Fill holes (Optional)\n\n    Parameters\n    ----------\n    mesh :              trimesh.Trimesh | navis.MeshNeuron\n    fill_holes :        bool\n                        If True will try to fix holes in the mesh.\n    remove_fragments :  False | int\n                        If a number is given, will iterate over the mesh's\n                        connected components and remove those consisting of less\n                        than the given number of vertices. For example,\n                        `remove_fragments=5` will drop parts of the mesh\n                        that consist of five or less connected vertices.\n    inplace :           bool\n                        If True, will perform fixes on the input mesh. If False,\n                        will make a copy and leave the original untouched.\n\n    Returns\n    -------\n    fixed object :      trimesh.Trimesh or navis.MeshNeuron\n\n    \"\"\"\n    if not inplace:\n        mesh = mesh.copy()\n\n    if isinstance(mesh, core.MeshNeuron):\n        m = mesh.trimesh\n    else:\n        m = mesh\n\n    assert isinstance(m, tm.Trimesh)\n\n    if remove_fragments:\n        to_drop = []\n        for c in nx.connected_components(m.vertex_adjacency_graph):\n            if len(c) &lt;= remove_fragments:\n                to_drop += list(c)\n\n        # Remove dropped vertices\n        remove = np.isin(np.arange(m.vertices.shape[0]), to_drop)\n        m.update_vertices(~remove)\n\n    if fill_holes:\n        m.fill_holes()\n\n    m.remove_infinite_values()\n    m.merge_vertices()\n    m.remove_duplicate_faces()\n    m.remove_degenerate_faces()\n    m.fix_normals()\n    m.remove_unreferenced_vertices()\n\n    # If we started with a MeshNeuron, map back the verts/faces\n    if isinstance(mesh, core.MeshNeuron):\n        mesh.vertices, mesh.faces = m.vertices, m.faces\n        mesh._clear_temp_attr()\n\n    return mesh\n</code></pre>"},{"location":"reference/navis/#navis.flow_centrality","title":"<code>navis.flow_centrality</code>","text":"<p>Calculate flow between leaf nodes.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Neuron(s) to calculate flow centrality for.\n</code></pre> <p> TYPE: <code>        TreeNeuron | MeshNeuron | NeuronList</code> </p> RETURNS DESCRIPTION <code>neuron</code> <p>Adds \"flow_centrality\" as column in the node table (for TreeNeurons) or as <code>.flow_centrality</code> property (for MeshNeurons).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(2)\n&gt;&gt;&gt; n.reroot(n.soma, inplace=True)\n&gt;&gt;&gt; _ = navis.flow_centrality(n)\n&gt;&gt;&gt; n[0].nodes.flow_centrality.max()\n91234\n</code></pre> See Also <p><code>navis.synapse_flow_centrality</code>         Synapse-based flow centrality.</p> Source code in <code>navis/morpho/mmetrics.py</code> <pre><code>@utils.map_neuronlist(desc=\"Calc. flow\", allow_parallel=True)\n@utils.meshneuron_skeleton(\n    method=\"node_properties\",\n    include_connectors=True,\n    heal=True,\n    node_props=[\"flow_centrality\"],\n)\ndef flow_centrality(x: \"core.NeuronObject\") -&gt; \"core.NeuronObject\":\n    \"\"\"Calculate flow between leaf nodes.\n\n    Parameters\n    ----------\n    x :         TreeNeuron | MeshNeuron | NeuronList\n                Neuron(s) to calculate flow centrality for.\n\n    Returns\n    -------\n    neuron\n                Adds \"flow_centrality\" as column in the node table (for\n                TreeNeurons) or as `.flow_centrality` property\n                (for MeshNeurons).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(2)\n    &gt;&gt;&gt; n.reroot(n.soma, inplace=True)\n    &gt;&gt;&gt; _ = navis.flow_centrality(n)\n    &gt;&gt;&gt; n[0].nodes.flow_centrality.max()\n    91234\n\n    See Also\n    --------\n    [`navis.synapse_flow_centrality`][]\n            Synapse-based flow centrality.\n\n    \"\"\"\n    # Quick disclaimer:\n    # This function may look unnecessarily complicated. I did also try out an\n    # implementation using igraph + shortest paths which works like a charm and\n    # causes less headaches. It is, however, about &gt;10X slower than this version!\n    # Note to self: do not go down that rabbit hole again!\n    msg = (\n        \"Synapse-based flow centrality has been moved to \"\n        \"`navis.synapse_flow_centrality` in navis \"\n        \"version 1.4.0. `navis.flow_centrality` now calculates \"\n        \"morphology-only flow. \"\n        \"This warning will be removed in a future version of navis.\"\n    )\n    warnings.warn(msg, DeprecationWarning)\n    logger.warning(msg)\n\n    if not isinstance(x, core.TreeNeuron):\n        raise ValueError(f'Expected TreeNeuron(s), got \"{type(x)}\"')\n\n    if np.any(x.soma) and not np.all(np.isin(x.soma, x.root)):\n        logger.warning(f\"Neuron {x.id} is not rooted to its soma!\")\n\n    # Get list of leafs\n    leafs = x.leafs.node_id.values\n    total_leafs = len(leafs)\n\n    # Get list of points to calculate flow centrality for:\n    calc_node_ids = x.branch_points.node_id.values\n\n    # We will be processing a super downsampled version of the neuron to\n    # speed up calculations\n    current_level = logger.level\n    current_state = config.pbar_hide\n    logger.setLevel(\"ERROR\")\n    config.pbar_hide = True\n    y = sampling.downsample_neuron(\n        x=x,\n        downsampling_factor=float(\"inf\"),\n        inplace=False,\n        preserve_nodes=calc_node_ids,\n    )\n    logger.setLevel(current_level)\n    config.pbar_hide = current_state\n\n    # Get number of leafs distal to each branch's childs\n    # Note that we're using geodesic matrix here because it is much more\n    # efficient than for `distal_to` for larger queries/neurons\n    dists = graph.geodesic_matrix(y, from_=leafs, directed=True, weight=None)\n    distal = (dists[calc_node_ids] &lt; np.inf).sum(axis=0)\n\n    # Calculate the flow\n    flow = {n: (total_leafs - distal[n]) * distal[n] for n in calc_node_ids}\n\n    # At this point there is only flow for branch points and connectors nodes.\n    # Let's complete that mapping by adding flow for the nodes between branch points.\n    for s in x.small_segments:\n        # Segments' orientation goes from distal -&gt; proximal\n\n        # If first node in the segment has no flow, set to 0\n        flow[s[0]] = flow.get(s[0], 0)\n\n        # For each node get the flow of its child\n        for i in range(1, len(s)):\n            if s[i] not in flow:\n                flow[s[i]] = flow[s[i - 1]]\n\n    x.nodes[\"flow_centrality\"] = x.nodes.node_id.map(flow).fillna(0).astype(int)\n\n    # We need to add a restriction: a branchpoint cannot have a lower\n    # flow than its highest child -&gt; this happens at the main branch point to\n    # the cell body fiber because the flow doesn't go \"through\" it in\n    # child -&gt; parent direction but rather \"across\" it from one child to the\n    # other\n    is_bp = x.nodes[\"type\"] == \"branch\"\n    bp = x.nodes.loc[is_bp, \"node_id\"].values\n    bp_childs = x.nodes[x.nodes.parent_id.isin(bp)]\n    max_flow = bp_childs.groupby(\"parent_id\").flow_centrality.max()\n    x.nodes.loc[is_bp, \"flow_centrality\"] = max_flow.loc[bp].values\n    x.nodes[\"flow_centrality\"] = x.nodes.flow_centrality.astype(int)\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.form_factor","title":"<code>navis.form_factor</code>","text":"<p>Calculate form factor for given neuron.</p> <p>The form factor F(q) is a Fourier transform of density-density correlation of particles used to classify objects in polymer physics. Based on Choi et al., 2022 (bioRxiv). Code adapted from github.com/kirichoi/FqClustering.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Neurons to calculate form factor for. A few notes:\n      - data should be in micron - if not, you might want to adjust\n        start/stop/min!\n      - since this is all about density, it may make sense to\n        resample neurons\n</code></pre> <p> TYPE: <code>        TreeNeuron | Meshneuron | Dotprops | NeuronList</code> </p> <code>start</code> <pre><code>    Start/stop/num describe the (log) space over which to calculate\n    the form factor. Effectively determining the resolution.\n    Assuming `x` is in microns the defaults mean we pay attention\n    to densities between 1 nm (1e-3 microns) and 1 mm (1e+3 microns).\n    The x-value corresponding to the form factor(s) in `Fq` will\n    be `np.logspace(start, stop, num)`.\n</code></pre> <p> TYPE: <code>int</code> DEFAULT: <code>-3</code> </p> <code>parallel</code> <pre><code>    Whether to use multiple cores when `x` is a NeuronList.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>False</code> </p> <code>n_cores</code> <pre><code>    Number of cores to use when `x` is a NeuronList and\n    `parallel=True`. Even on a single core this function makes\n    heavy use of numpy which itself uses multiple threads - it is\n    therefore not advisable to use all your cores as this would\n    create a bottleneck.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>os.cpu_count() // 2</code> </p> <code>progress</code> <pre><code>    Whether to show a progress bar.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Fq</code> <p>For single neurons: <code>(num,)</code> array For Neuronlists: <code>(len(x), num)</code> array</p> <p> TYPE: <code>np.ndarray</code> </p> References <p>Polymer physics-based classification of neurons Kiri Choi, Won Kyu Kim, Changbong Hyeon bioRxiv 2022.04.07.487455; doi: https://doi.org/10.1101/2022.04.07.487455</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(3)\n&gt;&gt;&gt; nl = nl.convert_units('microns')\n&gt;&gt;&gt; # Resample to 1 node / micron\n&gt;&gt;&gt; rs = navis.resample_skeleton(nl, '1 micron')\n&gt;&gt;&gt; # Calculate form factor\n&gt;&gt;&gt; Fq = navis.form_factor(rs, start=-3, stop=3, num=301,\n...                        parallel=True, n_cores=3)\n&gt;&gt;&gt; # Plot\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; x = np.logspace(-3, 3,  301)\n&gt;&gt;&gt; fig, ax = plt.subplots()\n&gt;&gt;&gt; for i in range(len(Fq)):\n...     _ = ax.plot(x, Fq[i])\n&gt;&gt;&gt; # Make log-log\n&gt;&gt;&gt; ax.set_xscale('log')\n&gt;&gt;&gt; ax.set_yscale('log')\n&gt;&gt;&gt; plt.show()\n&gt;&gt;&gt; # Cluster\n&gt;&gt;&gt; from scipy.spatial.distance import pdist\n&gt;&gt;&gt; from scipy.cluster.hierarchy import dendrogram, linkage\n&gt;&gt;&gt; dists = pdist(Fq)\n&gt;&gt;&gt; Z = linkage(dists, method='ward')\n&gt;&gt;&gt; dn = dendrogram(Z)\n</code></pre> Source code in <code>navis/morpho/fq.py</code> <pre><code>def form_factor(x: Union['core.TreeNeuron', 'core.MeshNeuron'],\n                start: int = -3,\n                stop: int = 3,\n                num: int = 601,\n                parallel: bool = False,\n                n_cores: int = os.cpu_count() // 2,\n                progress=True):\n    \"\"\"Calculate form factor for given neuron.\n\n    The form factor F(q) is a Fourier transform of density-density correlation\n    of particles used to classify objects in polymer physics. Based on Choi et\n    al., 2022 (bioRxiv). Code adapted from github.com/kirichoi/FqClustering.\n\n    Parameters\n    ----------\n    x :         TreeNeuron | Meshneuron | Dotprops | NeuronList\n                Neurons to calculate form factor for. A few notes:\n                  - data should be in micron - if not, you might want to adjust\n                    start/stop/min!\n                  - since this is all about density, it may make sense to\n                    resample neurons\n    start/stop/num : int\n                Start/stop/num describe the (log) space over which to calculate\n                the form factor. Effectively determining the resolution.\n                Assuming `x` is in microns the defaults mean we pay attention\n                to densities between 1 nm (1e-3 microns) and 1 mm (1e+3 microns).\n                The x-value corresponding to the form factor(s) in `Fq` will\n                be `np.logspace(start, stop, num)`.\n    parallel :  bool\n                Whether to use multiple cores when `x` is a NeuronList.\n    n_cores :   bool\n                Number of cores to use when `x` is a NeuronList and\n                `parallel=True`. Even on a single core this function makes\n                heavy use of numpy which itself uses multiple threads - it is\n                therefore not advisable to use all your cores as this would\n                create a bottleneck.\n    progress :  bool\n                Whether to show a progress bar.\n\n    Returns\n    -------\n    Fq :        np.ndarray\n                For single neurons: `(num,)` array\n                For Neuronlists: `(len(x), num)` array\n\n    References\n    ----------\n    Polymer physics-based classification of neurons\n    Kiri Choi, Won Kyu Kim, Changbong Hyeon\n    bioRxiv 2022.04.07.487455; doi: https://doi.org/10.1101/2022.04.07.487455\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(3)\n    &gt;&gt;&gt; nl = nl.convert_units('microns')\n    &gt;&gt;&gt; # Resample to 1 node / micron\n    &gt;&gt;&gt; rs = navis.resample_skeleton(nl, '1 micron')\n    &gt;&gt;&gt; # Calculate form factor\n    &gt;&gt;&gt; Fq = navis.form_factor(rs, start=-3, stop=3, num=301,\n    ...                        parallel=True, n_cores=3)\n    &gt;&gt;&gt; # Plot\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; x = np.logspace(-3, 3,  301)\n    &gt;&gt;&gt; fig, ax = plt.subplots()\n    &gt;&gt;&gt; for i in range(len(Fq)):\n    ...     _ = ax.plot(x, Fq[i])\n    &gt;&gt;&gt; # Make log-log\n    &gt;&gt;&gt; ax.set_xscale('log')\n    &gt;&gt;&gt; ax.set_yscale('log')\n    &gt;&gt;&gt; plt.show()                                              # doctest: +SKIP\n    &gt;&gt;&gt; # Cluster\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import dendrogram, linkage\n    &gt;&gt;&gt; dists = pdist(Fq)\n    &gt;&gt;&gt; Z = linkage(dists, method='ward')\n    &gt;&gt;&gt; dn = dendrogram(Z)                                      # doctest: +SKIP\n\n    \"\"\"\n    if isinstance(x, core.NeuronList):\n        pbar = partial(\n            config.tqdm,\n            desc='Calc. form factor',\n            total=len(x),\n            disable=config.pbar_hide or not progress,\n            leave=config.pbar_leave\n        )\n        _calc_form_factor = partial(form_factor, progress=False,\n                                    start=start, stop=stop, num=num)\n\n        if parallel:\n            with mp.Pool(processes=n_cores) as pool:\n                results = pool.imap(_calc_form_factor, x)\n                Fq = list(pbar(results))\n        else:\n            Fq = [_calc_form_factor(n) for n in pbar(x)]\n\n        return np.vstack(Fq)\n\n    utils.eval_param(x, name='x', allowed_types=(core.TreeNeuron,\n                                                 core.Dotprops,\n                                                 core.MeshNeuron))\n\n    if isinstance(x, core.TreeNeuron):\n        coor = x.nodes[['x', 'y', 'z']].values\n    elif isinstance(x, core.MeshNeuron):\n        coor = x.vertices\n    elif isinstance(x, core.Dotprops):\n        coor = x.points\n\n    ucoor = np.unique(coor, axis=0)\n    lenucoor = len(ucoor)\n\n    q_range = np.logspace(start, stop, num)\n    Fq = np.empty(len(q_range))\n    ccdisttri = scipy.spatial.distance.pdist(ucoor)\n\n    for q in config.trange(len(q_range),\n                           desc='Calc. form factor',\n                           disable=config.pbar_hide or not progress,\n                           leave=config.pbar_leave):\n        qrvec = q_range[q] * ccdisttri\n        Fq[q] = np.divide(np.divide(2 * np.sum(np.sin(qrvec) / qrvec), lenucoor), lenucoor) + 1 / lenucoor\n\n    return Fq\n</code></pre>"},{"location":"reference/navis/#navis.geodesic_matrix","title":"<code>navis.geodesic_matrix</code>","text":"<p>Generate geodesic (\"along-the-arbor\") distance matrix between nodes/vertices.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    If list, must contain a SINGLE neuron.\n</code></pre> <p> TYPE: <code>        TreeNeuron | MeshNeuron | NeuronList</code> </p> <code>from_</code> <pre><code>    Node IDs (for TreeNeurons) or vertex indices (for MeshNeurons).\n    If provided, will compute distances only FROM this subset to\n    all other nodes/vertices.\n</code></pre> <p> TYPE: <code>    list | numpy.ndarray</code> DEFAULT: <code>None</code> </p> <code>directed</code> <pre><code>    For TreeNeurons only: if True, pairs without a child-&gt;parent\n    path will be returned with `distance = \"inf\"`.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>False</code> </p> <code>weight</code> <pre><code>    If \"weight\" distances are given as physical length.\n    If `None` distance is the number of nodes.\n</code></pre> <p> TYPE: <code>   'weight' | None</code> DEFAULT: <code>'weight'</code> </p> <code>limit</code> <pre><code>    Use to limit distance calculations. Nodes that are not within\n    `limit` will have distance `np.inf`. If neuron has its\n    `.units` set, you can also pass a string such as \"10 microns\".\n</code></pre> <p> TYPE: <code>    int | float</code> DEFAULT: <code>np.inf</code> </p> RETURNS DESCRIPTION <code>pd.DataFrame</code> <p>Geodesic distance matrix. If the neuron is fragmented or <code>directed=True</code>, unreachable node pairs will have distance <code>np.inf</code>.</p> See Also <p><code>navis.distal_to</code>     Check if a node A is distal to node B. <code>navis.dist_between</code>     Get point-to-point geodesic distances. <code>navis.dist_to_root</code>     Distances from all skeleton node to their root(s). <code>navis.graph.skeleton_adjacency_matrix</code>     Generate adjacency matrix for a skeleton.</p> <p>Examples:</p> <p>Find average geodesic distance between all leaf nodes</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; # Generate distance matrix\n&gt;&gt;&gt; m = navis.geodesic_matrix(n)\n&gt;&gt;&gt; # Subset matrix to leaf nodes\n&gt;&gt;&gt; leafs = n.nodes[n.nodes.type=='end'].node_id.values\n&gt;&gt;&gt; l_dist = m.loc[leafs, leafs]\n&gt;&gt;&gt; # Get mean\n&gt;&gt;&gt; round(l_dist.mean().mean())\n12983\n</code></pre> Source code in <code>navis/graph/graph_utils.py</code> <pre><code>def geodesic_matrix(\n    x: \"core.NeuronObject\",\n    from_: Optional[Iterable[int]] = None,\n    directed: bool = False,\n    weight: Optional[str] = \"weight\",\n    limit: Union[float, int] = np.inf,\n) -&gt; pd.DataFrame:\n    \"\"\"Generate geodesic (\"along-the-arbor\") distance matrix between nodes/vertices.\n\n    Parameters\n    ----------\n    x :         TreeNeuron | MeshNeuron | NeuronList\n                If list, must contain a SINGLE neuron.\n    from_ :     list | numpy.ndarray, optional\n                Node IDs (for TreeNeurons) or vertex indices (for MeshNeurons).\n                If provided, will compute distances only FROM this subset to\n                all other nodes/vertices.\n    directed :  bool, optional\n                For TreeNeurons only: if True, pairs without a child-&gt;parent\n                path will be returned with `distance = \"inf\"`.\n    weight :    'weight' | None, optional\n                If \"weight\" distances are given as physical length.\n                If `None` distance is the number of nodes.\n    limit :     int | float, optional\n                Use to limit distance calculations. Nodes that are not within\n                `limit` will have distance `np.inf`. If neuron has its\n                `.units` set, you can also pass a string such as \"10 microns\".\n\n    Returns\n    -------\n    pd.DataFrame\n                Geodesic distance matrix. If the neuron is fragmented or\n                `directed=True`, unreachable node pairs will have distance `np.inf`.\n\n    See Also\n    --------\n    [`navis.distal_to`][]\n        Check if a node A is distal to node B.\n    [`navis.dist_between`][]\n        Get point-to-point geodesic distances.\n    [`navis.dist_to_root`][]\n        Distances from all skeleton node to their root(s).\n    [`navis.graph.skeleton_adjacency_matrix`][]\n        Generate adjacency matrix for a skeleton.\n\n    Examples\n    --------\n    Find average geodesic distance between all leaf nodes\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; # Generate distance matrix\n    &gt;&gt;&gt; m = navis.geodesic_matrix(n)\n    &gt;&gt;&gt; # Subset matrix to leaf nodes\n    &gt;&gt;&gt; leafs = n.nodes[n.nodes.type=='end'].node_id.values\n    &gt;&gt;&gt; l_dist = m.loc[leafs, leafs]\n    &gt;&gt;&gt; # Get mean\n    &gt;&gt;&gt; round(l_dist.mean().mean())\n    12983\n\n    \"\"\"\n    if isinstance(x, core.NeuronList):\n        if len(x) != 1:\n            raise ValueError(\"Input must be a single neuron.\")\n        x = x[0]\n\n    if not isinstance(x, (core.TreeNeuron, core.MeshNeuron)):\n        raise ValueError(f'Unable to process data of type \"{type(x)}\"')\n\n    limit = x.map_units(limit, on_error=\"raise\")\n\n    # Use fastcore if available\n    if utils.fastcore and isinstance(x, core.TreeNeuron):\n        # Calculate node distances\n        if weight == \"weight\":\n            weight = utils.fastcore.dag.parent_dist(\n                x.nodes.node_id.values,\n                x.nodes.parent_id.values,\n                x.nodes[[\"x\", \"y\", \"z\"]].values,\n                root_dist=0,\n            )\n\n        # Check for missing sources\n        if not isinstance(from_, type(None)):\n            from_ = np.unique(utils.make_iterable(from_))\n\n            miss = from_[~np.isin(from_, x.nodes.node_id.values)]\n            if len(miss):\n                raise ValueError(\n                    f\"Node/vertex IDs not present: {', '.join(miss.astype(str))}\"\n                )\n            ix = from_\n        else:\n            ix = x.nodes.node_id.values\n\n        dmat = utils.fastcore.geodesic_matrix(\n            x.nodes.node_id.values,\n            x.nodes.parent_id.values,\n            weights=weight,\n            directed=directed,\n            sources=from_,\n        )\n\n        # Fastcore returns -1 for unreachable node pairs\n        dmat[dmat &lt; 0] = np.inf\n\n        if limit is not None and limit is not np.inf:\n            dmat[dmat &gt; limit] = np.inf\n\n        return pd.DataFrame(dmat, index=ix, columns=x.nodes.node_id.values)\n\n    # Makes no sense to use directed for MeshNeurons\n    if isinstance(x, core.MeshNeuron):\n        directed = False\n\n    if x.igraph and config.use_igraph:\n        if isinstance(x, core.TreeNeuron):\n            nodeList = np.array(x.igraph.vs.get_attribute_values(\"node_id\"))\n        else:\n            nodeList = np.arange(len(x.igraph.vs))\n\n        # Matrix is ordered by vertex number\n        m = _igraph_to_sparse(x.igraph, weight_attr=weight)\n    else:\n        nodeList = np.array(x.graph.nodes())\n\n        if hasattr(nx, \"to_scipy_sparse_matrix\"):\n            m = nx.to_scipy_sparse_matrix(x.graph, nodeList, weight=weight)\n        else:\n            m = nx.to_scipy_sparse_array(x.graph, nodeList, weight=weight)\n\n    if not isinstance(from_, type(None)):\n        from_ = np.unique(utils.make_iterable(from_))\n\n        miss = from_[~np.isin(from_, nodeList)].astype(str)\n        if len(miss):\n            raise ValueError(f'Node/vertex IDs not present: {\", \".join(miss)}')\n\n        indices = np.where(np.isin(nodeList, from_))[0]\n        ix = nodeList[indices]\n    else:\n        indices = None\n        ix = nodeList\n\n    # For some reason csgraph.dijkstra expects indices/indptr as int32\n    # igraph seems to do that by default but networkx uses int64 for indices\n    m.indptr = m.indptr.astype(\"int32\", copy=False)\n    m.indices = m.indices.astype(\"int32\", copy=False)\n    dmat = csgraph.dijkstra(m, directed=directed, indices=indices, limit=limit)\n\n    return pd.DataFrame(dmat, columns=nodeList, index=ix)  # type: ignore  # no stubs\n</code></pre>"},{"location":"reference/navis/#navis.get_viewer","title":"<code>navis.get_viewer</code>","text":"<p>Grab active 3D viewer.</p> RETURNS DESCRIPTION <code>[`navis.Viewer`][]</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; from vispy import scene\n&gt;&gt;&gt; # Get and plot neuron in 3d\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; _ = n.plot3d(color='red', backend='vispy')\n&gt;&gt;&gt; # Grab active viewer and add custom text\n&gt;&gt;&gt; viewer = navis.get_viewer()\n&gt;&gt;&gt; text = scene.visuals.Text(text='TEST',\n...                           pos=(0, 0, 0))\n&gt;&gt;&gt; viewer.add(text)\n&gt;&gt;&gt; # Close viewer\n&gt;&gt;&gt; viewer.close()\n</code></pre> Source code in <code>navis/plotting/vispy/vputils.py</code> <pre><code>def get_viewer():\n    \"\"\"Grab active 3D viewer.\n\n    Returns\n    -------\n    [`navis.Viewer`][]\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; from vispy import scene\n    &gt;&gt;&gt; # Get and plot neuron in 3d\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; _ = n.plot3d(color='red', backend='vispy')\n    &gt;&gt;&gt; # Grab active viewer and add custom text\n    &gt;&gt;&gt; viewer = navis.get_viewer()\n    &gt;&gt;&gt; text = scene.visuals.Text(text='TEST',\n    ...                           pos=(0, 0, 0))\n    &gt;&gt;&gt; viewer.add(text)\n    &gt;&gt;&gt; # Close viewer\n    &gt;&gt;&gt; viewer.close()\n\n    \"\"\"\n    return getattr(config, 'primary_viewer', None)\n</code></pre>"},{"location":"reference/navis/#navis.guess_radius","title":"<code>navis.guess_radius</code>","text":"<p>Guess radii for skeleton nodes.</p> <p>Uses distance between connectors and nodes to guess radii. Interpolate for nodes without connectors. Fills in <code>radius</code> column in node table.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Neuron(s) to be processed.\n</code></pre> <p> TYPE: <code>            TreeNeuron | NeuronList</code> </p> <code>method</code> <pre><code>        Method to be used to interpolate unknown radii. See\n        `pandas.DataFrame.interpolate` for details.\n</code></pre> <p> TYPE: <code>       str</code> DEFAULT: <code>'linear'</code> </p> <code>limit</code> <pre><code>        Maximum number of consecutive missing radii to fill.\n        Must be greater than 0.\n</code></pre> <p> TYPE: <code>        int</code> DEFAULT: <code>None</code> </p> <code>smooth</code> <pre><code>        If True, will smooth radii after interpolation using a\n        rolling window. If `int`, will use to define size of\n        window.\n</code></pre> <p> TYPE: <code>       bool | int</code> DEFAULT: <code>True</code> </p> <code>inplace</code> <pre><code>        If False, will use and return copy of original neuron(s).\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>TreeNeuron / List</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(2)\n&gt;&gt;&gt; nl_radius = navis.guess_radius(nl)\n</code></pre> Source code in <code>navis/morpho/manipulation.py</code> <pre><code>@utils.map_neuronlist(desc=\"Guessing\", allow_parallel=True)\ndef guess_radius(\n    x: NeuronObject,\n    method: str = \"linear\",\n    limit: Optional[int] = None,\n    smooth: bool = True,\n    inplace: bool = False,\n) -&gt; Optional[NeuronObject]:\n    \"\"\"Guess radii for skeleton nodes.\n\n    Uses distance between connectors and nodes to guess radii. Interpolate for\n    nodes without connectors. Fills in `radius` column in node table.\n\n    Parameters\n    ----------\n    x :             TreeNeuron | NeuronList\n                    Neuron(s) to be processed.\n    method :        str, optional\n                    Method to be used to interpolate unknown radii. See\n                    `pandas.DataFrame.interpolate` for details.\n    limit :         int, optional\n                    Maximum number of consecutive missing radii to fill.\n                    Must be greater than 0.\n    smooth :        bool | int, optional\n                    If True, will smooth radii after interpolation using a\n                    rolling window. If `int`, will use to define size of\n                    window.\n    inplace :       bool, optional\n                    If False, will use and return copy of original neuron(s).\n\n    Returns\n    -------\n    TreeNeuron/List\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(2)\n    &gt;&gt;&gt; nl_radius = navis.guess_radius(nl)\n\n    \"\"\"\n    # The decorator makes sure that at this point we have single neurons\n    if not isinstance(x, core.TreeNeuron):\n        raise TypeError(f\"Can only process TreeNeurons, not {type(x)}\")\n\n    if not hasattr(x, \"connectors\") or x.connectors.empty:\n        raise ValueError(\"Neuron must have connectors!\")\n\n    if not inplace:\n        x = x.copy()\n\n    # Set default rolling window size\n    if isinstance(smooth, bool) and smooth:\n        smooth = 5\n\n    # We will be using the index as distance to interpolate. For this we have\n    # to change method 'linear' to 'index'\n    method = \"index\" if method == \"linear\" else method\n\n    # Collect connectors and calc distances\n    cn = x.connectors.copy()\n\n    # Prepare nodes (add parent_dist for later, set index)\n    x.nodes[\"parent_dist\"] = mmetrics.parent_dist(x, root_dist=0)\n    nodes = x.nodes.set_index(\"node_id\", inplace=False)\n\n    # For each connector (pre and post), get the X/Y distance to its node\n    cn_locs = cn[[\"x\", \"y\"]].values\n    tn_locs = nodes.loc[cn.node_id.values, [\"x\", \"y\"]].values\n    dist = np.sqrt(np.sum((tn_locs - cn_locs) ** 2, axis=1).astype(int))\n    cn[\"dist\"] = dist\n\n    # Get max distance per node (in case of multiple connectors per\n    # node)\n    cn_grouped = cn.groupby(\"node_id\").dist.max()\n\n    # Set undefined radii to None so that they are ignored for interpolation\n    nodes.loc[nodes.radius &lt;= 0, \"radius\"] = None\n\n    # Assign radii to nodes\n    nodes.loc[cn_grouped.index, \"radius\"] = cn_grouped.values.astype(\n        nodes.radius.dtype, copy=False\n    )\n\n    # Go over each segment and interpolate radii\n    for s in config.tqdm(\n        x.segments, desc=\"Interp.\", disable=config.pbar_hide, leave=config.pbar_leave\n    ):\n        # Get this segments radii and parent dist\n        this_radii = nodes.loc[s, [\"radius\", \"parent_dist\"]]\n        this_radii[\"parent_dist_cum\"] = this_radii.parent_dist.cumsum()\n\n        # Set cumulative distance as index and drop parent_dist\n        this_radii = this_radii.set_index(\"parent_dist_cum\", drop=True).drop(\n            \"parent_dist\", axis=1\n        )\n\n        # Interpolate missing radii\n        interp = this_radii.interpolate(\n            method=method, limit_direction=\"both\", limit=limit\n        )\n\n        if smooth:\n            interp = interp.rolling(smooth, min_periods=1).max()\n\n        nodes.loc[s, \"radius\"] = interp.values\n\n    # Set non-interpolated radii back to -1\n    nodes.loc[nodes.radius.isnull(), \"radius\"] = -1\n\n    # Reassign nodes\n    x.nodes = nodes.reset_index(drop=False, inplace=False)\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.heal_skeleton","title":"<code>navis.heal_skeleton</code>","text":"<p>Heal fragmented skeleton(s).</p> <p>Tries to heal a fragmented skeleton (i.e. a neuron with multiple roots) using a minimum spanning tree.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Fragmented skeleton(s).\n</code></pre> <p> TYPE: <code>        TreeNeuron/List</code> </p> <code>method</code> <pre><code>    Method used to heal fragments:\n     - 'LEAFS': Only leaf (including root) nodes will be used to\n       heal gaps. This can be much faster depending on the size of\n       the neuron\n     - 'ALL': All nodes can be used to reconnect fragments.\n</code></pre> <p> TYPE: <code>   'LEAFS' | 'ALL'</code> DEFAULT: <code>'ALL'</code> </p> <code>max_dist</code> <pre><code>    This effectively sets the max length for newly added edges. Use\n    it to prevent far away fragments to be forcefully connected.\n    If the neurons have `.units` set, you can also pass a string\n    such as e.g. \"2 microns\".\n</code></pre> <p> TYPE: <code> float | str</code> DEFAULT: <code>None</code> </p> <code>min_size</code> <pre><code>    Minimum size in nodes for fragments to be reattached. Fragments\n    smaller than `min_size` will be ignored during stitching and\n    hence remain disconnected.\n</code></pre> <p> TYPE: <code> int</code> DEFAULT: <code>None</code> </p> <code>drop_disc</code> <pre><code>    If True and the neuron remains fragmented after healing (i.e.\n    `max_dist` or `min_size` prevented a full connect), we will\n    keep only the largest (by number of nodes) connected component\n    and discard all other fragments.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>mask</code> <pre><code>    Either a boolean mask or a list of node IDs. If provided will\n    only heal breaks between these nodes.\n</code></pre> <p> TYPE: <code>     list-like</code> DEFAULT: <code>None</code> </p> <code>inplace</code> <pre><code>    If False, will perform healing on and return a copy.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>None</code> <p>If <code>inplace=True</code>.</p> <code>CatmaidNeuron / List</code> <p>If <code>inplace=False</code>.</p> See Also <p><code>navis.stitch_skeletons</code>             Use to stitch multiple skeletons together. <code>navis.break_fragments</code>             Use to produce individual neurons from disconnected fragments.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1, kind='skeleton')\n&gt;&gt;&gt; # Disconnect parts of the neuron\n&gt;&gt;&gt; n.nodes.loc[100, 'parent_id'] = -1\n&gt;&gt;&gt; len(n.root)\n2\n&gt;&gt;&gt; # Heal neuron\n&gt;&gt;&gt; healed = navis.heal_skeleton(n)\n&gt;&gt;&gt; len(healed.root)\n1\n</code></pre> Source code in <code>navis/morpho/manipulation.py</code> <pre><code>@utils.map_neuronlist(desc=\"Healing\", allow_parallel=True)\ndef heal_skeleton(\n    x: \"core.NeuronList\",\n    method: Union[Literal[\"LEAFS\"], Literal[\"ALL\"]] = \"ALL\",\n    max_dist: Optional[float] = None,\n    min_size: Optional[float] = None,\n    drop_disc: float = False,\n    mask: Optional[Sequence] = None,\n    inplace: bool = False,\n) -&gt; Optional[NeuronObject]:\n    \"\"\"Heal fragmented skeleton(s).\n\n    Tries to heal a fragmented skeleton (i.e. a neuron with multiple roots)\n    using a minimum spanning tree.\n\n    Parameters\n    ----------\n    x :         TreeNeuron/List\n                Fragmented skeleton(s).\n    method :    'LEAFS' | 'ALL', optional\n                Method used to heal fragments:\n                 - 'LEAFS': Only leaf (including root) nodes will be used to\n                   heal gaps. This can be much faster depending on the size of\n                   the neuron\n                 - 'ALL': All nodes can be used to reconnect fragments.\n    max_dist :  float | str, optional\n                This effectively sets the max length for newly added edges. Use\n                it to prevent far away fragments to be forcefully connected.\n                If the neurons have `.units` set, you can also pass a string\n                such as e.g. \"2 microns\".\n    min_size :  int, optional\n                Minimum size in nodes for fragments to be reattached. Fragments\n                smaller than `min_size` will be ignored during stitching and\n                hence remain disconnected.\n    drop_disc : bool\n                If True and the neuron remains fragmented after healing (i.e.\n                `max_dist` or `min_size` prevented a full connect), we will\n                keep only the largest (by number of nodes) connected component\n                and discard all other fragments.\n    mask :      list-like, optional\n                Either a boolean mask or a list of node IDs. If provided will\n                only heal breaks between these nodes.\n    inplace :   bool, optional\n                If False, will perform healing on and return a copy.\n\n    Returns\n    -------\n    None\n                If `inplace=True`.\n    CatmaidNeuron/List\n                If `inplace=False`.\n\n\n    See Also\n    --------\n    [`navis.stitch_skeletons`][]\n                Use to stitch multiple skeletons together.\n    [`navis.break_fragments`][]\n                Use to produce individual neurons from disconnected fragments.\n\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1, kind='skeleton')\n    &gt;&gt;&gt; # Disconnect parts of the neuron\n    &gt;&gt;&gt; n.nodes.loc[100, 'parent_id'] = -1\n    &gt;&gt;&gt; len(n.root)\n    2\n    &gt;&gt;&gt; # Heal neuron\n    &gt;&gt;&gt; healed = navis.heal_skeleton(n)\n    &gt;&gt;&gt; len(healed.root)\n    1\n\n    \"\"\"\n    method = str(method).upper()\n\n    if method not in (\"LEAFS\", \"ALL\"):\n        raise ValueError(f'Unknown method \"{method}\"')\n\n    # The decorator makes sure that at this point we have single neurons\n    if not isinstance(x, core.TreeNeuron):\n        raise TypeError(f'Expected TreeNeuron(s), got \"{type(x)}\"')\n\n    if not isinstance(max_dist, type(None)):\n        max_dist = x.map_units(max_dist, on_error=\"raise\")\n\n    if not inplace:\n        x = x.copy()\n\n    _ = _stitch_mst(\n        x, nodes=method, max_dist=max_dist, min_size=min_size, mask=mask, inplace=True\n    )\n\n    # See if we need to drop remaining disconnected fragments\n    if drop_disc:\n        # Compute this property only once\n        trees = x.subtrees\n        if len(trees) &gt; 1:\n            # Tree is sorted such that the largest component is the first\n            _ = subset.subset_neuron(x, subset=trees[0], inplace=True)\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.health_check","title":"<code>navis.health_check</code>","text":"<p>Run a health check on TreeNeurons and flag potential issues.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Neuron(s) whose nodes to classify nodes.\n</code></pre> <p> TYPE: <code>        TreeNeuron | NeuronList</code> </p> <code>verbose</code> <pre><code>    If True, will print errors in addition to returning them.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>list of issues or None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; navis.health_check(n)\nNeuron 1734350788 seems perfectly fine.\n</code></pre> Source code in <code>navis/graph/clinic.py</code> <pre><code>def health_check(x: 'core.NeuronObject', verbose: bool = True) -&gt; None:\n    \"\"\"Run a health check on TreeNeurons and flag potential issues.\n\n    Parameters\n    ----------\n    x :         TreeNeuron | NeuronList\n                Neuron(s) whose nodes to classify nodes.\n    verbose :   bool\n                If True, will print errors in addition to returning them.\n\n    Returns\n    -------\n    list of issues or None\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; navis.health_check(n)\n    Neuron 1734350788 seems perfectly fine.\n\n    \"\"\"\n    if isinstance(x, core.NeuronList):\n        for n in x:\n            _ = health_check(x)\n        return\n    elif not isinstance(x, core.TreeNeuron):\n        raise TypeError(f'Excepted TreeNeuron/List, got \"{type(x)}\"')\n\n    issues = []\n\n    # Check if neuron is not a tree\n    if not x.is_tree:\n        issues.append('is not a tree (networkx.is_forest)')\n    # See if there are any cycles\n    if x.cycles:\n        issues.append(f'has cycles (networkx.find_cycles): {str(x.cycles)}')\n    # See if any node has more than one parent\n    od = [n[0] for n in x.graph.out_degree if n[1] &gt; 1]\n    if od:\n        issues.append(f'has nodes with multiple parents (graph.out_degree): {\", \".join(od)}')\n\n    locs, counts = np.unique(x.nodes[['x', 'y', 'z']].values,\n                             axis=0,\n                             return_counts=True)\n    dupl = counts &gt; 1\n    if any(dupl):\n        issues.append(f'has {sum(dupl)} node positions that are occupied by multiple nodes')\n\n    if verbose:\n        if issues:\n            print(f'Neuron {str(x.id)} has issues:')\n            for i in issues:\n                print(f' - {i}')\n        else:\n            print(f'Neuron {str(x.id)} seems perfectly fine.')\n\n    return issues if issues else None\n</code></pre>"},{"location":"reference/navis/#navis.in_volume","title":"<code>navis.in_volume</code>","text":"<pre><code>in_volume\n</code></pre><pre><code>in_volume\n</code></pre><pre><code>in_volume\n</code></pre><pre><code>in_volume\n</code></pre><pre><code>in_volume\n</code></pre> <p>Test if points/neurons are within a given volume.</p> Notes <p>This function requires ncollpyde (recommended and installed with <code>navis</code>) or pyoctree as backends for raycasting. If neither is installed, we can fall back to using scipy's ConvexHull instead. This is, however, slower and will give wrong positives for concave meshes!</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            Object(s) to intersect with the volume.\n             - Neuron(s) will be subset to parts within the volume\n             - Array-like is treated as list of x/y/z coordinates;\n               has to be of shape `(N, 3)`\n             - `pandas.DataFrame` needs to have `x, y, z`\n               columns\n</code></pre> <p> TYPE: <code>                (N, 3) array-like | pandas.DataFrame | Neuron/List</code> </p> <code>volume</code> <pre><code>            Multiple volumes can be given as list\n            (`[volume1, volume2, ...]`) or dict\n            (`{'label1': volume1, ...}`).\n</code></pre> <p> TYPE: <code>           Volume | mesh-like | dict or list thereof</code> </p> <code>mode</code> <pre><code>            If 'IN', parts of the neuron that are within the volume\n            are kept.\n</code></pre> <p> TYPE: <code>             'IN' | 'OUT'</code> DEFAULT: <code>'IN'</code> </p> <code>backend</code> <pre><code>            Which backend so be used (see Notes). If multiple\n            backends are given, will use the first backend that is\n            available.\n</code></pre> <p> TYPE: <code>          'ncollpyde' | 'pyoctree' | 'scipy' | iterable thereof</code> DEFAULT: <code>('ncollpyde', 'pyoctree')</code> </p> <code>n_rays</code> <pre><code>            Number of rays used to determine if a point is inside\n            a volume. More rays give more reliable results but are\n            slower (especially with pyoctree backend). If `None`\n            will use default number of rays (3 for ncollpyde, 1 for\n            pyoctree).\n</code></pre> <p> TYPE: <code>           int | None</code> DEFAULT: <code>None</code> </p> <code>prevent_fragments</code> <pre><code>            Only relevant if input is TreeNeuron(s). If True, will\n            attempt to keep neuron from fragmenting.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>validate</code> <pre><code>            If True, validate `volume` and try to fix issues using\n            trimesh. Will raise ValueError if issue could not be\n            fixed.\n</code></pre> <p> TYPE: <code>         bool</code> DEFAULT: <code>False</code> </p> <code>inplace</code> <pre><code>            Only relevant if input is Neuron/List. Ignored\n            if multiple volumes are provided.\n</code></pre> <p> TYPE: <code>          bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Neuron</code> <p>If input is a single neuron or NeuronList, will return subset of the neuron(s) (nodes and connectors) that are within given volume.</p> <code>list of bools</code> <p>If input is <code>(N, 3)</code> array of coordinates, returns a <code>(N, )</code> boolean array: <code>True</code> if in volume, <code>False</code> if not in order.</p> <code>dict</code> <p>If multiple volumes are provided, results will be returned in dictionary with volumes as keys::</p> <p>{'volume1': in_volume(x, volume1),    'volume2': in_volume(x, volume2),    ... }</p> <p>Examples:</p> <p>Prune neuron to volume</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; lh = navis.example_volume('LH')\n&gt;&gt;&gt; n_lh = navis.in_volume(n, lh, inplace=False)\n&gt;&gt;&gt; n_lh\ntype            navis.TreeNeuron\nname                  1734350788\nid                    1734350788\nn_nodes                      344\nn_connectors                None\nn_branches                    49\nn_leafs                       50\ncable_length             32313.5\nsoma                        None\nunits                8 nanometer\ndtype: object\n</code></pre> <p>Find out which points are inside a volume</p> <pre><code>&gt;&gt;&gt; in_v = navis.in_volume(n.nodes[['x', 'y', 'z']].values, lh)\n&gt;&gt;&gt; in_v\narray([False, False, False, ..., False, False, False])\n&gt;&gt;&gt; in_v.sum()\n344\n</code></pre> Source code in <code>navis/intersection/intersect.py</code> <pre><code>def in_volume(x: Union['core.NeuronObject', Sequence, pd.DataFrame],\n              volume: Union[core.Volume,\n                            Dict[str, core.Volume],\n                            Sequence[core.Volume]],\n              mode: Modes = 'IN',\n              backend: Backends = ('ncollpyde', 'pyoctree'),\n              n_rays: Optional[int] = None,\n              prevent_fragments: bool = False,\n              validate: bool = False,\n              inplace: bool = False,) -&gt; Optional[Union['core.NeuronObject',\n                                                        Sequence[bool],\n                                                        Dict[str, Union[Sequence[bool],\n                                                                        'core.NeuronObject']]\n                                                        ]]:\n    \"\"\"Test if points/neurons are within a given volume.\n\n    Notes\n    -----\n    This function requires [ncollpyde](https://github.com/clbarnes/ncollpyde)\n    (recommended and installed with `navis`) or\n    [pyoctree](https://github.com/mhogg/pyoctree) as backends for raycasting.\n    If neither is installed, we can fall back to using scipy's ConvexHull\n    instead. This is, however, slower and will give wrong positives for concave\n    meshes!\n\n    Parameters\n    ----------\n    x :                 (N, 3) array-like | pandas.DataFrame | Neuron/List\n                        Object(s) to intersect with the volume.\n                         - Neuron(s) will be subset to parts within the volume\n                         - Array-like is treated as list of x/y/z coordinates;\n                           has to be of shape `(N, 3)`\n                         - `pandas.DataFrame` needs to have `x, y, z`\n                           columns\n\n    volume :            Volume | mesh-like | dict or list thereof\n                        Multiple volumes can be given as list\n                        (`[volume1, volume2, ...]`) or dict\n                        (`{'label1': volume1, ...}`).\n    mode :              'IN' | 'OUT', optional\n                        If 'IN', parts of the neuron that are within the volume\n                        are kept.\n    backend :           'ncollpyde' | 'pyoctree' | 'scipy' | iterable thereof\n                        Which backend so be used (see Notes). If multiple\n                        backends are given, will use the first backend that is\n                        available.\n    n_rays :            int | None, optional\n                        Number of rays used to determine if a point is inside\n                        a volume. More rays give more reliable results but are\n                        slower (especially with pyoctree backend). If `None`\n                        will use default number of rays (3 for ncollpyde, 1 for\n                        pyoctree).\n    prevent_fragments : bool, optional\n                        Only relevant if input is TreeNeuron(s). If True, will\n                        attempt to keep neuron from fragmenting.\n    validate :          bool, optional\n                        If True, validate `volume` and try to fix issues using\n                        trimesh. Will raise ValueError if issue could not be\n                        fixed.\n    inplace :           bool, optional\n                        Only relevant if input is Neuron/List. Ignored\n                        if multiple volumes are provided.\n\n    Returns\n    -------\n    Neuron\n                      If input is a single neuron or NeuronList, will return\n                      subset of the neuron(s) (nodes and connectors) that are\n                      within given volume.\n    list of bools\n                      If input is `(N, 3)` array of coordinates, returns a `(N, )`\n                      boolean array: `True` if in volume, `False` if not in\n                      order.\n    dict\n                      If multiple volumes are provided, results will be\n                      returned in dictionary with volumes as keys::\n\n                        {'volume1': in_volume(x, volume1),\n                         'volume2': in_volume(x, volume2),\n                         ... }\n\n    Examples\n    --------\n    Prune neuron to volume\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; lh = navis.example_volume('LH')\n    &gt;&gt;&gt; n_lh = navis.in_volume(n, lh, inplace=False)\n    &gt;&gt;&gt; n_lh                                                    # doctest: +SKIP\n    type            navis.TreeNeuron\n    name                  1734350788\n    id                    1734350788\n    n_nodes                      344\n    n_connectors                None\n    n_branches                    49\n    n_leafs                       50\n    cable_length             32313.5\n    soma                        None\n    units                8 nanometer\n    dtype: object\n\n    Find out which points are inside a volume\n\n    &gt;&gt;&gt; in_v = navis.in_volume(n.nodes[['x', 'y', 'z']].values, lh)\n    &gt;&gt;&gt; in_v\n    array([False, False, False, ..., False, False, False])\n    &gt;&gt;&gt; in_v.sum()\n    344\n\n    \"\"\"\n    allowed_backends = ('ncollpyde', 'pyoctree', 'scipy')\n\n    if not utils.is_iterable(backend):\n        backend = [backend]\n\n    if any(set(backend) - set(allowed_backends)):\n        raise ValueError(f'Unknown backend in \"{backend}\". Allowed backends: '\n                         f'{allowed_backends}')\n\n    if mode not in ('IN', 'OUT'):\n        raise ValueError(f'`mode` must be \"IN\" or \"OUT\", not \"{mode}\"')\n\n    # If we are given multiple volumes\n    if isinstance(volume, (list, dict, np.ndarray)):\n        # Force into dict\n        if not isinstance(volume, dict):\n            # Make sure all Volumes can be uniquely indexed\n            vnames = [getattr(v, 'name', i) for i, v in enumerate(volume)]\n            dupli = [str(v) for v in set(vnames) if vnames.count(v) &gt; 1]\n            if dupli:\n                raise ValueError('Duplicate Volume names detected: '\n                                 f'{\", \".join(dupli)}. Volume.name must be '\n                                 'unique.')\n\n            volume = {getattr(v, 'name', i): v for i, v in enumerate(volume)}\n\n        # Make sure everything is a volume\n        volume = {k: utils.make_volume(v) for k, v in volume.items()}\n\n        # Validate now - this might safe us troubles later\n        if validate:\n            for v in volume.values():\n                msg = 'Mesh is not a volume ' \\\n                      '(e.g. not watertight, incorrect ' \\\n                      'winding) and could not be fixed. ' \\\n                      'Use `validate=False` to skip validation and ' \\\n                      'perform intersection regardless.'\n                try:\n                    v.validate()\n                except utils.VolumeError as e:\n                    raise utils.VolumeError(f'{v}: {msg}') from e\n                except BaseException:\n                    raise\n\n        data: Dict[str, Any] = dict()\n        for v in config.tqdm(volume, desc='Volumes', disable=config.pbar_hide,\n                             leave=config.pbar_leave):\n            data[v] = in_volume(x,\n                                volume=volume[v],\n                                inplace=False,\n                                n_rays=n_rays,\n                                mode=mode,\n                                validate=False,\n                                backend=backend)\n        return data\n\n    # Coerce volume into navis.Volume\n    volume = utils.make_volume(volume)\n\n    if not isinstance(volume, core.Volume):\n        raise TypeError(f'Expected navis.Volume, got \"{type(volume)}\"')\n\n    # From here on out volume is a single core.Volume\n    vol: 'core.Volume' = volume  # type: ignore\n\n    if validate:\n        msg = 'Mesh is not a volume ' \\\n              '(e.g. not watertight, incorrect ' \\\n              'winding) and could not be fixed. ' \\\n              'Use `validate=False` to skip validation and ' \\\n              'perform intersection regardless.'\n        try:\n            vol.validate()\n        except utils.VolumeError as e:\n            raise utils.VolumeError(f'{vol}: {msg}') from e\n        except BaseException:\n            raise\n\n    # Make copy if necessary\n    if isinstance(x, (core.NeuronList, core.BaseNeuron)):\n        if inplace is False:\n            x = x.copy()\n\n    if isinstance(x, (core.BaseNeuron)):\n        if isinstance(x, core.TreeNeuron):\n            data = x.nodes[['x', 'y', 'z']].values\n        elif isinstance(x, core.Dotprops):\n            data = x.points\n        elif isinstance(x, core.MeshNeuron):\n            data = x.vertices\n        elif isinstance(x, core.VoxelNeuron):\n            data = x.voxels * x.units_xyz.magnitude + x.units_xyz.magnitude / 2\n            data += x.offset\n\n        in_v = in_volume(data,\n                         vol,\n                         mode='IN',\n                         n_rays=n_rays,\n                         validate=False,\n                         backend=backend)\n\n        # If mode is OUT, invert selection\n        if mode == 'OUT':\n            in_v = ~np.array(in_v)\n\n        # Only subset if there are actually nodes to remove\n        if not all(in_v):\n            if isinstance(x, core.TreeNeuron):\n                _ = morpho.subset_neuron(x,\n                                         subset=x.nodes[in_v].node_id.values,\n                                         inplace=True,\n                                         prevent_fragments=prevent_fragments)\n            elif isinstance(x, (core.MeshNeuron, core.Dotprops)):\n                _ = morpho.subset_neuron(x,\n                                         subset=in_v,\n                                         inplace=True,\n                                         prevent_fragments=prevent_fragments)\n            elif isinstance(x, core.VoxelNeuron):\n                values = x.values[in_v]\n                x._data = x.voxels[in_v]\n                x.values = values\n                x._clear_temp_attr()\n\n        return x\n    elif isinstance(x, core.NeuronList):\n        for n in config.tqdm(x, desc='Subsetting',\n                             leave=config.pbar_leave,\n                             disable=config.pbar_hide):\n            in_volume(n, vol, inplace=True, mode=mode, backend=backend,\n                      validate=False, n_rays=n_rays,\n                      prevent_fragments=prevent_fragments)\n\n        return x\n    elif isinstance(x, pd.DataFrame):\n        points = x[['x', 'y', 'z']].values\n    elif isinstance(x, np.ndarray):\n        points = x\n    elif isinstance(x, (list, tuple)):\n        points = np.array(x)\n\n    if points.ndim != 2 or points.shape[1] != 3:  # type: ignore  # does not know about numpy\n        raise ValueError('Points must be array of shape (N,3).')\n\n    for b in backend:\n        if b == 'ncollpyde' and ncollpyde:\n            return in_volume_ncoll(points, vol,\n                                   n_rays=n_rays)\n        elif b == 'pyoctree' and pyoctree:\n            return in_volume_pyoc(points, vol,\n                                  n_rays=n_rays)\n        elif b == 'scipy':\n            return in_volume_convex(points, vol, approximate=False)\n\n    raise ValueError(f'None of the specified backends were available: {backend}')\n</code></pre>"},{"location":"reference/navis/#navis.insert_nodes","title":"<code>navis.insert_nodes</code>","text":"<p>Insert new nodes between existing nodes.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Neuron to insert new nodes into.\n</code></pre> <p> TYPE: <code>        TreeNeuron</code> </p> <code>where</code> <pre><code>    Must be a list of node ID pairs. A new node will be added\n    between the nodes of each pair (see examples).\n</code></pre> <p> TYPE: <code>    list of node pairs</code> </p> <code>coords</code> <pre><code>    Can be:\n     - `None`: new nodes will be inserted exactly between the two\n                 nodes\n     - (N, 3) array of coordinates for the newly inserted nodes\n     - (N, ) array of fractional distances [0-1]: e.g. 0.25 means\n       that a new node will be inserted a quarter of the way between\n       the two nodes (from the child's perspective)\n</code></pre> <p> TYPE: <code>   None | list of (x, y, z) coordinates | list of fractions</code> DEFAULT: <code>None</code> </p> <code>validate</code> <pre><code>    If True, will make sure that pairs in `where` are always\n    in (parent, child) order. If you know this to already be the\n    case, set `validate=False` to save some time.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>True</code> </p> <code>inplace</code> <pre><code>    If True, will rewire the neuron inplace. If False, will return\n    a rewired copy of the neuron.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>TreeNeuron</code> <p>Examples:</p> <p>Insert new nodes between some random points</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; n.n_nodes\n4465\n&gt;&gt;&gt; where = n.nodes[['parent_id', 'node_id']].values[100:200]\n&gt;&gt;&gt; _ = navis.insert_nodes(n, where=where, inplace=True)\n&gt;&gt;&gt; n.n_nodes\n4565\n</code></pre> Source code in <code>navis/graph/graph_utils.py</code> <pre><code>def insert_nodes(\n    x: \"core.TreeNeuron\",\n    where: List[tuple],\n    coords: List[tuple] = None,\n    validate: bool = True,\n    inplace: bool = False,\n) -&gt; Optional[\"core.TreeNeuron\"]:\n    \"\"\"Insert new nodes between existing nodes.\n\n    Parameters\n    ----------\n    x :         TreeNeuron\n                Neuron to insert new nodes into.\n    where :     list of node pairs\n                Must be a list of node ID pairs. A new node will be added\n                between the nodes of each pair (see examples).\n    coords :    None | list of (x, y, z) coordinates | list of fractions\n                Can be:\n                 - `None`: new nodes will be inserted exactly between the two\n                             nodes\n                 - (N, 3) array of coordinates for the newly inserted nodes\n                 - (N, ) array of fractional distances [0-1]: e.g. 0.25 means\n                   that a new node will be inserted a quarter of the way between\n                   the two nodes (from the child's perspective)\n    validate :  bool\n                If True, will make sure that pairs in `where` are always\n                in (parent, child) order. If you know this to already be the\n                case, set `validate=False` to save some time.\n    inplace :   bool\n                If True, will rewire the neuron inplace. If False, will return\n                a rewired copy of the neuron.\n\n    Returns\n    -------\n    TreeNeuron\n\n    Examples\n    --------\n    Insert new nodes between some random points\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; n.n_nodes\n    4465\n    &gt;&gt;&gt; where = n.nodes[['parent_id', 'node_id']].values[100:200]\n    &gt;&gt;&gt; _ = navis.insert_nodes(n, where=where, inplace=True)\n    &gt;&gt;&gt; n.n_nodes\n    4565\n\n    \"\"\"\n    utils.eval_param(x, name=\"x\", allowed_types=(core.TreeNeuron,))\n\n    where = np.asarray(where)\n    if where.ndim != 2 or where.shape[1] != 2:\n        raise ValueError(\n            \"Expected `where` to be a (N, 2) list of pairs. \" f\"Got {where.shape}\"\n        )\n\n    # Validate if that's desired\n    if validate:\n        # Setup to get parents\n        parent = x.nodes.set_index(\"node_id\").parent_id\n\n        # Get parents of the left and the right nodes of each pair\n        parent_left = parent.loc[where[:, 0]].values\n        parent_right = parent.loc[where[:, 1]].values\n\n        # Check if the right node is parent of the left or the other way around\n        correct_order = where[:, 0] == parent_right\n        swapped = where[:, 1] == parent_left\n        not_connected = ~(correct_order | swapped)\n\n        if np.any(not_connected):\n            raise ValueError(\n                \"The following pairs are not connected: \" f\"{where[not_connected]}\"\n            )\n\n        # Flip nodes where necessary to sure we have (parent, child) order\n        if np.any(swapped):\n            where[swapped, :] = where[swapped][:, [1, 0]]\n\n    # If not provided, generate coordinates in the center between each node pair\n    if isinstance(coords, type(None)):\n        node_locs = x.nodes.set_index(\"node_id\")[[\"x\", \"y\", \"z\"]]\n        left_loc = node_locs.loc[where[:, 0]].values\n        right_loc = node_locs.loc[where[:, 1]].values\n\n        # Find center between each node\n        coords = left_loc + (right_loc - left_loc) / 2\n\n    coords = np.asarray(coords)\n    # Make sure we have correct coordinates\n    if coords.shape[0] != where.shape[0]:\n        raise ValueError(\n            f\"Expected {where.shape[0]} coordinates or distances, \"\n            f\"got {coords.shape[0]}\"\n        )\n\n    # If array of fractional distances translate to coordinates\n    if coords.ndim == 1:\n        node_locs = x.nodes.set_index(\"node_id\")[[\"x\", \"y\", \"z\"]]\n        left_loc = node_locs.loc[where[:, 0]].values\n        right_loc = node_locs.loc[where[:, 1]].values\n\n        # Find center between each node\n        coords = left_loc + (right_loc - left_loc) * coords.reshape(-1, 1)\n\n    # For the moment, we will interpolate the radius\n    rad = x.nodes.set_index(\"node_id\").radius\n    new_rad = (rad.loc[where[:, 0]].values + rad.loc[where[:, 1]].values) / 2\n\n    # Generate table for new nodes\n    new_nodes = pd.DataFrame()\n    max_id = x.nodes.node_id.max() + 1\n    new_nodes[\"node_id\"] = np.arange(max_id, max_id + where.shape[0]).astype(int)\n    new_nodes[\"parent_id\"] = where[:, 0]\n    new_nodes[\"x\"] = coords[:, 0]\n    new_nodes[\"y\"] = coords[:, 1]\n    new_nodes[\"z\"] = coords[:, 2]\n    new_nodes[\"radius\"] = new_rad\n\n    # Merge tables\n    nodes = pd.concat(\n        [x.nodes, new_nodes], join=\"outer\", axis=0, sort=True, ignore_index=True\n    )\n\n    # Remap nodes\n    new_parents = dict(zip(where[:, 1], new_nodes.node_id.values))\n    to_rewire = nodes.node_id.isin(new_parents)\n    nodes.loc[to_rewire, \"parent_id\"] = nodes.loc[to_rewire, \"node_id\"].map(new_parents).values.astype(\n        nodes.dtypes[\"parent_id\"], copy=False\n    )\n\n    if not inplace:\n        x = x.copy()\n\n    x._nodes = nodes\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.inspect_h5","title":"<code>navis.inspect_h5</code>","text":"<p>Extract basic info from Hdf5 file.</p> PARAMETER DESCRIPTION <code>filepath</code> <pre><code>                Path to HDF5 file.\n</code></pre> <p> TYPE: <code>             str</code> </p> <code>inspect_neurons</code> <pre><code>                If True, will return info about the neurons contained.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>True</code> </p> <code>inspect_annotations</code> <pre><code>                If True, include info about annotations associated\n                with each neuron.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>Returns a dictionary with basic info about the file. An example::</p> <p>{   'format_spec': 'hnf_v1', # format specifier   'neurons': {               'someID': {'skeleton': True,                          'mesh': False,                          'dotprops': True,                          'annotations': ['connectors']},               'someID2': {'skeleton': False,                           'mesh': False,                           'dotprops': True,                           'annotations': ['connectors']}              }     }</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def inspect_h5(filepath, inspect_neurons=True, inspect_annotations=True):\n    \"\"\"Extract basic info from Hdf5 file.\n\n    Parameters\n    ----------\n    filepath :              str\n                            Path to HDF5 file.\n    inspect_neurons :       bool\n                            If True, will return info about the neurons contained.\n    inspect_annotations :   bool\n                            If True, include info about annotations associated\n                            with each neuron.\n\n    Returns\n    -------\n    dict\n                        Returns a dictionary with basic info about the file.\n                        An example::\n\n                         {\n                          'format_spec': 'hnf_v1', # format specifier\n                          'neurons': {\n                                      'someID': {'skeleton': True,\n                                                 'mesh': False,\n                                                 'dotprops': True,\n                                                 'annotations': ['connectors']},\n                                      'someID2': {'skeleton': False,\n                                                  'mesh': False,\n                                                  'dotprops': True,\n                                                  'annotations': ['connectors']}\n                                     }\n                            }\n\n\n    \"\"\"\n    if not isinstance(filepath, str):\n        raise TypeError(f'`filepath` must be str, got \"{type(filepath)}\"')\n\n    if not os.path.isfile(filepath):\n        raise ValueError(f'{filepath} does not exist')\n\n    info = dict()\n    with h5py.File(filepath, 'r') as f:\n        info['format_spec'] = f.attrs.get('format_spec')\n        # R strings are automatically stored as vectors\n        info['format_spec'] = utils.make_non_iterable(info['format_spec'])\n\n        if inspect_neurons:\n            info['neurons'] = {}\n            # Go over all top level groups\n            for id, grp in f.items():\n                # Skip if not a group\n                if not isinstance(grp, h5py.Group):\n                    continue\n\n                # Do not change this test\n                this = {}\n                if 'skeleton' in grp:\n                    this['skeleton'] = True\n                if 'mesh' in grp:\n                    this['mesh'] = True\n                if 'dotprops' in grp:\n                    this['dotprops'] = True\n\n                if this:\n                    info['neurons'][id] = this\n                    if inspect_annotations:\n                        annotations = grp.get('annotations', None)\n                        if annotations:\n                            info['neurons'][id]['annotations'] = list(annotations.keys())\n\n    return info\n</code></pre>"},{"location":"reference/navis/#navis.intersection_matrix","title":"<code>navis.intersection_matrix</code>","text":"<p>Compute intersection matrix between a set of neurons and volumes.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>          Neuron(s) to intersect.\n</code></pre> <p> TYPE: <code>              NeuronList | single neuron</code> </p> <code>volume</code> <p> TYPE: <code>         list or dict of navis.Volume</code> </p> <code>attr</code> <pre><code>          Attribute to return for intersected neurons (e.g.\n          'cable_length' for TreeNeurons). If None, will return\n          the neuron subset to the volumes.\n</code></pre> <p> TYPE: <code>           str | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <pre><code>          Keyword arguments passed to [`navis.in_volume`][].\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>pandas DataFrame</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; # Grab neurons\n&gt;&gt;&gt; nl = navis.example_neurons(3)\n&gt;&gt;&gt; # Grab a single volume\n&gt;&gt;&gt; lh = navis.example_volume(\"LH\")\n&gt;&gt;&gt; # Re-use for testing\n&gt;&gt;&gt; vols = {'lh1': lh, 'lh2': lh}\n&gt;&gt;&gt; # Generate intersection matrix with cable length\n&gt;&gt;&gt; m = navis.intersection_matrix(nl, vols, attr='cable_length')\n</code></pre> Source code in <code>navis/intersection/intersect.py</code> <pre><code>def intersection_matrix(x: 'core.NeuronObject',\n                        volumes: Union[List[core.Volume],\n                                       Dict[str, core.Volume]],\n                        attr: Optional[str] = None,\n                        **kwargs\n                        ) -&gt; pd.DataFrame:\n    \"\"\"Compute intersection matrix between a set of neurons and volumes.\n\n    Parameters\n    ----------\n    x :               NeuronList | single neuron\n                      Neuron(s) to intersect.\n    volume :          list or dict of navis.Volume\n    attr :            str | None, optional\n                      Attribute to return for intersected neurons (e.g.\n                      'cable_length' for TreeNeurons). If None, will return\n                      the neuron subset to the volumes.\n    **kwargs\n                      Keyword arguments passed to [`navis.in_volume`][].\n\n    Returns\n    -------\n    pandas DataFrame\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; # Grab neurons\n    &gt;&gt;&gt; nl = navis.example_neurons(3)\n    &gt;&gt;&gt; # Grab a single volume\n    &gt;&gt;&gt; lh = navis.example_volume(\"LH\")\n    &gt;&gt;&gt; # Re-use for testing\n    &gt;&gt;&gt; vols = {'lh1': lh, 'lh2': lh}\n    &gt;&gt;&gt; # Generate intersection matrix with cable length\n    &gt;&gt;&gt; m = navis.intersection_matrix(nl, vols, attr='cable_length')\n\n    \"\"\"\n    # Volumes should be a dict at some point\n    volumes_dict: Dict[str, core.Volume]\n\n    if isinstance(x, core.BaseNeuron):\n        x = core.NeuronList(x)\n\n    if not isinstance(x, core.NeuronList):\n        raise TypeError(f'x must be Neuron/List, not \"{type(x)}\"')\n\n    if not isinstance(volumes, (list, dict)):\n        raise TypeError('Volumes must be given as list or dict, not '\n                        f'\"{type(volumes)}\"')\n\n    if isinstance(volumes, list):\n        volumes_dict = {v.name: v for v in volumes}\n    else:\n        volumes_dict = volumes\n\n    for v in volumes_dict.values():\n        if not isinstance(v, core.Volume):\n            raise TypeError(f'Wrong data type found in volumes: \"{type(v)}\"')\n\n    data = in_volume(x, volumes_dict, inplace=False, **kwargs)\n\n    if not attr:\n        df = pd.DataFrame([[n for n in data[v]] for v in data],\n                          index=list(data.keys()),\n                          columns=x.id)\n    else:\n        df = pd.DataFrame([[getattr(n, attr) for n in data[v]] for v in data],\n                          index=list(data.keys()),\n                          columns=x.id)\n\n    return df\n</code></pre>"},{"location":"reference/navis/#navis.ivscc_features","title":"<code>navis.ivscc_features</code>","text":"<p>Calculate IVSCC features for neuron(s).</p> <p>Please see the <code>IVSCC</code> tutorial for details.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>                Neuron(s) to calculate IVSCC for.\n</code></pre> <p> TYPE: <code>                    TreeNeuron | NeuronList</code> </p> <code>features</code> <pre><code>                Provide specific features to calculate.\n                Must be subclasses of `BasicFeatures`.\n                If `None`, will use default features.\n</code></pre> <p> TYPE: <code>             Sequence[Features]</code> DEFAULT: <code>None</code> </p> <code>missing_compartments</code> <pre><code>                What to do if a neuron is missing a compartment\n                (e.g. no axon or basal dendrite):\n                 - \"ignore\" (default): ignore that compartment\n                 - \"skip\": skip the entire neuron\n                 - \"raise\": raise an exception\n</code></pre> <p> TYPE: <code>ignore | skip | 'raise'</code> DEFAULT: <code>'ignore'</code> </p> RETURNS DESCRIPTION <code>ivscc</code> <p>IVSCC features for the neuron(s).</p> <p> TYPE: <code>pd.DataFrame</code> </p> Source code in <code>navis/morpho/ivscc.py</code> <pre><code>def ivscc_features(\n    x: \"core.TreeNeuron\",\n    features=None,\n    missing_compartments=\"ignore\",\n    verbose=False,\n    progress=True,\n) -&gt; Union[float, pd.DataFrame]:\n    \"\"\"Calculate IVSCC features for neuron(s).\n\n    Please see the `IVSCC` tutorial for details.\n\n    Parameters\n    ----------\n    x :                     TreeNeuron | NeuronList\n                            Neuron(s) to calculate IVSCC for.\n    features :              Sequence[Features], optional\n                            Provide specific features to calculate.\n                            Must be subclasses of `BasicFeatures`.\n                            If `None`, will use default features.\n    missing_compartments : \"ignore\" | \"skip\" | \"raise\"\n                            What to do if a neuron is missing a compartment\n                            (e.g. no axon or basal dendrite):\n                             - \"ignore\" (default): ignore that compartment\n                             - \"skip\": skip the entire neuron\n                             - \"raise\": raise an exception\n\n    Returns\n    -------\n    ivscc :                 pd.DataFrame\n                            IVSCC features for the neuron(s).\n\n    \"\"\"\n\n    if isinstance(x, core.TreeNeuron):\n        x = core.NeuronList([x])\n\n    if features is None:\n        features = DEFAULT_FEATURES\n\n    data = {}\n    for n in config.tqdm(\n        x, desc=\"Calculating IVSCC features\", disable=not progress or config.pbar_hide\n    ):\n        data[n.id] = {}\n        for feat in features:\n            try:\n                f = feat(n, verbose=verbose)\n            except CompartmentNotFoundError as e:\n                if missing_compartments == \"ignore\":\n                    continue\n                elif missing_compartments == \"skip\":\n                    if verbose:\n                        print(f\"Skipping neuron {n.id}: {e}\")\n                    data.pop(n.id)\n                    break\n                else:\n                    raise e\n\n            data[n.id].update(f.extract_features())\n\n    return pd.DataFrame(data)\n</code></pre>"},{"location":"reference/navis/#navis.longest_neurite","title":"<code>navis.longest_neurite</code>","text":"<p>Return a neuron consisting of only the longest neurite(s).</p> <p>Based on geodesic distances.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            Neuron(s) to prune.\n</code></pre> <p> TYPE: <code>                TreeNeuron | NeuronList</code> </p> <code>n</code> <pre><code>            Number of longest neurites to preserve. For example:\n             - `n=1` keeps the longest neurites\n             - `n=2` keeps the two longest neurites\n             - `n=slice(1, None)` removes the longest neurite\n</code></pre> <p> TYPE: <code>                int | slice</code> DEFAULT: <code>1</code> </p> <code>reroot_soma</code> <pre><code>            If True, neuron will be rerooted to soma.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> <code>from_root</code> <pre><code>            If True, will look for longest neurite from root.\n            If False, will look for the longest neurite between any\n            two tips.\n</code></pre> <p> TYPE: <code>        bool</code> DEFAULT: <code>True</code> </p> <code>inverse</code> <pre><code>            If True, will instead *remove* the longest neurite.\n</code></pre> <p> TYPE: <code>          bool</code> DEFAULT: <code>False</code> </p> <code>inplace</code> <pre><code>            If False, copy of the neuron will be trimmed down to\n            longest neurite and returned.\n</code></pre> <p> TYPE: <code>          bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>TreeNeuron / List</code> <p>Pruned neuron.</p> See Also <p><code>navis.split_into_fragments</code>         Split neuron into fragments based on longest neurites.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; # Keep only the longest neurite\n&gt;&gt;&gt; ln1 = navis.longest_neurite(n, n=1, reroot_soma=True)\n&gt;&gt;&gt; # Keep the two longest neurites\n&gt;&gt;&gt; ln2 = navis.longest_neurite(n, n=2, reroot_soma=True)\n&gt;&gt;&gt; # Keep everything but the longest neurite\n&gt;&gt;&gt; ln3 = navis.longest_neurite(n, n=slice(1, None), reroot_soma=True)\n</code></pre> Source code in <code>navis/graph/graph_utils.py</code> <pre><code>@utils.map_neuronlist(desc=\"Pruning\", allow_parallel=True)\n@utils.meshneuron_skeleton(method=\"subset\")\ndef longest_neurite(\n    x: \"core.NeuronObject\",\n    n: int = 1,\n    reroot_soma: bool = False,\n    from_root: bool = True,\n    inverse: bool = False,\n    inplace: bool = False,\n) -&gt; \"core.NeuronObject\":\n    \"\"\"Return a neuron consisting of only the longest neurite(s).\n\n    Based on geodesic distances.\n\n    Parameters\n    ----------\n    x :                 TreeNeuron | NeuronList\n                        Neuron(s) to prune.\n    n :                 int | slice\n                        Number of longest neurites to preserve. For example:\n                         - `n=1` keeps the longest neurites\n                         - `n=2` keeps the two longest neurites\n                         - `n=slice(1, None)` removes the longest neurite\n    reroot_soma :       bool\n                        If True, neuron will be rerooted to soma.\n    from_root :         bool\n                        If True, will look for longest neurite from root.\n                        If False, will look for the longest neurite between any\n                        two tips.\n    inverse :           bool\n                        If True, will instead *remove* the longest neurite.\n    inplace :           bool\n                        If False, copy of the neuron will be trimmed down to\n                        longest neurite and returned.\n\n    Returns\n    -------\n    TreeNeuron/List\n                        Pruned neuron.\n\n    See Also\n    --------\n    [`navis.split_into_fragments`][]\n            Split neuron into fragments based on longest neurites.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; # Keep only the longest neurite\n    &gt;&gt;&gt; ln1 = navis.longest_neurite(n, n=1, reroot_soma=True)\n    &gt;&gt;&gt; # Keep the two longest neurites\n    &gt;&gt;&gt; ln2 = navis.longest_neurite(n, n=2, reroot_soma=True)\n    &gt;&gt;&gt; # Keep everything but the longest neurite\n    &gt;&gt;&gt; ln3 = navis.longest_neurite(n, n=slice(1, None), reroot_soma=True)\n\n    \"\"\"\n    if not isinstance(x, core.TreeNeuron):\n        raise TypeError(f'Expected TreeNeuron(s), got \"{type(x)}\"')\n\n    if isinstance(n, numbers.Number) and n &lt; 1:\n        raise ValueError(\"Number of longest neurites to preserve must be &gt;=1\")\n\n    # At this point x is TreeNeuron\n    x: core.TreeNeuron\n\n    if not inplace:\n        x = x.copy()\n\n    if not from_root:\n        # Find the two most distal points (N.B. roots can also be \"ends\")\n        leafs = x.nodes.loc[x.nodes.type.isin((\"root\", \"end\")), 'node_id'].values\n        dists = geodesic_matrix(x, from_=leafs)[leafs]\n\n        # If the neuron is fragmented, we will have infinite distances\n        dists[dists == np.inf] = -1\n\n        # This might be multiple values\n        mx = np.where(dists == np.max(dists.values))\n        start = dists.columns[mx[0][0]]  # translate to node ID\n\n        # Reroot to one of the nodes that gives the longest distance\n        x.reroot(start, inplace=True)\n    elif reroot_soma and not isinstance(x.soma, type(None)):\n        x.reroot(x.soma, inplace=True)\n\n    segments = _generate_segments(x, weight=\"weight\")\n\n    if isinstance(n, (int, np.integer)):\n        tn_to_preserve: List[int] = [tn for s in segments[:n] for tn in s]\n    elif isinstance(n, slice):\n        tn_to_preserve = [tn for s in segments[n] for tn in s]\n    else:\n        raise TypeError(f'Unable to use `n` of type \"{type(n)}\"')\n\n    if not inverse:\n        _ = morpho.subset_neuron(x, tn_to_preserve, inplace=True)\n    else:\n        _ = morpho.subset_neuron(\n            x, ~np.isin(x.nodes.node_id.values, tn_to_preserve), inplace=True\n        )\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.make_dotprops","title":"<code>navis.make_dotprops</code>","text":"<p>Produce dotprops from neurons or point clouds.</p> <p>This is following the implementation in R's <code>nat</code> library.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Data/object to generate dotprops from. DataFrame must have\n    'x', 'y' and 'z' columns.\n</code></pre> <p> TYPE: <code>        Neuron | NeuronList | pandas.DataFrame | numpy.ndarray</code> </p> <code>k</code> <pre><code>    Number of nearest neighbours to use for tangent vector\n    calculation:\n\n      - `k=0` or `k=None` is possible but only for\n        `TreeNeurons` where we then use the midpoints between\n        child -&gt; parent nodes and their vectors\n      - `k` is only guaranteed if the input has at least `k`\n        points\n      - `k` includes self-hits and while `k=1` is not\n        strictly forbidden, it makes little sense and will\n        likely produce nonsense dotprops\n</code></pre> <p> TYPE: <code>        int (&gt; 1)</code> DEFAULT: <code>20</code> </p> <code>resample</code> <pre><code>    If provided will resample neurons to the given resolution:\n\n      - for `MeshNeurons`, `VoxelNeurons` and point clouds, we are using\n        `trimesh.points.remove_close` to remove surface vertices\n        closer than the given resolution. Note that this is only\n        approximate and also means that `Mesh/VoxelNeurons`\n        can not be up-sampled!\n      - if the neuron has `.units` set you can also provide this\n        as string, e.g. \"1 micron\".\n</code></pre> <p> TYPE: <code> float | int | str</code> DEFAULT: <code>False</code> </p> <code>threshold</code> <pre><code>    Only for `VoxelNeurons`: determines which voxels will be\n    converted to dotprops points.\n</code></pre> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>on_issue</code> <pre><code>    What to do if issues are found during processing:\n      - \"warn\": log a warning, fix the issue and continue\n      - \"raise\": raise an error and stop\n      - \"fix\": silently fix the issue and continue\n</code></pre> <p> TYPE: <code> \"warn\" (default) | \"raise\" | \"fix\"</code> DEFAULT: <code>'warn'</code> </p> <code>make_using</code> <pre><code>        Function or class used to construct dotprops. Must take\n        `points`, `alpha` and `vect` as inputs. By default, will use\n        `navis.Dotprops` but you can use this to e.g. produce\n        custom subclasses.\n</code></pre> <p> TYPE: <code>   function | class</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>navis.Dotprops</code> <p>If input is multiple neurons, will return a <code>navis.NeuronList</code> of <code>navis.Dotprops</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; dp = navis.make_dotprops(n)\n&gt;&gt;&gt; dp\ntype        navis.Dotprops\nname             DA1_lPN_R\nid              1734350788\nk                       20\nunits          8 nanometer\nn_points              4465\ndtype: object\n</code></pre> Source code in <code>navis/core/core_utils.py</code> <pre><code>@utils.map_neuronlist(desc=\"Dotprops\", allow_parallel=True)\ndef make_dotprops(\n    x: Union[\n        pd.DataFrame,\n        np.ndarray,\n        \"core.TreeNeuron\",\n        \"core.MeshNeuron\",\n        \"core.VoxelNeuron\",\n        \"core.NeuronList\",\n    ],\n    k: int = 20,\n    resample: Union[float, int, bool, str] = False,\n    threshold: float = None,\n    on_issue: Union[Literal[\"warn\", \"raise\", \"fix\"]] = \"warn\",\n    make_using: Optional = None,\n) -&gt; Union[\"core.Dotprops\", \"core.NeuronList\"]:\n    \"\"\"Produce dotprops from neurons or point clouds.\n\n    This is following the implementation in R's `nat` library.\n\n    Parameters\n    ----------\n    x :         Neuron | NeuronList | pandas.DataFrame | numpy.ndarray\n                Data/object to generate dotprops from. DataFrame must have\n                'x', 'y' and 'z' columns.\n    k :         int (&gt; 1), optional\n                Number of nearest neighbours to use for tangent vector\n                calculation:\n\n                  - `k=0` or `k=None` is possible but only for\n                    `TreeNeurons` where we then use the midpoints between\n                    child -&gt; parent nodes and their vectors\n                  - `k` is only guaranteed if the input has at least `k`\n                    points\n                  - `k` includes self-hits and while `k=1` is not\n                    strictly forbidden, it makes little sense and will\n                    likely produce nonsense dotprops\n\n    resample :  float | int | str, optional\n                If provided will resample neurons to the given resolution:\n\n                  - for `MeshNeurons`, `VoxelNeurons` and point clouds, we are using\n                    `trimesh.points.remove_close` to remove surface vertices\n                    closer than the given resolution. Note that this is only\n                    approximate and also means that `Mesh/VoxelNeurons`\n                    can not be up-sampled!\n                  - if the neuron has `.units` set you can also provide this\n                    as string, e.g. \"1 micron\".\n\n    threshold : float, optional\n                Only for `VoxelNeurons`: determines which voxels will be\n                converted to dotprops points.\n    on_issue :  \"warn\" (default) | \"raise\" | \"fix\"\n                What to do if issues are found during processing:\n                  - \"warn\": log a warning, fix the issue and continue\n                  - \"raise\": raise an error and stop\n                  - \"fix\": silently fix the issue and continue\n    make_using :    function | class, optional\n                    Function or class used to construct dotprops. Must take\n                    `points`, `alpha` and `vect` as inputs. By default, will use\n                    `navis.Dotprops` but you can use this to e.g. produce\n                    custom subclasses.\n\n    Returns\n    -------\n    navis.Dotprops\n\n                If input is multiple neurons, will return a\n                [`navis.NeuronList`][] of [`navis.Dotprops`][].\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; dp = navis.make_dotprops(n)\n    &gt;&gt;&gt; dp\n    type        navis.Dotprops\n    name             DA1_lPN_R\n    id              1734350788\n    k                       20\n    units          8 nanometer\n    n_points              4465\n    dtype: object\n\n    \"\"\"\n    if k and k == 1:\n        logger.warning(\"`k=1` is likely to produce nonsense dotprops\")\n\n    utils.eval_param(\n        resample, name=\"resample\", allowed_types=(numbers.Number, type(None), str)\n    )\n    utils.eval_param(on_issue, name=\"on_issue\", allowed_values=(\"warn\", \"raise\", \"fix\"))\n\n    if make_using is None:\n        make_using = core.Dotprops\n    elif not isinstance(make_using, type) and not callable(make_using):\n        make_using = make_using.__class__\n\n    properties = {}\n    if isinstance(x, pd.DataFrame):\n        if not all(np.isin([\"x\", \"y\", \"z\"], x.columns)):\n            raise ValueError('DataFrame must contain \"x\", \"y\" and \"z\" columns.')\n        x = x[[\"x\", \"y\", \"z\"]].values\n    elif isinstance(x, core.TreeNeuron):\n        if resample:\n            x = x.resample(resample_to=resample, inplace=False)\n        properties.update({\"units\": x.units, \"name\": x.name, \"id\": x.id})\n\n        if isinstance(k, type(None)) or k &lt;= 0:\n            points, vect, length = graph.neuron2tangents(x)\n            return make_using(\n                points=points,\n                vect=vect,\n                length=length,\n                alpha=None,\n                k=None,\n                **properties,\n            )\n\n        x = x.nodes[[\"x\", \"y\", \"z\"]].values\n    elif isinstance(x, core.MeshNeuron):\n        properties.update({\"units\": x.units, \"name\": x.name, \"id\": x.id})\n        x = x.vertices\n        if resample:\n            x, _ = tm.points.remove_close(x, resample)\n    elif isinstance(x, core.Dotprops):\n        properties.update({\"units\": x.units, \"name\": x.name, \"id\": x.id})\n        x = x.points\n        if resample:\n            x, _ = tm.points.remove_close(x, resample)\n    elif isinstance(x, core.VoxelNeuron):\n        properties.update({\"name\": x.name, \"id\": x.id})\n        if not x.units.dimensionless:\n            # We are scaling the units - hence all are set to 1\n            properties[\"units\"] = [f\"1 {u.units}\" for u in x.units_xyz]\n\n        if threshold:\n            x = x.voxels[x.values &gt;= threshold] * x.units.magnitude\n        else:\n            x = x.voxels * x.units.magnitude\n\n        if resample:\n            x, _ = tm.points.remove_close(x, resample)\n    elif isinstance(x, np.ndarray) and resample:\n        x, _ = tm.points.remove_close(x, resample)\n\n    if not isinstance(x, np.ndarray):\n        raise TypeError(f'Unable to generate dotprops from data of type \"{type(x)}\"')\n\n    if x.ndim != 2 or x.shape[1] != 3:\n        raise ValueError(f\"Expected input of shape (N, 3), got {x.shape}\")\n\n    if isinstance(k, type(None)) or k &lt;= 0:\n        raise ValueError(\"`k` must be &gt; 0 when converting non-TreeNeurons to Dotprops.\")\n\n    # Drop rows with NAs\n    contains_nan = np.any(np.isnan(x), axis=1)\n    if contains_nan.any():\n        if on_issue == \"raise\":\n            raise ValueError(\n                \"Input coordinates contains NaNs. Please remove them before \"\n                \"calculating dotprops, or set `on_issue` to 'fix' or 'warn'.\"\n            )\n        elif on_issue == \"warn\":\n            logger.warning(\n                f\"Dropping {np.sum(contains_nan)} coordinates containing NaNs. \"\n                \"You can silence warnings in `make_dotprops` by setting `on_issue='fix'`.\"\n            )\n        x = x[~contains_nan]\n\n    # Checks and balances\n    n_points = x.shape[0]\n\n    # Make sure we don't ask for more nearest neighbors than we have points\n    if k &gt; n_points:\n        if on_issue == \"raise\":\n            raise ValueError(\n                f\"Requested {k} nearest-neighbors but only {n_points} points \"\n                \"are available. Please reduce `k` (or provide more points).\"\n            )\n        elif on_issue == \"warn\":\n            logger.warning(\n                f\"Requested {k} nearest-neighbors but only {n_points} points \"\n                \"are available. Reducing `k` to match number of points. \"\n                \"You can silence warnings in `make_dotprops` by setting `on_issue='fix'`.\"\n            )\n        k = min(n_points, k)\n\n    properties[\"k\"] = k\n\n    # Create the KDTree and get the k-nearest neighbors for each point\n    tree = cKDTree(x)\n    dist, ix = tree.query(x, k=k)\n\n    # If we have duplicated x/y/z coordinates, we may end up with garbage\n    # vectors (1, 0, 0) and alpha values (NaN). This doesn't have to cause\n    # issues further down the line but it is better to warn the user.\n    max_zero = dist.max(axis=1) == 0\n    if max_zero.any():\n        n_zero = np.sum(max_zero)\n        if on_issue == \"raise\":\n            raise ValueError(\n                f\"Found {n_zero} points with zero distance to all k-nearest-neighbors. \"\n                \"This means that the input data contains duplicate coordinates and will \"\n                \"result in nonsense dotprops vectors and alpha values for these points. \"\n                \"While this does not have to cause issues further down the line, you \"\n                \"may still want to consider removing duplicate coordinates from your \"\n                \"input data.\"\n            )\n        elif on_issue == \"warn\":\n            logger.warning(\n                f\"Dropping {n_zero} points with zero distance to all k-nearest-neighbors. \"\n                \"This is likely the result of duplicate coordinates in the input data. \"\n                \"You can silence warnings in `make_dotprops` by setting `on_issue='fix'`.\"\n            )\n\n        x = x[~max_zero]\n        ix = ix[~max_zero] - max_zero.sum()  # Adjust indices\n\n    # This makes sure we have (N, k) shaped array even if k = 1\n    ix = ix.reshape(x.shape[0], k)\n\n    # Get points: array of (N, k, 3)\n    pt = x[ix]\n\n    # Generate centers for each cloud of k nearest neighbors\n    centers = np.mean(pt, axis=1)\n\n    # Generate vector from center\n    cpt = pt - centers.reshape((pt.shape[0], 1, 3))\n\n    # Get inertia (N, 3, 3)\n    inertia = cpt.transpose((0, 2, 1)) @ cpt\n\n    # Extract vector and alpha\n    u, s, vh = np.linalg.svd(inertia)\n    vect = vh[:, 0, :]\n    alpha = (s[:, 0] - s[:, 1]) / np.sum(s, axis=1)\n\n    return make_using(points=x, alpha=alpha, vect=vect, **properties)\n</code></pre>"},{"location":"reference/navis/#navis.mesh","title":"<code>navis.mesh</code>","text":"<p>Generate mesh from object(s).</p> <p>VoxelNeurons or (N, 3) arrays of voxel coordinates will be meshed using a marching cubes algorithm. TreeNeurons will be meshed by creating cylinders using the radii.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Object to mesh. See notes above.\n</code></pre> <p> TYPE: <code>            VoxelNeuron | (N, 3) np.array | TreeNeuron</code> </p> <code>**kwargs</code> <pre><code>        Keyword arguments are passed through to the respective\n        converters: [`navis.conversion.voxels2mesh`][] and\n        [`navis.conversion.tree2meshneuron`][], respectively.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>mesh</code> <p>Returns a trimesh or MeshNeuron depending on the input. Data tables (e.g. <code>connectors</code>) are not carried over from input neuron.</p> <p> TYPE: <code>trimesh.Trimesh | MeshNeuron</code> </p> Source code in <code>navis/conversion/wrappers.py</code> <pre><code>@utils.map_neuronlist(desc='Meshing', allow_parallel=True)\ndef mesh(x: Union['core.VoxelNeuron', np.ndarray, 'core.TreeNeuron'],\n         **kwargs) -&gt; Union[tm.Trimesh, 'core.MeshNeuron']:\n    \"\"\"Generate mesh from object(s).\n\n    VoxelNeurons or (N, 3) arrays of voxel coordinates will be meshed using\n    a marching cubes algorithm. TreeNeurons will be meshed by creating\n    cylinders using the radii.\n\n    Parameters\n    ----------\n    x :             VoxelNeuron | (N, 3) np.array | TreeNeuron\n                    Object to mesh. See notes above.\n    **kwargs\n                    Keyword arguments are passed through to the respective\n                    converters: [`navis.conversion.voxels2mesh`][] and\n                    [`navis.conversion.tree2meshneuron`][], respectively.\n\n    Returns\n    -------\n    mesh :          trimesh.Trimesh | MeshNeuron\n                    Returns a trimesh or MeshNeuron depending on the input.\n                    Data tables (e.g. `connectors`) are not carried over from\n                    input neuron.\n\n    \"\"\"\n    if isinstance(x, core.VoxelNeuron) or (isinstance(x, np.ndarray) and x.ndims == 2 and x.shape[1] == 3):\n        return voxels2mesh(x, **kwargs)\n    elif isinstance(x, core.TreeNeuron):\n        return tree2meshneuron(x, **kwargs)\n\n    raise TypeError(f'Unable to create mesh from data of type {type(x)}')\n</code></pre>"},{"location":"reference/navis/#navis.mirror","title":"<code>navis.mirror</code>","text":"<p>Mirror 3D coordinates about given axis.</p> <p>This is a lower level version of <code>navis.mirror_brain</code> that:  1. Flips object along midpoint of axis using a affine transformation.  2. (Optional) Applies a warp transform that corrects asymmetries.</p> PARAMETER DESCRIPTION <code>points</code> <pre><code>            3D coordinates to mirror.\n</code></pre> <p> TYPE: <code>           (N, 3) numpy array</code> </p> <code>mirror_axis_size</code> <pre><code>            A single number specifying the size of the mirror axis.\n            This is used to find the midpoint to mirror about.\n</code></pre> <p> TYPE: <code> int | float</code> </p> <code>mirror_axis</code> <pre><code>            Axis to mirror. Defaults to `x`.\n</code></pre> <p> TYPE: <code>      'x' | 'y' | 'z'</code> DEFAULT: <code>'x'</code> </p> <code>warp</code> <pre><code>            If provided, will apply this warp transform after the\n            affine flipping. Typically this will be a mirror\n            registration to compensate for left/right asymmetries.\n</code></pre> <p> TYPE: <code>             Transform</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>points_mirrored</code> <p>Mirrored coordinates.</p> See Also <p><code>navis.mirror_brain</code>                 Higher level function that uses meta data from registered                 template brains to transform data for you.</p> Source code in <code>navis/transforms/xfm_funcs.py</code> <pre><code>def mirror(points: np.ndarray, mirror_axis_size: float,\n           mirror_axis: str = 'x',\n           warp: Optional['BaseTransform'] = None) -&gt; np.ndarray:\n    \"\"\"Mirror 3D coordinates about given axis.\n\n    This is a lower level version of `navis.mirror_brain` that:\n     1. Flips object along midpoint of axis using a affine transformation.\n     2. (Optional) Applies a warp transform that corrects asymmetries.\n\n    Parameters\n    ----------\n    points :            (N, 3) numpy array\n                        3D coordinates to mirror.\n    mirror_axis_size :  int | float\n                        A single number specifying the size of the mirror axis.\n                        This is used to find the midpoint to mirror about.\n    mirror_axis :       'x' | 'y' | 'z', optional\n                        Axis to mirror. Defaults to `x`.\n    warp :              Transform, optional\n                        If provided, will apply this warp transform after the\n                        affine flipping. Typically this will be a mirror\n                        registration to compensate for left/right asymmetries.\n\n    Returns\n    -------\n    points_mirrored\n                        Mirrored coordinates.\n\n    See Also\n    --------\n    [`navis.mirror_brain`][]\n                    Higher level function that uses meta data from registered\n                    template brains to transform data for you.\n\n    \"\"\"\n    utils.eval_param(mirror_axis, name='mirror_axis',\n                     allowed_values=('x', 'y', 'z'), on_error='raise')\n\n    # At this point we expect numpy arrays\n    points = np.asarray(points)\n    if not points.ndim == 2 or points.shape[1] != 3:\n        raise ValueError('Array must be of shape (N, 3).')\n\n    # Translate mirror axis to index\n    mirror_ix = {'x': 0, 'y': 1, 'z': 2}[mirror_axis]\n\n    # Construct homogeneous affine mirroring transform\n    mirrormat = np.eye(4, 4)\n    mirrormat[mirror_ix, 3] = mirror_axis_size\n    mirrormat[mirror_ix, mirror_ix] = -1\n\n    # Turn into affine transform\n    flip_transform = AffineTransform(mirrormat)\n\n    # Flip about mirror axis\n    points_mirrored = flip_transform.xform(points)\n\n    if isinstance(warp, (BaseTransform, TransformSequence)):\n        points_mirrored = warp.xform(points_mirrored)\n\n    # Note that we are enforcing the same data type as the input data here.\n    # This is unlike in `xform` or `xform_brain` where data might genuinely\n    # end up in a space that requires higher precision (e.g. going from\n    # nm to microns).\n    return points_mirrored.astype(points.dtype)\n</code></pre>"},{"location":"reference/navis/#navis.mirror_brain","title":"<code>navis.mirror_brain</code>","text":"<p>Mirror 3D object (neuron, coordinates) about given axis.</p> <p>The way this works is:  1. Look up the length of the template space along the given axis. For this,     the template space has to be registered (see docs for details).  2. Flip object along midpoint of axis using a affine transformation.  3. (Optional) Apply a warp transform that corrects asymmetries.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Data to transform. Dataframe must contain `['x', 'y', 'z']`\n        columns. Numpy array must be shape `(N, 3)`.\n</code></pre> <p> TYPE: <code>            Neuron/List | Volume/trimesh | numpy.ndarray | pandas.DataFrame</code> </p> <code>template</code> <pre><code>        Source template brain space that the data is in. If string\n        will be searched against registered template brains.\n        Alternatively check out [`navis.transforms.mirror`][]\n        for a lower level interface.\n</code></pre> <p> TYPE: <code>     str | TemplateBrain</code> </p> <code>mirror_axis</code> <pre><code>        Axis to mirror. Defaults to `x`.\n</code></pre> <p> TYPE: <code>  'x' | 'y' | 'z'</code> DEFAULT: <code>'x'</code> </p> <code>warp</code> <pre><code>        If 'auto', will check if a non-rigid mirror transformation\n        exists for the given `template` and apply it after the\n        flipping. Alternatively, you can also pass a Transform or\n        TransformSequence directly.\n</code></pre> <p> TYPE: <code>         bool | \"auto\" | Transform</code> DEFAULT: <code>'auto'</code> </p> <code>via</code> <pre><code>        If provided, (e.g. \"FCWB\") will first transform coordinates\n        into that space, then mirror and transform back.\n        Use this if there is no mirror registration for the original\n        template, or to transform to a symmetrical template in which\n        flipping is sufficient.\n</code></pre> <p> TYPE: <code>          str | None</code> DEFAULT: <code>None</code> </p> <code>verbose</code> <pre><code>        If True, will print some useful info on the transform(s).\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> <code>progress</code> <pre><code>        Whether to show a progress bar when mirroring multiple\n        neurons.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>xf</code> <p>Same object type as input (array, neurons, etc) but with transformed coordinates.</p> <p>Examples:</p> <p>This example requires the flybrains library to be installed: <code>pip3 install flybrains</code></p> <p>Also, if you haven't already, you will need to have the optional Saalfeld lab (Janelia Research Campus) transforms installed (this is a one-off):</p> <pre><code>&gt;&gt;&gt; import flybrains\n&gt;&gt;&gt; flybrains.download_jrc_transforms()\n</code></pre> <p>Once <code>flybrains</code> is installed and you have downloaded the registrations, you can run this:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; import flybrains\n&gt;&gt;&gt; # navis example neurons are in raw hemibrain (JRCFIB2018Fraw) space\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; # Mirror about x axis (this is a simple flip in this case)\n&gt;&gt;&gt; mirrored = navis.mirror_brain(n * 8 / 1000, tem plate='JRCFIB2018F', via='JRC2018F')\n&gt;&gt;&gt; # We also need to get back to raw coordinates\n&gt;&gt;&gt; mirrored = mirrored / 8 * 1000\n</code></pre> See Also <p><code>navis.mirror</code>                 Lower level function for mirroring. You can use this if                 you want to mirror data without having a registered                 template for it.</p> Source code in <code>navis/transforms/templates.py</code> <pre><code>def mirror_brain(\n    x: Union[\"core.NeuronObject\", \"pd.DataFrame\", \"np.ndarray\"],\n    template: Union[str, \"TemplateBrain\"],\n    mirror_axis: Union[Literal[\"x\"], Literal[\"y\"], Literal[\"z\"]] = \"x\",\n    warp: Union[Literal[\"auto\"], bool] = \"auto\",\n    via: Optional[str] = None,\n    verbose: bool = False,\n    progress: bool = True,\n) -&gt; Union[\"core.NeuronObject\", \"pd.DataFrame\", \"np.ndarray\"]:\n    \"\"\"Mirror 3D object (neuron, coordinates) about given axis.\n\n    The way this works is:\n     1. Look up the length of the template space along the given axis. For this,\n        the template space has to be registered (see docs for details).\n     2. Flip object along midpoint of axis using a affine transformation.\n     3. (Optional) Apply a warp transform that corrects asymmetries.\n\n    Parameters\n    ----------\n    x :             Neuron/List | Volume/trimesh | numpy.ndarray | pandas.DataFrame\n                    Data to transform. Dataframe must contain `['x', 'y', 'z']`\n                    columns. Numpy array must be shape `(N, 3)`.\n    template :      str | TemplateBrain\n                    Source template brain space that the data is in. If string\n                    will be searched against registered template brains.\n                    Alternatively check out [`navis.transforms.mirror`][]\n                    for a lower level interface.\n    mirror_axis :   'x' | 'y' | 'z', optional\n                    Axis to mirror. Defaults to `x`.\n    warp :          bool | \"auto\" | Transform, optional\n                    If 'auto', will check if a non-rigid mirror transformation\n                    exists for the given `template` and apply it after the\n                    flipping. Alternatively, you can also pass a Transform or\n                    TransformSequence directly.\n    via :           str | None\n                    If provided, (e.g. \"FCWB\") will first transform coordinates\n                    into that space, then mirror and transform back.\n                    Use this if there is no mirror registration for the original\n                    template, or to transform to a symmetrical template in which\n                    flipping is sufficient.\n    verbose :       bool\n                    If True, will print some useful info on the transform(s).\n    progress :      bool\n                    Whether to show a progress bar when mirroring multiple\n                    neurons.\n\n    Returns\n    -------\n    xf\n                    Same object type as input (array, neurons, etc) but with\n                    transformed coordinates.\n\n    Examples\n    --------\n    This example requires the\n    [flybrains](https://github.com/navis-org/navis-flybrains)\n    library to be installed: `pip3 install flybrains`\n\n    Also, if you haven't already, you will need to have the optional Saalfeld\n    lab (Janelia Research Campus) transforms installed (this is a one-off):\n\n    &gt;&gt;&gt; import flybrains                                        # doctest: +SKIP\n    &gt;&gt;&gt; flybrains.download_jrc_transforms()                     # doctest: +SKIP\n\n    Once `flybrains` is installed and you have downloaded the registrations,\n    you can run this:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; import flybrains\n    &gt;&gt;&gt; # navis example neurons are in raw hemibrain (JRCFIB2018Fraw) space\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; # Mirror about x axis (this is a simple flip in this case)\n    &gt;&gt;&gt; mirrored = navis.mirror_brain(n * 8 / 1000, tem plate='JRCFIB2018F', via='JRC2018F') # doctest: +SKIP\n    &gt;&gt;&gt; # We also need to get back to raw coordinates\n    &gt;&gt;&gt; mirrored = mirrored / 8 * 1000                          # doctest: +SKIP\n\n    See Also\n    --------\n    [`navis.mirror`][]\n                    Lower level function for mirroring. You can use this if\n                    you want to mirror data without having a registered\n                    template for it.\n\n    \"\"\"\n    utils.eval_param(\n        mirror_axis,\n        name=\"mirror_axis\",\n        allowed_values=(\"x\", \"y\", \"z\"),\n        on_error=\"raise\",\n    )\n    if not isinstance(warp, (BaseTransform, TransformSequence)):\n        utils.eval_param(\n            warp, name=\"warp\", allowed_values=(\"auto\", True, False), on_error=\"raise\"\n        )\n\n    # If we go via another brain space\n    if via and via != template:\n        # Xform to \"via\" space\n        xf = xform_brain(x, source=template, target=via, verbose=verbose)\n        # Mirror\n        xfm = mirror_brain(\n            xf,\n            template=via,\n            mirror_axis=mirror_axis,\n            warp=warp,\n            progress=progress,\n            via=None,\n        )\n        # Xform back to original template space\n        xfm_inv = xform_brain(xfm, source=via, target=template, verbose=verbose)\n        return xfm_inv\n\n    if isinstance(x, core.NeuronList):\n        if len(x) == 1:\n            x = x[0]\n        else:\n            xf = []\n            for n in config.tqdm(\n                x,\n                desc=\"Mirroring\",\n                disable=config.pbar_hide or not progress,\n                leave=config.pbar_leave,\n            ):\n                xf.append(\n                    mirror_brain(\n                        n, template=template, mirror_axis=mirror_axis, warp=warp\n                    )\n                )\n            return core.NeuronList(xf)\n\n    if isinstance(x, core.BaseNeuron):\n        x = x.copy()\n        if isinstance(x, core.TreeNeuron):\n            x.nodes = mirror_brain(\n                x.nodes, template=template, mirror_axis=mirror_axis, warp=warp\n            )\n        elif isinstance(x, core.Dotprops):\n            if isinstance(x.k, type(None)) or x.k &lt;= 0:\n                # If no k, we need to mirror vectors too. Note that this is less\n                # than ideal though! Here, we are scaling the vector by the\n                # dotprop's sampling resolution (i.e. ideally a representative\n                # distance between the points) because if the vectors are too\n                # small any warping transform will make them go haywire\n                hp = mirror_brain(\n                    x.points + x.vect * x.sampling_resolution * 2,\n                    template=template,\n                    mirror_axis=mirror_axis,\n                    warp=warp,\n                )\n\n            x.points = mirror_brain(\n                x.points, template=template, mirror_axis=mirror_axis, warp=warp\n            )\n\n            if isinstance(x.k, type(None)) or x.k &lt;= 0:\n                # Re-generate vectors\n                vect = x.points - hp\n                vect = vect / np.linalg.norm(vect, axis=1).reshape(-1, 1)\n                x._vect = vect\n            else:\n                # Set tangent vectors and alpha to None so they will be\n                # regenerated on demand\n                x._vect = x._alpha = None\n        elif isinstance(x, core.MeshNeuron):\n            x.vertices = mirror_brain(\n                x.vertices, template=template, mirror_axis=mirror_axis, warp=warp\n            )\n            # We also need to flip the normals\n            x.faces = x.faces[:, ::-1]\n        else:\n            raise TypeError(f\"Don't know how to transform neuron of type '{type(x)}'\")\n\n        if x.has_connectors:\n            x.connectors = mirror_brain(\n                x.connectors, template=template, mirror_axis=mirror_axis, warp=warp\n            )\n        return x\n    elif isinstance(x, tm.Trimesh):\n        x = x.copy()\n        x.vertices = mirror_brain(\n            x.vertices, template=template, mirror_axis=mirror_axis, warp=warp\n        )\n\n        # We also need to flip the normals\n        x.faces = x.faces[:, ::-1]\n        return x\n    elif isinstance(x, pd.DataFrame):\n        if any([c not in x.columns for c in [\"x\", \"y\", \"z\"]]):\n            raise ValueError(\"DataFrame must have x, y and z columns.\")\n        x = x.copy()\n        x[[\"x\", \"y\", \"z\"]] = mirror_brain(\n            x[[\"x\", \"y\", \"z\"]].values,\n            template=template,\n            mirror_axis=mirror_axis,\n            warp=warp,\n        )\n        return x\n    else:\n        try:\n            # At this point we expect numpy arrays\n            x = np.asarray(x)\n        except BaseException:\n            raise TypeError(f'Unable to transform data of type \"{type(x)}\"')\n\n        if not x.ndim == 2 or x.shape[1] != 3:\n            raise ValueError(\"Array must be of shape (N, 3).\")\n\n    if not isinstance(template, str):\n        TypeError(f'Expected template of type str, got \"{type(template)}\"')\n\n    if isinstance(warp, (BaseTransform, TransformSequence)):\n        mirror_trans = warp\n    elif warp:\n        # See if there is a mirror registration\n        mirror_trans = registry.find_mirror_reg(template, non_found=\"ignore\")\n\n        # Get actual transform from tuple\n        if mirror_trans:\n            mirror_trans = mirror_trans.transform\n        # If warp was not \"auto\" and we didn't find a registration, raise\n        elif warp != \"auto\" and not mirror_trans:\n            raise ValueError(f'No mirror transform found for \"{template}\"')\n    else:\n        mirror_trans = None\n\n    # Now find the meta info about the template brain\n    if isinstance(template, TemplateBrain):\n        tb = template\n    else:\n        tb = registry.find_template(template, non_found=\"raise\")\n\n    # Get the bounding box\n    if not hasattr(tb, \"boundingbox\"):\n        raise ValueError(f'Template \"{tb.label}\" has no bounding box info.')\n\n    if not isinstance(tb.boundingbox, (list, tuple, np.ndarray)):\n        raise TypeError(\n            \"Expected the template brain's bounding box to be a \"\n            f\"list, tuple or array - got '{type(tb.boundingbox)}'\"\n        )\n\n    # Get bounding box of template brain\n    bbox = np.asarray(tb.boundingbox)\n\n    # Reshape if flat array\n    if bbox.ndim == 1:\n        bbox = bbox.reshape(3, 2)\n\n    # Index of mirror axis\n    ix = {\"x\": 0, \"y\": 1, \"z\": 2}[mirror_axis]\n\n    if bbox.shape == (3, 2):\n        # In nat.templatebrains this is using the sum (min+max) but have a\n        # suspicion that this should be the difference (max-min)\n        mirror_axis_size = bbox[ix, :].sum()\n    elif bbox.shape == (2, 3):\n        mirror_axis_size = bbox[:, ix].sum()\n    else:\n        raise ValueError(\n            f\"Expected bounding box to be of shape (3, 2) or (2, 3) got {bbox.shape}\"\n        )\n\n    return mirror(\n        x, mirror_axis=mirror_axis, mirror_axis_size=mirror_axis_size, warp=mirror_trans\n    )\n</code></pre>"},{"location":"reference/navis/#navis.nblast","title":"<code>navis.nblast</code>","text":"<p>NBLAST query against target neurons.</p> <p>This implements the NBLAST algorithm from Costa et al. (2016) (see references) and mirror the implementation in R's <code>nat.nblast</code> (https://github.com/natverse/nat.nblast).</p> PARAMETER DESCRIPTION <code>query</code> <pre><code>        Query neuron(s) to NBLAST against the targets. Neurons\n        should be in microns as NBLAST is optimized for that and\n        have similar sampling resolutions.\n</code></pre> <p> TYPE: <code>        Dotprops | NeuronList</code> </p> <code>target</code> <pre><code>        Target neuron(s) to NBLAST against. Neurons should be in\n        microns as NBLAST is optimized for that and have\n        similar sampling resolutions. If not provided, will NBLAST\n        queries against themselves.\n</code></pre> <p> TYPE: <code>       Dotprops | NeuronList</code> DEFAULT: <code>None</code> </p> <code>scores</code> <pre><code>        Determines the final scores:\n\n          - 'forward' (default) returns query-&gt;target scores\n          - 'mean' returns the mean of query-&gt;target and\n            target-&gt;query scores\n          - 'min' returns the minium between query-&gt;target and\n            target-&gt;query scores\n          - 'max' returns the maximum between query-&gt;target and\n            target-&gt;query scores\n          - 'both' will return foward and reverse scores as\n            multi-index DataFrame\n</code></pre> <p> TYPE: <code>       'forward' | 'mean' | 'min' | 'max' | 'both'</code> DEFAULT: <code>'forward'</code> </p> <code>use_alpha</code> <pre><code>        Emphasizes neurons' straight parts (backbone) over parts\n        that have lots of branches.\n</code></pre> <p> TYPE: <code>    bool</code> DEFAULT: <code>False</code> </p> <code>normalized</code> <pre><code>        Whether to return normalized NBLAST scores.\n</code></pre> <p> TYPE: <code>   bool</code> DEFAULT: <code>True</code> </p> <code>smat</code> <pre><code>        Score matrix. If 'auto' (default), will use scoring matrices\n        from FCWB. Same behaviour as in R's nat.nblast\n        implementation.\n        If `smat='v1'` it uses the analytic formulation of the\n        NBLAST scoring from Kohl et. al (2013). You can adjust parameter\n        `sigma_scaling` (default to 10) using `smat_kwargs`.\n        If `Callable` given, it passes distance and dot products as\n        first and second argument respectively.\n        If `smat=None` the scores will be\n        generated as the product of the distances and the dotproduct\n        of the vectors of nearest-neighbor pairs.\n</code></pre> <p> TYPE: <code>         str | pd.DataFrame | Callable</code> DEFAULT: <code>'auto'</code> </p> <code>limit_dist</code> <pre><code>        Sets the max distance for the nearest neighbor search\n        (`distance_upper_bound`). Typically this should be the\n        highest distance considered by the scoring function. If\n        \"auto\", will extract that value from the scoring matrix.\n        While this can give a ~2X speed up, it will introduce slight\n        inaccuracies because we won't have a vector component for\n        points without a nearest neighbour within the distance\n        limits. The impact depends on the scoring function but with\n        the default FCWB `smat`, this is typically limited to the\n        third decimal (0.0086 +/- 0.0027 for an all-by-all of 1k\n        neurons).\n</code></pre> <p> TYPE: <code>   float | \"auto\" | None</code> DEFAULT: <code>None</code> </p> <code>approx_nn</code> <pre><code>        If True, will use approximate nearest neighbors. This gives\n        a &gt;2X speed up but also produces only approximate scores.\n        Impact depends on the use case - testing highly recommended!\n</code></pre> <p> TYPE: <code>    bool</code> DEFAULT: <code>False</code> </p> <code>n_cores</code> <pre><code>        Max number of cores to use for nblasting. Default is\n        `os.cpu_count() // 2`. This should ideally be an even\n        number as that allows optimally splitting queries onto\n        individual processes.\n</code></pre> <p> TYPE: <code>      int</code> DEFAULT: <code>os.cpu_count() // 2</code> </p> <code>precision</code> <pre><code>        Precision for scores. Defaults to 64 bit (double) floats.\n        This is useful to reduce the memory footprint for very large\n        matrices. In real-world scenarios 32 bit (single)- and\n        depending on the purpose even 16 bit (half) - are typically\n        sufficient.\n</code></pre> <p> TYPE: <code>    int [16, 32, 64] | str [e.g. \"float64\"] | np.dtype</code> DEFAULT: <code>64</code> </p> <code>progress</code> <pre><code>        Whether to show progress bars. This may cause some overhead,\n        so switch off if you don't really need it.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> <code>smat_kwargs</code> <pre><code>        functions.\n</code></pre> <p> TYPE: <code>Optional[Dict]</code> DEFAULT: <code>dict()</code> </p> RETURNS DESCRIPTION <code>scores</code> <p>Matrix with NBLAST scores. Rows are query neurons, columns are targets. The order is the same as in <code>query</code>/<code>target</code> and the labels are based on the neurons' <code>.id</code> property.</p> <p> TYPE: <code>pandas.DataFrame</code> </p> References <p>Costa M, Manton JD, Ostrovsky AD, Prohaska S, Jefferis GS. NBLAST: Rapid, Sensitive Comparison of Neuronal Structure and Construction of Neuron Family Databases. Neuron. 2016 Jul 20;91(2):293-311. doi: 10.1016/j.neuron.2016.06.012.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(n=5)\n&gt;&gt;&gt; nl.units\n&lt;Quantity([8 8 8 8 8], 'nanometer')&gt;\n&gt;&gt;&gt; # Convert to microns\n&gt;&gt;&gt; nl_um = nl * (8 / 1000)\n&gt;&gt;&gt; # Convert to dotprops\n&gt;&gt;&gt; dps = navis.make_dotprops(nl_um)\n&gt;&gt;&gt; # Run the nblast\n&gt;&gt;&gt; scores = navis.nblast(dps[:3], dps[3:])\n</code></pre> See Also <p><code>navis.nblast_allbyall</code>             A more efficient way than <code>nblast(query=x, target=x)</code>. <code>navis.nblast_smart</code>             A smart(er) NBLAST suited for very large NBLAST. <code>navis.synblast</code>             A synapse-based variant of NBLAST.</p> Source code in <code>navis/nbl/nblast_funcs.py</code> <pre><code>def nblast(query: Union[Dotprops, NeuronList],\n           target: Optional[str] = None,\n           scores: Union[Literal['forward'],\n                         Literal['mean'],\n                         Literal['min'],\n                         Literal['max']] = 'forward',\n           normalized: bool = True,\n           use_alpha: bool = False,\n           smat: Optional[Union[str, pd.DataFrame, Callable]] = 'auto',\n           limit_dist: Optional[Union[Literal['auto'], int, float]] = None,\n           approx_nn: bool = False,\n           precision: Union[int, str, np.dtype] = 64,\n           n_cores: int = os.cpu_count() // 2,\n           progress: bool = True,\n           smat_kwargs: Optional[Dict] = dict()) -&gt; pd.DataFrame:\n    \"\"\"NBLAST query against target neurons.\n\n    This implements the NBLAST algorithm from Costa et al. (2016) (see\n    references) and mirror the implementation in R's `nat.nblast`\n    (https://github.com/natverse/nat.nblast).\n\n    Parameters\n    ----------\n    query :         Dotprops | NeuronList\n                    Query neuron(s) to NBLAST against the targets. Neurons\n                    should be in microns as NBLAST is optimized for that and\n                    have similar sampling resolutions.\n    target :        Dotprops | NeuronList, optional\n                    Target neuron(s) to NBLAST against. Neurons should be in\n                    microns as NBLAST is optimized for that and have\n                    similar sampling resolutions. If not provided, will NBLAST\n                    queries against themselves.\n    scores :        'forward' | 'mean' | 'min' | 'max' | 'both'\n                    Determines the final scores:\n\n                      - 'forward' (default) returns query-&gt;target scores\n                      - 'mean' returns the mean of query-&gt;target and\n                        target-&gt;query scores\n                      - 'min' returns the minium between query-&gt;target and\n                        target-&gt;query scores\n                      - 'max' returns the maximum between query-&gt;target and\n                        target-&gt;query scores\n                      - 'both' will return foward and reverse scores as\n                        multi-index DataFrame\n\n    use_alpha :     bool, optional\n                    Emphasizes neurons' straight parts (backbone) over parts\n                    that have lots of branches.\n    normalized :    bool, optional\n                    Whether to return normalized NBLAST scores.\n    smat :          str | pd.DataFrame | Callable\n                    Score matrix. If 'auto' (default), will use scoring matrices\n                    from FCWB. Same behaviour as in R's nat.nblast\n                    implementation.\n                    If `smat='v1'` it uses the analytic formulation of the\n                    NBLAST scoring from Kohl et. al (2013). You can adjust parameter\n                    `sigma_scaling` (default to 10) using `smat_kwargs`.\n                    If `Callable` given, it passes distance and dot products as\n                    first and second argument respectively.\n                    If `smat=None` the scores will be\n                    generated as the product of the distances and the dotproduct\n                    of the vectors of nearest-neighbor pairs.\n    limit_dist :    float | \"auto\" | None\n                    Sets the max distance for the nearest neighbor search\n                    (`distance_upper_bound`). Typically this should be the\n                    highest distance considered by the scoring function. If\n                    \"auto\", will extract that value from the scoring matrix.\n                    While this can give a ~2X speed up, it will introduce slight\n                    inaccuracies because we won't have a vector component for\n                    points without a nearest neighbour within the distance\n                    limits. The impact depends on the scoring function but with\n                    the default FCWB `smat`, this is typically limited to the\n                    third decimal (0.0086 +/- 0.0027 for an all-by-all of 1k\n                    neurons).\n    approx_nn :     bool\n                    If True, will use approximate nearest neighbors. This gives\n                    a &gt;2X speed up but also produces only approximate scores.\n                    Impact depends on the use case - testing highly recommended!\n    n_cores :       int, optional\n                    Max number of cores to use for nblasting. Default is\n                    `os.cpu_count() // 2`. This should ideally be an even\n                    number as that allows optimally splitting queries onto\n                    individual processes.\n    precision :     int [16, 32, 64] | str [e.g. \"float64\"] | np.dtype\n                    Precision for scores. Defaults to 64 bit (double) floats.\n                    This is useful to reduce the memory footprint for very large\n                    matrices. In real-world scenarios 32 bit (single)- and\n                    depending on the purpose even 16 bit (half) - are typically\n                    sufficient.\n    progress :      bool\n                    Whether to show progress bars. This may cause some overhead,\n                    so switch off if you don't really need it.\n    smat_kwargs:    Dictionary with additional parameters passed to scoring\n                    functions.\n\n    Returns\n    -------\n    scores :        pandas.DataFrame\n                    Matrix with NBLAST scores. Rows are query neurons, columns\n                    are targets. The order is the same as in `query`/`target`\n                    and the labels are based on the neurons' `.id` property.\n\n    References\n    ----------\n    Costa M, Manton JD, Ostrovsky AD, Prohaska S, Jefferis GS. NBLAST: Rapid,\n    Sensitive Comparison of Neuronal Structure and Construction of Neuron\n    Family Databases. Neuron. 2016 Jul 20;91(2):293-311.\n    doi: 10.1016/j.neuron.2016.06.012.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(n=5)\n    &gt;&gt;&gt; nl.units\n    &lt;Quantity([8 8 8 8 8], 'nanometer')&gt;\n    &gt;&gt;&gt; # Convert to microns\n    &gt;&gt;&gt; nl_um = nl * (8 / 1000)\n    &gt;&gt;&gt; # Convert to dotprops\n    &gt;&gt;&gt; dps = navis.make_dotprops(nl_um)\n    &gt;&gt;&gt; # Run the nblast\n    &gt;&gt;&gt; scores = navis.nblast(dps[:3], dps[3:])\n\n    See Also\n    --------\n    [`navis.nblast_allbyall`][]\n                A more efficient way than `nblast(query=x, target=x)`.\n    [`navis.nblast_smart`][]\n                A smart(er) NBLAST suited for very large NBLAST.\n    [`navis.synblast`][]\n                A synapse-based variant of NBLAST.\n\n    \"\"\"\n    utils.eval_param(scores, name='scores', allowed_values=ALLOWED_SCORES)\n\n    if isinstance(target, type(None)):\n        target = query\n\n    # Make sure we're working on NeuronLists\n    query_dps = NeuronList(query)\n    target_dps = NeuronList(target)\n\n    # Run NBLAST preflight checks\n    nblast_preflight(query_dps, target_dps, n_cores,\n                     req_unique_ids=True,\n                     req_microns=isinstance(smat, str) and smat=='auto')\n\n    # Find a partition that produces batches that each run in approximately\n    # 10 seconds\n    if n_cores and n_cores &gt; 1:\n        if progress:\n            # If progress bar, we need to make smaller mini batches.\n            # These mini jobs must not be too small - otherwise the overhead\n            # from spawning and sending results between processes slows things\n            # down dramatically. Hence we want to make sure that each job runs\n            # for &gt;10s. The run time depends on the system and how big the neurons\n            # are. Here, we run a quick test and try to extrapolate from there\n            n_rows, n_cols = find_batch_partition(query_dps, target_dps,\n                                                  T=10 * JOB_SIZE_MULTIPLIER)\n        else:\n            # If no progress bar needed, we could just split neurons evenly across\n            # all available cores but that can lead to one core lagging behind\n            # and finishing much later than all the others. To avoid this, we\n            # should aim for each batch to finish in a certain amount of time\n            n_rows, n_cols = find_batch_partition(query_dps, target_dps,\n                                                  T=JOB_MAX_TIME_SECONDS)\n            if (n_rows * n_cols) &lt; n_cores:\n                n_rows, n_cols = find_optimal_partition(n_cores, query_dps, target_dps)\n    else:\n        n_rows = n_cols = 1\n\n    # Calculate self-hits once for all neurons\n    nb = NBlaster(use_alpha=use_alpha,\n                  normalized=normalized,\n                  smat=smat,\n                  limit_dist=limit_dist,\n                  dtype=precision,\n                  approx_nn=approx_nn,\n                  progress=progress,\n                  smat_kwargs=smat_kwargs)\n    query_self_hits = np.array([nb.calc_self_hit(n) for n in query_dps])\n    target_self_hits = np.array([nb.calc_self_hit(n) for n in target_dps])\n\n    # This makes sure we don't run into multiple layers of concurrency\n    with set_omp_flag(limits=OMP_NUM_THREADS_LIMIT if n_cores and (n_cores &gt; 1) else None):\n        # Initialize a pool of workers\n        # Note that we're forcing \"spawn\" instead of \"fork\" (default on linux)!\n        # This is to reduce the memory footprint since \"fork\" appears to inherit all\n        # variables (including all neurons) while \"spawn\" appears to get only\n        # what's required to run the job?\n        with ProcessPoolExecutor(max_workers=n_cores,\n                                 mp_context=mp.get_context('spawn')) as pool:\n            with config.tqdm(desc='Preparing',\n                             total=n_rows * n_cols,\n                             leave=False,\n                             disable=not progress) as pbar:\n                futures = {}\n                nblasters = []\n                for qix in np.array_split(np.arange(len(query_dps)), n_rows):\n                    for tix in np.array_split(np.arange(len(target_dps)), n_cols):\n                        # Initialize NBlaster\n                        this = NBlaster(use_alpha=use_alpha,\n                                        normalized=normalized,\n                                        smat=smat,\n                                        limit_dist=limit_dist,\n                                        dtype=precision,\n                                        approx_nn=approx_nn,\n                                        progress=progress,\n                                        smat_kwargs=smat_kwargs)\n\n                        # Add queries and targets\n                        for i, ix in enumerate(qix):\n                            this.append(query_dps[ix], query_self_hits[ix])\n                        for i, ix in enumerate(tix):\n                            this.append(target_dps[ix], target_self_hits[ix])\n\n                        # Keep track of indices of queries and targets\n                        this.queries = np.arange(len(qix))\n                        this.targets = np.arange(len(tix)) + len(qix)\n                        this.queries_ix = qix  # this facilitates filling in the big matrix later\n                        this.targets_ix = tix  # this facilitates filling in the big matrix later\n                        this.pbar_position = len(nblasters) if not utils.is_jupyter() else None\n\n                        nblasters.append(this)\n                        pbar.update()\n\n                        # If multiple cores requested, submit job to the pool right away\n                        if n_cores and n_cores &gt; 1 and (n_cols &gt; 1 or n_rows &gt; 1):\n                            this.progress=False  # no progress bar for individual NBLASTERs\n                            futures[pool.submit(this.multi_query_target,\n                                                q_idx=this.queries,\n                                                t_idx=this.targets,\n                                                scores=scores)] = this\n\n            # Collect results\n            if futures and len(futures) &gt; 1:\n                # Prepare empty score matrix\n                scores = pd.DataFrame(np.empty((len(query_dps), len(target_dps)),\n                                               dtype=this.dtype),\n                                      index=query_dps.id, columns=target_dps.id)\n                scores.index.name = 'query'\n                scores.columns.name = 'target'\n\n                # Collect results\n                # We're dropping the \"N / N_total\" bit from the progress bar because\n                # it's not helpful here\n                fmt = ('{desc}: {percentage:3.0f}%|{bar}| [{elapsed}&lt;{remaining}]')\n                for f in config.tqdm(as_completed(futures),\n                                     desc='NBLASTing',\n                                     bar_format=fmt,\n                                     total=len(futures),\n                                     smoothing=0,\n                                     disable=not progress,\n                                     leave=False):\n                    res = f.result()\n                    this = futures[f]\n                    # Fill-in big score matrix\n                    scores.iloc[this.queries_ix, this.targets_ix] = res.values\n            else:\n                scores = this.multi_query_target(this.queries,\n                                                 this.targets,\n                                                 scores=scores)\n\n    return scores\n</code></pre>"},{"location":"reference/navis/#navis.nblast_align","title":"<code>navis.nblast_align</code>","text":"<p>Run NBLAST on pairwise-aligned neurons.</p> <p>Requires the <code>pycpd</code> library at least version 2.0.1 which at the time of writing is only available from Github (not PyPI):</p> <p>https://github.com/siavashk/pycpd</p> PARAMETER DESCRIPTION <code>query</code> <pre><code>        Query neuron(s) to NBLAST against the targets. Neurons\n        should be in microns as NBLAST is optimized for that and\n        have similar sampling resolutions.\n</code></pre> <p> TYPE: <code>        Neuron | NeuronList</code> </p> <code>target</code> <pre><code>        Target neuron(s) to NBLAST against. Neurons should be in\n        microns as NBLAST is optimized for that and have\n        similar sampling resolutions. If not provided, will NBLAST\n        queries against themselves.\n</code></pre> <p> TYPE: <code>       Neuron | NeuronList</code> DEFAULT: <code>None</code> </p> <code>align_method</code> <pre><code>        Which method to use for alignment. Maps to the respective\n        `navis.align_{method}` function.\n</code></pre> <p> TYPE: <code> \"rigid\" | \"deform\" | \"pca\" | \"rigid+deform\"</code> DEFAULT: <code>'rigid'</code> </p> <code>two_way_align</code> <pre><code>        If True, will run the alignment + NBLAST in both,\n        query-&gt;target as well as query-&gt;target direction. This is\n        highly recommended because it reduces the chance that a\n        single bad alignment will mess up your scores.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>sample_align</code> <pre><code>        If provided, will calculate an initial alignment on just a\n        fraction of the points followed by a landmark transform\n        to transform the rest. Use this to speed things up.\n</code></pre> <p> TYPE: <code> float [0-1]</code> DEFAULT: <code>None</code> </p> <code>scores</code> <pre><code>        Determines the final scores:\n\n          - 'forward' (default) returns query-&gt;target scores\n          - 'mean' returns the mean of query-&gt;target and\n            target-&gt;query scores\n          - 'min' returns the minium between query-&gt;target and\n            target-&gt;query scores\n          - 'max' returns the maximum between query-&gt;target and\n            target-&gt;query scores\n          - 'both' will return foward and reverse scores as\n            multi-index DataFrame\n</code></pre> <p> TYPE: <code>       'forward' | 'mean' | 'min' | 'max' | 'both'</code> DEFAULT: <code>'mean'</code> </p> <code>use_alpha</code> <pre><code>        Emphasizes neurons' straight parts (backbone) over parts\n        that have lots of branches for the NBLAST.\n</code></pre> <p> TYPE: <code>    bool</code> DEFAULT: <code>False</code> </p> <code>normalized</code> <pre><code>        Whether to return normalized NBLAST scores.\n</code></pre> <p> TYPE: <code>   bool</code> DEFAULT: <code>True</code> </p> <code>smat</code> <pre><code>        Score matrix. If 'auto' (default), will use scoring matrices\n        from FCWB. Same behaviour as in R's nat.nblast\n        implementation.\n        If `smat='v1'` it uses the analytic formulation of the\n        NBLAST scoring from Kohl et. al (2013). You can adjust parameter\n        `sigma_scaling` (default to 10) using `smat_kwargs`.\n        If `Callable` given, it passes distance and dot products as\n        first and second argument respectively.\n        If `smat=None` the scores will be\n        generated as the product of the distances and the dotproduct\n        of the vectors of nearest-neighbor pairs.\n</code></pre> <p> TYPE: <code>         str | pd.DataFrame | Callable</code> DEFAULT: <code>'auto'</code> </p> <code>limit_dist</code> <pre><code>        Sets the max distance for the nearest neighbor search\n        (`distance_upper_bound`). Typically this should be the\n        highest distance considered by the scoring function. If\n        \"auto\", will extract that value from the scoring matrix.\n        While this can give a ~2X speed up, it will introduce slight\n        inaccuracies because we won't have a vector component for\n        points without a nearest neighbour within the distance\n        limits. The impact depends on the scoring function but with\n        the default FCWB `smat`, this is typically limited to the\n        third decimal (0.0086 +/- 0.0027 for an all-by-all of 1k\n        neurons).\n</code></pre> <p> TYPE: <code>   float | \"auto\" | None</code> DEFAULT: <code>None</code> </p> <code>approx_nn</code> <pre><code>        If True, will use approximate nearest neighbors. This gives\n        a &gt;2X speed up but also produces only approximate scores.\n        Impact depends on the use case - testing highly recommended!\n</code></pre> <p> TYPE: <code>    bool</code> DEFAULT: <code>False</code> </p> <code>n_cores</code> <pre><code>        Max number of cores to use for nblasting. Default is\n        `os.cpu_count() // 2`. This should ideally be an even\n        number as that allows optimally splitting queries onto\n        individual processes. Also note that due to multiple layers\n        of concurrency using all available cores might not be the\n        fastest option.\n</code></pre> <p> TYPE: <code>      int</code> DEFAULT: <code>os.cpu_count() // 2</code> </p> <code>precision</code> <pre><code>        Precision for scores. Defaults to 64 bit (double) floats.\n        This is useful to reduce the memory footprint for very large\n        matrices. In real-world scenarios 32 bit (single)- and\n        depending on the purpose even 16 bit (half) - are typically\n        sufficient.\n</code></pre> <p> TYPE: <code>    int [16, 32, 64] | str [e.g. \"float64\"] | np.dtype</code> DEFAULT: <code>64</code> </p> <code>progress</code> <pre><code>        Whether to show progress bars. This may cause some overhead,\n        so switch off if you don't really need it.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> <code>smat_kwargs</code> <pre><code>        Dictionary with additional parameters passed to scoring\n        functions.\n</code></pre> <p> TYPE: <code>  dict</code> DEFAULT: <code>dict()</code> </p> <code>align_kwargs</code> <pre><code>        Dictionary with additional parameters passed to alignment\n        function.\n</code></pre> <p> TYPE: <code> dict</code> DEFAULT: <code>dict()</code> </p> <code>dotprop_kwargs</code> <pre><code>        Dictionary with additional parameters passed to\n        `navis.make_dotprops`. Only relevant if inputs aren't\n        already dotprops.\n</code></pre> <p> TYPE: <code>dict</code> DEFAULT: <code>dict()</code> </p> RETURNS DESCRIPTION <code>scores</code> <p>Matrix with NBLAST scores. Rows are query neurons, columns are targets. The order is the same as in <code>query</code>/<code>target</code> and the labels are based on the neurons' <code>.id</code> property. Important to note that even when <code>q == t</code> and with <code>scores=mean</code> the matrix will not be symmetrical because we run separate alignments for the forward and the reverse comparisons.</p> <p> TYPE: <code>pandas.DataFrame</code> </p> References <p>Costa M, Manton JD, Ostrovsky AD, Prohaska S, Jefferis GS. NBLAST: Rapid, Sensitive Comparison of Neuronal Structure and Construction of Neuron Family Databases. Neuron. 2016 Jul 20;91(2):293-311. doi: 10.1016/j.neuron.2016.06.012.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(n=5)\n&gt;&gt;&gt; nl.units\n&lt;Quantity([8 8 8 8 8], 'nanometer')&gt;\n&gt;&gt;&gt; # Convert to microns\n&gt;&gt;&gt; nl_um = nl * (8 / 1000)\n&gt;&gt;&gt; # Run the align nblast\n&gt;&gt;&gt; scores = navis.nblast_align(nl_um[:3], nl_um[3:],\n...                             dotprop_kwargs=dict(k=5),\n...                             sample_align=.2)\n</code></pre> See Also <p><code>navis.nblast</code>             The vanilla version of NBLAST. <code>navis.nblast_allbyall</code>             A more efficient way than <code>nblast(query=x, target=x)</code>. <code>navis.nblast_smart</code>             A smart(er) NBLAST suited for very large NBLAST. <code>navis.synblast</code>             A synapse-based variant of NBLAST.</p> Source code in <code>navis/nbl/ablast_funcs.py</code> <pre><code>def nblast_align(query: Union[core.BaseNeuron, core.NeuronList],\n                 target: Optional[str] = None,\n                 align_method: Union[Literal['rigid'],\n                                     Literal['deform'],\n                                     Literal['pca']] = 'rigid',\n                 two_way_align: bool = True,\n                 sample_align: Optional[float] = None,\n                 scores: Union[Literal['forward'],\n                               Literal['mean'],\n                               Literal['min'],\n                               Literal['max']] = 'mean',\n                 normalized: bool = True,\n                 use_alpha: bool = False,\n                 smat: Optional[Union[str, pd.DataFrame, Callable]] = 'auto',\n                 limit_dist: Optional[Union[Literal['auto'], int, float]] = None,\n                 approx_nn: bool = False,\n                 precision: Union[int, str, np.dtype] = 64,\n                 n_cores: int = os.cpu_count() // 2,\n                 progress: bool = True,\n                 dotprop_kwargs: Optional[Dict] = dict(),\n                 align_kwargs: Optional[Dict] = dict(),\n                 smat_kwargs: Optional[Dict] = dict()) -&gt; pd.DataFrame:\n    \"\"\"Run NBLAST on pairwise-aligned neurons.\n\n    Requires the `pycpd` library at least version 2.0.1 which at the time of\n    writing is only available from Github (not PyPI):\n\n      https://github.com/siavashk/pycpd\n\n    Parameters\n    ----------\n    query :         Neuron | NeuronList\n                    Query neuron(s) to NBLAST against the targets. Neurons\n                    should be in microns as NBLAST is optimized for that and\n                    have similar sampling resolutions.\n    target :        Neuron | NeuronList, optional\n                    Target neuron(s) to NBLAST against. Neurons should be in\n                    microns as NBLAST is optimized for that and have\n                    similar sampling resolutions. If not provided, will NBLAST\n                    queries against themselves.\n    align_method :  \"rigid\" | \"deform\" | \"pca\" | \"rigid+deform\"\n                    Which method to use for alignment. Maps to the respective\n                    `navis.align_{method}` function.\n    two_way_align : bool\n                    If True, will run the alignment + NBLAST in both,\n                    query-&gt;target as well as query-&gt;target direction. This is\n                    highly recommended because it reduces the chance that a\n                    single bad alignment will mess up your scores.\n    sample_align :  float [0-1], optional\n                    If provided, will calculate an initial alignment on just a\n                    fraction of the points followed by a landmark transform\n                    to transform the rest. Use this to speed things up.\n    scores :        'forward' | 'mean' | 'min' | 'max' | 'both'\n                    Determines the final scores:\n\n                      - 'forward' (default) returns query-&gt;target scores\n                      - 'mean' returns the mean of query-&gt;target and\n                        target-&gt;query scores\n                      - 'min' returns the minium between query-&gt;target and\n                        target-&gt;query scores\n                      - 'max' returns the maximum between query-&gt;target and\n                        target-&gt;query scores\n                      - 'both' will return foward and reverse scores as\n                        multi-index DataFrame\n\n    use_alpha :     bool, optional\n                    Emphasizes neurons' straight parts (backbone) over parts\n                    that have lots of branches for the NBLAST.\n    normalized :    bool, optional\n                    Whether to return normalized NBLAST scores.\n    smat :          str | pd.DataFrame | Callable\n                    Score matrix. If 'auto' (default), will use scoring matrices\n                    from FCWB. Same behaviour as in R's nat.nblast\n                    implementation.\n                    If `smat='v1'` it uses the analytic formulation of the\n                    NBLAST scoring from Kohl et. al (2013). You can adjust parameter\n                    `sigma_scaling` (default to 10) using `smat_kwargs`.\n                    If `Callable` given, it passes distance and dot products as\n                    first and second argument respectively.\n                    If `smat=None` the scores will be\n                    generated as the product of the distances and the dotproduct\n                    of the vectors of nearest-neighbor pairs.\n    limit_dist :    float | \"auto\" | None\n                    Sets the max distance for the nearest neighbor search\n                    (`distance_upper_bound`). Typically this should be the\n                    highest distance considered by the scoring function. If\n                    \"auto\", will extract that value from the scoring matrix.\n                    While this can give a ~2X speed up, it will introduce slight\n                    inaccuracies because we won't have a vector component for\n                    points without a nearest neighbour within the distance\n                    limits. The impact depends on the scoring function but with\n                    the default FCWB `smat`, this is typically limited to the\n                    third decimal (0.0086 +/- 0.0027 for an all-by-all of 1k\n                    neurons).\n    approx_nn :     bool\n                    If True, will use approximate nearest neighbors. This gives\n                    a &gt;2X speed up but also produces only approximate scores.\n                    Impact depends on the use case - testing highly recommended!\n    n_cores :       int, optional\n                    Max number of cores to use for nblasting. Default is\n                    `os.cpu_count() // 2`. This should ideally be an even\n                    number as that allows optimally splitting queries onto\n                    individual processes. Also note that due to multiple layers\n                    of concurrency using all available cores might not be the\n                    fastest option.\n    precision :     int [16, 32, 64] | str [e.g. \"float64\"] | np.dtype\n                    Precision for scores. Defaults to 64 bit (double) floats.\n                    This is useful to reduce the memory footprint for very large\n                    matrices. In real-world scenarios 32 bit (single)- and\n                    depending on the purpose even 16 bit (half) - are typically\n                    sufficient.\n    progress :      bool\n                    Whether to show progress bars. This may cause some overhead,\n                    so switch off if you don't really need it.\n    smat_kwargs :   dict, optional\n                    Dictionary with additional parameters passed to scoring\n                    functions.\n    align_kwargs :  dict, optional\n                    Dictionary with additional parameters passed to alignment\n                    function.\n    dotprop_kwargs : dict, optional\n                    Dictionary with additional parameters passed to\n                    `navis.make_dotprops`. Only relevant if inputs aren't\n                    already dotprops.\n\n\n    Returns\n    -------\n    scores :        pandas.DataFrame\n                    Matrix with NBLAST scores. Rows are query neurons, columns\n                    are targets. The order is the same as in `query`/`target`\n                    and the labels are based on the neurons' `.id` property.\n                    Important to note that even when `q == t` and with\n                    `scores=mean` the matrix will not be symmetrical because\n                    we run separate alignments for the forward and the reverse\n                    comparisons.\n\n    References\n    ----------\n    Costa M, Manton JD, Ostrovsky AD, Prohaska S, Jefferis GS. NBLAST: Rapid,\n    Sensitive Comparison of Neuronal Structure and Construction of Neuron\n    Family Databases. Neuron. 2016 Jul 20;91(2):293-311.\n    doi: 10.1016/j.neuron.2016.06.012.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(n=5)\n    &gt;&gt;&gt; nl.units\n    &lt;Quantity([8 8 8 8 8], 'nanometer')&gt;\n    &gt;&gt;&gt; # Convert to microns\n    &gt;&gt;&gt; nl_um = nl * (8 / 1000)\n    &gt;&gt;&gt; # Run the align nblast\n    &gt;&gt;&gt; scores = navis.nblast_align(nl_um[:3], nl_um[3:],\n    ...                             dotprop_kwargs=dict(k=5),\n    ...                             sample_align=.2)\n\n    See Also\n    --------\n    [`navis.nblast`][]\n                The vanilla version of NBLAST.\n    [`navis.nblast_allbyall`][]\n                A more efficient way than `nblast(query=x, target=x)`.\n    [`navis.nblast_smart`][]\n                A smart(er) NBLAST suited for very large NBLAST.\n    [`navis.synblast`][]\n                A synapse-based variant of NBLAST.\n\n    \"\"\"\n    if isinstance(target, type(None)):\n        target = query\n\n    # Make sure we're working on NeuronLists\n    query = core.NeuronList(query)\n    target = core.NeuronList(target)\n\n    if not callable(align_method):\n        align_func = {'rigid': align_rigid,\n                      'deform': align_deform,\n                      'pca': align_pca,\n                      'rigid+deform': _align_rigid_deform}[align_method]\n    else:\n        align_func = align_method\n\n    # Run NBLAST preflight checks\n    nblast_preflight(query, target, n_cores,\n                     req_dotprops=False,\n                     req_unique_ids=True,\n                     req_microns=isinstance(smat, str) and smat=='auto')\n\n    # Find a partition that produces batches that each run in approximately\n    # 10 seconds\n    if n_cores and n_cores &gt; 1:\n        n_rows, n_cols = find_optimal_partition(n_cores, query, target)\n        if progress:\n            # If progress bar, we need to make smaller mini batches.\n            # These mini jobs must not be too small - otherwise the overhead\n            # from spawning and sending results between processes slows things\n            # down dramatically. Here we hardcode such that we get updates\n            # at most every 1%\n            n_rows = max(n_rows, len(query) // 10)\n            n_cols = max(n_cols, len(target) // 10)\n    else:\n        n_rows = n_cols = 1\n\n    # This makes sure we don't run into multiple layers of concurrency\n    # Note that it doesn't do anything for the parent process (which is great\n    # if we end up not actually using multiple cores)\n    with set_omp_flag(limits=1):\n        # Initialize a pool of workers\n        # Note that we're forcing \"spawn\" instead of \"fork\" (default on linux)!\n        # This is to reduce the memory footprint since \"fork\" appears to inherit all\n        # variables (including all neurons) while \"spawn\" appears to get only\n        # what's required to run the job?\n        with ProcessPoolExecutor(max_workers=n_cores,\n                                 mp_context=mp.get_context('spawn')) as pool:\n            with config.tqdm(desc='Preparing',\n                             total=n_rows * n_cols,\n                             leave=False,\n                             disable=not progress) as pbar:\n                futures = {}\n                nblasters = []\n                for qix in np.array_split(np.arange(len(query)), n_rows):\n                    for tix in np.array_split(np.arange(len(target)), n_cols):\n                        # Initialize NBlaster\n                        this = NBlasterAlign(align_func=align_func,\n                                             two_way_align=two_way_align,\n                                             sample_align=sample_align,\n                                             use_alpha=use_alpha,\n                                             normalized=normalized,\n                                             smat=smat,\n                                             limit_dist=limit_dist,\n                                             dtype=precision,\n                                             approx_nn=approx_nn,\n                                             progress=progress,\n                                             align_kwargs=align_kwargs,\n                                             dotprop_kwargs=dotprop_kwargs,\n                                             smat_kwargs=smat_kwargs)\n\n                        # Add queries and targets\n                        for i, ix in enumerate(qix):\n                            this.append(query[ix])\n                        for i, ix in enumerate(tix):\n                            this.append(target[ix])\n\n                        # Keep track of indices of queries and targets\n                        this.queries = np.arange(len(qix))\n                        this.targets = np.arange(len(tix)) + len(qix)\n                        this.queries_ix = qix  # this facilitates filling in the big matrix later\n                        this.targets_ix = tix  # this facilitates filling in the big matrix later\n                        this.pbar_position = len(nblasters) if not utils.is_jupyter() else None\n\n                        nblasters.append(this)\n                        pbar.update()\n\n                        # If multiple cores requested, submit job to the pool right away\n                        if n_cores and n_cores &gt; 1 and (n_cols &gt; 1 or n_rows &gt; 1):\n                            this.progress=False  # no progress bar for individual NBLASTERs\n                            futures[pool.submit(this.multi_query_target,\n                                                q_idx=this.queries,\n                                                t_idx=this.targets,\n                                                scores=scores)] = this\n\n            # Collect results\n            if futures and len(futures) &gt; 1:\n                # Prepare empty score matrix\n                scores = pd.DataFrame(np.empty((len(query), len(target)),\n                                               dtype=this.dtype),\n                                      index=query.id, columns=target.id)\n                scores.index.name = 'query'\n                scores.columns.name = 'target'\n\n                # Collect results\n                # We're dropping the \"N / N_total\" bit from the progress bar because\n                # it's not helpful here\n                fmt = ('{desc}: {percentage:3.0f}%|{bar}| [{elapsed}&lt;{remaining}]')\n                for f in config.tqdm(as_completed(futures),\n                                     desc='NBLASTing',\n                                     bar_format=fmt,\n                                     total=len(futures),\n                                     smoothing=0,\n                                     disable=not progress,\n                                     leave=False):\n                    res = f.result()\n                    this = futures[f]\n                    # Fill-in big score matrix\n                    scores.iloc[this.queries_ix, this.targets_ix] = res.values\n            else:\n                scores = this.multi_query_target(this.queries,\n                                                 this.targets,\n                                                 scores=scores)\n\n    return scores\n</code></pre>"},{"location":"reference/navis/#navis.nblast_allbyall","title":"<code>navis.nblast_allbyall</code>","text":"<p>All-by-all NBLAST of inputs neurons.</p> <p>A more efficient way than running <code>nblast(query=x, target=x)</code>.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Neuron(s) to NBLAST against each other. Neurons should\n        be in microns as NBLAST is optimized for that and have\n        similar sampling resolutions.\n</code></pre> <p> TYPE: <code>            Dotprops | NeuronList</code> </p> <code>n_cores</code> <pre><code>        Max number of cores to use for nblasting. Default is\n        `os.cpu_count() // 2`. This should ideally be an even\n        number as that allows optimally splitting queries onto\n        individual processes.\n</code></pre> <p> TYPE: <code>      int</code> DEFAULT: <code>os.cpu_count() // 2</code> </p> <code>use_alpha</code> <pre><code>        Emphasizes neurons' straight parts (backbone) over parts\n        that have lots of branches.\n</code></pre> <p> TYPE: <code>    bool</code> DEFAULT: <code>False</code> </p> <code>normalized</code> <pre><code>        Whether to return normalized NBLAST scores.\n</code></pre> <p> TYPE: <code>   bool</code> DEFAULT: <code>True</code> </p> <code>smat</code> <pre><code>        Score matrix/function:\n         - If `smat='auto'` (default), will use scoring matrices\n           based on flycircuit data. Same behaviour as in R's\n           nat.nblast implementation.\n         - For `smat='v1'`, uses the analytic formulation of the\n           NBLAST scoring from Kohl et. al (2013). You can adjust\n           parameter `sigma_scaling` (default to 10) using `smat_kwargs`.\n         - For `smat=None` the scores will be generated as the product\n           of the distances and the dotproduct of the vectors of\n           nearest-neighbor pairs.\n         - If function, must consume distance and dot products as\n           first and second argument, respectively and return float.\n</code></pre> <p> TYPE: <code>         str | pd.DataFrame | Callable</code> DEFAULT: <code>'auto'</code> </p> <code>limit_dist</code> <pre><code>        Sets the max distance for the nearest neighbor search\n        (`distance_upper_bound`). Typically this should be the\n        highest distance considered by the scoring function. If\n        \"auto\", will extract that value from the scoring matrix.\n        While this can give a ~2X speed up, it will introduce slight\n        inaccuracies because we won't have a vector component for\n        points without a nearest neighbour within the distance\n        limits. The impact depends on the scoring function but with\n        the default FCWB `smat`, this is typically limited to the\n        third decimal (0.0086 +/- 0.0027 for an all-by-all of 1k\n        neurons).\n</code></pre> <p> TYPE: <code>   float | \"auto\" | None</code> DEFAULT: <code>None</code> </p> <code>approx_nn</code> <pre><code>        If True, will use approximate nearest neighbors. This gives\n        a &gt;2X speed up but also produces only approximate scores.\n        Impact depends on the use case - testing highly recommended!\n</code></pre> <p> TYPE: <code>    bool</code> DEFAULT: <code>False</code> </p> <code>precision</code> <pre><code>        Precision for scores. Defaults to 64 bit (double) floats.\n        This is useful to reduce the memory footprint for very large\n        matrices. In real-world scenarios 32 bit (single)- and\n        depending on the purpose even 16 bit (half) - are typically\n        sufficient.\n</code></pre> <p> TYPE: <code>    int [16, 32, 64] | str [e.g. \"float64\"] | np.dtype</code> DEFAULT: <code>64</code> </p> <code>progress</code> <pre><code>        Whether to show progress bars. This cause may some overhead,\n        so switch off if you don't really need it.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> <code>smat_kwargs</code> <pre><code>        functions.\n</code></pre> <p> TYPE: <code>Optional[Dict]</code> DEFAULT: <code>dict()</code> </p> RETURNS DESCRIPTION <code>scores</code> <p>Matrix with NBLAST scores. Rows are query neurons, columns are targets. The order is the same as in <code>x</code> and the labels are based on the neurons' <code>.id</code> property.</p> <p> TYPE: <code>pandas.DataFrame</code> </p> References <p>Costa M, Manton JD, Ostrovsky AD, Prohaska S, Jefferis GS. NBLAST: Rapid, Sensitive Comparison of Neuronal Structure and Construction of Neuron Family Databases. Neuron. 2016 Jul 20;91(2):293-311. doi: 10.1016/j.neuron.2016.06.012.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(n=5)\n&gt;&gt;&gt; nl.units\n&lt;Quantity([8 8 8 8 8], 'nanometer')&gt;\n&gt;&gt;&gt; # Convert to microns\n&gt;&gt;&gt; nl_um = nl * (8 / 1000)\n&gt;&gt;&gt; # Make dotprops\n&gt;&gt;&gt; dps = navis.make_dotprops(nl_um)\n&gt;&gt;&gt; # Run the nblast\n&gt;&gt;&gt; scores = navis.nblast_allbyall(dps)\n</code></pre> See Also <p><code>navis.nblast</code>             For generic query -&gt; target nblasts.</p> Source code in <code>navis/nbl/nblast_funcs.py</code> <pre><code>def nblast_allbyall(x: NeuronList,\n                    normalized: bool = True,\n                    use_alpha: bool = False,\n                    smat: Optional[Union[str, pd.DataFrame, Callable]] = 'auto',\n                    limit_dist: Optional[Union[Literal['auto'], int, float]] = None,\n                    approx_nn: bool = False,\n                    precision: Union[int, str, np.dtype] = 64,\n                    n_cores: int = os.cpu_count() // 2,\n                    progress: bool = True,\n                    smat_kwargs: Optional[Dict] = dict()) -&gt; pd.DataFrame:\n    \"\"\"All-by-all NBLAST of inputs neurons.\n\n    A more efficient way than running `nblast(query=x, target=x)`.\n\n    Parameters\n    ----------\n    x :             Dotprops | NeuronList\n                    Neuron(s) to NBLAST against each other. Neurons should\n                    be in microns as NBLAST is optimized for that and have\n                    similar sampling resolutions.\n    n_cores :       int, optional\n                    Max number of cores to use for nblasting. Default is\n                    `os.cpu_count() // 2`. This should ideally be an even\n                    number as that allows optimally splitting queries onto\n                    individual processes.\n    use_alpha :     bool, optional\n                    Emphasizes neurons' straight parts (backbone) over parts\n                    that have lots of branches.\n    normalized :    bool, optional\n                    Whether to return normalized NBLAST scores.\n    smat :          str | pd.DataFrame | Callable, optional\n                    Score matrix/function:\n                     - If `smat='auto'` (default), will use scoring matrices\n                       based on flycircuit data. Same behaviour as in R's\n                       nat.nblast implementation.\n                     - For `smat='v1'`, uses the analytic formulation of the\n                       NBLAST scoring from Kohl et. al (2013). You can adjust\n                       parameter `sigma_scaling` (default to 10) using `smat_kwargs`.\n                     - For `smat=None` the scores will be generated as the product\n                       of the distances and the dotproduct of the vectors of\n                       nearest-neighbor pairs.\n                     - If function, must consume distance and dot products as\n                       first and second argument, respectively and return float.\n    limit_dist :    float | \"auto\" | None\n                    Sets the max distance for the nearest neighbor search\n                    (`distance_upper_bound`). Typically this should be the\n                    highest distance considered by the scoring function. If\n                    \"auto\", will extract that value from the scoring matrix.\n                    While this can give a ~2X speed up, it will introduce slight\n                    inaccuracies because we won't have a vector component for\n                    points without a nearest neighbour within the distance\n                    limits. The impact depends on the scoring function but with\n                    the default FCWB `smat`, this is typically limited to the\n                    third decimal (0.0086 +/- 0.0027 for an all-by-all of 1k\n                    neurons).\n    approx_nn :     bool\n                    If True, will use approximate nearest neighbors. This gives\n                    a &gt;2X speed up but also produces only approximate scores.\n                    Impact depends on the use case - testing highly recommended!\n    precision :     int [16, 32, 64] | str [e.g. \"float64\"] | np.dtype\n                    Precision for scores. Defaults to 64 bit (double) floats.\n                    This is useful to reduce the memory footprint for very large\n                    matrices. In real-world scenarios 32 bit (single)- and\n                    depending on the purpose even 16 bit (half) - are typically\n                    sufficient.\n    progress :      bool\n                    Whether to show progress bars. This cause may some overhead,\n                    so switch off if you don't really need it.\n    smat_kwargs:    Dictionary with additional parameters passed to scoring\n                    functions.\n\n    Returns\n    -------\n    scores :        pandas.DataFrame\n                    Matrix with NBLAST scores. Rows are query neurons, columns\n                    are targets. The order is the same as in `x`\n                    and the labels are based on the neurons' `.id` property.\n\n    References\n    ----------\n    Costa M, Manton JD, Ostrovsky AD, Prohaska S, Jefferis GS. NBLAST: Rapid,\n    Sensitive Comparison of Neuronal Structure and Construction of Neuron\n    Family Databases. Neuron. 2016 Jul 20;91(2):293-311.\n    doi: 10.1016/j.neuron.2016.06.012.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(n=5)\n    &gt;&gt;&gt; nl.units\n    &lt;Quantity([8 8 8 8 8], 'nanometer')&gt;\n    &gt;&gt;&gt; # Convert to microns\n    &gt;&gt;&gt; nl_um = nl * (8 / 1000)\n    &gt;&gt;&gt; # Make dotprops\n    &gt;&gt;&gt; dps = navis.make_dotprops(nl_um)\n    &gt;&gt;&gt; # Run the nblast\n    &gt;&gt;&gt; scores = navis.nblast_allbyall(dps)\n\n    See Also\n    --------\n    [`navis.nblast`][]\n                For generic query -&gt; target nblasts.\n\n    \"\"\"\n    # Check if pykdtree flag needed to be set\n    #if n_cores and n_cores &gt; 1:\n    #    check_pykdtree_flag()\n\n    # Make sure we're working on NeuronLists\n    dps = NeuronList(x)\n\n    # Run NBLAST preflight checks\n    # Note that we are passing the same dotprops twice to avoid having to\n    # change the function's signature. Should have little to no overhead.\n    nblast_preflight(dps, dps, n_cores,\n                     req_unique_ids=True,\n                     req_microns=isinstance(smat, str) and smat=='auto')\n\n    # Find a partition that produces batches that each run in approximately\n    # 10 seconds\n    if n_cores and n_cores &gt; 1:\n        if progress:\n            # If progress bar, we need to make smaller mini batches.\n            # These mini jobs must not be too small - otherwise the overhead\n            # from spawning and sending results between processes slows things\n            # down dramatically. Hence we want to make sure that each job runs\n            # for &gt;10s. The run time depends on the system and how big the neurons\n            # are. Here, we run a quick test and try to extrapolate from there:\n            n_rows, n_cols = find_batch_partition(dps, dps,\n                                                  T=10 * JOB_SIZE_MULTIPLIER)\n        else:\n            # If no progress bar needed, we can just split neurons evenly across\n            # all available cores\n            n_rows, n_cols = find_optimal_partition(n_cores, dps, dps)\n    else:\n        n_rows = n_cols = 1\n\n    # Calculate self-hits once for all neurons\n    nb = NBlaster(use_alpha=use_alpha,\n                  normalized=normalized,\n                  smat=smat,\n                  limit_dist=limit_dist,\n                  dtype=precision,\n                  approx_nn=approx_nn,\n                  progress=progress,\n                  smat_kwargs=smat_kwargs)\n    self_hits = np.array([nb.calc_self_hit(n) for n in dps])\n\n    # This makes sure we don't run into multiple layers of concurrency\n    with set_omp_flag(limits=OMP_NUM_THREADS_LIMIT if n_cores and (n_cores &gt; 1) else None):\n        # Initialize a pool of workers\n        # Note that we're forcing \"spawn\" instead of \"fork\" (default on linux)!\n        # This is to reduce the memory footprint since \"fork\" appears to inherit all\n        # variables (including all neurons) while \"spawn\" appears to get only\n        # what's required to run the job?\n        with ProcessPoolExecutor(max_workers=n_cores,\n                                 mp_context=mp.get_context('spawn')) as pool:\n            with config.tqdm(desc='Preparing',\n                             total=n_rows * n_cols,\n                             leave=False,\n                             disable=not progress) as pbar:\n                futures = {}\n                nblasters = []\n                for qix in np.array_split(np.arange(len(dps)), n_rows):\n                    for tix in np.array_split(np.arange(len(dps)), n_cols):\n                        # Initialize NBlaster\n                        this = NBlaster(use_alpha=use_alpha,\n                                        normalized=normalized,\n                                        smat=smat,\n                                        limit_dist=limit_dist,\n                                        dtype=precision,\n                                        approx_nn=approx_nn,\n                                        progress=progress,\n                                        smat_kwargs=smat_kwargs)\n\n                        # Make sure we don't add the same neuron twice\n                        # Map indices to neurons\n                        to_add = list(set(qix) | set(tix))\n\n                        # Add neurons\n                        ixmap = {}\n                        for i, ix in enumerate(to_add):\n                            this.append(dps[ix], self_hits[ix])\n                            ixmap[ix] = i\n\n                        # Keep track of indices of queries and targets\n                        this.queries = [ixmap[ix] for ix in qix]\n                        this.targets = [ixmap[ix] for ix in tix]\n                        this.queries_ix = qix  # this facilitates filling in the big matrix later\n                        this.targets_ix = tix  # this facilitates filling in the big matrix later\n                        this.pbar_position = len(nblasters) if not utils.is_jupyter() else None\n\n                        nblasters.append(this)\n                        pbar.update()\n\n                        # If multiple cores requested, submit job to the pool right away\n                        if n_cores and n_cores &gt; 1 and (n_cols &gt; 1 or n_rows &gt; 1):\n                            this.progress=False  # no progress bar for individual NBLASTERs\n                            futures[pool.submit(this.multi_query_target,\n                                                q_idx=this.queries,\n                                                t_idx=this.targets,\n                                                scores='forward')] = this\n\n            # Collect results\n            if futures and len(futures) &gt; 1:\n                # Prepare empty score matrix\n                scores = pd.DataFrame(np.empty((len(dps), len(dps)),\n                                               dtype=this.dtype),\n                                      index=dps.id, columns=dps.id)\n                scores.index.name = 'query'\n                scores.columns.name = 'target'\n\n                # Collect results\n                # We're dropping the \"N / N_total\" bit from the progress bar because\n                # it's not helpful here\n                fmt = ('{desc}: {percentage:3.0f}%|{bar}| [{elapsed}&lt;{remaining}]')\n                for f in config.tqdm(as_completed(futures),\n                                     desc='NBLASTing',\n                                     bar_format=fmt,\n                                     total=len(futures),\n                                     smoothing=0,\n                                     disable=not progress,\n                                     leave=False):\n                    res = f.result()\n                    this = futures[f]\n                    # Fill-in big score matrix\n                    scores.iloc[this.queries_ix, this.targets_ix] = res.values\n            else:\n                scores = this.all_by_all()\n\n    return scores\n</code></pre>"},{"location":"reference/navis/#navis.nblast_smart","title":"<code>navis.nblast_smart</code>","text":"<p>Smart(er) NBLAST query against target neurons.</p> <p>In contrast to <code>navis.nblast</code>, this function will first run a \"pre-NBLAST\" in which only 10% of the query dotprops' points are used. Using those initial scores we select, for each query, the highest scoring targets and run the full NBLAST only on those query-target pairs (see <code>t</code> and <code>criterion</code> for fine-tuning).</p> PARAMETER DESCRIPTION <code>query</code> <pre><code>        Query neuron(s) to NBLAST against the targets. Neurons\n        should be in microns as NBLAST is optimized for that and\n        have similar sampling resolutions.\n</code></pre> <p> TYPE: <code>        Dotprops | NeuronList</code> </p> <code>target</code> <pre><code>        Target neuron(s) to NBLAST against. Neurons should be in\n        microns as NBLAST is optimized for that and have\n        similar sampling resolutions. If not provided, will NBLAST\n        queries against themselves.\n</code></pre> <p> TYPE: <code>       Dotprops | NeuronList</code> DEFAULT: <code>None</code> </p> <code>t</code> <pre><code>        Determines for which pairs we will run a full NBLAST. See\n        `criterion` parameter for details.\n</code></pre> <p> TYPE: <code>            int | float</code> DEFAULT: <code>90</code> </p> <code>criterion</code> <pre><code>        Criterion for selecting query-target pairs for full NBLAST:\n          - \"percentile\" runs full NBLAST on the `t`-th percentile\n          - \"score\" runs full NBLAST on all scores above `t`\n          - \"N\" runs full NBLAST on top `t` targets\n</code></pre> <p> TYPE: <code>    \"percentile\" | \"score\" | \"N\"</code> DEFAULT: <code>'percentile'</code> </p> <code>return_mask</code> <pre><code>        If True, will also return a boolean mask that shows which\n        scores are based on a full NBLAST and which ones only on\n        the pre-NBLAST.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> <code>scores</code> <pre><code>        Determines the final scores:\n          - 'forward' (default) returns query-&gt;target scores\n          - 'mean' returns the mean of query-&gt;target and\n            target-&gt;query scores\n          - 'min' returns the minium between query-&gt;target and\n            target-&gt;query scores\n          - 'max' returns the maximum between query-&gt;target and\n            target-&gt;query scores\n</code></pre> <p> TYPE: <code>       'forward' | 'mean' | 'min' | 'max'</code> DEFAULT: <code>'forward'</code> </p> <code>use_alpha</code> <pre><code>        Emphasizes neurons' straight parts (backbone) over parts\n        that have lots of branches.\n</code></pre> <p> TYPE: <code>    bool</code> DEFAULT: <code>False</code> </p> <code>normalized</code> <pre><code>        Whether to return normalized NBLAST scores.\n</code></pre> <p> TYPE: <code>   bool</code> DEFAULT: <code>True</code> </p> <code>smat</code> <pre><code>        Score matrix. If 'auto' (default), will use scoring matrices\n        from FCWB. Same behaviour as in R's nat.nblast\n        implementation.\n        If `smat='v1'` it uses the analytic formulation of the\n        NBLAST scoring from Kohl et. al (2013). You can adjust parameter\n        `sigma_scaling` (default to 10) using `smat_kwargs`.\n        If `smat=None` the scores will be\n        generated as the product of the distances and the dotproduct\n        of the vectors of nearest-neighbor pairs.\n        If `Callable` given, it passes distance and dot products as\n        first and second argument respectively.\n</code></pre> <p> TYPE: <code>         str | pd.DataFrame | Callable</code> DEFAULT: <code>'auto'</code> </p> <code>smat_kwargs</code> <pre><code>        functions.\n</code></pre> <p> TYPE: <code>Optional[Dict]</code> DEFAULT: <code>dict()</code> </p> <code>limit_dist</code> <pre><code>        Sets the max distance for the nearest neighbor search\n        (`distance_upper_bound`). Typically this should be the\n        highest distance considered by the scoring function. If\n        \"auto\", will extract that value from the scoring matrix.\n        While this can give a ~2X speed up, it will introduce slight\n        inaccuracies because we won't have a vector component for\n        points without a nearest neighbour within the distance\n        limits. The impact depends on the scoring function but with\n        the default FCWB `smat`, this is typically limited to the\n        third decimal (0.0086 +/- 0.0027 for an all-by-all of 1k\n        neurons).\n</code></pre> <p> TYPE: <code>   float | \"auto\" | None</code> DEFAULT: <code>'auto'</code> </p> <code>approx_nn</code> <pre><code>        If True, will use approximate nearest neighbors. This gives\n        a &gt;2X speed up but also produces only approximate scores.\n        Impact depends on the use case - testing highly recommended!\n</code></pre> <p> TYPE: <code>    bool</code> DEFAULT: <code>False</code> </p> <code>precision</code> <pre><code>        Precision for scores. Defaults to 64 bit (double) floats.\n        This is useful to reduce the memory footprint for very large\n        matrices. In real-world scenarios 32 bit (single)- and\n        depending on the purpose even 16 bit (half) - are typically\n        sufficient.\n</code></pre> <p> TYPE: <code>    int [16, 32, 64] | str [e.g. \"float64\"] | np.dtype</code> DEFAULT: <code>64</code> </p> <code>n_cores</code> <pre><code>        Max number of cores to use for nblasting. Default is\n        `os.cpu_count() // 2`. This should ideally be an even\n        number as that allows optimally splitting queries onto\n        individual processes.\n</code></pre> <p> TYPE: <code>      int</code> DEFAULT: <code>os.cpu_count() // 2</code> </p> <code>progress</code> <pre><code>        Whether to show progress bars.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>scores</code> <p>Matrix with NBLAST scores. Rows are query neurons, columns are targets. The order is the same as in <code>query</code>/<code>target</code> and the labels are based on the neurons' <code>.id</code> property.</p> <p> TYPE: <code>pandas.DataFrame</code> </p> <code>mask</code> <p>Only if <code>return_mask=True</code>: a boolean mask with same shape as <code>scores</code> that shows which scores are based on a full NBLAST and which ones only on the pre-NBLAST.</p> <p> TYPE: <code>np.ndarray</code> </p> References <p>Costa M, Manton JD, Ostrovsky AD, Prohaska S, Jefferis GS. NBLAST: Rapid, Sensitive Comparison of Neuronal Structure and Construction of Neuron Family Databases. Neuron. 2016 Jul 20;91(2):293-311. doi: 10.1016/j.neuron.2016.06.012.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(n=5)\n&gt;&gt;&gt; nl.units\n&lt;Quantity([8 8 8 8 8], 'nanometer')&gt;\n&gt;&gt;&gt; # Convert to microns\n&gt;&gt;&gt; nl_um = nl * (8 / 1000)\n&gt;&gt;&gt; # Convert to dotprops\n&gt;&gt;&gt; dps = navis.make_dotprops(nl_um)\n&gt;&gt;&gt; # Run a NBLAST where only the top target from the pre-NBLAST is run\n&gt;&gt;&gt; # through a full NBLAST\n&gt;&gt;&gt; scores = navis.nblast_smart(dps[:3], dps[3:], t=1, criterion='N')\n</code></pre> See Also <p><code>navis.nblast</code>             The conventional full NBLAST. <code>navis.nblast_allbyall</code>             A more efficient way than <code>nblast(query=x, target=x)</code>. <code>navis.synblast</code>             A synapse-based variant of NBLAST.</p> Source code in <code>navis/nbl/nblast_funcs.py</code> <pre><code>def nblast_smart(query: Union[Dotprops, NeuronList],\n                 target: Optional[str] = None,\n                 t: Union[int, float] = 90,\n                 criterion: Union[Literal['percentile'],\n                                  Literal['score'],\n                                  Literal['N']] = 'percentile',\n                 scores: Union[Literal['forward'],\n                               Literal['mean'],\n                               Literal['min'],\n                               Literal['max']] = 'forward',\n                 return_mask: bool = False,\n                 normalized: bool = True,\n                 use_alpha: bool = False,\n                 smat: Optional[Union[str, pd.DataFrame]] = 'auto',\n                 limit_dist: Optional[Union[Literal['auto'], int, float]] = 'auto',\n                 approx_nn: bool = False,\n                 precision: Union[int, str, np.dtype] = 64,\n                 n_cores: int = os.cpu_count() // 2,\n                 progress: bool = True,\n                 smat_kwargs: Optional[Dict] = dict()) -&gt; pd.DataFrame:\n    \"\"\"Smart(er) NBLAST query against target neurons.\n\n    In contrast to [`navis.nblast`][], this function will first run a\n    \"pre-NBLAST\" in which only 10% of the query dotprops' points are used.\n    Using those initial scores we select, for each query, the highest scoring\n    targets and run the full NBLAST only on those query-target pairs (see\n    `t` and `criterion` for fine-tuning).\n\n    Parameters\n    ----------\n    query :         Dotprops | NeuronList\n                    Query neuron(s) to NBLAST against the targets. Neurons\n                    should be in microns as NBLAST is optimized for that and\n                    have similar sampling resolutions.\n    target :        Dotprops | NeuronList, optional\n                    Target neuron(s) to NBLAST against. Neurons should be in\n                    microns as NBLAST is optimized for that and have\n                    similar sampling resolutions. If not provided, will NBLAST\n                    queries against themselves.\n    t :             int | float\n                    Determines for which pairs we will run a full NBLAST. See\n                    `criterion` parameter for details.\n    criterion :     \"percentile\" | \"score\" | \"N\"\n                    Criterion for selecting query-target pairs for full NBLAST:\n                      - \"percentile\" runs full NBLAST on the `t`-th percentile\n                      - \"score\" runs full NBLAST on all scores above `t`\n                      - \"N\" runs full NBLAST on top `t` targets\n    return_mask :   bool\n                    If True, will also return a boolean mask that shows which\n                    scores are based on a full NBLAST and which ones only on\n                    the pre-NBLAST.\n    scores :        'forward' | 'mean' | 'min' | 'max'\n                    Determines the final scores:\n                      - 'forward' (default) returns query-&gt;target scores\n                      - 'mean' returns the mean of query-&gt;target and\n                        target-&gt;query scores\n                      - 'min' returns the minium between query-&gt;target and\n                        target-&gt;query scores\n                      - 'max' returns the maximum between query-&gt;target and\n                        target-&gt;query scores\n    use_alpha :     bool, optional\n                    Emphasizes neurons' straight parts (backbone) over parts\n                    that have lots of branches.\n    normalized :    bool, optional\n                    Whether to return normalized NBLAST scores.\n    smat :          str | pd.DataFrame | Callable\n                    Score matrix. If 'auto' (default), will use scoring matrices\n                    from FCWB. Same behaviour as in R's nat.nblast\n                    implementation.\n                    If `smat='v1'` it uses the analytic formulation of the\n                    NBLAST scoring from Kohl et. al (2013). You can adjust parameter\n                    `sigma_scaling` (default to 10) using `smat_kwargs`.\n                    If `smat=None` the scores will be\n                    generated as the product of the distances and the dotproduct\n                    of the vectors of nearest-neighbor pairs.\n                    If `Callable` given, it passes distance and dot products as\n                    first and second argument respectively.\n    smat_kwargs:    Dictionary with additional parameters passed to scoring\n                    functions.\n    limit_dist :    float | \"auto\" | None\n                    Sets the max distance for the nearest neighbor search\n                    (`distance_upper_bound`). Typically this should be the\n                    highest distance considered by the scoring function. If\n                    \"auto\", will extract that value from the scoring matrix.\n                    While this can give a ~2X speed up, it will introduce slight\n                    inaccuracies because we won't have a vector component for\n                    points without a nearest neighbour within the distance\n                    limits. The impact depends on the scoring function but with\n                    the default FCWB `smat`, this is typically limited to the\n                    third decimal (0.0086 +/- 0.0027 for an all-by-all of 1k\n                    neurons).\n    approx_nn :     bool\n                    If True, will use approximate nearest neighbors. This gives\n                    a &gt;2X speed up but also produces only approximate scores.\n                    Impact depends on the use case - testing highly recommended!\n    precision :     int [16, 32, 64] | str [e.g. \"float64\"] | np.dtype\n                    Precision for scores. Defaults to 64 bit (double) floats.\n                    This is useful to reduce the memory footprint for very large\n                    matrices. In real-world scenarios 32 bit (single)- and\n                    depending on the purpose even 16 bit (half) - are typically\n                    sufficient.\n    n_cores :       int, optional\n                    Max number of cores to use for nblasting. Default is\n                    `os.cpu_count() // 2`. This should ideally be an even\n                    number as that allows optimally splitting queries onto\n                    individual processes.\n    progress :      bool\n                    Whether to show progress bars.\n\n    Returns\n    -------\n    scores :        pandas.DataFrame\n                    Matrix with NBLAST scores. Rows are query neurons, columns\n                    are targets. The order is the same as in `query`/`target`\n                    and the labels are based on the neurons' `.id` property.\n    mask :          np.ndarray\n                    Only if `return_mask=True`: a boolean mask with same shape\n                    as `scores` that shows which scores are based on a full\n                    NBLAST and which ones only on the pre-NBLAST.\n\n    References\n    ----------\n    Costa M, Manton JD, Ostrovsky AD, Prohaska S, Jefferis GS. NBLAST: Rapid,\n    Sensitive Comparison of Neuronal Structure and Construction of Neuron\n    Family Databases. Neuron. 2016 Jul 20;91(2):293-311.\n    doi: 10.1016/j.neuron.2016.06.012.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(n=5)\n    &gt;&gt;&gt; nl.units\n    &lt;Quantity([8 8 8 8 8], 'nanometer')&gt;\n    &gt;&gt;&gt; # Convert to microns\n    &gt;&gt;&gt; nl_um = nl * (8 / 1000)\n    &gt;&gt;&gt; # Convert to dotprops\n    &gt;&gt;&gt; dps = navis.make_dotprops(nl_um)\n    &gt;&gt;&gt; # Run a NBLAST where only the top target from the pre-NBLAST is run\n    &gt;&gt;&gt; # through a full NBLAST\n    &gt;&gt;&gt; scores = navis.nblast_smart(dps[:3], dps[3:], t=1, criterion='N')\n\n    See Also\n    --------\n    [`navis.nblast`][]\n                The conventional full NBLAST.\n    [`navis.nblast_allbyall`][]\n                A more efficient way than `nblast(query=x, target=x)`.\n    [`navis.synblast`][]\n                A synapse-based variant of NBLAST.\n\n    \"\"\"\n    utils.eval_param(criterion, name='criterion',\n                     allowed_values=(\"percentile\", \"score\", \"N\"))\n    utils.eval_param(scores, name='scores', allowed_values=ALLOWED_SCORES)\n\n    # We will make a couple tweaks for speed things up if this is\n    # an all-by-all NBLAST\n    aba = False\n    pre_scores = scores\n    if isinstance(target, type(None)):\n        target = query\n        aba = True\n        # For all-by-all's we can compute only forward scores and\n        # produce the mean later\n        if scores == 'mean':\n            pre_scores = 'forward'\n\n    try:\n        t = int(t)\n    except BaseException:\n        raise TypeError(f'`t` must be (convertable to) integer - got \"{type(t)}\"')\n\n    if criterion == 'percentile':\n        if (t &lt;= 0 or t &gt;= 100):\n            raise ValueError('Expected `t` to be integer between 0 and 100 for '\n                             f'criterion \"percentile\", got {t}')\n    elif criterion == 'N':\n        if (t &lt; 0 or t &gt; len(target)):\n            raise ValueError('`t` must be between 0 and the total number of '\n                             f'targets ({len(target)}) for criterion \"N\", '\n                             f'got {t}')\n\n    # Make sure we're working on NeuronLists\n    query_dps = NeuronList(query)\n    target_dps = NeuronList(target)\n\n    # Run NBLAST preflight checks\n    nblast_preflight(query_dps, target_dps, n_cores,\n                     req_unique_ids=True,\n                     req_microns=isinstance(smat, str) and smat=='auto')\n\n    # Make simplified dotprops\n    query_dps_simp = query_dps.downsample(10, inplace=False)\n    if not aba:\n        target_dps_simp = target_dps.downsample(10, inplace=False)\n    else:\n        target_dps_simp = query_dps_simp\n\n    # Find a partition that produces batches that each run in approximately\n    # 10 seconds\n    if n_cores and n_cores &gt; 1:\n        if progress:\n            # If progress bar, we need to make smaller mini batches.\n            # These mini jobs must not be too small - otherwise the overhead\n            # from spawning and sending results between processes slows things\n            # down dramatically. Hence we want to make sure that each job runs\n            # for &gt;10s. The run time depends on the system and how big the neurons\n            # are. Here, we run a quick test and try to extrapolate from there\n            n_rows, n_cols = find_batch_partition(query_dps_simp, target_dps_simp,\n                                                  T=10 * JOB_SIZE_MULTIPLIER)\n        else:\n            # If no progress bar needed, we can just split neurons evenly across\n            # all available cores\n            n_rows, n_cols = find_optimal_partition(n_cores, query_dps_simp, target_dps_simp)\n    else:\n        n_rows = n_cols = 1\n\n    # Calculate self-hits once for all neurons\n    nb = NBlaster(use_alpha=use_alpha,\n                  normalized=normalized,\n                  smat=smat,\n                  limit_dist=limit_dist,\n                  dtype=precision,\n                  approx_nn=approx_nn,\n                  progress=progress,\n                  smat_kwargs=smat_kwargs)\n    query_self_hits = np.array([nb.calc_self_hit(n) for n in query_dps_simp])\n    target_self_hits = np.array([nb.calc_self_hit(n) for n in target_dps_simp])\n\n    # This makes sure we don't run into multiple layers of concurrency\n    with set_omp_flag(limits=OMP_NUM_THREADS_LIMIT if n_cores and (n_cores &gt; 1) else None):\n        # Initialize a pool of workers\n        # Note that we're forcing \"spawn\" instead of \"fork\" (default on linux)!\n        # This is to reduce the memory footprint since \"fork\" appears to inherit all\n        # variables (including all neurons) while \"spawn\" appears to get only\n        # what's required to run the job?\n        with ProcessPoolExecutor(max_workers=n_cores,\n                                 mp_context=mp.get_context('spawn')) as pool:\n            with config.tqdm(desc='Prep. pre-NBLAST',\n                             total=n_rows * n_cols,\n                             leave=False,\n                             disable=not progress) as pbar:\n                futures = {}\n                nblasters = []\n                for qix in np.array_split(np.arange(len(query_dps_simp)), n_rows):\n                    for tix in np.array_split(np.arange(len(target_dps_simp)), n_cols):\n                        # Initialize NBlaster\n                        this = NBlaster(use_alpha=use_alpha,\n                                        normalized=normalized,\n                                        smat=smat,\n                                        limit_dist=limit_dist,\n                                        dtype=precision,\n                                        approx_nn=approx_nn,\n                                        progress=progress,\n                                        smat_kwargs=smat_kwargs)\n\n                        # Add queries and targets\n                        for i, ix in enumerate(qix):\n                            this.append(query_dps_simp[ix], query_self_hits[ix])\n                        for i, ix in enumerate(tix):\n                            this.append(target_dps_simp[ix], target_self_hits[ix])\n\n                        # Keep track of indices of queries and targets\n                        this.queries = np.arange(len(qix))\n                        this.targets = np.arange(len(tix)) + len(qix)\n                        this.queries_ix = qix  # this facilitates filling in the big matrix later\n                        this.targets_ix = tix  # this facilitates filling in the big matrix later\n                        this.pbar_position = len(nblasters) if not utils.is_jupyter() else None\n\n                        nblasters.append(this)\n                        pbar.update()\n\n                        # If multiple cores requested, submit job to the pool right away\n                        if n_cores and n_cores &gt; 1 and (n_cols &gt; 1 or n_rows &gt; 1):\n                            this.progress=False  # no progress bar for individual NBLASTERs\n                            futures[pool.submit(this.multi_query_target,\n                                                q_idx=this.queries,\n                                                t_idx=this.targets,\n                                                scores=pre_scores)] = this\n\n            # Collect results\n            if futures and len(futures) &gt; 1:\n                # Prepare empty score matrix\n                scr = pd.DataFrame(np.empty((len(query_dps_simp),\n                                             len(target_dps_simp)),\n                                            dtype=this.dtype),\n                                      index=query_dps_simp.id,\n                                      columns=target_dps_simp.id)\n                scr.index.name = 'query'\n                scr.columns.name = 'target'\n\n                # Collect results\n                # We're dropping the \"N / N_total\" bit from the progress bar because\n                # it's not helpful here\n                fmt = ('{desc}: {percentage:3.0f}%|{bar}| [{elapsed}&lt;{remaining}]')\n                for f in config.tqdm(as_completed(futures),\n                                     desc='Pre-NBLASTs',\n                                     bar_format=fmt,\n                                     total=len(futures),\n                                     smoothing=0,\n                                     disable=not progress,\n                                     leave=False):\n                    res = f.result()\n                    this = futures[f]\n                    # Fill-in big score matrix\n                    scr.iloc[this.queries_ix, this.targets_ix] = res.values\n            else:\n                scr = this.multi_query_target(this.queries,\n                                              this.targets,\n                                              scores=scores)\n\n    # If this is an all-by-all and we would have computed only forward scores\n    # during pre-NBLAST\n    if aba and scores == 'mean':\n        scr = (scr + scr.T.values) / 2\n\n    # Now select targets of interest for each query\n    if criterion == 'percentile':\n        # Generate a mask for the scores we want to recalculate from full dotprops\n        sel = np.percentile(scr, q=t, axis=1)\n        mask = scr &gt;= sel.reshape(-1, 1)\n    elif criterion == 'score':\n        # Generate a mask for the scores we want to recalculate from full dotprops\n        sel = np.full(scr.shape[0], fill_value=t)\n        mask = scr &gt;= sel.reshape(-1, 1)\n    else:\n        # Sort such that the top hit is to the left\n        srt = np.argsort(scr.values, axis=1)[:, ::-1]\n        # Generate the mask\n        mask = pd.DataFrame(np.zeros(scr.shape, dtype=bool),\n                            columns=scr.columns, index=scr.index)\n        _ = np.arange(mask.shape[0])\n        for N in range(t):\n            mask.iloc[_, srt[:, N]] = True\n\n    # Calculate self-hits for full neurons\n    query_self_hits = np.array([nb.calc_self_hit(n) for n in query_dps])\n    target_self_hits = np.array([nb.calc_self_hit(n) for n in target_dps])\n\n    # This makes sure we don't run into multiple layers of concurrency\n    with set_omp_flag(limits=OMP_NUM_THREADS_LIMIT if n_cores and (n_cores &gt; 1) else None):\n        # Initialize a pool of workers\n        # Note that we're forcing \"spawn\" instead of \"fork\" (default on linux)!\n        # This is to reduce the memory footprint since \"fork\" appears to inherit all\n        # variables (including all neurons) while \"spawn\" appears to get only\n        # what's required to run the job?\n        with ProcessPoolExecutor(max_workers=n_cores,\n                                 mp_context=mp.get_context('spawn')) as pool:\n            with config.tqdm(desc='Prep. full NBLAST',\n                             total=n_rows * n_cols,\n                             leave=False,\n                             disable=not progress) as pbar:\n                futures = {}\n                nblasters = []\n                for qix in np.array_split(np.arange(len(query_dps)), n_rows):\n                    for tix in np.array_split(np.arange(len(target_dps)), n_cols):\n                        # Initialize NBlaster\n                        this = NBlaster(use_alpha=use_alpha,\n                                        normalized=normalized,\n                                        smat=smat,\n                                        limit_dist=limit_dist,\n                                        dtype=precision,\n                                        approx_nn=approx_nn,\n                                        progress=progress,\n                                        smat_kwargs=smat_kwargs)\n                        # Add queries and targets\n                        for i, ix in enumerate(qix):\n                            this.append(query_dps[ix], query_self_hits[ix])\n                        for i, ix in enumerate(tix):\n                            this.append(target_dps[ix], target_self_hits[ix])\n\n                        # Find the pairs to NBLAST in this part of the matrix\n                        submask = mask.loc[query_dps[qix].id,\n                                           target_dps[tix].id]\n                        # `pairs` is an array of `[[query, target], [...]]` pairs\n                        this.pairs = np.vstack(np.where(submask)).T\n\n                        # Offset the query indices\n                        this.pairs[:, 1] += len(qix)\n\n                        # Track this NBLASTER's mask relative to the original big one\n                        this.mask = np.zeros(mask.shape, dtype=bool)\n                        this.mask[qix[0]:qix[-1]+1, tix[0]:tix[-1]+1] = submask\n\n                        # Make sure position of progress bar checks out\n                        this.pbar_position = len(nblasters) if not utils.is_jupyter() else None\n                        this.desc = 'Full NBLAST'\n\n                        nblasters.append(this)\n                        pbar.update()\n\n                        # If multiple cores requested, submit job to the pool right away\n                        if n_cores and n_cores &gt; 1 and (n_cols &gt; 1 or n_rows &gt; 1):\n                            this.progress=False  # no progress bar for individual NBLASTERs\n                            futures[pool.submit(this.pair_query_target,\n                                                pairs=this.pairs,\n                                                scores=scores)] = this\n\n            # Collect results\n            if futures and len(futures) &gt; 1:\n                # Collect results\n                # We're dropping the \"N / N_total\" bit from the progress bar because\n                # it's not helpful here\n                fmt = ('{desc}: {percentage:3.0f}%|{bar}| [{elapsed}&lt;{remaining}]')\n                for f in config.tqdm(as_completed(futures),\n                                     desc='NBLASTing',\n                                     bar_format=fmt,\n                                     total=len(futures),\n                                     smoothing=0,\n                                     disable=not progress,\n                                     leave=False):\n                    res = f.result()\n                    this = futures[f]\n\n                    # Fill-in big score matrix\n                    scr[this.mask] = res\n            else:\n                scr[mask] = this.pair_query_target(this.pairs, scores=scores)\n\n    if return_mask:\n        return scr, mask\n\n    return scr\n</code></pre>"},{"location":"reference/navis/#navis.network2igraph","title":"<code>navis.network2igraph</code>","text":"<p>Generate iGraph graph from edge list or adjacency.</p> <p>Requires iGraph to be installed.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            Connectivity information:\n\n             1. List of edges (columns: 'source', 'target', 'weight')\n             2. Adjacency matrix (pd.DataFrame, rows=sources,\n                columns=targets)\n</code></pre> <p> TYPE: <code>                pandas.DataFrame | np.array</code> </p> <code>threshold</code> <pre><code>            Connections weaker than this will be excluded.\n</code></pre> <p> TYPE: <code>        float | int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>igraph.Graph(directed=True)</code> <p>iGraph representation of the network.</p> Source code in <code>navis/graph/converters.py</code> <pre><code>def network2igraph(\n    x: Union[pd.DataFrame, Iterable], threshold: Optional[float] = None\n) -&gt; \"igraph.Graph\":\n    \"\"\"Generate iGraph graph from edge list or adjacency.\n\n    Requires iGraph to be installed.\n\n    Parameters\n    ----------\n    x :                 pandas.DataFrame | np.array\n                        Connectivity information:\n\n                         1. List of edges (columns: 'source', 'target', 'weight')\n                         2. Adjacency matrix (pd.DataFrame, rows=sources,\n                            columns=targets)\n\n    threshold :         float | int, optional\n                        Connections weaker than this will be excluded.\n\n    Returns\n    -------\n    igraph.Graph(directed=True)\n                        iGraph representation of the network.\n\n    \"\"\"\n    if igraph is None:\n        raise ModuleNotFoundError(\"igraph must be installed to use this function.\")\n\n    if isinstance(x, pd.DataFrame):\n        present = [c in x.columns for c in [\"source\", \"target\", \"weight\"]]\n        if all(present):\n            edges = x[[\"source\", \"target\", \"weight\"]].values\n        else:\n            edges = (\n                x.reset_index(inplace=False, drop=False)\n                .melt(id_vars=\"index\", inplace=False)\n                .values\n            )\n    elif isinstance(x, (list, np.ndarray)):\n        edges = np.array(x)\n    else:\n        raise TypeError(f'Expected numpy array or pandas DataFrame, got \"{type(x)}\"')\n\n    if edges.ndim != 2 or edges.shape[1] != 3:\n        raise ValueError(\n            \"Edges must be (N, 3) array containing source, \" \"target, weight\"\n        )\n\n    if not isinstance(threshold, (type(None), bool)):\n        edges = edges[edges[:, 2] &gt;= threshold]\n\n    names = list(set(np.array(edges)[:, 0]) | set(np.array(edges)[:, 1]))\n\n    edges_by_index = [[names.index(e[0]), names.index(e[1])] for e in edges]\n\n    # Generate igraph and assign custom properties\n    g = igraph.Graph(directed=True)\n    g.add_vertices(len(names))\n    g.add_edges(edges_by_index)\n\n    g.vs[\"node_id\"] = names\n    # g.vs['neuron_name'] = g.vs['label'] = neuron_names\n    g.es[\"weight\"] = edges[:, 2]\n\n    return g\n</code></pre>"},{"location":"reference/navis/#navis.network2nx","title":"<code>navis.network2nx</code>","text":"<p>Generate NetworkX graph from edge list or adjacency.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            Connectivity information:\n\n             1. List of edges (columns: 'source', 'target', 'weight')\n             2. Adjacency matrix (pd.DataFrame, rows=sources,\n                columns=targets)\n</code></pre> <p> TYPE: <code>                pandas.DataFrame</code> </p> <code>threshold</code> <pre><code>            Connections weaker than this will be excluded.\n</code></pre> <p> TYPE: <code>        float | int</code> DEFAULT: <code>None</code> </p> <code>group_by</code> <pre><code>            Provide a dictionary `{group_name: [skid1, skid2, ...]}`\n            to collapse sets of nodes into groups.\n</code></pre> <p> TYPE: <code>         None | dict</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>networkx.DiGraph</code> <p>NetworkX representation of the network.</p> Source code in <code>navis/graph/converters.py</code> <pre><code>def network2nx(\n    x: Union[pd.DataFrame, Iterable],\n    threshold: Optional[float] = None,\n    group_by: Union[dict, None] = None,\n) -&gt; nx.DiGraph:\n    \"\"\"Generate NetworkX graph from edge list or adjacency.\n\n    Parameters\n    ----------\n    x :                 pandas.DataFrame\n                        Connectivity information:\n\n                         1. List of edges (columns: 'source', 'target', 'weight')\n                         2. Adjacency matrix (pd.DataFrame, rows=sources,\n                            columns=targets)\n\n    threshold :         float | int, optional\n                        Connections weaker than this will be excluded.\n    group_by :          None | dict, optional\n                        Provide a dictionary `{group_name: [skid1, skid2, ...]}`\n                        to collapse sets of nodes into groups.\n\n    Returns\n    -------\n    networkx.DiGraph\n                        NetworkX representation of the network.\n\n    \"\"\"\n    if isinstance(x, pd.DataFrame):\n        present = [c in x.columns for c in [\"source\", \"target\", \"weight\"]]\n        if all(present):\n            edges = x[[\"source\", \"target\", \"weight\"]].values\n        else:\n            # Assume it's an adjacency matrix\n            ix_name = x.index.name if x.index.name else \"index\"\n            edges = (\n                x.reset_index(inplace=False, drop=False).melt(id_vars=ix_name).values\n            )\n    elif isinstance(x, (list, np.ndarray)):\n        edges = np.array(x)\n    else:\n        raise TypeError(f'Expected numpy array or pandas DataFrame, got \"{type(x)}\"')\n\n    if edges.ndim != 2 or edges.shape[1] != 3:\n        raise ValueError(\n            \"Edges must be (N, 3) array containing source, \" \"target, weight\"\n        )\n\n    if not isinstance(threshold, (type(None), bool)):\n        edges = edges[edges[:, 2] &gt;= threshold]\n\n    # Generate graph and assign custom properties\n    g = nx.DiGraph()\n    g.add_weighted_edges_from(edges)\n\n    # Group nodes\n    if group_by:\n        for n, skids in group_by.items():\n            # First collapse all nodes into the first of each group\n            for s in skids[1:]:\n                g = nx.contracted_nodes(g, str(skids[0]), str(s))\n            # Now relabel the first node\n            g = nx.relabel_nodes(g, {str(skids[0]): str(n)})\n            g.nodes[str(n)][\"neuron_name\"] = str(n)\n\n    return g\n</code></pre>"},{"location":"reference/navis/#navis.neuron2KDTree","title":"<code>navis.neuron2KDTree</code>","text":"<p>Turn neuron into scipy KDTree.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    A single neuron to turn into a KDTree.\n</code></pre> <p> TYPE: <code>        TreeNeuron | MeshNeuron | VoxelNeuron | Dotprops</code> </p> <code>tree_type</code> <pre><code>    Type of KDTree:\n      1. `'c'` = `scipy.spatial.cKDTree` (faster)\n      2. `'normal'` = `scipy.spatial.KDTree` (more functions)\n</code></pre> <p> TYPE: <code>c | normal</code> DEFAULT: <code>'c'</code> </p> <code>data</code> <pre><code>    Data used to generate tree. \"auto\" will pick the core data\n    depending on neuron type: `nodes`, `vertices`, `voxels` and\n    `points` for TreeNeuron, MeshNeuron, VoxelNeuron and Dotprops,\n    respectively. Other values (e.g. \"connectors\" or \"nodes\") must\n    map to a neuron property that is either (N, 3) array or\n    DataFrame with x/y/z columns.\n</code></pre> <p> TYPE: <code>     'auto' | str</code> DEFAULT: <code>'auto'</code> </p> <code>**kwargs</code> <pre><code>    Keyword arguments passed at KDTree initialization.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>`scipy.spatial.cKDTree` or `scipy.spatial.KDTree`</code> Source code in <code>navis/graph/converters.py</code> <pre><code>def neuron2KDTree(\n    x: \"core.NeuronObject\", tree_type: str = \"c\", data: str = \"auto\", **kwargs\n) -&gt; Union[scipy.spatial.cKDTree, scipy.spatial.KDTree]:\n    \"\"\"Turn neuron into scipy KDTree.\n\n    Parameters\n    ----------\n    x :         TreeNeuron | MeshNeuron | VoxelNeuron | Dotprops\n                A single neuron to turn into a KDTree.\n    tree_type : 'c' | 'normal'\n                Type of KDTree:\n                  1. `'c'` = `scipy.spatial.cKDTree` (faster)\n                  2. `'normal'` = `scipy.spatial.KDTree` (more functions)\n    data :      'auto' | str\n                Data used to generate tree. \"auto\" will pick the core data\n                depending on neuron type: `nodes`, `vertices`, `voxels` and\n                `points` for TreeNeuron, MeshNeuron, VoxelNeuron and Dotprops,\n                respectively. Other values (e.g. \"connectors\" or \"nodes\") must\n                map to a neuron property that is either (N, 3) array or\n                DataFrame with x/y/z columns.\n    **kwargs\n                Keyword arguments passed at KDTree initialization.\n\n\n    Returns\n    -------\n    `scipy.spatial.cKDTree` or `scipy.spatial.KDTree`\n\n    \"\"\"\n    if tree_type not in [\"c\", \"normal\"]:\n        raise ValueError('\"tree_type\" needs to be either \"c\" or \"normal\"')\n\n    if isinstance(x, core.NeuronList):\n        if len(x) == 1:\n            x = x[0]\n        else:\n            raise ValueError(\"Need a single TreeNeuron\")\n    elif not isinstance(x, core.BaseNeuron):\n        raise TypeError(f'Need Neuron, got \"{type(x)}\"')\n\n    if data == \"auto\":\n        if isinstance(x, core.TreeNeuron):\n            data = \"nodes\"\n        if isinstance(x, core.MeshNeuron):\n            data = \"vertices\"\n        if isinstance(x, core.VoxelNeuron):\n            data = \"voxels\"\n        if isinstance(x, core.Dotprops):\n            data = \"points\"\n\n    if not hasattr(x, data):\n        raise ValueError(f\"Neuron does not have a {data} property\")\n\n    data = getattr(x, data)\n\n    if isinstance(data, pd.DataFrame):\n        if not all(np.isin([\"x\", \"y\", \"z\"], data.columns)):\n            raise ValueError(\n                f'\"{data}\" DataFrame must contain \"x\", \"y\" and ' '\"z\" columns.'\n            )\n        data = data[[\"x\", \"y\", \"z\"]].values\n\n    if not isinstance(data, np.ndarray) or data.ndim != 2 or data.shape[1] != 3:\n        raise ValueError(\n            f'\"{data}\" must be DataFrame or (N, 3) array, got {type(data)}'\n        )\n\n    if tree_type == \"c\":\n        return scipy.spatial.cKDTree(data=data, **kwargs)\n    else:\n        return scipy.spatial.KDTree(data=data, **kwargs)\n</code></pre>"},{"location":"reference/navis/#navis.neuron2igraph","title":"<code>navis.neuron2igraph</code>","text":"<p>Turn Tree-, Mesh- or VoxelNeuron(s) into an iGraph graph.</p> <p>Requires iGraph to be installed.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>                Neuron(s) to convert.\n</code></pre> <p> TYPE: <code>                    TreeNeuron | MeshNeuron | VoxelNeuron | NeuronList</code> </p> <code>simplify</code> <pre><code>                For TreeNeurons only: simplify the graph by keeping only roots,\n                leaves and branching points. Preserves the original branch\n                lengths (i.e. weights).\n</code></pre> <p> TYPE: <code>             bool</code> DEFAULT: <code>False</code> </p> <code>connectivity</code> <pre><code>                For VoxelNeurons only. Defines the connectedness:\n                 - 6 = faces\n                 - 18 = faces + edges\n                 - 26 = faces + edges + vertices\n</code></pre> <p> TYPE: <code>         6 | 18 | 26</code> DEFAULT: <code>18</code> </p> <code>raise_not_installed</code> <pre><code>                If False and igraph is not installed will silently\n                return `None`.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>igraph.Graph</code> <p>Representation of the neuron. Returns list of graphs if x is multiple neurons. Directed for TreeNeurons, undirected for MeshNeurons.</p> <code>None</code> <p>If igraph not installed.</p> Source code in <code>navis/graph/converters.py</code> <pre><code>def neuron2igraph(\n    x: \"core.NeuronObject\",\n    simplify: bool = False,\n    connectivity: int = 18,\n    raise_not_installed: bool = True,\n) -&gt; \"igraph.Graph\":\n    \"\"\"Turn Tree-, Mesh- or VoxelNeuron(s) into an iGraph graph.\n\n    Requires iGraph to be installed.\n\n    Parameters\n    ----------\n    x :                     TreeNeuron | MeshNeuron | VoxelNeuron | NeuronList\n                            Neuron(s) to convert.\n    simplify :              bool\n                            For TreeNeurons only: simplify the graph by keeping only roots,\n                            leaves and branching points. Preserves the original branch\n                            lengths (i.e. weights).\n    connectivity :          6 | 18 | 26\n                            For VoxelNeurons only. Defines the connectedness:\n                             - 6 = faces\n                             - 18 = faces + edges\n                             - 26 = faces + edges + vertices\n    raise_not_installed :   bool\n                            If False and igraph is not installed will silently\n                            return `None`.\n\n    Returns\n    -------\n    igraph.Graph\n                Representation of the neuron. Returns list of graphs\n                if x is multiple neurons. Directed for TreeNeurons, undirected\n                for MeshNeurons.\n    None\n                If igraph not installed.\n\n    \"\"\"\n    # If iGraph is not installed return nothing\n    if igraph is None:\n        if not raise_not_installed:\n            return None\n        else:\n            raise ModuleNotFoundError(\n                \"iGraph appears to not be installed (properly). \"\n                'Make sure \"import igraph\" works.'\n            )\n\n    if isinstance(x, core.NeuronList):\n        return [\n            neuron2igraph(x.loc[i], connectivity=connectivity)\n            for i in range(x.shape[0])\n        ]\n\n    if isinstance(x, core.TreeNeuron):\n        # Make sure we have correctly numbered indices\n        nodes = x.nodes.reset_index(inplace=False, drop=True)\n\n        # Generate list of vertices -&gt; this order is retained\n        vlist = nodes.node_id.values\n\n        # Get list of edges as indices (needs to exclude root node)\n        tn_index_with_parent = nodes.index.values[nodes.parent_id &gt;= 0]\n        parent_ids = nodes.parent_id.values[nodes.parent_id &gt;= 0]\n        nodes[\"temp_index\"] = nodes.index  # add temporary index column\n        try:\n            parent_index = (\n                nodes.set_index(\"node_id\", inplace=False)\n                .loc[parent_ids, \"temp_index\"]\n                .values\n            )\n        except KeyError:\n            miss = nodes[~nodes.parent_id.isin(nodes.node_id)].node_id.unique()\n            raise KeyError(\n                f\"{len(miss)} nodes (e.g. {miss[0]}) in TreeNeuron \"\n                f\"{x.id} connect to non-existent parent nodes.\"\n            )\n        except BaseException:\n            raise\n\n        # Generate list of edges based on index of vertices\n        elist = np.vstack((tn_index_with_parent, parent_index)).T\n\n        # iGraph &lt; 0.8.0 does not like arrays as edge list\n        if getattr(igraph, \"__version_info__\", (0, 0, 0))[1] &lt; 8:\n            elist = elist.tolist()\n\n        # Generate graph and assign custom properties\n        G = igraph.Graph(elist, n=len(vlist), directed=True)\n\n        G.vs[\"node_id\"] = G.vs[\"name\"] = nodes.node_id.values\n        G.vs[\"parent_id\"] = nodes.parent_id.values\n\n        # Generate weights by calculating edge lengths = distance between nodes\n        tn_coords = nodes[[\"x\", \"y\", \"z\"]].values[tn_index_with_parent, :]\n        parent_coords = nodes[[\"x\", \"y\", \"z\"]].values[parent_index.astype(int), :]\n\n        w = np.sqrt(np.sum((tn_coords - parent_coords) ** 2, axis=1))\n        G.es[\"weight\"] = w\n\n        if simplify:\n            simplify_graph(G, inplace=True)\n    elif isinstance(x, core.MeshNeuron):\n        elist = x.trimesh.edges_unique\n        G = igraph.Graph(elist, n=x.n_vertices, directed=False)\n        G.es[\"weight\"] = x.trimesh.edges_unique_length\n    elif isinstance(x, core.VoxelNeuron):\n        edges = _voxels2edges(x, connectivity=connectivity)\n        G = igraph.Graph(edges, n=len(x.voxels), directed=False)\n    else:\n        raise ValueError(f'Unable to convert data of type \"{type(x)}\" to igraph.')\n\n    return G\n</code></pre>"},{"location":"reference/navis/#navis.neuron2nx","title":"<code>navis.neuron2nx</code>","text":"<p>Turn Tree-, Mesh- or VoxelNeuron into an NetworkX graph.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Uses simple 6-connectedness for voxels.\n</code></pre> <p> TYPE: <code>        TreeNeuron | MeshNeuron | VoxelNeuron | NeuronList</code> </p> <code>simplify</code> <pre><code>    For TreeNeurons only: simplify the graph by keeping only roots,\n    leaves and branching points. Preserves the original\n    branch lengths (i.e. weights).\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>False</code> </p> <code>epsilon</code> <pre><code>    For Dotprops only: maximum distance between two points to\n    connect them. If `None`, will use 5x the average distance\n    between points (i.e. `5 * x.sampling_resolution`).\n</code></pre> <p> TYPE: <code>  float</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>graph</code> <p>NetworkX representation of the neuron. Returns list of graphs if x is multiple neurons. Graph is directed for TreeNeurons and undirected for Mesh- and VoxelNeurons. Graph is weighted for Tree- and MeshNeurons.</p> <p> TYPE: <code>networkx.Graph | networkx.DiGraph</code> </p> Source code in <code>navis/graph/converters.py</code> <pre><code>def neuron2nx(x: \"core.NeuronObject\", simplify=False, epsilon=None) -&gt; nx.DiGraph:\n    \"\"\"Turn Tree-, Mesh- or VoxelNeuron into an NetworkX graph.\n\n    Parameters\n    ----------\n    x :         TreeNeuron | MeshNeuron | VoxelNeuron | NeuronList\n                Uses simple 6-connectedness for voxels.\n    simplify :  bool\n                For TreeNeurons only: simplify the graph by keeping only roots,\n                leaves and branching points. Preserves the original\n                branch lengths (i.e. weights).\n    epsilon :   float, optional\n                For Dotprops only: maximum distance between two points to\n                connect them. If `None`, will use 5x the average distance\n                between points (i.e. `5 * x.sampling_resolution`).\n\n    Returns\n    -------\n    graph:      networkx.Graph | networkx.DiGraph\n                NetworkX representation of the neuron. Returns list of graphs\n                if x is multiple neurons. Graph is directed for TreeNeurons\n                and undirected for Mesh- and VoxelNeurons. Graph is weighted\n                for Tree- and MeshNeurons.\n\n    \"\"\"\n    if isinstance(x, core.NeuronList):\n        return [neuron2nx(x.loc[i]) for i in range(x.shape[0])]\n\n    if isinstance(x, core.TreeNeuron):\n        # Collect nodes\n        nodes = x.nodes.set_index(\"node_id\", inplace=False)\n        # Collect edges\n        edges = x.nodes[x.nodes.parent_id &gt;= 0][[\"node_id\", \"parent_id\"]].values\n        # Collect weight\n        weights = np.sqrt(\n            np.sum(\n                (\n                    nodes.loc[edges[:, 0], [\"x\", \"y\", \"z\"]].values.astype(float)\n                    - nodes.loc[edges[:, 1], [\"x\", \"y\", \"z\"]].values.astype(float)\n                )\n                ** 2,\n                axis=1,\n            )\n        )\n        # It's fastest to generate a list of (source, target, weight) tuples to pass to networkX\n        elist = [(e[0], e[1], l) for e, l in zip(edges, weights)]\n        # Create empty directed Graph\n        G = nx.DiGraph()\n        # Add nodes (in case we have disconnected nodes)\n        G.add_nodes_from(x.nodes.node_id.values)\n        # Add edges\n        G.add_weighted_edges_from(elist)\n\n        if simplify:\n            simplify_graph(G, inplace=True)\n    elif isinstance(x, core.MeshNeuron):\n        G = nx.Graph()\n        G.add_nodes_from(np.arange(x.n_vertices))\n        edges = [\n            (e[0], e[1], l)\n            for e, l in zip(x.trimesh.edges_unique, x.trimesh.edges_unique_length)\n        ]\n        G.add_weighted_edges_from(edges)\n    elif isinstance(x, core.Dotprops):\n        if epsilon is None:\n            epsilon = 5 * x.sampling_resolution\n\n        # Generate KDTree\n        tree = neuron2KDTree(x)\n\n        # Generate graph and assign custom properties\n        G = nx.Graph()\n        G.add_nodes_from(np.arange(x.n_points))\n        G.add_edges_from(tree.query_pairs(epsilon))\n    elif isinstance(x, core.VoxelNeuron):\n        # First we need to determine the 6-connecivity between voxels\n        edges = []\n        # Go over each axis\n        for i in range(3):\n            # Generate an offset of 1 voxel along given axis\n            offset = np.zeros(3, dtype=int)\n            offset[i] = 1\n            # Combine real and offset voxels\n            vox_off = x.voxels + offset\n            # Find out which voxels overlap (i.e. count == 2 after offset)\n            unique, cnt = np.unique(\n                np.append(x.voxels, vox_off, axis=0), axis=0, return_counts=True\n            )\n\n            connected = unique[cnt &gt; 1]\n            for vox in connected:\n                edges.append([tuple(vox), tuple(vox - offset)])\n        G = nx.Graph()\n        G.add_nodes_from([tuple(v) for v in x.voxels])\n        G.add_edges_from(edges)\n    else:\n        raise ValueError(\n            f'Unable to convert data of type \"{type(x)}\" to networkx graph.'\n        )\n\n    return G\n</code></pre>"},{"location":"reference/navis/#navis.neuron2tangents","title":"<code>navis.neuron2tangents</code>","text":"<p>Turn skeleton(s) into points + tangent vectors.</p> <p>This will drop zero-length vectors (i.e when node and parent occupy the exact same position).</p> PARAMETER DESCRIPTION <code>x</code> <p> TYPE: <code>        TreeNeuron | NeuronList</code> </p> RETURNS DESCRIPTION <code>points</code> <p>Midpoints for each child-&gt;parent node pair.</p> <p> TYPE: <code>(N, 3) array</code> </p> <code>vect</code> <p>Normalized child-&gt; parent vectors.</p> <p> TYPE: <code>(N, 3) array</code> </p> <code>length</code> <p>Distance between parent and child</p> <p> TYPE: <code>(N, ) array</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; t = navis.neuron2tangents(n)\n</code></pre> Source code in <code>navis/graph/converters.py</code> <pre><code>def neuron2tangents(x: \"core.NeuronObject\") -&gt; \"core.Dotprops\":\n    \"\"\"Turn skeleton(s) into points + tangent vectors.\n\n    This will drop zero-length vectors (i.e when node and parent occupy the\n    exact same position).\n\n    Parameters\n    ----------\n    x :         TreeNeuron | NeuronList\n\n    Returns\n    -------\n    points :    (N, 3) array\n                Midpoints for each child-&gt;parent node pair.\n    vect :      (N, 3) array\n                Normalized child-&gt; parent vectors.\n    length :    (N, ) array\n                Distance between parent and child\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; t = navis.neuron2tangents(n)\n\n    \"\"\"\n    if isinstance(x, core.NeuronList):\n        return [neuron2tangents(n) for n in x]\n    elif not isinstance(x, core.TreeNeuron):\n        raise TypeError(f'Expected TreeNeuron/List, got \"{type(x)}\"')\n\n    # Collect nodes\n    nodes = x.nodes[x.nodes.parent_id &gt;= 0]\n\n    # Get child-&gt;parent vectors\n    parent_locs = (\n        x.nodes.set_index(\"node_id\").loc[nodes.parent_id, [\"x\", \"y\", \"z\"]].values\n    )\n    child_locs = nodes[[\"x\", \"y\", \"z\"]].values\n    vect = child_locs - parent_locs\n\n    # Get mid point\n    points = child_locs + (parent_locs - child_locs) / 2\n\n    # Get length\n    length = np.sqrt(np.sum(vect**2, axis=1))\n\n    # Drop zero length points\n    points = points[length != 0]\n    vect = vect[length != 0]\n    length = length[length != 0]\n\n    # Normalize vector\n    vect = vect / np.linalg.norm(vect, axis=1).reshape(-1, 1)\n\n    return points, vect, length\n</code></pre>"},{"location":"reference/navis/#navis.nx2neuron","title":"<code>navis.nx2neuron</code>","text":"<p>Create TreeNeuron from NetworkX Graph.</p> <p>This function will try to generate a neuron-like tree structure from the Graph. Therefore the graph must not contain loops!</p> <p>All node attributes (e.g. <code>x</code>, <code>y</code>, <code>z</code>, <code>radius</code>) will be added to the neuron's <code>.nodes</code> table.</p> PARAMETER DESCRIPTION <code>G</code> <pre><code>        Graph to convert to neuron.\n</code></pre> <p> TYPE: <code>            networkx.Graph</code> </p> <code>root</code> <pre><code>        Node in graph to use as root for neuron. If not provided,\n        will use first node in `g.nodes`. Ignored if graph\n        consists of several disconnected components.\n</code></pre> <p> TYPE: <code>         str | int | list</code> DEFAULT: <code>None</code> </p> <code>break_cycles</code> <pre><code>        The input graph must not contain cycles. We can break them\n        up at risk of disconnecting parts of the graph.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <pre><code>        Keyword arguments are passed to the construction of\n        [`navis.TreeNeuron`][].\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>TreeNeuron</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; import networkx as nx\n&gt;&gt;&gt; G = nx.balanced_tree(2, 3)\n&gt;&gt;&gt; tn = navis.nx2neuron(G)\n&gt;&gt;&gt; tn\ntype            navis.TreeNeuron\nname                        None\nn_nodes                       15\nn_connectors                None\nn_branches                     6\nn_leafs                        8\ncable_length                 0.0\nsoma                        None\nunits            1 dimensionless\ndtype: object\n</code></pre> Source code in <code>navis/graph/converters.py</code> <pre><code>def nx2neuron(\n    G: nx.Graph,\n    root: Optional[Union[int, str]] = None,\n    break_cycles: bool = False,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"Create TreeNeuron from NetworkX Graph.\n\n    This function will try to generate a neuron-like tree structure from\n    the Graph. Therefore the graph must not contain loops!\n\n    All node attributes (e.g. `x`, `y`, `z`, `radius`) will be added to\n    the neuron's `.nodes` table.\n\n    Parameters\n    ----------\n    G :             networkx.Graph\n                    Graph to convert to neuron.\n    root :          str | int | list, optional\n                    Node in graph to use as root for neuron. If not provided,\n                    will use first node in `g.nodes`. Ignored if graph\n                    consists of several disconnected components.\n    break_cycles :  bool\n                    The input graph must not contain cycles. We can break them\n                    up at risk of disconnecting parts of the graph.\n    **kwargs\n                    Keyword arguments are passed to the construction of\n                    [`navis.TreeNeuron`][].\n\n    Returns\n    -------\n    TreeNeuron\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; import networkx as nx\n    &gt;&gt;&gt; G = nx.balanced_tree(2, 3)\n    &gt;&gt;&gt; tn = navis.nx2neuron(G)\n    &gt;&gt;&gt; tn\n    type            navis.TreeNeuron\n    name                        None\n    n_nodes                       15\n    n_connectors                None\n    n_branches                     6\n    n_leafs                        8\n    cable_length                 0.0\n    soma                        None\n    units            1 dimensionless\n    dtype: object\n\n    \"\"\"\n    # First some sanity checks\n    if not isinstance(G, nx.Graph):\n        raise TypeError(f'`G` must be NetworkX Graph, got \"{type(G)}\"')\n\n    # We need an undirected Graph\n    if isinstance(G, nx.DiGraph):\n        G = G.to_undirected(as_view=True)\n\n    if not nx.is_forest(G):\n        if not break_cycles:\n            raise TypeError(\n                \"Graph must be tree-like. You can try setting \"\n                \"the `cut_cycles` parameter to True.\"\n            )\n        else:\n            if break_cycles:\n                while True:\n                    try:\n                        # Find cycle\n                        cycle = nx.find_cycle(G)\n                    except nx.exception.NetworkXNoCycle:\n                        break\n                    except BaseException:\n                        raise\n\n                    # Sort by degree\n                    cycle = sorted(cycle, key=lambda x: G.degree[x[0]])\n\n                    # Remove the edge with the lowest degree\n                    G.remove_edge(cycle[0][0], cycle[0][1])\n\n    # Ignore root if this is a forest\n    if not nx.is_tree(G):\n        root = None\n\n    # This effectively makes sure that all edges point in the same direction\n    lop = {}\n    for c in nx.connected_components(G):\n        sg = nx.subgraph(G, c)\n        # Pick a random root if not explicitly provided\n        if not root:\n            r = list(sg.nodes)[0]\n        elif root not in sg.nodes:\n            raise ValueError(f'Node \"{root}\" not in graph.')\n        else:\n            r = root\n\n        # Generate parent-&gt;child dictionary\n        this_lop = nx.predecessor(sg, r)\n\n        # Make sure no node has more than one parent\n        if any((len(v) &gt; 1 for v in this_lop.values())):\n            raise ValueError(\n                \"Nodes with multiple parents found. Make sure graph is tree-like.\"\n            )\n\n        # Note that we assign -1 as root's parent\n        lop.update({k: v[0] if v else -1 for k, v in this_lop.items()})\n\n    # Generate node table\n    tn_table = pd.DataFrame(index=list(G.nodes))\n    tn_table.index = tn_table.index.set_names(\"node_id\", inplace=False)\n\n    # Add parents - use -1 for root's parent\n    tn_table[\"parent_id\"] = tn_table.index.map(lop)\n\n    try:\n        tn_table.index = tn_table.index.astype(int)\n        tn_table[\"parent_id\"] = tn_table.parent_id.astype(int)\n    except (ValueError, TypeError):\n        raise ValueError(\"Node IDs must be convertible to integers.\")\n    except BaseException:\n        raise\n\n    # Add additional generic attribute -&gt; will skip node_id and parent_id\n    # if they exist\n    all_attr = set([k for n in G.nodes for k in G.nodes[n].keys()])\n\n    # Remove some that we don't need\n    all_attr -= set([\"parent_id\", \"node_id\"])\n    # Add some that we want as columns even if they don't exist\n    all_attr |= set([\"x\", \"y\", \"z\", \"radius\"])\n\n    # For some we want to have set default values\n    defaults = {\"x\": 0, \"y\": 0, \"z\": 0, \"radius\": -1}\n\n    # Now map the attributes onto node table\n    for at in all_attr:\n        vals = nx.get_node_attributes(G, at)\n        tn_table[at] = tn_table.index.map(vals).fillna(defaults.get(at, None))\n\n    return core.TreeNeuron(tn_table.reset_index(drop=False, inplace=False), **kwargs)\n</code></pre>"},{"location":"reference/navis/#navis.patch_cloudvolume","title":"<code>navis.patch_cloudvolume</code>","text":"<p>Monkey patch cloud-volume to return navis neurons.</p> <p>This function must be run before initializing the <code>CloudVolume</code>! Adds new methods/parameters to <code>CloudVolume.mesh.get</code> and <code>CloudVolume.skeleton.get</code>. See examples for details.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; import cloudvolume as cv\n&gt;&gt;&gt; # Monkey patch cloudvolume\n&gt;&gt;&gt; navis.patch_cloudvolume()\n&gt;&gt;&gt; # Connect to the Google segmentation of FAFB\n&gt;&gt;&gt; vol = cv.CloudVolume('precomputed://gs://fafb-ffn1-20200412/segmentation',\n...                       use_https=True,\n...                       progress=False)\n&gt;&gt;&gt; ids = [2137190164, 2268989790]\n&gt;&gt;&gt; # Fetch as navis neuron using newly added method or ...\n&gt;&gt;&gt; nl = vol.mesh.get_navis(ids, lod=3)\n&gt;&gt;&gt; # ... alternatively use `as_navis` keyword argument in original method\n&gt;&gt;&gt; nl = vol.mesh.get(ids, lod=3, as_navis=True)\n&gt;&gt;&gt; type(nl)\n&lt;class 'navis.core.neuronlist.NeuronList'&gt;\n&gt;&gt;&gt; # The same works for skeletons\n&gt;&gt;&gt; skels = vol.skeleton.get_navis(ids)\n</code></pre> Source code in <code>navis/utils/cv.py</code> <pre><code>def patch_cloudvolume():\n    \"\"\"Monkey patch cloud-volume to return navis neurons.\n\n    This function must be run before initializing the `CloudVolume`! Adds new\n    methods/parameters to `CloudVolume.mesh.get` and `CloudVolume.skeleton.get`.\n    See examples for details.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; import cloudvolume as cv\n    &gt;&gt;&gt; # Monkey patch cloudvolume\n    &gt;&gt;&gt; navis.patch_cloudvolume()\n    &gt;&gt;&gt; # Connect to the Google segmentation of FAFB\n    &gt;&gt;&gt; vol = cv.CloudVolume('precomputed://gs://fafb-ffn1-20200412/segmentation',\n    ...                       use_https=True,\n    ...                       progress=False)\n    &gt;&gt;&gt; ids = [2137190164, 2268989790]\n    &gt;&gt;&gt; # Fetch as navis neuron using newly added method or ...\n    &gt;&gt;&gt; nl = vol.mesh.get_navis(ids, lod=3)\n    &gt;&gt;&gt; # ... alternatively use `as_navis` keyword argument in original method\n    &gt;&gt;&gt; nl = vol.mesh.get(ids, lod=3, as_navis=True)\n    &gt;&gt;&gt; type(nl)\n    &lt;class 'navis.core.neuronlist.NeuronList'&gt;\n    &gt;&gt;&gt; # The same works for skeletons\n    &gt;&gt;&gt; skels = vol.skeleton.get_navis(ids)\n\n    \"\"\"\n    global cv\n    try:\n        import cloudvolume as cv\n    except ModuleNotFoundError:\n        cv = None\n\n    # If CV not installed do nothing\n    if not cv:\n        logger.info(\"cloud-volume appears to not be installed?\")\n        return\n\n    for ds in [\n        cv.datasource.graphene.mesh.sharded.GrapheneShardedMeshSource,\n        cv.datasource.graphene.mesh.unsharded.GrapheneUnshardedMeshSource,\n        cv.datasource.precomputed.mesh.unsharded.UnshardedLegacyPrecomputedMeshSource,\n        cv.datasource.precomputed.mesh.multilod.UnshardedMultiLevelPrecomputedMeshSource,\n        cv.datasource.precomputed.mesh.multilod.ShardedMultiLevelPrecomputedMeshSource,\n        cv.datasource.precomputed.skeleton.sharded.ShardedPrecomputedSkeletonSource,\n        cv.datasource.precomputed.skeleton.unsharded.UnshardedPrecomputedSkeletonSource,\n    ]:\n        ds.get_navis = return_navis(ds.get, only_on_kwarg=False)\n        ds.get = return_navis(ds.get, only_on_kwarg=True)\n\n    logger.info(\"cloud-volume successfully patched!\")\n</code></pre>"},{"location":"reference/navis/#navis.persistence_distances","title":"<code>navis.persistence_distances</code>","text":"<p>Calculate morphological similarity using persistence diagrams.</p> <p>This works by:   1. Generate persistence points for each neuron.   2. Create a weighted Gaussian from persistence points and sample 100      evenly spaced points to create a feature vector.   3. Calculate Euclidean distance.</p> PARAMETER DESCRIPTION <code>q</code> <pre><code>    Queries and targets, respectively. If `t=None` will run\n    queries against queries. Neurons should have the same units,\n    ideally nanometers.\n</code></pre> <p> TYPE: <code>core.NeuronObject</code> </p> <code>normalize</code> <pre><code>    If True, will normalized the vector for each neuron to be within\n    0-1. Set to False if the total number of linear segments matter.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>bw</code> <pre><code>    Bandwidth for Gaussian kernel: larger = smoother, smaller =\n    more detailed.\n</code></pre> <p> TYPE: <code>       float</code> DEFAULT: <code>0.2</code> </p> <code>augment</code> <pre><code>    Whether to augment the persistence vectors with other neuron\n    properties (number of branch points &amp; leafs and cable length).\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>True</code> </p> <code>**persistence_kwargs</code> <pre><code>    Keyword arguments are passed to [`navis.persistence_points`][].\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>distances</code> <p> TYPE: <code>pandas.DataFrame</code> </p> See Also <p><code>navis.persistence_points</code>             The function to calculate the persistence points. <code>navis.persistence_vectors</code>             Use this to get and inspect the actual vectors used here.</p> Source code in <code>navis/morpho/persistence.py</code> <pre><code>def persistence_distances(q: 'core.NeuronObject',\n                          t: Optional['core.NeuronObject'] = None,\n                          augment: bool = True,\n                          normalize: bool = True,\n                          bw: float = .2,\n                          **persistence_kwargs):\n    \"\"\"Calculate morphological similarity using persistence diagrams.\n\n    This works by:\n      1. Generate persistence points for each neuron.\n      2. Create a weighted Gaussian from persistence points and sample 100\n         evenly spaced points to create a feature vector.\n      3. Calculate Euclidean distance.\n\n    Parameters\n    ----------\n    q/t :       NeuronList\n                Queries and targets, respectively. If `t=None` will run\n                queries against queries. Neurons should have the same units,\n                ideally nanometers.\n    normalize : bool\n                If True, will normalized the vector for each neuron to be within\n                0-1. Set to False if the total number of linear segments matter.\n    bw :        float\n                Bandwidth for Gaussian kernel: larger = smoother, smaller =\n                more detailed.\n    augment :   bool\n                Whether to augment the persistence vectors with other neuron\n                properties (number of branch points &amp; leafs and cable length).\n    **persistence_kwargs\n                Keyword arguments are passed to [`navis.persistence_points`][].\n\n    Returns\n    -------\n    distances : pandas.DataFrame\n\n    See Also\n    --------\n    [`navis.persistence_points`][]\n                The function to calculate the persistence points.\n    [`navis.persistence_vectors`][]\n                Use this to get and inspect the actual vectors used here.\n\n    \"\"\"\n    q = core.NeuronList(q)\n    all_n = q\n\n    if t:\n        t = core.NeuronList(t)\n        all_n += t\n\n    # Some sanity checks\n    if len(all_n) &lt;= 1:\n        raise ValueError('Need more than one neuron.')\n\n    soma_warn = False\n    root_warn = False\n    for n in all_n:\n        if not soma_warn:\n            if n.has_soma and n.soma not in n.root:\n                soma_warn = True\n        if not root_warn:\n            if len(n.root) &gt; 1:\n                root_warn = True\n\n        if root_warn and soma_warn:\n            break\n\n    if soma_warn:\n        logger.warning('At least some neurons are not rooted to their soma.')\n    if root_warn:\n        logger.warning('At least some neurons are fragmented.')\n\n    # Get persistence points for each skeleton\n    pers = persistence_points(all_n, **persistence_kwargs)\n\n    # Get the vectors\n    vectors, samples = persistence_vectors(pers, samples=100, bw=bw)\n\n    # Normalizing the vectors will produce more useful distances\n    if normalize:\n        vectors = vectors / vectors.max(axis=1).reshape(-1, 1)\n    else:\n        vectors = vectors / vectors.max()\n\n    if augment:\n        # Collect extra data. Note that this adds only 3 more to the existing\n        # 100 observations\n        vec_aug = np.vstack((all_n.cable_length,\n                             all_n.n_leafs,\n                             all_n.n_branches)).T\n\n        # Normalize per metric\n        vec_aug = vec_aug / vec_aug.max(axis=0)\n\n        # If we wanted to weigh those observation equal to the 100 topology\n        # observations:\n        # vec_aug *= 100 / vec_aug.shape[1]\n\n        vectors = np.append(vectors, vec_aug, axis=1)\n\n    if t:\n        # Extract source and target vectors\n        q_vec = vectors[:len(q)]\n        t_vec = vectors[len(q):]\n        return pd.DataFrame(cdist(q_vec, t_vec), index=q.id, columns=t.id)\n    else:\n        return pd.DataFrame(squareform(pdist(vectors)), index=q.id, columns=q.id)\n</code></pre>"},{"location":"reference/navis/#navis.persistence_points","title":"<code>navis.persistence_points</code>","text":"<p>Calculate points for a persistence diagram.</p> <p>Based on Li et al., PLoS One (2017). Briefly, this cuts the neuron into linear segments, the start (birth) and end (death) of which are assigned a value (see <code>descriptor</code> parameter). In combination, these points represent a fingerprint for the topology of the neuron.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Neuron(s) to calculate persistence poinst for. For MeshNeurons,\n    we will use the skeleton produced by/associated with its\n    `.skeleton` property.\n</code></pre> <p> TYPE: <code>        TreeNeuron | MeshNeuron | NeuronList</code> </p> <code>descriptor</code> <pre><code>    Descriptor function used to calculate birth and death \"time\" of\n    the segments:\n      - `root_dist` distance from root\n</code></pre> <p> TYPE: <code>root_dist</code> DEFAULT: <code>'root_dist'</code> </p> <code>remove_cbf</code> <pre><code>    In unipolar neurons (e.g. in insects) the soma is separate and\n    connects to the neuron's backbone via \"cell body fiber\" (CBF).\n    The length of the CBF can vary quite a bit. Discounting the\n    CBF can make the persistence points more stable.\n    If `remove_cbf=True` and the neuron has a soma (!) we ignore\n    the CBF for the birth &amp; death times. Neurons will also be\n    automatically be rooted onto their soma!\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>pandas.DataFrame</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; n.reroot(n.soma, inplace=True)\n&gt;&gt;&gt; p = navis.persistence_points(n)\n</code></pre> References <p>Li Y, Wang D, Ascoli GA, Mitra P, Wang Y (2017) Metrics for comparing neuronal tree shapes based on persistent homology. PLOS ONE 12(8): e0182184. https://doi.org/10.1371/journal.pone.0182184</p> Source code in <code>navis/morpho/persistence.py</code> <pre><code>@utils.map_neuronlist(desc='Calc. persistence', allow_parallel=True)\ndef persistence_points(x: 'core.NeuronObject',\n                       descriptor: Union[\n                                         Literal['root_dist']\n                                         ] = 'root_dist',\n                       remove_cbf: bool = False\n                       ) -&gt; pd.DataFrame:\n    \"\"\"Calculate points for a persistence diagram.\n\n    Based on Li et al., PLoS One (2017). Briefly, this cuts the neuron into\n    linear segments, the start (birth) and end (death) of which are assigned a\n    value (see `descriptor` parameter). In combination, these points represent\n    a fingerprint for the topology of the neuron.\n\n    Parameters\n    ----------\n    x :         TreeNeuron | MeshNeuron | NeuronList\n                Neuron(s) to calculate persistence poinst for. For MeshNeurons,\n                we will use the skeleton produced by/associated with its\n                `.skeleton` property.\n    descriptor : 'root_dist'\n                Descriptor function used to calculate birth and death \"time\" of\n                the segments:\n                  - `root_dist` distance from root\n    remove_cbf : bool\n                In unipolar neurons (e.g. in insects) the soma is separate and\n                connects to the neuron's backbone via \"cell body fiber\" (CBF).\n                The length of the CBF can vary quite a bit. Discounting the\n                CBF can make the persistence points more stable.\n                If `remove_cbf=True` and the neuron has a soma (!) we ignore\n                the CBF for the birth &amp; death times. Neurons will also be\n                automatically be rooted onto their soma!\n\n    Returns\n    -------\n    pandas.DataFrame\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; n.reroot(n.soma, inplace=True)\n    &gt;&gt;&gt; p = navis.persistence_points(n)\n\n    References\n    ----------\n    Li Y, Wang D, Ascoli GA, Mitra P, Wang Y (2017) Metrics for comparing\n    neuronal tree shapes based on persistent homology.\n    PLOS ONE 12(8): e0182184. https://doi.org/10.1371/journal.pone.0182184\n\n    \"\"\"\n    if descriptor not in ('root_dist', ):\n        raise ValueError(f'Unknown \"descriptor\" parameter: {descriptor}')\n\n    if isinstance(x, core.MeshNeuron):\n        x = x.skeleton\n    elif not isinstance(x, core.TreeNeuron):\n        raise ValueError(f'Expected TreeNeuron(s), got \"{type(x)}\"')\n\n    if remove_cbf and x.has_soma:\n        # Reroot to soma\n        x.reroot(x.soma, inplace=True)\n        # Find the main branch point\n        mbp = graph.find_main_branchpoint(x)\n\n    # Generate segments\n    segs = graph._generate_segments(x, weight='weight')\n\n    # Grab starts and ends of each segment\n    ends = np.array([s[0] for s in segs])\n    starts = np.array([s[-1] for s in segs])\n\n    if descriptor == 'root_dist':\n        # Get geodesic distances to roots\n        dist = graph.dist_to_root(x, weight='weight')\n        death = np.array([dist[e] for e in ends])\n        birth = np.array([dist[s] for s in starts])\n\n        if remove_cbf and x.has_soma:\n            # Subtract length of CBF\n            cbf_length = graph.dist_between(x, mbp, x.soma)\n            birth -= cbf_length\n            death -= cbf_length\n\n            # Drop segments that are entirely on the CBF\n            starts = starts[death &gt;= 0]\n            ends = ends[death &gt;= 0]\n            birth = birth[death &gt;= 0]\n            death = death[death &gt;= 0]\n\n            # Clip negative births\n            birth[birth &lt; 0] = 0\n\n    # Compile into a DataFrame\n    pers = pd.DataFrame()\n    pers['start_node'] = starts\n    pers['end_node'] = ends\n    pers['birth'] = birth\n    pers['death'] = death\n\n    return pers\n</code></pre>"},{"location":"reference/navis/#navis.persistence_vectors","title":"<code>navis.persistence_vectors</code>","text":"<p>Produce vectors from persistence points.</p> <p>Works by creating a Gaussian and sampling <code>samples</code> evenly spaced points across it.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    The persistence points (see [navis.persistence_points][]).\n    For vectors for multiple neurons, provide either a list of\n    persistence points DataFrames or a single DataFrame with a\n    \"neuron_id\" column.\n</code></pre> <p> TYPE: <code>        navis.NeuronList | pd.DataFrame | list thereof</code> </p> <code>threshold</code> <pre><code>    If provided, segments shorter (death - birth) than this will not\n    be used to create the Gaussian.\n</code></pre> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>samples</code> <pre><code>    Either the number of points sampled across the Gaussian or an\n    iterable of points to sample.\n</code></pre> <p> TYPE: <code>  int | iterable</code> DEFAULT: <code>100</code> </p> <code>bw</code> <pre><code>    Bandwidth for Gaussian kernel: larger = smoother, smaller =\n    more detailed.\n</code></pre> <p> TYPE: <code>       float</code> DEFAULT: <code>0.2</code> </p> <code>center</code> <pre><code>    Whether to center the individual curves on their highest value.\n    This is done by \"rolling\" the axis (using `np.roll`) which\n    means that elements that roll beyond the last position are\n    re-introduced at the first.\n</code></pre> <p> TYPE: <code>   bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>vectors</code> <p> TYPE: <code>np.ndarray</code> </p> <code>samples</code> <p>Sampled distances. If <code>center=True</code> the absolute values don't make much sense anymore.</p> <p> TYPE: <code>np.ndarray</code> </p> References <p>Li Y, Wang D, Ascoli GA, Mitra P, Wang Y (2017) Metrics for comparing neuronal tree shapes based on persistent homology. PLOS ONE 12(8): e0182184. https://doi.org/10.1371/journal.pone.0182184</p> See Also <p><code>navis.persistence_points</code>             The function to calculate the persistence points. <code>navis.persistence_distances</code>             Get distances based on (augmented) persistence vectors.</p> Source code in <code>navis/morpho/persistence.py</code> <pre><code>def persistence_vectors(x,\n                        threshold: Optional[float] = None,\n                        samples: int = 100,\n                        bw: float = .2,\n                        center: bool = False,\n                        **kwargs):\n    \"\"\"Produce vectors from persistence points.\n\n    Works by creating a Gaussian and sampling `samples` evenly spaced\n    points across it.\n\n    Parameters\n    ----------\n    x :         navis.NeuronList | pd.DataFrame | list thereof\n                The persistence points (see [navis.persistence_points][]).\n                For vectors for multiple neurons, provide either a list of\n                persistence points DataFrames or a single DataFrame with a\n                \"neuron_id\" column.\n    threshold : float, optional\n                If provided, segments shorter (death - birth) than this will not\n                be used to create the Gaussian.\n    samples :   int | iterable\n                Either the number of points sampled across the Gaussian or an\n                iterable of points to sample.\n    bw :        float\n                Bandwidth for Gaussian kernel: larger = smoother, smaller =\n                more detailed.\n    center :    bool\n                Whether to center the individual curves on their highest value.\n                This is done by \"rolling\" the axis (using `np.roll`) which\n                means that elements that roll beyond the last position are\n                re-introduced at the first.\n\n    Returns\n    -------\n    vectors :   np.ndarray\n    samples :   np.ndarray\n                Sampled distances. If `center=True` the absolute values don't\n                make much sense anymore.\n\n    References\n    ----------\n    Li Y, Wang D, Ascoli GA, Mitra P, Wang Y (2017) Metrics for comparing\n    neuronal tree shapes based on persistent homology.\n    PLOS ONE 12(8): e0182184. https://doi.org/10.1371/journal.pone.0182184\n\n    See Also\n    --------\n    [`navis.persistence_points`][]\n                The function to calculate the persistence points.\n    [`navis.persistence_distances`][]\n                Get distances based on (augmented) persistence vectors.\n\n    \"\"\"\n    if isinstance(x, core.BaseNeuron):\n        x = core.NeuronList(x)\n\n    if isinstance(x, pd.DataFrame):\n        pers = [x]\n    elif isinstance(x, core.NeuronList):\n        pers = [persistence_points(n, **kwargs) for n in x]\n    elif isinstance(x, list):\n        if not all([isinstance(l, pd.DataFrame) for l in x]):\n            raise ValueError('Expected lists to contain only DataFrames')\n        pers = x\n    else:\n        raise TypeError('Unable to work extract persistence vectors from data '\n                        f'of type \"{x}\"')\n\n    if isinstance(samples, (int, np.integer)):\n        # Get the max distance\n        max_pdist = max([p.birth.max() for p in pers])\n        samples = np.linspace(0, max_pdist * 1.05, samples)\n    elif not isinstance(samples, (np.ndarray, list, tuple)):\n        raise ValueError(\n            f'Expected `samples` to be an integer or iterable, got \"{type(samples)}\"'\n            )\n\n    # Now get a persistence vector\n    vectors = []\n    for p in pers:\n        weights = p.death.values - p.birth.values\n        if threshold:\n            p = p.loc[weights &gt;= threshold]\n            weights = weights[weights &gt;= threshold]\n\n        # For each persistence generate a weighted Gaussian kernel\n        kernel = gaussian_kde(p.birth.values,\n                              weights=weights,\n                              bw_method=bw)\n\n        # And sample probabilities at the sample points\n        vectors.append(kernel(samples))\n    vectors = np.array(vectors)\n\n    if center:\n        # Shift each vector such that the highest value lies in the center.\n        # Note that we are \"rolling\" the array which means that elements that\n        # drop off to the right are reintroduced on the left\n        for i in range(len(vectors)):\n            vectors[i] = np.roll(vectors[i],\n                                 -np.argmax(vectors[i]) + len(samples) // 2)\n\n    return vectors, samples\n</code></pre>"},{"location":"reference/navis/#navis.plot1d","title":"<code>navis.plot1d</code>","text":"<p>Plot neuron topology in 1D according to Cuntz et al. (2010).</p> <p>This function breaks a neurons into segments between branch points. See Cuntz et al., PLoS Computational Biology (2010) for detailed explanation. For very complex neurons, this neuron \"barcode\" can get fairly complicated - make sure to zoom in.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Neuron(s) to plot.\n</code></pre> <p> TYPE: <code>        TreeNeuron | NeuronList</code> </p> <code>ax</code> <p> TYPE: <code>       matplotlib.ax</code> DEFAULT: <code>None</code> </p> <code>color</code> <pre><code>    Color. If dict must map neuron UUID to color.\n</code></pre> <p> TYPE: <code>    tuple | dict</code> DEFAULT: <code>None</code> </p> <code>palette</code> <pre><code>    Name of a matplotlib or seaborn palette. If `color` is\n    not specified will pick colors from this palette.\n</code></pre> <p> TYPE: <code>  str | array | list of arrays</code> DEFAULT: <code>None</code> </p> <code>color_by</code> <pre><code>    Can be the name of a column in the node table of\n    `TreeNeurons` or an array of (numerical or categorical)\n    values for each node. Numerical values will be normalized.\n    You can control the normalization by passing a `vmin`\n    and/or `vmax` parameter.\n</code></pre> <p> TYPE: <code> str | array | list of arrays</code> DEFAULT: <code>= None</code> </p> <code>**kwargs</code> <pre><code>    Will be passed to `matplotlib.patches.Rectangle`.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>matplotlib.ax</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; n = navis.example_neurons(2)\n&gt;&gt;&gt; ax = navis.plot1d(n)\n&gt;&gt;&gt; plt.show()\n</code></pre> <p>Close figures (only relevant for doctests)</p> <pre><code>&gt;&gt;&gt; plt.close('all')\n</code></pre> <p>See the <code>flat plotting tutorial</code> for more examples.</p> Source code in <code>navis/plotting/d.py</code> <pre><code>def plot1d(x: 'core.NeuronObject',\n           ax: Optional[mpl.axes.Axes] = None,\n           color: Optional[Union['str',\n                                 colortype,\n                                 Dict[Any, colortype],\n                                 ]\n                           ] = None,\n           color_by: Optional[Union[str, np.ndarray]] = None,\n           palette: Optional[str] = None,\n           **kwargs) -&gt; mpl.axes.Axes:\n    \"\"\"Plot neuron topology in 1D according to Cuntz et al. (2010).\n\n    This function breaks a neurons into segments between branch points.\n    See Cuntz et al., PLoS Computational Biology (2010) for detailed\n    explanation. For very complex neurons, this neuron \"barcode\" can get\n    fairly complicated - make sure to zoom in.\n\n    Parameters\n    ----------\n    x :         TreeNeuron | NeuronList\n                Neuron(s) to plot.\n    ax :        matplotlib.ax, optional\n    color :     tuple | dict\n                Color. If dict must map neuron UUID to color.\n    palette :   str | array | list of arrays, default=None\n                Name of a matplotlib or seaborn palette. If `color` is\n                not specified will pick colors from this palette.\n    color_by :  str | array | list of arrays, default = None\n                Can be the name of a column in the node table of\n                `TreeNeurons` or an array of (numerical or categorical)\n                values for each node. Numerical values will be normalized.\n                You can control the normalization by passing a `vmin`\n                and/or `vmax` parameter.\n    **kwargs\n                Will be passed to `matplotlib.patches.Rectangle`.\n\n    Returns\n    -------\n    matplotlib.ax\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; n = navis.example_neurons(2)\n    &gt;&gt;&gt; ax = navis.plot1d(n)\n    &gt;&gt;&gt; plt.show() # doctest: +SKIP\n\n    Close figures (only relevant for doctests)\n\n    &gt;&gt;&gt; plt.close('all')\n\n    See the [`flat plotting tutorial`](generated/gallery/1_plotting/tutorial_plotting_02_1d/)\n    for more examples.\n\n    \"\"\"\n    if isinstance(x, core.NeuronList):\n        if x.is_mixed:\n            raise TypeError('NeuronList contains MeshNeuron(s). Unable to plot1d.')\n    elif isinstance(x, core.TreeNeuron):\n        x = core.NeuronList(x)\n    else:\n        raise TypeError(f'Unable plot1d data of type \"{type(x)}\"')\n\n    if isinstance(color, type(None)) and isinstance(palette, type(None)):\n        color = (0.56, 0.86, 0.34)\n\n    color, _ =  prepare_colormap(color,\n                                 neurons=x,\n                                 palette=palette,\n                                 color_range=1)\n\n    if not isinstance(color_by, type(None)):\n        if not palette:\n            raise ValueError('Must provide `palette` (e.g. \"viridis\") argument '\n                             'if using `color_by`')\n\n        vertex_map = vertex_colors(x,\n                                   by=color_by,\n                                   use_alpha=False,\n                                   palette=palette,\n                                   vmin=kwargs.pop('vmin', None),\n                                   vmax=kwargs.pop('vmax', None),\n                                   na=kwargs.pop('na', 'raise'),\n                                   color_range=1)\n    else:\n        vertex_map = None\n\n    if not ax:\n        fig, ax = plt.subplots(figsize=(8, len(x)))\n        # Make background transparent (nicer for dark themes)\n        fig.patch.set_alpha(0)\n        ax.patch.set_alpha(0)\n\n    # Add some default parameters for the plotting to kwargs\n    DEFAULTS = {'lw': kwargs.pop('lw', kwargs.pop('linewidth', .2)),\n                'ec': kwargs.pop('ec', kwargs.pop('edgecolor', (1, 1, 1))),\n    }\n    kwargs.update(DEFAULTS)\n\n    max_x = []\n    for ix, n in enumerate(config.tqdm(x, desc='Processing',\n                                       disable=config.pbar_hide,\n                                       leave=config.pbar_leave)):\n        if isinstance(color, dict):\n            this_c = color[n.id]\n        else:\n            this_c = color[ix]\n\n        # Get topological sort (root -&gt; terminals)\n        topology = graph.node_label_sorting(n, weighted=True)\n\n        # Get terminals and branch points\n        roots = n.nodes[n.nodes.type == 'root'].node_id.values\n        bp = n.nodes[n.nodes.type == 'branch'].node_id.values\n        term = n.nodes[n.nodes.type == 'end'].node_id.values\n        breaks = np.concatenate((bp, term, roots))\n\n        # Order this neuron's segments by topology (remember that segments are\n        # sorted child -&gt; parent, i.e. distal to proximal)\n        topo_ix = dict(zip(topology, range(len(topology))))\n        segs = sorted(n.small_segments, key=lambda x: topo_ix[x[0]])\n\n        # Keep only the first and the last node in each segment\n        segs = [[s[0], s[1]] for s in segs]\n\n        # Now get distances for each segment\n        if 'nodes_geodesic_distance_matrix' in n.__dict__:\n            # If available, use existing geodesic distance matrix\n            dist_mat = n.nodes_geodesic_distance_matrix\n        else:\n            # If not, compute matrix for subset of nodes\n            dist_mat = graph.geodesic_matrix(n, from_=breaks, directed=False)\n\n        # Get length of each segment\n        lengths = np.array([dist_mat.loc[s[0], s[1]] for s in segs])\n        max_x.append(sum(lengths))\n\n        # Plot\n        curr_dist = 0\n        id2ix = dict(zip(n.nodes.node_id.values, range(n.n_nodes)))\n        for k, le in enumerate(lengths):\n            if isinstance(vertex_map, type(None)):\n                c = this_c\n\n                if segs[k][0] in term:\n                    c = tuple(np.array(c) / 2)\n            else:\n                # Get this segments vertex colors\n                node_ix = [id2ix[i] for i in segs[k]]\n                vc = vertex_map[ix][node_ix]\n                c = vc[-1]\n\n            p = mpatches.Rectangle((curr_dist, ix), le, 1, fc=c, **kwargs)\n            ax.add_patch(p)\n            curr_dist += le\n    ax.set_xlim(0, max(max_x))\n    ax.set_ylim(0, len(x))\n\n    ax.set_yticks(np.array(range(0, len(x))) + .5)\n    ax.set_yticklabels(x.name)\n\n    dstring = 'distance'\n    ax.set_xlabel(dstring)\n\n    ax.set_frame_on(False)\n\n    return ax\n</code></pre>"},{"location":"reference/navis/#navis.plot2d","title":"<code>navis.plot2d</code>","text":"<p>Generate 2D plots of neurons and neuropils.</p> <p>The main advantage of this is that you can save plot as vector graphics.</p> Note <p>This function uses <code>matplotlib</code> which \"fakes\" 3D as it has only very limited control over layering objects in 3D. Therefore neurites are not necessarily plotted in the right Z order. This becomes especially troublesome when plotting a complex scene with lots of neurons criss-crossing. See the <code>method</code> parameter for details.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            Objects to plot:\n             - multiple objects can be passed as list (see examples)\n             - numpy array of shape (N, 3) is intepreted as points for\n               scatter plots\n</code></pre> <p> TYPE: <code>                TreeNeuron | MeshNeuron | NeuronList | Volume | Dotprops | np.ndarray</code> </p> Object parameters <p>soma :              bool | dict, default=True</p> <pre><code>                Plot soma if one exists. Size of the soma is determined\n                by the neuron's `.soma_radius` property which defaults\n                to the \"radius\" column for `TreeNeurons`. You can also\n                pass `soma` as a dictionary to customize the appearance\n                of the soma - for example `soma={\"color\": \"red\", \"lw\": 2, \"ec\": 1}`.\n</code></pre> <p>radius :            bool | \"auto\", default=False</p> <pre><code>                If \"auto\" will plot neurites of `TreeNeurons` with radius\n                if they have radii. If True, will try plotting neurites of\n                `TreeNeurons` with radius regardless. The radius can be\n                scaled by `linewidth`. Note that this will increase rendering\n                time.\n</code></pre> <p>linewidth :         int | float, default=.5</p> <pre><code>                Width of neurites. Also accepts alias `lw`.\n</code></pre> <p>linestyle :         str, default='-'</p> <pre><code>                Line style of neurites. Also accepts alias `ls`.\n</code></pre> <p>color :             None | str | tuple | list | dict, default=None</p> <pre><code>                Use single str (e.g. `'red'`) or `(r, g, b)` tuple\n                to give all neurons the same color. Use `list` of\n                colors to assign colors: `['red', (1, 0, 1), ...].\n                Use `dict` to map colors to neuron IDs:\n                `{id: (r, g, b), ...}`.\n</code></pre> <p>palette :           str | array | list of arrays, default=None</p> <pre><code>                Name of a matplotlib or seaborn palette. If `color` is\n                not specified will pick colors from this palette.\n</code></pre> <p>color_by :          str | array | list of arrays, default = None</p> <pre><code>                Color neurons by a property. Can be:\n                  - a list/array of labels, one per each neuron\n                  - a neuron property (str)\n                  - a column name in the node table of `TreeNeurons`\n                  - a list/array of values for each node\n                Numerical values will be normalized. You can control\n                the normalization by passing a `vmin` and/or `vmax` parameter.\n</code></pre> <p>shade_by :          str | array | list of arrays, default=None</p> <pre><code>                Similar to `color_by` but will affect only the alpha\n                channel of the color. If `shade_by='strahler'` will\n                compute Strahler order if not already part of the node\n                table (TreeNeurons only). Numerical values will be\n                normalized. You can control the normalization by passing\n                a `smin` and/or `smax` parameter.\n</code></pre> <p>alpha :             float [0-1], default=1</p> <pre><code>                Alpha value for neurons. Overriden if alpha is provided\n                as fourth value in `color` (rgb*a*). You can override\n                alpha value for connectors by using `cn_alpha`.\n</code></pre> <p>mesh_shade :        bool, default=False</p> <pre><code>                Only relevant for meshes (e.g. `MeshNeurons`) and\n                `TreeNeurons` with radius, and when method is 3d or\n                3d complex. Whether to shade the object which will give it\n                a 3D look.\n</code></pre> <p>depth_coloring :    bool, default=False</p> <pre><code>                If True, will use neuron color to encode depth (Z).\n                Overrides `color` argument. Does not work with\n                `method = '3d_complex'`.\n</code></pre> <p>depth_scale :       bool, default=True</p> <pre><code>                If True and `depth_coloring=True` will plot a scale.\n</code></pre> <p>connectors :        bool | \"presynapses\" | \"postsynapses\" | str | list, default=True</p> <pre><code>                Plot connectors. This can either be `True` (plot all\n                connectors), `\"presynapses\"` (only presynaptic connectors)\n                or `\"postsynapses\"` (only postsynaptic connectors). If\n                a string or a list is provided, it will be used to filter the\n                `type` column in the connectors table.\n</code></pre> <p>connectors :        bool | \"presynapses\" | \"postsynapses\" | str | list, default=True</p> <pre><code>                Plot connectors. This can either be `True` (plot all\n                connectors), `\"presynapses\"` (only presynaptic connectors)\n                or `\"postsynapses\"` (only postsynaptic connectors). If\n                a string or a list is provided, it will be used to filter the\n                `type` column in the connectors table.\n\n                Use these parameters to adjust the way connectors are plotted:\n\n                  - `cn_colors` (str | tuple | dict | \"neuron\" ) overrides\n                    the default connector (e.g. synpase) colors:\n                      - single color as str (e.g. `'red'`) or rgb tuple\n                        (e.g. `(1, 0, 0)`)\n                      - dict mapping the connectors tables `type` column to\n                        a color (e.g. `{\"pre\": (1, 0, 0)}`)\n                      - with \"neuron\", connectors will receive the same color\n                        as their neuron\n                  - `cn_layout` (dict): Layout of the connectors. See\n                    `navis.config.default_connector_colors` for options.\n                  - `cn_size` (float): Size of the connectors.\n                  - `cn_alpha` (float): Transparency of the connectors.\n                  - `cn_mesh_colors` (bool): Whether to color the connectors\n                    by the neuron's color.\n</code></pre> <p>connectors_only :   boolean, default=False</p> <pre><code>                Plot only connectors, not the neuron.\n</code></pre> <p>scatter_kws :       dict, default={}</p> <pre><code>                Parameters to be used when plotting points. Accepted\n                keywords are: `size` and `color`.\n</code></pre> <p>volume_outlines :   bool | \"both\", default=False</p> <pre><code>                If True will plot volume outline with no fill. Only\n                works with `method=\"2d\"`. Requires the `shapely` package.\n</code></pre> <p>dps_scale_vec :     float</p> <pre><code>                Scale vector for dotprops.\n</code></pre> Figure parameters <p>method :            '2d' | '3d' (default) | '3d_complex'</p> <pre><code>                Method used to generate plot. Comes in three flavours:\n                 1. `2d` uses normal matplotlib. Neurons are plotted on\n                    top of one another in the order their are passed to\n                    the function. Use the `view` parameter (below) to\n                    set the view (default = xy).\n                 2. `3d` uses matplotlib's 3D axis. Here, matplotlib\n                    decide the depth order (zorder) of plotting. Can\n                    change perspective either interacively or by code\n                    (see examples).\n                 3. `3d_complex` same as 3d but each neuron segment is\n                    added individually. This allows for more complex\n                    zorders to be rendered correctly. Slows down\n                    rendering!\n</code></pre> <p>view :              tuple, default = (\"x\", \"y\")</p> <pre><code>                Sets view for `method='2d'`. Can be any combination of\n                \"x\", \"y\", \"z\" and their negations. For example, to plot\n                from the top, use `view=('x', '-y')`. For 3D `methods`,\n                this will set the initial view which can be changed by\n                adjusting `ax.azim`, `ax.elev` and `ax.roll` (see examples).\n</code></pre> <p>non_view_axes3d :   \"show\" | \"hide\" (default) | \"fade\"</p> <pre><code>                Only relevant for methods '3d' and '3d_complex': what to\n                do with the axis that are not in the view. If 'hide', will\n                hide them. If 'show', will show them. If 'fade', will\n                make them semi-transparent. This is relevant if you\n                intend if you intend to customize the view after plotting.\n</code></pre> <p>autoscale :         bool, default=True</p> <pre><code>                If True, will scale the axes to fit the data.\n</code></pre> <p>scalebar :          int | float | str | pint.Quantity | dict, default=False</p> <pre><code>                Adds a scale bar. Provide integer, float or str to set\n                size of scalebar. Int|float are assumed to be in same\n                units as data. You can specify units in as string:\n                e.g. \"1 um\". For methods '3d' and '3d_complex', this\n                will create an axis object.\n\n                You can customize the scalebar by passing a dictionary.\n                For example:\n\n                `{size: \"1 micron\", color: 'k', lw: 3, alpha: 0.9}`\n</code></pre> <p>ax :                matplotlib.Axes, default=None</p> <pre><code>                Pass an axis object if you want to plot on an existing\n                canvas. Must match `method` - i.e. 2D or 3D axis.\n</code></pre> <p>figsize :           tuple, default=None</p> <pre><code>                Size of figure. Ignored if `ax` is provided.\n</code></pre> <p>rasterize :         bool, default=False</p> <pre><code>                Neurons produce rather complex vector graphics which can\n                lead to large files when saving to SVG, PDF or PS. Use\n                this parameter to rasterize neurons and meshes/volumes\n                (but not axes or labels) to reduce file size.\n</code></pre> <p>orthogonal :        bool, default=True</p> <pre><code>                Whether to use orthogonal or perspective view for\n                methods '3d' and '3d_complex'.\n</code></pre> <p>group_neurons :     bool, default=False</p> <pre><code>                If True, neurons will be grouped. Works with SVG export\n                but not PDF. Does NOT work with `method='3d_complex'`.\n</code></pre> RETURNS DESCRIPTION <code>fig</code> <p> TYPE: <code>matplotlib.Figure</code> </p> <code>ax</code> <p> TYPE: <code>matplotlib.Axes</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n</code></pre> <p>Plot list of neurons as simple 2d:</p> <pre><code>&gt;&gt;&gt; nl = navis.example_neurons()\n&gt;&gt;&gt; fig, ax = navis.plot2d(nl, method='2d', view=('x', '-z'))\n&gt;&gt;&gt; plt.show()\n</code></pre> <p>Add a volume:</p> <pre><code>&gt;&gt;&gt; vol = navis.example_volume('LH')\n&gt;&gt;&gt; fig, ax = navis.plot2d([nl, vol], method='2d', view=('x', '-z'))\n&gt;&gt;&gt; plt.show()\n</code></pre> <p>Change neuron colors:</p> <pre><code>&gt;&gt;&gt; fig, ax = navis.plot2d(\n...              nl,\n...              method='2d',\n...              view=('x', '-z'),\n...              color=['r', 'g', 'b', 'm', 'c', 'y']\n...          )\n&gt;&gt;&gt; plt.show()\n</code></pre> <p>Plot in \"fake\" 3D:</p> <pre><code>&gt;&gt;&gt; fig, ax = navis.plot2d(nl, method='3d', view=('x', '-z'))\n&gt;&gt;&gt; plt.show()\n&gt;&gt;&gt; # In an interactive window you can dragging the plot to rotate\n</code></pre> <p>Plot in \"fake\" 3D and change perspective:</p> <pre><code>&gt;&gt;&gt; fig, ax = navis.plot2d(nl, method='3d', view=('x', '-z'))\n&gt;&gt;&gt; # Change view\n&gt;&gt;&gt; ax.elev = -20\n&gt;&gt;&gt; ax.azim = 45\n&gt;&gt;&gt; ax.roll = 180\n&gt;&gt;&gt; plt.show()\n</code></pre> <p>Plot using depth-coloring:</p> <pre><code>&gt;&gt;&gt; fig, ax = navis.plot2d(nl, method='3d', depth_coloring=True, view=('x', '-z'))\n&gt;&gt;&gt; plt.show()\n</code></pre> <p>See the plotting intro for more examples.</p> See Also <p><code>navis.plot3d</code>         Use this if you want interactive, perspectively correct renders         and if you don't need vector graphics as outputs. <code>navis.plot1d</code>         A nifty way to visualise neurons in a single dimension. <code>navis.plot_flat</code>         Plot neurons as flat structures (e.g. dendrograms).</p> Source code in <code>navis/plotting/dd.py</code> <pre><code>def plot2d(\n    x: Union[\n        core.NeuronObject,\n        core.Volume,\n        np.ndarray,\n        List[Union[core.NeuronObject, np.ndarray, core.Volume]],\n    ],\n    **kwargs,\n) -&gt; Tuple[mpl.figure.Figure, mpl.axes.Axes]:\n    \"\"\"Generate 2D plots of neurons and neuropils.\n\n    The main advantage of this is that you can save plot as vector graphics.\n\n    Note\n    ----\n    This function uses `matplotlib` which \"fakes\" 3D as it has only very limited\n    control over layering objects in 3D. Therefore neurites are not necessarily\n    plotted in the right Z order. This becomes especially troublesome when\n    plotting a complex scene with lots of neurons criss-crossing. See the\n    `method` parameter for details.\n\n    Parameters\n    ----------\n    x :                 TreeNeuron | MeshNeuron | NeuronList | Volume | Dotprops | np.ndarray\n                        Objects to plot:\n                         - multiple objects can be passed as list (see examples)\n                         - numpy array of shape (N, 3) is intepreted as points for\n                           scatter plots\n\n    Object parameters\n    -----------------\n    soma :              bool | dict, default=True\n\n                        Plot soma if one exists. Size of the soma is determined\n                        by the neuron's `.soma_radius` property which defaults\n                        to the \"radius\" column for `TreeNeurons`. You can also\n                        pass `soma` as a dictionary to customize the appearance\n                        of the soma - for example `soma={\"color\": \"red\", \"lw\": 2, \"ec\": 1}`.\n\n    radius :            bool | \"auto\", default=False\n\n                        If \"auto\" will plot neurites of `TreeNeurons` with radius\n                        if they have radii. If True, will try plotting neurites of\n                        `TreeNeurons` with radius regardless. The radius can be\n                        scaled by `linewidth`. Note that this will increase rendering\n                        time.\n\n    linewidth :         int | float, default=.5\n\n                        Width of neurites. Also accepts alias `lw`.\n\n    linestyle :         str, default='-'\n\n                        Line style of neurites. Also accepts alias `ls`.\n\n    color :             None | str | tuple | list | dict, default=None\n\n                        Use single str (e.g. `'red'`) or `(r, g, b)` tuple\n                        to give all neurons the same color. Use `list` of\n                        colors to assign colors: `['red', (1, 0, 1), ...].\n                        Use `dict` to map colors to neuron IDs:\n                        `{id: (r, g, b), ...}`.\n\n    palette :           str | array | list of arrays, default=None\n\n                        Name of a matplotlib or seaborn palette. If `color` is\n                        not specified will pick colors from this palette.\n\n    color_by :          str | array | list of arrays, default = None\n\n                        Color neurons by a property. Can be:\n                          - a list/array of labels, one per each neuron\n                          - a neuron property (str)\n                          - a column name in the node table of `TreeNeurons`\n                          - a list/array of values for each node\n                        Numerical values will be normalized. You can control\n                        the normalization by passing a `vmin` and/or `vmax` parameter.\n\n    shade_by :          str | array | list of arrays, default=None\n\n                        Similar to `color_by` but will affect only the alpha\n                        channel of the color. If `shade_by='strahler'` will\n                        compute Strahler order if not already part of the node\n                        table (TreeNeurons only). Numerical values will be\n                        normalized. You can control the normalization by passing\n                        a `smin` and/or `smax` parameter.\n\n    alpha :             float [0-1], default=1\n\n                        Alpha value for neurons. Overriden if alpha is provided\n                        as fourth value in `color` (rgb*a*). You can override\n                        alpha value for connectors by using `cn_alpha`.\n\n    mesh_shade :        bool, default=False\n\n                        Only relevant for meshes (e.g. `MeshNeurons`) and\n                        `TreeNeurons` with radius, and when method is 3d or\n                        3d complex. Whether to shade the object which will give it\n                        a 3D look.\n\n    depth_coloring :    bool, default=False\n\n                        If True, will use neuron color to encode depth (Z).\n                        Overrides `color` argument. Does not work with\n                        `method = '3d_complex'`.\n\n    depth_scale :       bool, default=True\n\n                        If True and `depth_coloring=True` will plot a scale.\n\n    connectors :        bool | \"presynapses\" | \"postsynapses\" | str | list, default=True\n\n                        Plot connectors. This can either be `True` (plot all\n                        connectors), `\"presynapses\"` (only presynaptic connectors)\n                        or `\"postsynapses\"` (only postsynaptic connectors). If\n                        a string or a list is provided, it will be used to filter the\n                        `type` column in the connectors table.\n\n    connectors :        bool | \"presynapses\" | \"postsynapses\" | str | list, default=True\n\n                        Plot connectors. This can either be `True` (plot all\n                        connectors), `\"presynapses\"` (only presynaptic connectors)\n                        or `\"postsynapses\"` (only postsynaptic connectors). If\n                        a string or a list is provided, it will be used to filter the\n                        `type` column in the connectors table.\n\n                        Use these parameters to adjust the way connectors are plotted:\n\n                          - `cn_colors` (str | tuple | dict | \"neuron\" ) overrides\n                            the default connector (e.g. synpase) colors:\n                              - single color as str (e.g. `'red'`) or rgb tuple\n                                (e.g. `(1, 0, 0)`)\n                              - dict mapping the connectors tables `type` column to\n                                a color (e.g. `{\"pre\": (1, 0, 0)}`)\n                              - with \"neuron\", connectors will receive the same color\n                                as their neuron\n                          - `cn_layout` (dict): Layout of the connectors. See\n                            `navis.config.default_connector_colors` for options.\n                          - `cn_size` (float): Size of the connectors.\n                          - `cn_alpha` (float): Transparency of the connectors.\n                          - `cn_mesh_colors` (bool): Whether to color the connectors\n                            by the neuron's color.\n\n    connectors_only :   boolean, default=False\n\n                        Plot only connectors, not the neuron.\n\n    scatter_kws :       dict, default={}\n\n                        Parameters to be used when plotting points. Accepted\n                        keywords are: `size` and `color`.\n\n    volume_outlines :   bool | \"both\", default=False\n\n                        If True will plot volume outline with no fill. Only\n                        works with `method=\"2d\"`. Requires the `shapely` package.\n\n    dps_scale_vec :     float\n\n                        Scale vector for dotprops.\n\n    Figure parameters\n    -----------------\n    method :            '2d' | '3d' (default) | '3d_complex'\n\n                        Method used to generate plot. Comes in three flavours:\n                         1. `2d` uses normal matplotlib. Neurons are plotted on\n                            top of one another in the order their are passed to\n                            the function. Use the `view` parameter (below) to\n                            set the view (default = xy).\n                         2. `3d` uses matplotlib's 3D axis. Here, matplotlib\n                            decide the depth order (zorder) of plotting. Can\n                            change perspective either interacively or by code\n                            (see examples).\n                         3. `3d_complex` same as 3d but each neuron segment is\n                            added individually. This allows for more complex\n                            zorders to be rendered correctly. Slows down\n                            rendering!\n\n    view :              tuple, default = (\"x\", \"y\")\n\n                        Sets view for `method='2d'`. Can be any combination of\n                        \"x\", \"y\", \"z\" and their negations. For example, to plot\n                        from the top, use `view=('x', '-y')`. For 3D `methods`,\n                        this will set the initial view which can be changed by\n                        adjusting `ax.azim`, `ax.elev` and `ax.roll` (see examples).\n\n    non_view_axes3d :   \"show\" | \"hide\" (default) | \"fade\"\n\n                        Only relevant for methods '3d' and '3d_complex': what to\n                        do with the axis that are not in the view. If 'hide', will\n                        hide them. If 'show', will show them. If 'fade', will\n                        make them semi-transparent. This is relevant if you\n                        intend if you intend to customize the view after plotting.\n\n    autoscale :         bool, default=True\n\n                        If True, will scale the axes to fit the data.\n\n    scalebar :          int | float | str | pint.Quantity | dict, default=False\n\n                        Adds a scale bar. Provide integer, float or str to set\n                        size of scalebar. Int|float are assumed to be in same\n                        units as data. You can specify units in as string:\n                        e.g. \"1 um\". For methods '3d' and '3d_complex', this\n                        will create an axis object.\n\n                        You can customize the scalebar by passing a dictionary.\n                        For example:\n\n                        `{size: \"1 micron\", color: 'k', lw: 3, alpha: 0.9}`\n\n\n    ax :                matplotlib.Axes, default=None\n\n                        Pass an axis object if you want to plot on an existing\n                        canvas. Must match `method` - i.e. 2D or 3D axis.\n\n    figsize :           tuple, default=None\n\n                        Size of figure. Ignored if `ax` is provided.\n\n    rasterize :         bool, default=False\n\n                        Neurons produce rather complex vector graphics which can\n                        lead to large files when saving to SVG, PDF or PS. Use\n                        this parameter to rasterize neurons and meshes/volumes\n                        (but not axes or labels) to reduce file size.\n\n    orthogonal :        bool, default=True\n\n                        Whether to use orthogonal or perspective view for\n                        methods '3d' and '3d_complex'.\n\n    group_neurons :     bool, default=False\n\n                        If True, neurons will be grouped. Works with SVG export\n                        but not PDF. Does NOT work with `method='3d_complex'`.\n\n    Returns\n    -------\n    fig :               matplotlib.Figure\n    ax :                matplotlib.Axes\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n\n    Plot list of neurons as simple 2d:\n\n    &gt;&gt;&gt; nl = navis.example_neurons()\n    &gt;&gt;&gt; fig, ax = navis.plot2d(nl, method='2d', view=('x', '-z'))\n    &gt;&gt;&gt; plt.show() # doctest: +SKIP\n\n    Add a volume:\n\n    &gt;&gt;&gt; vol = navis.example_volume('LH')\n    &gt;&gt;&gt; fig, ax = navis.plot2d([nl, vol], method='2d', view=('x', '-z'))\n    &gt;&gt;&gt; plt.show() # doctest: +SKIP\n\n    Change neuron colors:\n\n    &gt;&gt;&gt; fig, ax = navis.plot2d(\n    ...              nl,\n    ...              method='2d',\n    ...              view=('x', '-z'),\n    ...              color=['r', 'g', 'b', 'm', 'c', 'y']\n    ...          )\n    &gt;&gt;&gt; plt.show() # doctest: +SKIP\n\n    Plot in \"fake\" 3D:\n\n    &gt;&gt;&gt; fig, ax = navis.plot2d(nl, method='3d', view=('x', '-z'))\n    &gt;&gt;&gt; plt.show() # doctest: +SKIP\n    &gt;&gt;&gt; # In an interactive window you can dragging the plot to rotate\n\n    Plot in \"fake\" 3D and change perspective:\n\n    &gt;&gt;&gt; fig, ax = navis.plot2d(nl, method='3d', view=('x', '-z'))\n    &gt;&gt;&gt; # Change view\n    &gt;&gt;&gt; ax.elev = -20\n    &gt;&gt;&gt; ax.azim = 45\n    &gt;&gt;&gt; ax.roll = 180\n    &gt;&gt;&gt; plt.show() # doctest: +SKIP\n\n    Plot using depth-coloring:\n\n    &gt;&gt;&gt; fig, ax = navis.plot2d(nl, method='3d', depth_coloring=True, view=('x', '-z'))\n    &gt;&gt;&gt; plt.show() # doctest: +SKIP\n\n    See the [plotting intro](../../generated/gallery/1_plotting/tutorial_plotting_00_intro)\n    for more examples.\n\n    See Also\n    --------\n    [`navis.plot3d`][]\n            Use this if you want interactive, perspectively correct renders\n            and if you don't need vector graphics as outputs.\n    [`navis.plot1d`][]\n            A nifty way to visualise neurons in a single dimension.\n    [`navis.plot_flat`][]\n            Plot neurons as flat structures (e.g. dendrograms).\n\n    \"\"\"\n    # This handles (1) checking for invalid arguments, (2) setting defaults and\n    # (3) synonyms\n    settings = Matplotlib2dSettings().update_settings(**kwargs)\n\n    _METHOD_OPTIONS = [\"2d\", \"3d\", \"3d_complex\"]\n    if settings.method not in _METHOD_OPTIONS:\n        raise ValueError(\n            f'Unknown method \"{settings.method}\". Please use either: '\n            f'{\",\".join(_METHOD_OPTIONS)}'\n        )\n\n    # Parse objects\n    (neurons, volumes, points, _) = utils.parse_objects(x)\n\n    # Here we check whether `color_by` is a neuron property which we\n    # want to translate into a single color per neuron, or a\n    # per node/vertex property which we will parse late\n    color_neurons_by = None\n    if settings.color_by is not None and neurons:\n        if not settings.palette:\n            raise ValueError(\n                'Must provide palette (via e.g. `palette=\"viridis\"`) '\n                \"when using `color_by` argument.\"\n            )\n\n        # Check if this may be a neuron property\n        if isinstance(settings.color_by, str):\n            # Check if this could be a neuron property\n            has_prop = hasattr(neurons[0], settings.color_by)\n\n            # For TreeNeurons, we also check if it is a node property\n            # If so, prioritize this.\n            if isinstance(neurons[0], core.TreeNeuron):\n                if settings.color_by in neurons[0].nodes.columns:\n                    has_prop = False\n\n            if has_prop:\n                # If it is, use it to color neurons\n                color_neurons_by = [\n                    getattr(neuron, settings.color_by) for neuron in neurons\n                ]\n                settings.color_by = None\n        elif isinstance(settings.color_by, (list, np.ndarray)):\n            if len(settings.color_by) == len(neurons):\n                color_neurons_by = settings.color_by\n                settings.color_by = None\n\n    # Generate the per-neuron colors\n    (neuron_cmap, volumes_cmap) = prepare_colormap(\n        settings.color,\n        neurons=neurons,\n        volumes=volumes,\n        palette=settings.palette,\n        color_by=color_neurons_by,\n        alpha=settings.alpha,\n        color_range=1,\n    )\n\n    if not isinstance(settings.color_by, type(None)):\n        neuron_cmap = vertex_colors(\n            neurons,\n            by=settings.color_by,\n            use_alpha=False,\n            palette=settings.palette,\n            norm_global=settings.norm_global,\n            vmin=settings.vmin,\n            vmax=settings.vmax,\n            na=\"raise\",\n            color_range=1,\n        )\n\n    if not isinstance(settings.shade_by, type(None)):\n        alphamap = vertex_colors(\n            neurons,\n            by=settings.shade_by,\n            use_alpha=True,\n            palette=\"viridis\",  # palette is irrelevant here\n            norm_global=settings.norm_global,\n            vmin=settings.smin,\n            vmax=settings.smax,\n            na=\"raise\",\n            color_range=1,\n        )\n\n        new_colormap = []\n        for c, a in zip(neuron_cmap, alphamap):\n            if not (isinstance(c, np.ndarray) and c.ndim == 2):\n                c = np.tile(c, (a.shape[0], 1))\n\n            if c.shape[1] == 4:\n                c[:, 3] = a[:, 3]\n            else:\n                c = np.insert(c, 3, a[:, 3], axis=1)\n\n            new_colormap.append(c)\n        neuron_cmap = new_colormap\n\n    # Generate axes\n    if not settings.ax:\n        if settings.method == \"2d\":\n            fig, ax = plt.subplots(figsize=settings.figsize)\n        elif settings.method in (\"3d\", \"3d_complex\"):\n            fig = plt.figure(\n                figsize=settings.figsize if settings.figsize else plt.figaspect(1) * 1.5\n            )\n            ax = fig.add_subplot(111, projection=\"3d\")\n        # Hide axes\n        # ax.set_axis_off()\n    else:\n        # Check if correct axis were provided\n        if not isinstance(settings.ax, mpl.axes.Axes):\n            raise TypeError('Ax must be of type \"mpl.axes.Axes\", ' f'not \"{type(ax)}\"')\n        ax = settings.ax\n        fig = ax.get_figure()\n        if settings.method in (\"3d\", \"3d_complex\") and ax.name != \"3d\":\n            raise TypeError(\"Axis must be 3d.\")\n        elif settings.method == \"2d\" and ax.name == \"3d\":\n            raise TypeError(\"Axis must be 2d.\")\n\n    # Set axis projection\n    if settings.method in (\"3d\", \"3d_complex\"):\n        # This sets the view\n        _set_view3d(ax, settings)\n\n        # Some styling:\n        # Make background transparent (nicer for dark themes)\n        fig.patch.set_alpha(0)\n        ax.patch.set_alpha(0)\n\n        # For 3d axes, we also need to set the pane color to transparent\n        if hasattr(ax, \"zaxis\"):\n            ax.xaxis.pane.fill = False\n            ax.xaxis.pane.set_edgecolor((1, 1, 1, 0))\n\n            ax.yaxis.pane.fill = False\n            ax.yaxis.pane.set_edgecolor((1, 1, 1, 0))\n\n            ax.zaxis.pane.set_edgecolor((1, 1, 1, 0))\n            ax.zaxis.pane.fill = False\n\n        if settings.orthogonal:\n            ax.set_proj_type(\"ortho\")\n        else:\n            ax.set_proj_type(\"persp\", focal_length=1)  # smaller = more perspective\n    else:\n        ax.set_aspect(\"equal\")\n        _set_view2d(ax, settings)\n\n    # Prepare some stuff for depth coloring\n    if settings.depth_coloring and not neurons.empty:\n        if settings.method == \"3d_complex\":\n            raise Exception(\n                f'Depth coloring unavailable for method \"{settings.method}\"'\n            )\n        elif settings.method == \"2d\":\n            bbox = neurons.bbox\n            # Add to kwargs\n            xy = [v.replace(\"-\", \"\").replace(\"+\", \"\") for v in settings.view]\n            depth_ix = [v[1] for v in [(\"x\", 0), (\"y\", 1), (\"z\", 2)] if v[0] not in xy]\n\n            # We use this to track the normaliser\n            settings.norm = plt.Normalize(\n                vmin=bbox[depth_ix, 0], vmax=bbox[depth_ix, 1]\n            )\n\n    # Plot volumes first\n    if volumes:\n        for i, v in enumerate(volumes):\n            _ = _plot_volume(v, volumes_cmap[i], ax, settings)\n\n    # Create lines from segments\n    visuals = {}\n    for i, neuron in enumerate(\n        config.tqdm(\n            neurons,\n            desc=\"Plot neurons\",\n            leave=False,\n            disable=config.pbar_hide | len(neurons) &lt;= 10,\n        )\n    ):\n        if not settings.connectors_only:\n            if isinstance(neuron, core.TreeNeuron) and neuron.nodes.empty:\n                logger.warning(f\"Skipping TreeNeuron w/o nodes: {neuron.label}\")\n                continue\n            if isinstance(neuron, core.TreeNeuron) and neuron.nodes.shape[0] == 1:\n                logger.warning(f\"Skipping single-node TreeNeuron: {neuron.label}\")\n                continue\n            elif isinstance(neuron, core.MeshNeuron) and neuron.faces.size == 0:\n                logger.warning(f\"Skipping MeshNeuron w/o faces: {neuron.label}\")\n                continue\n            elif isinstance(neuron, core.Dotprops) and neuron.points.size == 0:\n                logger.warning(f\"Skipping Dotprops w/o points: {neuron.label}\")\n                continue\n\n            if isinstance(neuron, core.TreeNeuron) and settings.radius == \"auto\":\n                # Number of nodes with radii\n                n_radii = (\n                    neuron.nodes.get(\"radius\", pd.Series([])).fillna(0) &gt; 0\n                ).sum()\n                # If less than 30% of nodes have a radius, we will fall back to lines\n                if n_radii / neuron.nodes.shape[0] &lt; 0.3:\n                    settings.radius = False\n\n            if isinstance(neuron, core.TreeNeuron) and settings.radius:\n                # Warn once if more than 5% of nodes have missing radii\n                if not getattr(fig, \"_radius_warned\", False):\n                    if (\n                        (neuron.nodes.radius.fillna(0).values &lt;= 0).sum()\n                        / neuron.n_nodes\n                    ) &gt; 0.05:\n                        logger.warning(\n                            \"Some skeleton nodes have radius &lt;= 0. This may lead to \"\n                            \"rendering artifacts. Set `radius=False` to plot skeletons \"\n                            \"as single-width lines instead.\"\n                        )\n                        fig._radius_warned = True\n\n                _neuron = conversion.tree2meshneuron(\n                    neuron,\n                    warn_missing_radii=False,\n                    radius_scale_factor=settings.get(\"linewidth\", 1),\n                )\n                _neuron.connectors = neuron.connectors\n                neuron = _neuron\n\n                # See if we need to map colors to vertices\n                if isinstance(neuron_cmap[i], np.ndarray) and neuron_cmap[i].ndim == 2:\n                    neuron_cmap[i] = neuron_cmap[i][neuron.vertex_map]\n\n            if isinstance(neuron, core.TreeNeuron):\n                lc, sc = _plot_skeleton(neuron, neuron_cmap[i], ax, settings)\n                # Keep track of visuals related to this neuron\n                visuals[neuron] = {\"skeleton\": lc, \"somata\": sc}\n            elif isinstance(neuron, core.MeshNeuron):\n                m = _plot_mesh(neuron, neuron_cmap[i], ax, settings)\n                visuals[neuron] = {\"mesh\": m}\n            elif isinstance(neuron, core.Dotprops):\n                dp = _plot_dotprops(neuron, neuron_cmap[i], ax, settings)\n                visuals[neuron] = {\"dotprop\": dp}\n            elif isinstance(neuron, core.VoxelNeuron):\n                dp = _plot_voxels(\n                    neuron,\n                    neuron_cmap[i],\n                    ax,\n                    settings,\n                    **settings.scatter_kws,\n                )\n                visuals[neuron] = {\"dotprop\": dp}\n            else:\n                raise TypeError(\n                    f\"Don't know how to plot neuron of type '{type(neuron)}' \"\n                )\n\n        if (settings.connectors or settings.connectors_only) and neuron.has_connectors:\n            _ = _plot_connectors(neuron, neuron_cmap[i], ax, settings)\n\n    # Plot points\n    for p in points:\n        _ = _plot_scatter(p, ax, settings)\n\n    # Note: autoscaling is a bitch for 3d. In particular when we use Collections, because\n    # these are currently ignored by matplotlib's built-in autoscaling.\n    if settings.autoscale:\n        ax.autoscale(tight=False)  # tight=False avoids clipping the neurons\n\n        if \"3d\" in settings.method:\n            update_axes3d_bounds(ax)\n\n        # This is apparently still required and has to happen AFTER updating axis bounds\n        ax.set_aspect(\"equal\", adjustable=\"box\")\n\n    # Add scalebar after the dust has settled\n    if settings.scalebar not in (False, None):\n        if not settings.orthogonal:\n            raise ValueError(\"Scalebar only available if `orthogonal=True`.\")\n\n        _ = _add_scalebar(settings.scalebar, neurons, ax, settings)\n\n    def set_depth():\n        \"\"\"Set depth information for neurons according to camera position.\"\"\"\n        # Get projected coordinates\n        proj_co = proj_points(all_co, ax.get_proj())\n\n        # Get min and max of z coordinates\n        z_min, z_max = min(proj_co[:, 2]), max(proj_co[:, 2])\n\n        # Generate a new normaliser\n        norm = plt.Normalize(vmin=z_min, vmax=z_max)\n\n        # Go over all neurons and update Z information\n        for neuron in visuals:\n            # Get this neurons colletion and coordinates\n            if \"skeleton\" in visuals[neuron]:\n                c = visuals[neuron][\"skeleton\"]\n                this_co = c._segments3d[:, 0, :]\n            elif \"mesh\" in visuals[neuron]:\n                c = visuals[neuron][\"mesh\"]\n                # Note that we only get every third position -&gt; that's because\n                # these vectors actually represent faces, i.e. each vertex\n                this_co = c._vec.T[::3, [0, 1, 2]]\n            else:\n                raise ValueError(\n                    f\"Neither mesh nor skeleton found for neuron {neuron.id}\"\n                )\n\n            # Get projected coordinates\n            this_proj = proj_points(this_co, ax.get_proj())\n\n            # Normalise z coordinates\n            ns = norm(this_proj[:, 2]).data\n\n            # Set array\n            c.set_array(ns)\n\n            # No need for normaliser - already happened\n            c.set_norm(None)\n\n            if isinstance(neuron, core.TreeNeuron) and not isinstance(\n                getattr(neuron, \"soma\", None), type(None)\n            ):\n                # Get depth of soma(s)\n                soma = utils.make_iterable(neuron.soma)\n                soma_co = (\n                    neuron.nodes.set_index(\"node_id\").loc[soma][[\"x\", \"y\", \"z\"]].values\n                )\n                soma_proj = proj_points(soma_co, ax.get_proj())\n                soma_cs = norm(soma_proj[:, 2]).data\n\n                # Set soma color\n                for cs, s in zip(soma_cs, visuals[neuron][\"somata\"]):\n                    s.set_color(cmap(cs))\n\n    def Update(event):\n        set_depth()\n\n    if settings.depth_coloring:\n        if settings.palette:\n            cmap = plt.get_cmap(settings.palette)\n        else:\n            cmap = DEPTH_CMAP\n        if settings.method == \"2d\" and settings.depth_scale:\n            sm = ScalarMappable(norm=settings.norm, cmap=cmap)\n            fig.colorbar(sm, ax=ax, fraction=0.075, shrink=0.5, label=\"Depth\")\n        elif settings.method == \"3d\":\n            # Collect all coordinates\n            all_co = []\n            for n in visuals:\n                if \"skeleton\" in visuals[n]:\n                    all_co.append(visuals[n][\"skeleton\"]._segments3d[:, 0, :])\n                if \"mesh\" in visuals[n]:\n                    all_co.append(visuals[n][\"mesh\"]._vec.T[:, [0, 1, 2]])\n\n            all_co = np.concatenate(all_co, axis=0)\n            fig.canvas.mpl_connect(\"draw_event\", Update)\n            set_depth()\n\n    return fig, ax\n</code></pre>"},{"location":"reference/navis/#navis.plot3d","title":"<code>navis.plot3d</code>","text":"<p>Generate interactive 3D plot.</p> <p>Uses either octarine, vispy, k3d or plotly as backend. By default, the choice is automatic depending on what backends are installed and the context:</p> <ul> <li>Terminal: octarine &gt; vispy &gt; plotly</li> <li>Jupyter: plotly &gt; octarine &gt; k3d</li> </ul> <p>See the <code>backend</code> parameter on how to change this behavior.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>          The object(s) to plot. Can be:\n            - navis neurons, neuronlists or volumes\n            - numpy.array (N,3) is plotted as scatter plot\n            - multiple objects can be passed as list (see examples)\n          See parameters below for ways to customize the plot.\n</code></pre> <p> TYPE: <code>              Neuron/List | Volume | numpy.array | list thereof</code> </p> Object parameters <p>color :           None | str | tuple | list | dict, default=None</p> <pre><code>              Use single str (e.g. `'red'`) or `(r, g, b)` tuple\n              to give all neurons the same color. Use `list` of\n              colors to assign colors: `['red', (1, 0, 1), ...].\n              Use `dict` to map colors to neurons:\n              `{neuron.id: (r, g, b), ...}`.\n</code></pre> <p>palette :         str | array | list of arrays, default=None</p> <pre><code>              Name of a matplotlib or seaborn palette. If `color` is\n              not specified will pick colors from this palette.\n</code></pre> <p>alpha :           float [0-1], optional</p> <pre><code>              Alpha value for neurons. Overriden if alpha is provided\n              as color specified in `color` has an alpha channel.\n</code></pre> <p>connectors :      bool | \"presynapses\" | \"postsynapses\" | str | list, default=True</p> <pre><code>              Plot connectors. This can either be `True` (plot all\n              connectors), `\"presynapses\"` (only presynaptic connectors)\n              or `\"postsynapses\"` (only postsynaptic connectors). If\n              a string or a list is provided, it will be used to filter the\n              `type` column in the connectors table.\n\n              Use these parameters to adjust the way connectors are plotted:\n\n                - `cn_colors` (str | tuple | dict | \"neuron\" ) overrides\n                  the default connector (e.g. synpase) colors:\n                    - single color as str (e.g. `'red'`) or rgb tuple\n                      (e.g. `(1, 0, 0)`)\n                    - dict mapping the connectors tables `type` column to\n                      a color (e.g. `{\"pre\": (1, 0, 0)}`)\n                    - with \"neuron\", connectors will receive the same color\n                      as their neuron\n                - `cn_layout` (dict): Layout of the connectors. See\n                  `navis.config.default_connector_colors` for options.\n                - `cn_size` (float): Size of the connectors.\n                - `cn_alpha` (float): Transparency of the connectors.\n                - `cn_mesh_colors` (bool): Whether to color the connectors\n                  by the neuron's color.\n</code></pre> <p>connectors_only : bool, default=False</p> <pre><code>              Plot only connectors (e.g. synapses) if available and\n              ignore the neurons.\n</code></pre> <p>color_by :        str | array | list of arrays, default = None</p> <pre><code>              Color neurons by a property. Can be:\n\n                - a list/array of labels, one per each neuron\n                - a neuron property (str)\n                - a column name in the node table of `TreeNeurons`\n                - a list/array of values for each node\n\n              Numerical values will be normalized. You can control\n              the normalization by passing a `vmin` and/or `vmax`\n              parameter. Must specify a colormap via `palette`.\n</code></pre> <p>shade_by :        str | array | list of arrays, default=None</p> <pre><code>              Similar to `color_by` but will affect only the alpha\n              channel of the color. If `shade_by='strahler'` will\n              compute Strahler order if not already part of the node\n              table (TreeNeurons only). Numerical values will be\n              normalized. You can control the normalization by passing\n              a `smin` and/or `smax` parameter. Does not work with\n              `k3d` backend.\n</code></pre> <p>radius :          bool | \"auto\", default=False</p> <pre><code>              If \"auto\" will plot neurites of `TreeNeurons` with radius\n              if they have radii. If True, will try plotting neurites of\n              `TreeNeurons` with radius regardless. The radius can be\n              scaled by `linewidth`. Note that this will increase rendering\n              time.\n</code></pre> <p>soma :            bool, default=True</p> <pre><code>              TreeNeurons only: Whether to plot soma if it exists. Size\n              of the soma is determined by the neuron's `.soma_radius`\n              property which defaults to the \"radius\" column for\n              `TreeNeurons`.\n</code></pre> <p>linewidth :       float, default=3 for plotly and 1 for all others</p> <pre><code>              TreeNeurons only.\n</code></pre> <p>linestyle :       str, default='-'</p> <pre><code>              TreeNeurons only. Follows the same rules as in matplotlib.\n</code></pre> <p>scatter_kws :     dict, optional</p> <pre><code>              Use to modify scatter plots. Accepted parameters are:\n                - `size` to adjust size of dots\n                - `color` to adjust color\n</code></pre> Figure parameters <p>backend :         'auto' (default) | 'octarine' | 'vispy' | 'plotly' | 'k3d'</p> <pre><code>              Which backend to use for plotting. Note that there will\n              be minor differences in what feature/parameters are\n              supported depending on the backend:\n\n                - `auto` selects backend based on availability and\n                  context (see above). You can override this by setting an\n                  environment variable e.g. `NAVIS_PLOT3D_BACKEND=\"vispy\"`\n                  or `NAVIS_PLOT3D_JUPYTER_BACKEND=\"k3d\"`.\n                - `octarine` uses WGPU to generate high performances\n                  interactive 3D plots. Works both terminal and Jupyter.\n                - `vispy` similar to octarine but uses OpenGL: slower\n                  but runs on older systems. Works only from terminals.\n                - `plotly` generates 3D plots using WebGL. Works\n                  \"inline\" in Jupyter notebooks but can also produce a\n                  HTML file that can be opened in any browers.\n                - `k3d` generates 3D plots using k3d. Works only in\n                  Jupyter notebooks!\n</code></pre> <p>Below parameters are for plotly backend only:</p> <p>fig :             plotly.graph_objs.Figure</p> <pre><code>              Pass to add graph objects to existing plotly figure. Will\n              not change layout.\n</code></pre> <p>title :           str, default=None</p> <pre><code>              For plotly only! Change plot title.\n</code></pre> <p>width/height :    int, optional</p> <pre><code>              Use to adjust figure size.\n</code></pre> <p>fig_autosize :    bool, default=False</p> <pre><code>              For plotly only! Autoscale figure size.\n              Attention: autoscale overrides width and height\n</code></pre> <p>hover_name :      bool, default=False</p> <pre><code>              If True, hovering over neurons will show their label.\n</code></pre> <p>hover_id :        bool, default=False</p> <pre><code>              If True, hovering over skeleton nodes will show their ID.\n</code></pre> <p>legend :          bool, default=True</p> <pre><code>              Whether or not to show the legend.\n</code></pre> <p>legend_orientation : \"v\" (default) | \"h\"</p> <pre><code>              Orientation of the legend. Can be 'h' (horizontal) or 'v'\n              (vertical).\n</code></pre> <p>legend_group :    dict, default=None</p> <pre><code>              A dictionary mapping neuron IDs to labels (strings).\n              Use this to group neurons under a common label in the\n              legend.\n</code></pre> <p>inline :          bool, default=True</p> <pre><code>              If True and you are in an Jupyter environment, will\n              render plotly/k3d plots inline. If False, will generate\n              and return either a plotly Figure or a k3d Plot object\n              without immediately showing it.\n</code></pre> <p>Below parameters are for the Octarine/vispy backends only:</p> <p>clear :           bool, default = False</p> <pre><code>              If True, will clear the viewer before adding the new\n              objects.\n</code></pre> <p>center :          bool, default = True</p> <pre><code>              If True, will center camera on the newly added objects.\n</code></pre> <p>combine :         bool, default = False</p> <pre><code>              If True, will combine objects of the same type into a\n              single visual. This can greatly improve performance but\n              also means objects can't be selected individually\n              anymore. This is Vispy only.\n</code></pre> <p>size :            (width, height) tuple, optional</p> <pre><code>              Use to adjust figure/window size.\n</code></pre> <p>show :            bool, default=True</p> <pre><code>              Whether to immediately show the viewer.\n</code></pre> RETURNS DESCRIPTION <code>If `backend='octarine'`</code> <p>From terminal: opens a 3D window and returns :class:<code>octarine.Viewer</code>. From Jupyter: :class:<code>octarine.Viewer</code> displayed in an ipywidget.</p> <code>If `backend='vispy'`</code> <p>Opens a 3D window and returns <code>navis.Viewer</code>.</p> <code>If `backend='plotly'`</code> <p>Returns either <code>None</code> if you are in a Jupyter notebook (see also <code>inline</code> parameter) or a <code>plotly.graph_objects.Figure</code> (see examples).</p> <code>If `backend='k3d'`</code> <p>Returns either <code>None</code> and immediately displays the plot or a <code>k3d.plot</code> object that you can manipulate further (see <code>inline</code> parameter).</p> See Also <p><code>octarine.Viewer</code>     Interactive 3D viewer.</p> <p><code>navis.Viewer</code>     Interactive vispy 3D viewer.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n</code></pre> <p>In a Jupyter notebook using plotly as backend:</p> <pre><code>&gt;&gt;&gt; nl = navis.example_neurons()\n&gt;&gt;&gt; # Backend is automatically chosen but we can set it explicitly\n&gt;&gt;&gt; # Plot inline\n&gt;&gt;&gt; nl.plot3d(backend='plotly')\n&gt;&gt;&gt; # Plot as separate html in a new window\n&gt;&gt;&gt; fig = nl.plot3d(backend='plotly', inline=False)\n&gt;&gt;&gt; import plotly.offline\n&gt;&gt;&gt; _ = plotly.offline.plot(fig)\n</code></pre> <p>In a Jupyter notebook using k3d as backend:</p> <pre><code>&gt;&gt;&gt; nl = navis.example_neurons()\n&gt;&gt;&gt; # Plot inline\n&gt;&gt;&gt; nl.plot3d(backend='k3d')\n</code></pre> <p>In a terminal using octarine as backend:</p> <pre><code>&gt;&gt;&gt; # Plot list of neurons\n&gt;&gt;&gt; nl = navis.example_neurons()\n&gt;&gt;&gt; v = navis.plot3d(nl, backend='octarine')\n&gt;&gt;&gt; # Clear canvas\n&gt;&gt;&gt; navis.clear3d()\n</code></pre> <p>Some more advanced examples:</p> <pre><code>&gt;&gt;&gt; # plot3d() can deal with combinations of objects\n&gt;&gt;&gt; nl = navis.example_neurons()\n&gt;&gt;&gt; vol = navis.example_volume('LH')\n&gt;&gt;&gt; vol.color = (255, 0, 0, .5)\n&gt;&gt;&gt; # This plots a neuronlists, a single neuron and a volume\n&gt;&gt;&gt; v = navis.plot3d([nl[0:2], nl[3], vol])\n&gt;&gt;&gt; # Clear viewer (works only with octarine and vispy)\n&gt;&gt;&gt; v = navis.plot3d(nl, clear=True)\n</code></pre> <p>See the plotting intro for even more examples.</p> Source code in <code>navis/plotting/ddd.py</code> <pre><code>def plot3d(\n    x: Union[\n        core.NeuronObject,\n        core.Volume,\n        np.ndarray,\n        List[Union[core.NeuronObject, np.ndarray, core.Volume]],\n    ],\n    **kwargs,\n):\n    \"\"\"Generate interactive 3D plot.\n\n    Uses either [octarine], [vispy], [k3d] or [plotly] as backend.\n    By default, the choice is automatic depending on what backends\n    are installed and the context:\n\n      - Terminal: octarine &gt; vispy &gt; plotly\n      - Jupyter: plotly &gt; octarine &gt; k3d\n\n    See the `backend` parameter on how to change this behavior.\n\n    [octarine]: https://schlegelp.github.io/octarine/\n    [vispy]: http://vispy.org\n    [k3d]: https://k3d-jupyter.org/\n    [plotly]: http://plot.ly\n\n    Parameters\n    ----------\n    x :               Neuron/List | Volume | numpy.array | list thereof\n                      The object(s) to plot. Can be:\n                        - navis neurons, neuronlists or volumes\n                        - numpy.array (N,3) is plotted as scatter plot\n                        - multiple objects can be passed as list (see examples)\n                      See parameters below for ways to customize the plot.\n\n    Object parameters\n    -----------------\n    color :           None | str | tuple | list | dict, default=None\n\n                      Use single str (e.g. `'red'`) or `(r, g, b)` tuple\n                      to give all neurons the same color. Use `list` of\n                      colors to assign colors: `['red', (1, 0, 1), ...].\n                      Use `dict` to map colors to neurons:\n                      `{neuron.id: (r, g, b), ...}`.\n\n    palette :         str | array | list of arrays, default=None\n\n                      Name of a matplotlib or seaborn palette. If `color` is\n                      not specified will pick colors from this palette.\n\n    alpha :           float [0-1], optional\n\n                      Alpha value for neurons. Overriden if alpha is provided\n                      as color specified in `color` has an alpha channel.\n\n    connectors :      bool | \"presynapses\" | \"postsynapses\" | str | list, default=True\n\n                      Plot connectors. This can either be `True` (plot all\n                      connectors), `\"presynapses\"` (only presynaptic connectors)\n                      or `\"postsynapses\"` (only postsynaptic connectors). If\n                      a string or a list is provided, it will be used to filter the\n                      `type` column in the connectors table.\n\n                      Use these parameters to adjust the way connectors are plotted:\n\n                        - `cn_colors` (str | tuple | dict | \"neuron\" ) overrides\n                          the default connector (e.g. synpase) colors:\n                            - single color as str (e.g. `'red'`) or rgb tuple\n                              (e.g. `(1, 0, 0)`)\n                            - dict mapping the connectors tables `type` column to\n                              a color (e.g. `{\"pre\": (1, 0, 0)}`)\n                            - with \"neuron\", connectors will receive the same color\n                              as their neuron\n                        - `cn_layout` (dict): Layout of the connectors. See\n                          `navis.config.default_connector_colors` for options.\n                        - `cn_size` (float): Size of the connectors.\n                        - `cn_alpha` (float): Transparency of the connectors.\n                        - `cn_mesh_colors` (bool): Whether to color the connectors\n                          by the neuron's color.\n\n    connectors_only : bool, default=False\n\n                      Plot only connectors (e.g. synapses) if available and\n                      ignore the neurons.\n\n    color_by :        str | array | list of arrays, default = None\n\n                      Color neurons by a property. Can be:\n\n                        - a list/array of labels, one per each neuron\n                        - a neuron property (str)\n                        - a column name in the node table of `TreeNeurons`\n                        - a list/array of values for each node\n\n                      Numerical values will be normalized. You can control\n                      the normalization by passing a `vmin` and/or `vmax`\n                      parameter. Must specify a colormap via `palette`.\n\n    shade_by :        str | array | list of arrays, default=None\n\n                      Similar to `color_by` but will affect only the alpha\n                      channel of the color. If `shade_by='strahler'` will\n                      compute Strahler order if not already part of the node\n                      table (TreeNeurons only). Numerical values will be\n                      normalized. You can control the normalization by passing\n                      a `smin` and/or `smax` parameter. Does not work with\n                      `k3d` backend.\n\n    radius :          bool | \"auto\", default=False\n\n                      If \"auto\" will plot neurites of `TreeNeurons` with radius\n                      if they have radii. If True, will try plotting neurites of\n                      `TreeNeurons` with radius regardless. The radius can be\n                      scaled by `linewidth`. Note that this will increase rendering\n                      time.\n\n    soma :            bool, default=True\n\n                      TreeNeurons only: Whether to plot soma if it exists. Size\n                      of the soma is determined by the neuron's `.soma_radius`\n                      property which defaults to the \"radius\" column for\n                      `TreeNeurons`.\n\n    linewidth :       float, default=3 for plotly and 1 for all others\n\n                      TreeNeurons only.\n\n    linestyle :       str, default='-'\n\n                      TreeNeurons only. Follows the same rules as in matplotlib.\n\n    scatter_kws :     dict, optional\n\n                      Use to modify scatter plots. Accepted parameters are:\n                        - `size` to adjust size of dots\n                        - `color` to adjust color\n\n    Figure parameters\n    -----------------\n    backend :         'auto' (default) | 'octarine' | 'vispy' | 'plotly' | 'k3d'\n\n                      Which backend to use for plotting. Note that there will\n                      be minor differences in what feature/parameters are\n                      supported depending on the backend:\n\n                        - `auto` selects backend based on availability and\n                          context (see above). You can override this by setting an\n                          environment variable e.g. `NAVIS_PLOT3D_BACKEND=\"vispy\"`\n                          or `NAVIS_PLOT3D_JUPYTER_BACKEND=\"k3d\"`.\n                        - `octarine` uses WGPU to generate high performances\n                          interactive 3D plots. Works both terminal and Jupyter.\n                        - `vispy` similar to octarine but uses OpenGL: slower\n                          but runs on older systems. Works only from terminals.\n                        - `plotly` generates 3D plots using WebGL. Works\n                          \"inline\" in Jupyter notebooks but can also produce a\n                          HTML file that can be opened in any browers.\n                        - `k3d` generates 3D plots using k3d. Works only in\n                          Jupyter notebooks!\n\n    **Below parameters are for plotly backend only:**\n\n    fig :             plotly.graph_objs.Figure\n\n                      Pass to add graph objects to existing plotly figure. Will\n                      not change layout.\n\n    title :           str, default=None\n\n                      For plotly only! Change plot title.\n\n    width/height :    int, optional\n\n                      Use to adjust figure size.\n\n    fig_autosize :    bool, default=False\n\n                      For plotly only! Autoscale figure size.\n                      Attention: autoscale overrides width and height\n\n    hover_name :      bool, default=False\n\n                      If True, hovering over neurons will show their label.\n\n    hover_id :        bool, default=False\n\n                      If True, hovering over skeleton nodes will show their ID.\n\n    legend :          bool, default=True\n\n                      Whether or not to show the legend.\n\n    legend_orientation : \"v\" (default) | \"h\"\n\n                      Orientation of the legend. Can be 'h' (horizontal) or 'v'\n                      (vertical).\n\n    legend_group :    dict, default=None\n\n                      A dictionary mapping neuron IDs to labels (strings).\n                      Use this to group neurons under a common label in the\n                      legend.\n\n    inline :          bool, default=True\n\n                      If True and you are in an Jupyter environment, will\n                      render plotly/k3d plots inline. If False, will generate\n                      and return either a plotly Figure or a k3d Plot object\n                      without immediately showing it.\n\n    **Below parameters are for the Octarine/vispy backends only:**\n\n    clear :           bool, default = False\n\n                      If True, will clear the viewer before adding the new\n                      objects.\n\n    center :          bool, default = True\n\n                      If True, will center camera on the newly added objects.\n\n    combine :         bool, default = False\n\n                      If True, will combine objects of the same type into a\n                      single visual. This can greatly improve performance but\n                      also means objects can't be selected individually\n                      anymore. This is Vispy only.\n\n    size :            (width, height) tuple, optional\n\n                      Use to adjust figure/window size.\n\n    show :            bool, default=True\n\n                      Whether to immediately show the viewer.\n\n    Returns\n    -------\n    If `backend='octarine'`\n\n        From terminal: opens a 3D window and returns :class:`octarine.Viewer`.\n        From Jupyter: :class:`octarine.Viewer` displayed in an ipywidget.\n\n    If `backend='vispy'`\n\n        Opens a 3D window and returns [`navis.Viewer`][].\n\n    If `backend='plotly'`\n\n        Returns either `None` if you are in a Jupyter notebook (see also\n        `inline` parameter) or a `plotly.graph_objects.Figure`\n        (see examples).\n\n    If `backend='k3d'`\n\n        Returns either `None` and immediately displays the plot or a\n        `k3d.plot` object that you can manipulate further (see `inline`\n        parameter).\n\n    See Also\n    --------\n    [`octarine.Viewer`](https://schlegelp.github.io/octarine/)\n        Interactive 3D viewer.\n\n    [`navis.Viewer`][]\n        Interactive vispy 3D viewer.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n\n    In a Jupyter notebook using plotly as backend:\n\n    &gt;&gt;&gt; nl = navis.example_neurons()\n    &gt;&gt;&gt; # Backend is automatically chosen but we can set it explicitly\n    &gt;&gt;&gt; # Plot inline\n    &gt;&gt;&gt; nl.plot3d(backend='plotly')                             # doctest: +SKIP\n    &gt;&gt;&gt; # Plot as separate html in a new window\n    &gt;&gt;&gt; fig = nl.plot3d(backend='plotly', inline=False)\n    &gt;&gt;&gt; import plotly.offline\n    &gt;&gt;&gt; _ = plotly.offline.plot(fig)                            # doctest: +SKIP\n\n    In a Jupyter notebook using k3d as backend:\n\n    &gt;&gt;&gt; nl = navis.example_neurons()\n    &gt;&gt;&gt; # Plot inline\n    &gt;&gt;&gt; nl.plot3d(backend='k3d')                                # doctest: +SKIP\n\n    In a terminal using octarine as backend:\n\n    &gt;&gt;&gt; # Plot list of neurons\n    &gt;&gt;&gt; nl = navis.example_neurons()\n    &gt;&gt;&gt; v = navis.plot3d(nl, backend='octarine')                # doctest: +SKIP\n    &gt;&gt;&gt; # Clear canvas\n    &gt;&gt;&gt; navis.clear3d()\n\n    Some more advanced examples:\n\n    &gt;&gt;&gt; # plot3d() can deal with combinations of objects\n    &gt;&gt;&gt; nl = navis.example_neurons()\n    &gt;&gt;&gt; vol = navis.example_volume('LH')\n    &gt;&gt;&gt; vol.color = (255, 0, 0, .5)\n    &gt;&gt;&gt; # This plots a neuronlists, a single neuron and a volume\n    &gt;&gt;&gt; v = navis.plot3d([nl[0:2], nl[3], vol])\n    &gt;&gt;&gt; # Clear viewer (works only with octarine and vispy)\n    &gt;&gt;&gt; v = navis.plot3d(nl, clear=True)\n\n    See the [plotting intro](../../generated/gallery/1_plotting/tutorial_plotting_00_intro)\n    for even more examples.\n\n    \"\"\"\n    # Select backend\n    backend = kwargs.pop(\"backend\", \"auto\")\n    allowed_backends = (\"auto\", \"octarine\", \"vispy\", \"plotly\", \"k3d\")\n    if backend.lower() == \"auto\":\n        global AUTO_BACKEND\n        if AUTO_BACKEND is not None:\n            backend = AUTO_BACKEND\n        else:\n            if utils.is_jupyter():\n                if not len(JUPYTER_BACKENDS):\n                    raise ModuleNotFoundError(\n                        \"No 3D plotting backends available for Jupyter \"\n                        \"environment. Please install one of the following: \"\n                        \"plotly, octarine, k3d.\"\n                    )\n                backend = os.environ.get(\n                    \"NAVIS_PLOT3D_JUPYTER_BACKEND\", JUPYTER_BACKENDS[0]\n                )\n            else:\n                if not len(NON_JUPYTER_BACKENDS):\n                    raise ModuleNotFoundError(\n                        \"No 3D plotting backends available for REPL/script. Please \"\n                        \"install one of the following: octarine, vispy, plotly.\"\n                    )\n                backend = os.environ.get(\n                    \"NAVIS_PLOT3D_BACKEND\", NON_JUPYTER_BACKENDS[0]\n                )\n\n            # Set the backend for the next time\n            AUTO_BACKEND = backend\n\n            logger.info(f'Using \"{backend}\" backend for 3D plotting.')\n    elif backend.lower() not in allowed_backends:\n        raise ValueError(\n            f'Unknown backend \"{backend}\". ' f'Permitted: {\".\".join(allowed_backends)}.'\n        )\n    elif backend.lower() not in BACKENDS:\n        raise ModuleNotFoundError(\n            f'Backend \"{backend}\" not installed. Please install it via pip '\n            \"(see https://navis.readthedocs.io/en/latest/source/install.html#optional-dependencies \"\n            \"for more information).\"\n        )\n\n    if backend == \"vispy\":\n        return plot3d_vispy(x, **kwargs)\n    elif backend == \"k3d\":\n        if not utils.is_jupyter():\n            logger.warning(\"k3d backend only works in Jupyter environments\")\n        return plot3d_k3d(x, **kwargs)\n    elif backend == \"plotly\":\n        return plot3d_plotly(x, **kwargs)\n    elif backend == \"octarine\":\n        return plot3d_octarine(x, **kwargs)\n    else:\n        raise ValueError(\n            f'Unknown backend \"{backend}\". ' f'Permitted: {\".\".join(allowed_backends)}.'\n        )\n</code></pre>"},{"location":"reference/navis/#navis.plot_flat","title":"<code>navis.plot_flat</code>","text":"<p>Plot neuron as flat diagrams.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>                A single neuron to plot.\n</code></pre> <p> TYPE: <code>                    TreeNeuron</code> </p> <code>layout</code> <pre><code>                Layout to use. All but 'subway' require graphviz to\n                be installed. For the 'fdp' and 'neato' it is highly\n                recommended to downsample the neuron first.\n</code></pre> <p> TYPE: <code>               'subway' | 'dot' | 'neato' | 'fdp' | 'sfpd' | 'twopi' | 'circo'</code> DEFAULT: <code>'subway'</code> </p> <code>connectors</code> <pre><code>                If True (and neuron actually has connectors), will plot\n                connectors.\n</code></pre> <p> TYPE: <code>           bool</code> DEFAULT: <code>False</code> </p> <code>highlight_connectors</code> <pre><code>                Will highlight these connector IDs.\n</code></pre> <p> TYPE: <code> list of connector IDs</code> DEFAULT: <code>None</code> </p> <code>ax</code> <pre><code>                Ax to plot on. Will create new one if not provided.\n</code></pre> <p> TYPE: <code>                   matplotlib.ax</code> DEFAULT: <code>None</code> </p> <code>shade_by_length</code> <pre><code>                Change shade of branch with length. For layout\n                \"subway\" only.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> <code>normalize_distance</code> <pre><code>                If True, will normalise all distances to the longest\n                neurite. For layout \"subway\" only.\n</code></pre> <p> TYPE: <code>   bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <pre><code>                Keyword argument passed on to the respective\n                plotting functions.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ax</code> <p> TYPE: <code>matplotlib.ax</code> </p> <code>pos</code> <p>(X, Y) positions for each node: <code>{node_id: (x, y)}</code>.</p> <p> TYPE: <code>dict</code> </p> <p>Examples:</p> <p>Plot neuron in \"subway\" layout:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1).convert_units('nm')\n&gt;&gt;&gt; ax, pos = navis.plot_flat(n, layout='subway',\n...                           figsize=(12, 2),\n...                           connectors=True)\n&gt;&gt;&gt; _ = ax.set_xlabel('distance [nm]')\n&gt;&gt;&gt; plt.show()\n</code></pre> <p>Plot neuron in \"dot\" layout (requires pygraphviz and graphviz):</p> <pre><code>&gt;&gt;&gt; # First downsample to speed up processing\n&gt;&gt;&gt; ds = navis.downsample_neuron(n, 10, preserve_nodes='connectors')\n&gt;&gt;&gt; ax, pos = navis.plot_flat(ds, layout='dot', connectors=True)\n&gt;&gt;&gt; plt.show()\n</code></pre> <p>To close all figures (only for doctests)</p> <pre><code>&gt;&gt;&gt; plt.close('all')\n</code></pre> <p>See the plotting intro and the neuron topology tutorial for more examples.</p> Source code in <code>navis/plotting/flat.py</code> <pre><code>def plot_flat(\n    x,\n    layout: Union[\n        Literal[\"subway\"],\n        Literal[\"dot\"],\n        Literal[\"neato\"],\n        Literal[\"fpd\"],\n        Literal[\"sfpd\"],\n        Literal[\"twopi\"],\n        Literal[\"circo\"],\n    ] = \"subway\",\n    connectors: bool = False,\n    highlight_connectors: Optional[List[int]] = None,\n    shade_by_length: bool = False,\n    normalize_distance: bool = False,\n    reroot_soma: bool = False,\n    ax: Optional[Any] = None,\n    **kwargs,\n):\n    \"\"\"Plot neuron as flat diagrams.\n\n    Parameters\n    ----------\n    x :                     TreeNeuron\n                            A single neuron to plot.\n    layout :                'subway' | 'dot' | 'neato' | 'fdp' | 'sfpd' | 'twopi' | 'circo'\n                            Layout to use. All but 'subway' require graphviz to\n                            be installed. For the 'fdp' and 'neato' it is highly\n                            recommended to downsample the neuron first.\n    connectors :            bool\n                            If True (and neuron actually has connectors), will plot\n                            connectors.\n    highlight_connectors :  list of connector IDs, optional\n                            Will highlight these connector IDs.\n    ax :                    matplotlib.ax, optional\n                            Ax to plot on. Will create new one if not provided.\n    shade_by_length :       bool, optional\n                            Change shade of branch with length. For layout\n                            \"subway\" only.\n    normalize_distance :    bool, optional\n                            If True, will normalise all distances to the longest\n                            neurite. For layout \"subway\" only.\n    **kwargs\n                            Keyword argument passed on to the respective\n                            plotting functions.\n\n    Returns\n    -------\n    ax :                    matplotlib.ax\n    pos :                   dict\n                            (X, Y) positions for each node: `{node_id: (x, y)}`.\n\n\n    Examples\n    --------\n    Plot neuron in \"subway\" layout:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1).convert_units('nm')\n    &gt;&gt;&gt; ax, pos = navis.plot_flat(n, layout='subway',\n    ...                           figsize=(12, 2),\n    ...                           connectors=True)\n    &gt;&gt;&gt; _ = ax.set_xlabel('distance [nm]')\n    &gt;&gt;&gt; plt.show() # doctest: +SKIP\n\n    Plot neuron in \"dot\" layout (requires pygraphviz and graphviz):\n\n    &gt;&gt;&gt; # First downsample to speed up processing\n    &gt;&gt;&gt; ds = navis.downsample_neuron(n, 10, preserve_nodes='connectors')\n    &gt;&gt;&gt; ax, pos = navis.plot_flat(ds, layout='dot', connectors=True) # doctest: +SKIP\n    &gt;&gt;&gt; plt.show()                                                   # doctest: +SKIP\n\n    To close all figures (only for doctests)\n\n    &gt;&gt;&gt; plt.close('all')\n\n    See the [plotting intro](../../generated/gallery/1_plotting/tutorial_plotting_00_intro)\n    and the [neuron topology tutorial](../../generated/gallery/1_plotting/tutorial_plotting_03_dend)\n    for more examples.\n\n    \"\"\"\n    if isinstance(x, core.NeuronList) and len(x) == 1:\n        x = x[0]\n\n    utils.eval_param(x, name=\"x\", allowed_types=(core.TreeNeuron,))\n    utils.eval_param(\n        layout,\n        name=\"layout\",\n        allowed_values=(\"subway\", \"dot\", \"neato\", \"fdp\", \"sfdp\", \"twopi\", \"circo\"),\n    )\n\n    # Work on the copy of the neuron\n    x = x.copy()\n\n    # Reroot to soma (if applicable)\n    if reroot_soma and x.soma:\n        x.reroot(x.soma, inplace=True)\n\n    if layout == \"subway\":\n        return _plot_subway(\n            x,\n            connectors=connectors,\n            highlight_connectors=highlight_connectors,\n            shade_by_length=shade_by_length,\n            normalize_distance=normalize_distance,\n            ax=ax,\n            **kwargs,\n        )\n    else:\n        return _plot_force(\n            x,\n            prog=layout,\n            connectors=connectors,\n            highlight_connectors=highlight_connectors,\n            ax=ax,\n            **kwargs,\n        )\n</code></pre>"},{"location":"reference/navis/#navis.pop3d","title":"<code>navis.pop3d</code>","text":"<p>Remove the last item added to the 3D canvas.</p> Source code in <code>navis/plotting/vispy/vputils.py</code> <pre><code>def pop3d():\n    \"\"\"Remove the last item added to the 3D canvas.\"\"\"\n    viewer = get_viewer()\n    viewer.pop()\n</code></pre>"},{"location":"reference/navis/#navis.prune_at_depth","title":"<code>navis.prune_at_depth</code>","text":"<p>Prune all neurites past a given distance from a source.</p> PARAMETER DESCRIPTION <code>x</code> <p> TYPE: <code>            TreeNeuron | MeshNeuron | NeuronList</code> </p> <code>depth</code> <pre><code>        Distance from source at which to start pruning. If neuron\n        has its `.units` set, you can also pass this as a string such\n        as \"50 microns\".\n</code></pre> <p> TYPE: <code>        int | float | str</code> </p> <code>source</code> <pre><code>        Source node for depth calculation. If `None`, will use\n        root (first root if multiple). If `x` is a\n        list of neurons then must provide a source for each neuron.\n</code></pre> <p> TYPE: <code>       int</code> DEFAULT: <code>None</code> </p> <code>inplace</code> <pre><code>        If False, pruning is performed on copy of original neuron\n        which is then returned.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>TreeNeuron / List</code> <p>Pruned neuron(s).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(2)\n&gt;&gt;&gt; # Reroot to soma\n&gt;&gt;&gt; n.reroot(n.soma, inplace=True)\n&gt;&gt;&gt; # Prune all twigs farther from the root than 100 microns\n&gt;&gt;&gt; # (example neuron are in 8x8x8nm units)\n&gt;&gt;&gt; n_pr = navis.prune_at_depth(n,\n...                             depth=100e3 / 8,\n...                             inplace=False)\n&gt;&gt;&gt; all(n.n_nodes &gt; n_pr.n_nodes)\nTrue\n</code></pre> Source code in <code>navis/morpho/manipulation.py</code> <pre><code>@utils.map_neuronlist(desc=\"Pruning\", must_zip=[\"source\"], allow_parallel=True)\n@utils.meshneuron_skeleton(method=\"subset\")\ndef prune_at_depth(\n    x: NeuronObject,\n    depth: Union[float, int],\n    *,\n    source: Optional[int] = None,\n    inplace: bool = False,\n) -&gt; Optional[NeuronObject]:\n    \"\"\"Prune all neurites past a given distance from a source.\n\n    Parameters\n    ----------\n    x :             TreeNeuron | MeshNeuron | NeuronList\n    depth :         int | float | str\n                    Distance from source at which to start pruning. If neuron\n                    has its `.units` set, you can also pass this as a string such\n                    as \"50 microns\".\n    source :        int, optional\n                    Source node for depth calculation. If `None`, will use\n                    root (first root if multiple). If `x` is a\n                    list of neurons then must provide a source for each neuron.\n    inplace :       bool, optional\n                    If False, pruning is performed on copy of original neuron\n                    which is then returned.\n\n    Returns\n    -------\n    TreeNeuron/List\n                    Pruned neuron(s).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(2)\n    &gt;&gt;&gt; # Reroot to soma\n    &gt;&gt;&gt; n.reroot(n.soma, inplace=True)\n    &gt;&gt;&gt; # Prune all twigs farther from the root than 100 microns\n    &gt;&gt;&gt; # (example neuron are in 8x8x8nm units)\n    &gt;&gt;&gt; n_pr = navis.prune_at_depth(n,\n    ...                             depth=100e3 / 8,\n    ...                             inplace=False)\n    &gt;&gt;&gt; all(n.n_nodes &gt; n_pr.n_nodes)\n    True\n\n    \"\"\"\n    # The decorator makes sure that at this point we only have single neurons\n    if not isinstance(x, core.TreeNeuron):\n        raise TypeError(f\"Expected TreeNeuron, got {type(x)}\")\n\n    depth = x.map_units(depth, on_error=\"raise\")\n    if depth &lt; 0:\n        raise ValueError(f'`depth` must be &gt; 0, got \"{depth}\"')\n\n    if isinstance(source, type(None)):\n        source = x.root[0]\n    elif source not in x.nodes.node_id.values:\n        raise ValueError(f'Source \"{source}\" not among nodes')\n\n    # Get distance from source\n    dist = graph.geodesic_matrix(x, from_=source, directed=False, limit=depth)\n    keep = dist.columns[dist.values[0] &lt; np.inf]\n\n    if not inplace:\n        x = x.copy()\n\n    _ = subset.subset_neuron(x, subset=keep, inplace=True)\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.prune_by_strahler","title":"<code>navis.prune_by_strahler</code>","text":"<p>Prune neuron based on Strahler order.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Neuron(s) to prune.\n</code></pre> <p> TYPE: <code>            TreeNeuron | MeshNeuron | NeuronList</code> </p> <code>to_prune</code> <pre><code>        Strahler indices (SI) to prune. For example:\n          1. `to_prune=1` removes all leaf branches\n          2. `to_prune=[1, 2]` removes SI 1 and 2\n          3. `to_prune=range(1, 4)` removes SI 1, 2 and 3\n          4. `to_prune=slice(0, -1)` removes everything but the\n             highest SI\n          5. `to_prune=slice(-1, None)` removes only the highest\n             SI\n</code></pre> <p> TYPE: <code>     int | list | range | slice</code> </p> <code>reroot_soma</code> <pre><code>        If True, neuron will be rerooted to its soma.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>True</code> </p> <code>inplace</code> <pre><code>        If False, pruning is performed on copy of original neuron\n        which is then returned.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> <code>force_strahler_update</code> <pre><code>                If True, will force update of Strahler order even\n                if already exists in node table.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>relocate_connectors</code> <pre><code>              If True, connectors on removed nodes will be\n              reconnected to the closest still existing node.\n              Works only in child-&gt;parent direction.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>TreeNeuron / List</code> <p>Pruned neuron(s).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; n_pr = navis.prune_by_strahler(n, to_prune=1, inplace=False)\n&gt;&gt;&gt; n.n_nodes &gt; n_pr.n_nodes\nTrue\n</code></pre> Source code in <code>navis/morpho/manipulation.py</code> <pre><code>@utils.map_neuronlist(desc=\"Pruning\", allow_parallel=True)\n@utils.meshneuron_skeleton(method=\"subset\")\ndef prune_by_strahler(\n    x: NeuronObject,\n    to_prune: Union[int, List[int], range, slice],\n    inplace: bool = False,\n    reroot_soma: bool = True,\n    force_strahler_update: bool = False,\n    relocate_connectors: bool = False,\n) -&gt; NeuronObject:\n    \"\"\"Prune neuron based on [Strahler order](https://en.wikipedia.org/wiki/Strahler_number).\n\n    Parameters\n    ----------\n    x :             TreeNeuron | MeshNeuron | NeuronList\n                    Neuron(s) to prune.\n    to_prune :      int | list | range | slice\n                    Strahler indices (SI) to prune. For example:\n                      1. `to_prune=1` removes all leaf branches\n                      2. `to_prune=[1, 2]` removes SI 1 and 2\n                      3. `to_prune=range(1, 4)` removes SI 1, 2 and 3\n                      4. `to_prune=slice(0, -1)` removes everything but the\n                         highest SI\n                      5. `to_prune=slice(-1, None)` removes only the highest\n                         SI\n    reroot_soma :   bool, optional\n                    If True, neuron will be rerooted to its soma.\n    inplace :       bool, optional\n                    If False, pruning is performed on copy of original neuron\n                    which is then returned.\n    force_strahler_update : bool, optional\n                            If True, will force update of Strahler order even\n                            if already exists in node table.\n    relocate_connectors : bool, optional\n                          If True, connectors on removed nodes will be\n                          reconnected to the closest still existing node.\n                          Works only in child-&gt;parent direction.\n\n    Returns\n    -------\n    TreeNeuron/List\n                    Pruned neuron(s).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; n_pr = navis.prune_by_strahler(n, to_prune=1, inplace=False)\n    &gt;&gt;&gt; n.n_nodes &gt; n_pr.n_nodes\n    True\n\n    \"\"\"\n    # The decorator makes sure that at this point we have single neurons\n    if not isinstance(x, core.TreeNeuron):\n        raise TypeError(f\"Expected TreeNeuron(s), got {type(x)}\")\n\n    # Make a copy if necessary before making any changes\n    neuron = x\n    if not inplace:\n        neuron = neuron.copy()\n\n    if reroot_soma and not isinstance(neuron.soma, type(None)):\n        neuron.reroot(neuron.soma, inplace=True)\n\n    if \"strahler_index\" not in neuron.nodes or force_strahler_update:\n        mmetrics.strahler_index(neuron)\n\n    # Prepare indices\n    if isinstance(to_prune, int) and to_prune &lt; 0:\n        to_prune = range(1, int(neuron.nodes.strahler_index.max() + (to_prune + 1)))\n\n    if isinstance(to_prune, int):\n        if to_prune &lt; 1:\n            raise ValueError(\n                \"SI to prune must be positive. Please see docs\"\n                \"for additional options.\"\n            )\n        to_prune = [to_prune]\n    elif isinstance(to_prune, range):\n        to_prune = list(to_prune)\n    elif isinstance(to_prune, slice):\n        SI_range = range(1, int(neuron.nodes.strahler_index.max() + 1))\n        to_prune = list(SI_range)[to_prune]\n\n    # Prepare parent dict if needed later\n    if relocate_connectors:\n        parent_dict = {tn.node_id: tn.parent_id for tn in neuron.nodes.itertuples()}\n\n    # Avoid setting the nodes as this potentiall triggers a regeneration\n    # of the graph which in turn will raise an error because some nodes might\n    # still have parents that don't exist anymore\n    neuron._nodes = neuron._nodes[\n        ~neuron._nodes.strahler_index.isin(to_prune)\n    ].reset_index(drop=True, inplace=False)\n\n    if neuron.has_connectors:\n        if not relocate_connectors:\n            neuron._connectors = neuron._connectors[\n                neuron._connectors.node_id.isin(neuron._nodes.node_id.values)\n            ].reset_index(drop=True, inplace=False)\n        else:\n            remaining_tns = set(neuron._nodes.node_id.values)\n            for cn in neuron._connectors[\n                ~neuron.connectors.node_id.isin(neuron._nodes.node_id.values)\n            ].itertuples():\n                this_tn = parent_dict[cn.node_id]\n                while True:\n                    if this_tn in remaining_tns:\n                        break\n                    this_tn = parent_dict[this_tn]\n                neuron._connectors.loc[cn.Index, \"node_id\"] = this_tn\n\n    # Reset indices of node and connector tables (important for igraph!)\n    neuron._nodes.reset_index(inplace=True, drop=True)\n\n    if neuron.has_connectors:\n        neuron._connectors.reset_index(inplace=True, drop=True)\n\n    # Theoretically we can end up with disconnected pieces, i.e. with more\n    # than 1 root node -&gt; we have to fix the nodes that lost their parents\n    neuron._nodes.loc[\n        ~neuron._nodes.parent_id.isin(neuron._nodes.node_id.values), \"parent_id\"\n    ] = -1\n\n    # Remove temporary attributes\n    neuron._clear_temp_attr()\n\n    return neuron\n</code></pre>"},{"location":"reference/navis/#navis.prune_twigs","title":"<code>navis.prune_twigs</code>","text":"<p>Prune terminal twigs under a given size.</p> <p>By default this function will simply drop all terminal twigs shorter than <code>size</code>. This is very fast but rather stupid: for example, if a twig is just 1 nanometer longer than <code>size</code> it will not be touched at all. If you require precision, set <code>exact=True</code> which will prune exactly <code>size</code> off the terminals but is about an order of magnitude slower.</p> PARAMETER DESCRIPTION <code>x</code> <p> TYPE: <code>            TreeNeuron | MeshNeuron | NeuronList</code> </p> <code>size</code> <pre><code>        Twigs shorter than this will be pruned. If the neuron has\n        its `.units` set, you can also pass a string including the\n        units, e.g. '5 microns'.\n</code></pre> <p> TYPE: <code>         int | float | str</code> </p> <code>exact</code> <pre><code>        See notes above.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>mask</code> <pre><code>        Either a boolean mask, a list of node IDs or a callable taking\n        a neuron as input and returning one of the former. If provided,\n        only nodes that are in the mask will be considered for pruning.\n</code></pre> <p> TYPE: <code>         iterable | callable</code> DEFAULT: <code>None</code> </p> <code>inplace</code> <pre><code>        If False, pruning is performed on copy of original neuron\n        which is then returned.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> <code>recursive</code> <pre><code>        If `int` will undergo that many rounds of recursive\n        pruning. If True will prune iteratively until no more\n        terminal twigs under the given size are left. Only\n        relevant if `exact=False`.\n</code></pre> <p> TYPE: <code>    int | bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>TreeNeuron / List</code> <p>Pruned neuron(s).</p> <p>Examples:</p> <p>Simple pruning</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(2)\n&gt;&gt;&gt; # Prune twigs smaller than 5 microns\n&gt;&gt;&gt; # (example neuron are in 8x8x8nm units)\n&gt;&gt;&gt; n_pr = navis.prune_twigs(n,\n...                          size=5000 / 8,\n...                          recursive=float('inf'),\n...                          inplace=False)\n&gt;&gt;&gt; all(n.n_nodes &gt; n_pr.n_nodes)\nTrue\n</code></pre> <p>Exact pruning</p> <pre><code>&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; # Prune twigs by exactly 5 microns\n&gt;&gt;&gt; # (example neuron are in 8x8x8nm units)\n&gt;&gt;&gt; n_pr = navis.prune_twigs(n,\n...                          size=5000 / 8,\n...                          exact=True,\n...                          inplace=False)\n&gt;&gt;&gt; n.n_nodes &gt; n_pr.n_nodes\nTrue\n</code></pre> <p>Prune using units</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; # Example neurons are in 8x8x8nm units...\n&gt;&gt;&gt; n.units\n&lt;Quantity(8, 'nanometer')&gt;\n&gt;&gt;&gt; # ... therefore we can use units for `size`\n&gt;&gt;&gt; n_pr = navis.prune_twigs(n,\n...                          size='5 microns',\n...                          inplace=False)\n&gt;&gt;&gt; n.n_nodes &gt; n_pr.n_nodes\nTrue\n</code></pre> Source code in <code>navis/morpho/manipulation.py</code> <pre><code>@utils.map_neuronlist(desc=\"Pruning\", allow_parallel=True)\n@utils.meshneuron_skeleton(method=\"subset\")\ndef prune_twigs(\n    x: NeuronObject,\n    size: Union[float, str],\n    exact: bool = False,\n    mask: Optional[Union[Sequence[int], Callable]] = None,\n    inplace: bool = False,\n    recursive: Union[int, bool, float] = False,\n) -&gt; NeuronObject:\n    \"\"\"Prune terminal twigs under a given size.\n\n    By default this function will simply drop all terminal twigs shorter than\n    `size`. This is very fast but rather stupid: for example, if a twig is\n    just 1 nanometer longer than `size` it will not be touched at all. If you\n    require precision, set `exact=True` which will prune *exactly* `size`\n    off the terminals but is about an order of magnitude slower.\n\n    Parameters\n    ----------\n    x :             TreeNeuron | MeshNeuron | NeuronList\n    size :          int | float | str\n                    Twigs shorter than this will be pruned. If the neuron has\n                    its `.units` set, you can also pass a string including the\n                    units, e.g. '5 microns'.\n    exact:          bool\n                    See notes above.\n    mask :          iterable | callable, optional\n                    Either a boolean mask, a list of node IDs or a callable taking\n                    a neuron as input and returning one of the former. If provided,\n                    only nodes that are in the mask will be considered for pruning.\n    inplace :       bool, optional\n                    If False, pruning is performed on copy of original neuron\n                    which is then returned.\n    recursive :     int | bool, optional\n                    If `int` will undergo that many rounds of recursive\n                    pruning. If True will prune iteratively until no more\n                    terminal twigs under the given size are left. Only\n                    relevant if `exact=False`.\n\n    Returns\n    -------\n    TreeNeuron/List\n                    Pruned neuron(s).\n\n    Examples\n    --------\n    Simple pruning\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(2)\n    &gt;&gt;&gt; # Prune twigs smaller than 5 microns\n    &gt;&gt;&gt; # (example neuron are in 8x8x8nm units)\n    &gt;&gt;&gt; n_pr = navis.prune_twigs(n,\n    ...                          size=5000 / 8,\n    ...                          recursive=float('inf'),\n    ...                          inplace=False)\n    &gt;&gt;&gt; all(n.n_nodes &gt; n_pr.n_nodes)\n    True\n\n    Exact pruning\n\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; # Prune twigs by exactly 5 microns\n    &gt;&gt;&gt; # (example neuron are in 8x8x8nm units)\n    &gt;&gt;&gt; n_pr = navis.prune_twigs(n,\n    ...                          size=5000 / 8,\n    ...                          exact=True,\n    ...                          inplace=False)\n    &gt;&gt;&gt; n.n_nodes &gt; n_pr.n_nodes\n    True\n\n    Prune using units\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; # Example neurons are in 8x8x8nm units...\n    &gt;&gt;&gt; n.units\n    &lt;Quantity(8, 'nanometer')&gt;\n    &gt;&gt;&gt; # ... therefore we can use units for `size`\n    &gt;&gt;&gt; n_pr = navis.prune_twigs(n,\n    ...                          size='5 microns',\n    ...                          inplace=False)\n    &gt;&gt;&gt; n.n_nodes &gt; n_pr.n_nodes\n    True\n\n    \"\"\"\n    # The decorator makes sure that at this point we have single neurons\n    if not isinstance(x, core.TreeNeuron):\n        raise TypeError(f\"Expected TreeNeuron(s), got {type(x)}\")\n\n    # Convert to neuron units - numbers will be passed through\n    size = x.map_units(size, on_error=\"raise\")\n\n    if not exact:\n        return _prune_twigs_simple(\n            x, size=size, inplace=inplace, recursive=recursive, mask=mask\n        )\n    else:\n        return _prune_twigs_precise(x, size=size, mask=mask, inplace=inplace)\n</code></pre>"},{"location":"reference/navis/#navis.read_h5","title":"<code>navis.read_h5</code>","text":"<p>Read Neuron/List from Hdf5 file.</p> <p>This import is following the schema specified here</p> PARAMETER DESCRIPTION <code>filepath</code> <pre><code>            Path to HDF5 file.\n</code></pre> <p> TYPE: <code>         filepath</code> </p> <code>read</code> <pre><code>            The HDF5 file might contain skeleton, dotprops and/or\n            mesh representations for any given neuron. This\n            parameter determines which one are returned. Some\n            illustrative examples:\n\n              - 'mesh', 'skeleton' or 'dotprops' will return only\n                the given representation\n              - 'mesh-&gt;skeleton-&gt;dotprops' will return a mesh if the\n                neuron has one, a skeleton if it does not and\n                dotprops if it has neither mesh nor skeleton\n              - 'mesh,skeleton,dotprops' will return all available\n                representations\n              - 'mesh,dotprops' will only return meshes and dotprops\n              - 'mesh,skeleton-&gt;dotprops' will return the mesh\n                and a skeleton or alternatively the dotprops\n\n            Note that neurons which have none of the requested\n            representations are silently skipped!\n</code></pre> <p> TYPE: <code>             str</code> DEFAULT: <code>'mesh-&gt;skeleton-&gt;dotprops'</code> </p> <code>subset</code> <pre><code>            If provided, will read only a subset of neurons from the\n            file. IDs that don't exist are silently ignored. Also\n            note that due to HDF5 restrictions numeric IDs will be\n            converted to strings.\n</code></pre> <p> TYPE: <code>           list of IDs | slice</code> DEFAULT: <code>None</code> </p> <code>prefer_raw</code> <pre><code>            If True and a neuron is saved as both serialized and\n            raw data, will load the neuron from the raw data.\n</code></pre> <p> TYPE: <code>       bool</code> DEFAULT: <code>False</code> </p> <code>parallel</code> <pre><code>            Defaults to `auto` which means only use parallel\n            processing if more than 200 neurons are imported.\n            Spawning and joining processes causes overhead and is\n            considerably slower for imports of small numbers of\n            neurons. Integer will be interpreted as the\n            number of cores (otherwise defaults to\n            `os.cpu_count() - 2`).\n</code></pre> <p> TYPE: <code>         \"auto\" | bool | int</code> DEFAULT: <code>'auto'</code> </p> <code>on_error</code> <pre><code>            What to do if a neuron can not be parsed: \"stop\" and\n            raise an exception, \"warn\" and keep going or silently\n            \"ignore\" and skip.\n</code></pre> <p> TYPE: <code>         \"stop\" | \"warn\" | \"ignore\"</code> DEFAULT: <code>'stop'</code> </p> <code>ret_errors</code> <pre><code>            If True, will also return a list of errors encountered\n            while parsing the neurons.\n</code></pre> <p> TYPE: <code>       bool</code> DEFAULT: <code>False</code> </p> <code>Only</code> <p> </p> <code>annotations</code> <pre><code>            Whether to load annotations associated with the\n            neuron(s):\n\n             - `True` reads all annotations\n             - `False` reads no annotations\n             - e.g. `[\"connenctors\"]` reads only \"connectors\"\n\n            Non-existing annotations are silently ignored!\n</code></pre> <p> TYPE: <code>      bool | str | list of str</code> DEFAULT: <code>True</code> </p> <code>strict</code> <pre><code>            If True, will read only the attributes/columns which\n            are absolutely required to construct the respective\n            neuron representation. This is useful if you either want\n            to keep memory usage low or if any additional attributes\n            are causing troubles. If False (default), will read\n            every attribute and dataframe column and attach it to\n            the neuron.\n</code></pre> <p> TYPE: <code>           bool</code> DEFAULT: <code>False</code> </p> <code>reader</code> <pre><code>            Which reader to use to parse the given format. By\n            default (\"auto\") will try to pick the correct parser\n            for you depending on the `format_spec` attribute in\n            the HDF5 file. Alternatively, you can also provided either\n            a format version (e.g. \"v1\") or a subclass of BaseH5Reader\n            that is capable of reading neurons from the file.\n</code></pre> <p> TYPE: <code>           \"auto\" | str | subclass of BaseH5Reader</code> DEFAULT: <code>'auto'</code> </p> RETURNS DESCRIPTION <code>neurons</code> <p> TYPE: <code>navis.NeuronList</code> </p> <code>errors</code> <p>If <code>ret_errors=True</code> return dictionary with errors: <code>{id: \"error\"}</code>.</p> <p> TYPE: <code>dict</code> </p> <p>Examples:</p> <p>See <code>navis.write_h5</code> for examples.</p> See Also <p><code>navis.write_h5</code>                     Write neurons to HDF5 file. <code>navis.io.inspect_h5</code>                     Extract meta data (format, number of neurons,                     available annotations and representations) from                     HDF5 file. This is useful if you don't know what's                     actually contained within the HDF5 file.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def read_h5(filepath: str,\n            read='mesh-&gt;skeleton-&gt;dotprops',\n            subset=None,\n            prefer_raw=False,\n            annotations=True,\n            strict=False,\n            reader='auto',\n            on_error='stop',\n            ret_errors=False,\n            parallel='auto') -&gt; 'core.NeuronObject':\n    \"\"\"Read Neuron/List from Hdf5 file.\n\n    This import is following the schema specified\n    [here](https://github.com/flyconnectome/hnf)\n\n    Parameters\n    ----------\n    filepath :          filepath\n                        Path to HDF5 file.\n    read :              str\n                        The HDF5 file might contain skeleton, dotprops and/or\n                        mesh representations for any given neuron. This\n                        parameter determines which one are returned. Some\n                        illustrative examples:\n\n                          - 'mesh', 'skeleton' or 'dotprops' will return only\n                            the given representation\n                          - 'mesh-&gt;skeleton-&gt;dotprops' will return a mesh if the\n                            neuron has one, a skeleton if it does not and\n                            dotprops if it has neither mesh nor skeleton\n                          - 'mesh,skeleton,dotprops' will return all available\n                            representations\n                          - 'mesh,dotprops' will only return meshes and dotprops\n                          - 'mesh,skeleton-&gt;dotprops' will return the mesh\n                            and a skeleton or alternatively the dotprops\n\n                        Note that neurons which have none of the requested\n                        representations are silently skipped!\n    subset :            list of IDs | slice\n                        If provided, will read only a subset of neurons from the\n                        file. IDs that don't exist are silently ignored. Also\n                        note that due to HDF5 restrictions numeric IDs will be\n                        converted to strings.\n    prefer_raw :        bool\n                        If True and a neuron is saved as both serialized and\n                        raw data, will load the neuron from the raw data.\n    parallel :          \"auto\" | bool | int\n                        Defaults to `auto` which means only use parallel\n                        processing if more than 200 neurons are imported.\n                        Spawning and joining processes causes overhead and is\n                        considerably slower for imports of small numbers of\n                        neurons. Integer will be interpreted as the\n                        number of cores (otherwise defaults to\n                        `os.cpu_count() - 2`).\n    on_error :          \"stop\" | \"warn\" | \"ignore\"\n                        What to do if a neuron can not be parsed: \"stop\" and\n                        raise an exception, \"warn\" and keep going or silently\n                        \"ignore\" and skip.\n    ret_errors :        bool\n                        If True, will also return a list of errors encountered\n                        while parsing the neurons.\n\n    Only relevant for raw data:\n\n    annotations :       bool | str | list of str\n                        Whether to load annotations associated with the\n                        neuron(s):\n\n                         - `True` reads all annotations\n                         - `False` reads no annotations\n                         - e.g. `[\"connenctors\"]` reads only \"connectors\"\n\n                        Non-existing annotations are silently ignored!\n    strict :            bool\n                        If True, will read only the attributes/columns which\n                        are absolutely required to construct the respective\n                        neuron representation. This is useful if you either want\n                        to keep memory usage low or if any additional attributes\n                        are causing troubles. If False (default), will read\n                        every attribute and dataframe column and attach it to\n                        the neuron.\n    reader :            \"auto\" | str | subclass of BaseH5Reader\n                        Which reader to use to parse the given format. By\n                        default (\"auto\") will try to pick the correct parser\n                        for you depending on the `format_spec` attribute in\n                        the HDF5 file. Alternatively, you can also provided either\n                        a format version (e.g. \"v1\") or a subclass of BaseH5Reader\n                        that is capable of reading neurons from the file.\n\n    Returns\n    -------\n    neurons :           navis.NeuronList\n\n    errors :            dict\n                        If `ret_errors=True` return dictionary with errors:\n                        `{id: \"error\"}`.\n\n    Examples\n    --------\n    See [`navis.write_h5`][] for examples.\n\n\n    See Also\n    --------\n    [`navis.write_h5`][]\n                        Write neurons to HDF5 file.\n    [`navis.io.inspect_h5`][]\n                        Extract meta data (format, number of neurons,\n                        available annotations and representations) from\n                        HDF5 file. This is useful if you don't know what's\n                        actually contained within the HDF5 file.\n\n    \"\"\"\n    utils.eval_param(read, name='read', allowed_types=(str, ))\n    utils.eval_param(on_error, name='on_error',\n                     allowed_values=('stop', 'warn', 'ignore'))\n\n    # Make sure the read string is \"correct\"\n    for rep in read.split(','):\n        rep = rep.strip()\n        for prio in rep.split('-&gt;'):\n            prio = prio.strip()\n            if prio not in ('mesh', 'skeleton', 'dotprops'):\n                raise ValueError(f'Unexpected representation in `read` parameter: {prio}')\n\n    # Get info for this file\n    filepath = os.path.expanduser(filepath)\n    info = inspect_h5(filepath, inspect_neurons=True, inspect_annotations=False)\n\n    # Get a reader for these specs\n    if reader == 'auto':\n        if info['format_spec'] is None:\n            config.logger.warning(\n                'No format specifier found in file, suggesting this file may not have '\n                'been created using NAVis. We will try to read using the latest '\n                'version of the schema. If this fails you may have to specify a reader '\n                'or version manually (see the `reader` parameter).')\n            reader = READERS['latest']\n        elif info['format_spec'] not in READERS:\n            raise TypeError(f'No reader for HDF5 format {info[\"format_spec\"]}')\n        reader = READERS[info['format_spec']]\n    elif isinstance(reader, str):\n        if reader not in READERS:\n            raise TypeError(f'No reader for HDF5 format \"{reader}\"')\n        reader = READERS[reader]\n    elif not isinstance(reader, BaseH5Reader):\n        raise TypeError('If provided, the reader must be a subclass of '\n                        f'BaseH5Reader - got \"{type(reader)}\"')\n\n    # By default only use parallel if there are more than 200 neurons\n    if parallel == 'auto':\n        if len(info['neurons']) &gt; 200:\n            parallel = True\n        else:\n            parallel = False\n\n    if not parallel:\n        # This opens the file\n        with reader(filepath) as r:\n            nl, errors = r.read_neurons(subset=subset,\n                                        read=read,\n                                        strict=strict,\n                                        on_error=on_error,\n                                        annotations=annotations)\n    else:\n        # Do not swap this as `isinstance(True, int)` returns `True`\n        if isinstance(parallel, (bool, str)):\n            n_cores = os.cpu_count() - 2\n        else:\n            n_cores = int(parallel)\n\n        # If subset not specified, fetch all neurons\n        if isinstance(subset, type(None)):\n            subset = list(info['neurons'])\n        elif isinstance(subset, slice):\n            subset = list(info['neurons'])[subset]\n        else:\n            # Make sure it's an iterable and strings\n            subset = utils.make_iterable(subset).astype(str)\n\n        # Just to leave note that I tried splitting the array into\n        # `n_cores` chunks but that caused massive memory usage in the\n        # spawned processes without being any faster - reading and returning\n        # one neuron at a time seems to be the most efficient way\n        reader = READERS[info['format_spec']]\n        with mp.Pool(processes=n_cores) as pool:\n            futures = pool.imap(_h5_reader_worker, [dict(reader=reader,\n                                                         filepath=filepath,\n                                                         read=read,\n                                                         strict=strict,\n                                                         prefer_raw=prefer_raw,\n                                                         on_error=on_error,\n                                                         annotations=annotations,\n                                                         subset=[x]) for x in subset],\n                                chunksize=1)\n\n            # Wait for results and show progress bar althewhile\n            # Do not close the pool before doing this\n            res = list(config.tqdm(futures,\n                                   desc='Reading',\n                                   total=len(subset)))\n\n        # Unpack results\n        nl = []\n        errors = {}\n        for n, e in res:\n            nl += n\n            errors.update(e)\n\n        # Warnings will not have propagated\n        if on_error == 'warn':\n            for e in errors:\n                warnings.warn(f\"Error reading neuron {e}: {errors[e]}\")\n\n    if ret_errors:\n        return core.NeuronList(nl), errors\n    else:\n        return core.NeuronList(nl)\n</code></pre>"},{"location":"reference/navis/#navis.read_json","title":"<code>navis.read_json</code>","text":"<p>Load neuron from JSON (file or string).</p> PARAMETER DESCRIPTION <code>s</code> <pre><code>    Either filepath or JSON-formatted string.\n</code></pre> <p> TYPE: <code>        str</code> </p> <code>**kwargs</code> <pre><code>    Parameters passed to `json.loads()` and\n    `pandas.DataFrame.read_json()`.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>[`navis.NeuronList`][]</code> See Also <p>[<code>navis.neuron2json</code>][]             Turn neuron into json.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; js = navis.write_json(n, filepath=None)\n&gt;&gt;&gt; n2 = navis.read_json(js)\n</code></pre> Source code in <code>navis/io/json_io.py</code> <pre><code>def read_json(s: str, **kwargs) -&gt; 'core.NeuronList':\n    \"\"\"Load neuron from JSON (file or string).\n\n    Parameters\n    ----------\n    s :         str\n                Either filepath or JSON-formatted string.\n    **kwargs\n                Parameters passed to `json.loads()` and\n                `pandas.DataFrame.read_json()`.\n\n    Returns\n    -------\n    [`navis.NeuronList`][]\n\n    See Also\n    --------\n    [`navis.neuron2json`][]\n                Turn neuron into json.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; js = navis.write_json(n, filepath=None)\n    &gt;&gt;&gt; n2 = navis.read_json(js)\n\n    \"\"\"\n    if not isinstance(s, (str, Path)):\n        raise TypeError(f'Expected str, got \"{type(s)}\"')\n\n    # Try except is necessary because Path() throws a hissy fit if it is given\n    # a long ass json string as filepath\n    try:\n        is_file = Path(s).is_file()\n    except OSError:\n        is_file = False\n    except BaseException:\n        raise\n\n    if is_file:\n        with open(Path(s), 'r') as f:\n            data = json.load(f, **kwargs)\n    else:\n        data = json.loads(s, **kwargs)\n\n    nl = core.NeuronList([])\n\n    for n in data:\n        cn = core.TreeNeuron(None)\n\n        if '_nodes' in n:\n            try:\n                cn._nodes = pd.read_json(io.StringIO(n['_nodes']))\n            except ValueError:\n                cn._nodes = None\n\n        if '_connectors' in n:\n            try:\n                cn._connectors = pd.read_json(io.StringIO(n['_connectors']))\n            except ValueError:\n                cn._connectors = None\n\n        for key in n:\n            if key in ['_nodes', '_connectors']:\n                continue\n            setattr(cn, key, n[key])\n\n        nl += cn\n\n    return nl\n</code></pre>"},{"location":"reference/navis/#navis.read_mesh","title":"<code>navis.read_mesh</code>","text":"<p>Load mesh file into Neuron/List.</p> <p>This is a thin wrapper around <code>trimesh.load_mesh</code> which supports most commonly used formats (obj, ply, stl, etc.).</p> PARAMETER DESCRIPTION <code>f</code> <pre><code>            Filename(s) or folder. If folder should include file\n            extension (e.g. `my/dir/*.ply`) otherwise all\n            mesh files in the folder will be read.\n</code></pre> <p> TYPE: <code>                str | iterable</code> </p> <code>include_subdirs</code> <pre><code>            If True and `f` is a folder, will also search\n            subdirectories for meshes.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> <code>parallel</code> <pre><code>            Defaults to `auto` which means only use parallel\n            processing if more than 100 mesh files are imported.\n            Spawning and joining processes causes overhead and is\n            considerably slower for imports of small numbers of\n            neurons. Integer will be interpreted as the number of\n            cores (otherwise defaults to `os.cpu_count() - 2`).\n</code></pre> <p> TYPE: <code>         \"auto\" | bool | int,</code> DEFAULT: <code>'auto'</code> </p> <code>output</code> <pre><code>            Determines function's output - see `Returns`.\n</code></pre> <p> TYPE: <code>           \"neuron\" | \"volume\" | \"trimesh\"</code> DEFAULT: <code>'neuron'</code> </p> <code>errors</code> <pre><code>            If \"log\" or \"ignore\", errors will not be raised and the\n            mesh will be skipped. Can result in empty output.\n</code></pre> <p> TYPE: <code>           \"raise\" | \"log\" | \"ignore\"</code> DEFAULT: <code>'raise'</code> </p> <code>limit</code> <pre><code>            When reading from a folder or archive you can use this parameter to\n            restrict the which files read:\n             - if an integer, will read only the first `limit` mesh files\n               (useful to get a sample from a large library of meshes)\n             - if a string, will interpret it as filename (regex) pattern\n               and only read files that match the pattern; e.g. `limit='.*_R.*'`\n               will only read files that contain `_R` in their filename\n             - if a slice (e.g. `slice(10, 20)`) will read only the files in\n               that range\n             - a list is expected to be a list of filenames to read from\n               the folder/archive\n</code></pre> <p> TYPE: <code>            int | str | slice | list</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <pre><code>            Keyword arguments passed to [`navis.MeshNeuron`][]\n            or [`navis.Volume`][]. You can use this to e.g.\n            set the units on the neurons.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>MeshNeuron</code> <p>If <code>output=\"neuron\"</code> (default).</p> <code>Volume</code> <p>If <code>output=\"volume\"</code>.</p> <code>Trimesh</code> <p>If <code>output=\"trimesh\"</code>.</p> <code>NeuronList</code> <p>If <code>output=\"neuron\"</code> and import has multiple meshes will return NeuronList of MeshNeurons.</p> <code>list</code> <p>If <code>output!=\"neuron\"</code> and import has multiple meshes will return list of Volumes or Trimesh.</p> See Also <p><code>navis.read_precomputed</code>                     Read meshes and skeletons from Neuroglancer's precomputed format.</p> <p>Examples:</p> <p>Read a single file into <code>navis.MeshNeuron</code>:</p> <pre><code>&gt;&gt;&gt; m = navis.read_mesh('mesh.obj')\n</code></pre> <p>Read all e.g. .obj files in a directory:</p> <pre><code>&gt;&gt;&gt; nl = navis.read_mesh('/some/directory/*.obj')\n</code></pre> <p>Sample first 50 files in folder:</p> <pre><code>&gt;&gt;&gt; nl = navis.read_mesh('/some/directory/*.obj', limit=50)\n</code></pre> <p>Read single file into <code>navis.Volume</code>:</p> <pre><code>&gt;&gt;&gt; nl = navis.read_mesh('mesh.obj', output='volume')\n</code></pre> Source code in <code>navis/io/mesh_io.py</code> <pre><code>def read_mesh(\n    f: Union[str, Iterable],\n    include_subdirs: bool = False,\n    parallel: Union[bool, int] = \"auto\",\n    output: Union[Literal[\"neuron\"], Literal[\"volume\"], Literal[\"trimesh\"]] = \"neuron\",\n    errors: Literal[\"raise\", \"log\", \"ignore\"] = \"raise\",\n    limit: Optional[int] = None,\n    fmt: str = \"{name}.\",\n    **kwargs,\n) -&gt; \"core.NeuronObject\":\n    \"\"\"Load mesh file into Neuron/List.\n\n    This is a thin wrapper around `trimesh.load_mesh` which supports most\n    commonly used formats (obj, ply, stl, etc.).\n\n    Parameters\n    ----------\n    f :                 str | iterable\n                        Filename(s) or folder. If folder should include file\n                        extension (e.g. `my/dir/*.ply`) otherwise all\n                        mesh files in the folder will be read.\n    include_subdirs :   bool, optional\n                        If True and `f` is a folder, will also search\n                        subdirectories for meshes.\n    parallel :          \"auto\" | bool | int,\n                        Defaults to `auto` which means only use parallel\n                        processing if more than 100 mesh files are imported.\n                        Spawning and joining processes causes overhead and is\n                        considerably slower for imports of small numbers of\n                        neurons. Integer will be interpreted as the number of\n                        cores (otherwise defaults to `os.cpu_count() - 2`).\n    output :            \"neuron\" | \"volume\" | \"trimesh\"\n                        Determines function's output - see `Returns`.\n    errors :            \"raise\" | \"log\" | \"ignore\"\n                        If \"log\" or \"ignore\", errors will not be raised and the\n                        mesh will be skipped. Can result in empty output.\n    limit :             int | str | slice | list, optional\n                        When reading from a folder or archive you can use this parameter to\n                        restrict the which files read:\n                         - if an integer, will read only the first `limit` mesh files\n                           (useful to get a sample from a large library of meshes)\n                         - if a string, will interpret it as filename (regex) pattern\n                           and only read files that match the pattern; e.g. `limit='.*_R.*'`\n                           will only read files that contain `_R` in their filename\n                         - if a slice (e.g. `slice(10, 20)`) will read only the files in\n                           that range\n                         - a list is expected to be a list of filenames to read from\n                           the folder/archive\n    **kwargs\n                        Keyword arguments passed to [`navis.MeshNeuron`][]\n                        or [`navis.Volume`][]. You can use this to e.g.\n                        set the units on the neurons.\n\n    Returns\n    -------\n    MeshNeuron\n                        If `output=\"neuron\"` (default).\n    Volume\n                        If `output=\"volume\"`.\n    Trimesh\n                        If `output=\"trimesh\"`.\n    NeuronList\n                        If `output=\"neuron\"` and import has multiple meshes\n                        will return NeuronList of MeshNeurons.\n    list\n                        If `output!=\"neuron\"` and import has multiple meshes\n                        will return list of Volumes or Trimesh.\n\n    See Also\n    --------\n    [`navis.read_precomputed`][]\n                        Read meshes and skeletons from Neuroglancer's precomputed format.\n\n    Examples\n    --------\n\n    Read a single file into [`navis.MeshNeuron`][]:\n\n    &gt;&gt;&gt; m = navis.read_mesh('mesh.obj')                         # doctest: +SKIP\n\n    Read all e.g. .obj files in a directory:\n\n    &gt;&gt;&gt; nl = navis.read_mesh('/some/directory/*.obj')           # doctest: +SKIP\n\n    Sample first 50 files in folder:\n\n    &gt;&gt;&gt; nl = navis.read_mesh('/some/directory/*.obj', limit=50) # doctest: +SKIP\n\n    Read single file into [`navis.Volume`][]:\n\n    &gt;&gt;&gt; nl = navis.read_mesh('mesh.obj', output='volume')       # doctest: +SKIP\n\n    \"\"\"\n    utils.eval_param(\n        output, name=\"output\", allowed_values=(\"neuron\", \"volume\", \"trimesh\")\n    )\n\n    reader = MeshReader(fmt=fmt, output=output, errors=errors, attrs=kwargs)\n    return reader.read_any(f, include_subdirs, parallel, limit=limit)\n</code></pre>"},{"location":"reference/navis/#navis.read_nml","title":"<code>navis.read_nml</code>","text":"<p>Read xml-based NML files into Neuron/Lists.</p> PARAMETER DESCRIPTION <code>f</code> <pre><code>            Filename or folder. If folder, will import all `.nml`\n            files.\n</code></pre> <p> TYPE: <code>                str</code> </p> <code>include_subdirs</code> <pre><code>            If True and `f` is a folder, will also search\n            subdirectories for `.nml` files.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> <code>parallel</code> <pre><code>            Defaults to `auto` which means only use parallel\n            processing if more than 200 files are imported. Spawning\n            and joining processes causes overhead and is\n            considerably slower for imports of small numbers of\n            neurons. Integer will be interpreted as the\n            number of cores (otherwise defaults to\n            `os.cpu_count() // 2`).\n</code></pre> <p> TYPE: <code>         \"auto\" | bool | int</code> DEFAULT: <code>'auto'</code> </p> <code>precision</code> <pre><code>            Precision for data. Defaults to 32 bit integers/floats.\n            If `None` will let pandas infer data types - this\n            typically leads to higher than necessary precision.\n</code></pre> <p> TYPE: <code>        int [8, 16, 32, 64] | None</code> DEFAULT: <code>32</code> </p> <code>limit</code> <pre><code>            When reading from a folder or archive you can use this\n            parameter to restrict the which files read:\n             - if an integer, will read only the first `limit` NML files\n               (useful to get a sample from a large library of skeletons)\n             - if a string, will interpret it as filename (regex) pattern\n               and only read files that match the pattern; e.g. `limit='.*_R.*'`\n               will only read files that contain `_R` in their filename\n             - if a slice (e.g. `slice(10, 20)`) will read only the files in\n               that range\n             - a list is expected to be a list of filenames to read from\n               the folder/archive\n</code></pre> <p> TYPE: <code>            int | str | slice | list</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <pre><code>            Keyword arguments passed to the construction of\n            `navis.TreeNeuron`. You can use this to e.g. set\n            meta data.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>navis.NeuronList</code> See Also <p><code>navis.read_nmx</code>                     Read NMX files (collections of NML files).</p> Source code in <code>navis/io/nmx_io.py</code> <pre><code>def read_nml(\n    f: Union[str, pd.DataFrame, Iterable],\n    include_subdirs: bool = False,\n    parallel: Union[bool, int] = \"auto\",\n    precision: int = 32,\n    limit: Optional[int] = None,\n    **kwargs,\n) -&gt; \"core.NeuronObject\":\n    \"\"\"Read xml-based NML files into Neuron/Lists.\n\n    Parameters\n    ----------\n    f :                 str\n                        Filename or folder. If folder, will import all `.nml`\n                        files.\n    include_subdirs :   bool, optional\n                        If True and `f` is a folder, will also search\n                        subdirectories for `.nml` files.\n    parallel :          \"auto\" | bool | int\n                        Defaults to `auto` which means only use parallel\n                        processing if more than 200 files are imported. Spawning\n                        and joining processes causes overhead and is\n                        considerably slower for imports of small numbers of\n                        neurons. Integer will be interpreted as the\n                        number of cores (otherwise defaults to\n                        `os.cpu_count() // 2`).\n    precision :         int [8, 16, 32, 64] | None\n                        Precision for data. Defaults to 32 bit integers/floats.\n                        If `None` will let pandas infer data types - this\n                        typically leads to higher than necessary precision.\n    limit :             int | str | slice | list, optional\n                        When reading from a folder or archive you can use this\n                        parameter to restrict the which files read:\n                         - if an integer, will read only the first `limit` NML files\n                           (useful to get a sample from a large library of skeletons)\n                         - if a string, will interpret it as filename (regex) pattern\n                           and only read files that match the pattern; e.g. `limit='.*_R.*'`\n                           will only read files that contain `_R` in their filename\n                         - if a slice (e.g. `slice(10, 20)`) will read only the files in\n                           that range\n                         - a list is expected to be a list of filenames to read from\n                           the folder/archive\n    **kwargs\n                        Keyword arguments passed to the construction of\n                        `navis.TreeNeuron`. You can use this to e.g. set\n                        meta data.\n\n    Returns\n    -------\n    navis.NeuronList\n\n    See Also\n    --------\n    [`navis.read_nmx`][]\n                        Read NMX files (collections of NML files).\n\n    \"\"\"\n    reader = NMLReader(precision=precision, attrs=kwargs)\n    # Read neurons\n    neurons = reader.read_any(\n        f, parallel=parallel, limit=limit, include_subdirs=include_subdirs\n    )\n\n    return neurons\n</code></pre>"},{"location":"reference/navis/#navis.read_nmx","title":"<code>navis.read_nmx</code>","text":"<p>Read NMX files into Neuron/Lists.</p> <p>NMX is an xml-based format used by pyKNOSSOS. See e.g. here for a data dump of neurons from Wanner et al. (2016).</p> PARAMETER DESCRIPTION <code>f</code> <pre><code>            Filename or folder. If folder, will import all `.nmx`\n            files.\n</code></pre> <p> TYPE: <code>                str</code> </p> <code>include_subdirs</code> <pre><code>            If True and `f` is a folder, will also search\n            subdirectories for `.nmx` files.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> <code>parallel</code> <pre><code>            Defaults to `auto` which means only use parallel\n            processing if more than 200 files are imported. Spawning\n            and joining processes causes overhead and is\n            considerably slower for imports of small numbers of\n            neurons. Integer will be interpreted as the\n            number of cores (otherwise defaults to\n            `os.cpu_count() // 2`).\n</code></pre> <p> TYPE: <code>         \"auto\" | bool | int</code> DEFAULT: <code>'auto'</code> </p> <code>precision</code> <pre><code>            Precision for data. Defaults to 32 bit integers/floats.\n            If `None` will let pandas infer data types - this\n            typically leads to higher than necessary precision.\n</code></pre> <p> TYPE: <code>        int [8, 16, 32, 64] | None</code> DEFAULT: <code>32</code> </p> <code>limit</code> <pre><code>            When reading from a folder or archive you can use this parameter to\n            restrict the which files read:\n             - if an integer, will read only the first `limit` NMX files\n               (useful to get a sample from a large library of skeletons)\n             - if a string, will interpret it as filename (regex) pattern\n               and only read files that match the pattern; e.g. `limit='.*_R.*'`\n               will only read files that contain `_R` in their filename\n             - if a slice (e.g. `slice(10, 20)`) will read only the files in\n               that range\n             - a list is expected to be a list of filenames to read from\n               the folder/archive\n</code></pre> <p> TYPE: <code>            int | str | slice | list</code> DEFAULT: <code>None</code> </p> <code>errors</code> <pre><code>            If \"log\" or \"ignore\", errors will not be raised and the\n            mesh will be skipped. Can result in empty output.\n</code></pre> <p> TYPE: <code>           \"raise\" | \"log\" | \"ignore\"</code> DEFAULT: <code>'raise'</code> </p> <code>**kwargs</code> <pre><code>            Keyword arguments passed to the construction of\n            `navis.TreeNeuron`. You can use this to e.g. set\n            meta data.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>navis.NeuronList</code> See Also <p><code>navis.read_nml</code>                     Read NML file(s).</p> Source code in <code>navis/io/nmx_io.py</code> <pre><code>def read_nmx(\n    f: Union[str, pd.DataFrame, Iterable],\n    include_subdirs: bool = False,\n    parallel: Union[bool, int] = \"auto\",\n    precision: int = 32,\n    limit: Optional[int] = None,\n    errors: str = \"raise\",\n    **kwargs,\n) -&gt; \"core.NeuronObject\":\n    \"\"\"Read NMX files into Neuron/Lists.\n\n    NMX is an xml-based format used by pyKNOSSOS.\n    See e.g. [here](https://doi.org/10.5281/zenodo.58985) for a data dump\n    of neurons from Wanner et al. (2016).\n\n    Parameters\n    ----------\n    f :                 str\n                        Filename or folder. If folder, will import all `.nmx`\n                        files.\n    include_subdirs :   bool, optional\n                        If True and `f` is a folder, will also search\n                        subdirectories for `.nmx` files.\n    parallel :          \"auto\" | bool | int\n                        Defaults to `auto` which means only use parallel\n                        processing if more than 200 files are imported. Spawning\n                        and joining processes causes overhead and is\n                        considerably slower for imports of small numbers of\n                        neurons. Integer will be interpreted as the\n                        number of cores (otherwise defaults to\n                        `os.cpu_count() // 2`).\n    precision :         int [8, 16, 32, 64] | None\n                        Precision for data. Defaults to 32 bit integers/floats.\n                        If `None` will let pandas infer data types - this\n                        typically leads to higher than necessary precision.\n    limit :             int | str | slice | list, optional\n                        When reading from a folder or archive you can use this parameter to\n                        restrict the which files read:\n                         - if an integer, will read only the first `limit` NMX files\n                           (useful to get a sample from a large library of skeletons)\n                         - if a string, will interpret it as filename (regex) pattern\n                           and only read files that match the pattern; e.g. `limit='.*_R.*'`\n                           will only read files that contain `_R` in their filename\n                         - if a slice (e.g. `slice(10, 20)`) will read only the files in\n                           that range\n                         - a list is expected to be a list of filenames to read from\n                           the folder/archive\n    errors :            \"raise\" | \"log\" | \"ignore\"\n                        If \"log\" or \"ignore\", errors will not be raised and the\n                        mesh will be skipped. Can result in empty output.\n    **kwargs\n                        Keyword arguments passed to the construction of\n                        `navis.TreeNeuron`. You can use this to e.g. set\n                        meta data.\n\n    Returns\n    -------\n    navis.NeuronList\n\n    See Also\n    --------\n    [`navis.read_nml`][]\n                        Read NML file(s).\n\n    \"\"\"\n    reader = NMXReader(precision=precision, errors=errors, attrs=kwargs)\n    # Read neurons\n    neurons = reader.read_any(\n        f, parallel=parallel, limit=limit, include_subdirs=include_subdirs\n    )\n\n    # Failed reads will produce empty neurons which we need to remove\n    if isinstance(neurons, core.NeuronList):\n        neurons = neurons[neurons.has_nodes]\n\n    return neurons\n</code></pre>"},{"location":"reference/navis/#navis.read_nrrd","title":"<code>navis.read_nrrd</code>","text":"<p>Create Neuron/List from NRRD file.</p> <p>See here for specs of NRRD file format including description of the headers.</p> PARAMETER DESCRIPTION <code>f</code> <pre><code>            Filename, folder or URL:\n             - if folder, will import all `.nrrd` files\n             - if a `.zip`, `.tar` or `.tar.gz` archive will read all\n               NRRD files from the file\n             - if a URL (http:// or https://), will download the\n               file and import it\n             - FTP address (ftp://) can point to a folder or a single\n               file\n            See also `limit` parameter to read only a subset of files.\n</code></pre> <p> TYPE: <code>                str | list thereof</code> </p> <code>output</code> <pre><code>            Determines function's output. See Returns for details.\n</code></pre> <p> TYPE: <code>           \"voxels\" | \"dotprops\" | \"raw\"</code> DEFAULT: <code>'voxels'</code> </p> <code>threshold</code> <pre><code>            For `output='dotprops'` only: a threshold to filter\n            low intensity voxels.\n              - if `None`, all values &gt; 0 are converted to points\n              - if &gt;=1, all values &gt;= threshold are converted to points\n              - if &lt;1, all values &gt;= threshold * max(data) are converted\n</code></pre> <p> TYPE: <code>        int | float | None</code> DEFAULT: <code>None</code> </p> <code>thin</code> <pre><code>            For `output='dotprops'` only: if True, will thin the\n            point cloud using `skimage.morphology.skeletonize`\n            after thresholding. Requires `scikit-image`.\n</code></pre> <p> TYPE: <code>             bool</code> DEFAULT: <code>False</code> </p> <code>include_subdirs</code> <pre><code>            If True and `f` is a folder, will also search\n            subdirectories for `.nrrd` files.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> <code>parallel</code> <pre><code>            Defaults to `auto` which means only use parallel\n            processing if more than 10 NRRD files are imported.\n            Spawning and joining processes causes overhead and is\n            considerably slower for imports of small numbers of\n            neurons. Integer will be interpreted as the number of\n            cores (otherwise defaults to `os.cpu_count() - 2`).\n</code></pre> <p> TYPE: <code>         \"auto\" | bool | int,</code> DEFAULT: <code>'auto'</code> </p> <code>fmt</code> <pre><code>            Formatter to specify how filenames are parsed into neuron\n            attributes. Some illustrative examples:\n              - `{name}` (default) uses the filename\n                (minus the suffix) as the neuron's name property\n              - `{id}` (default) uses the filename as the neuron's ID\n                property\n              - `{name,id}` uses the filename as the neuron's\n                name and ID properties\n              - `{name}.{id}` splits the filename at a \".\"\n                and uses the first part as name and the second as ID\n              - `{name,id:int}` same as above but converts\n                into integer for the ID\n              - `{name}_{myproperty}` splits the filename at\n                \"_\" and uses the first part as name and as a\n                generic \"myproperty\" property\n              - `{name}_{}_{id}` splits the filename at\n                \"_\" and uses the first part as name and the last as\n                ID. The middle part is ignored.\n\n            Throws a ValueError if pattern can't be found in\n            filename.\n</code></pre> <p> TYPE: <code>              str</code> DEFAULT: <code>'{name}.nrrd'</code> </p> <code>limit</code> <pre><code>            When reading from a folder or archive you can use this parameter to\n            restrict the which files read:\n             - if an integer, will read only the first `limit` NMX files\n               (useful to get a sample from a large library of skeletons)\n             - if a string, will interpret it as filename (regex) pattern\n               and only read files that match the pattern; e.g. `limit='.*_R.*'`\n               will only read files that contain `_R` in their filename\n             - if a slice (e.g. `slice(10, 20)`) will read only the files in\n               that range\n             - a list is expected to be a list of filenames to read from\n               the folder/archive\n</code></pre> <p> TYPE: <code>            int | str | slice | list</code> DEFAULT: <code>None</code> </p> <code>errors</code> <pre><code>            If \"log\" or \"ignore\", errors will not be raised and the\n            mesh will be skipped. Can result in empty output.\n</code></pre> <p> TYPE: <code>           \"raise\" | \"log\" | \"ignore\"</code> DEFAULT: <code>'raise'</code> </p> <code>**dotprops_kwargs</code> <pre><code>            Keyword arguments passed to [`navis.make_dotprops`][]\n            if `output='dotprops'`. Use this to adjust e.g. the\n            number of nearest neighbors used for calculating the\n            tangent vector by passing e.g. `k=5`.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>navis.VoxelNeuron</code> <p>If <code>output=\"voxels\"</code> (default): requires NRRD data to be 3-dimensional voxels. VoxelNeuron will have NRRD file header as <code>.nrrd_header</code> attribute.</p> <code>navis.Dotprops</code> <p>If <code>output=\"dotprops\"</code>: requires NRRD data to be either:   - <code>(N, M, K)</code> (i.e. 3D) in which case we will turn     voxels into a point cloud (see also <code>threshold</code>     parameter)   - <code>(N, 3)</code> = x/y/z points   - <code>(N, 6)</code> = x/y/z points + x/y/z vectors   - <code>(N, 7)</code> = x/y/z points + x/y/z vectors + alpha</p> <p>Dotprops will contain NRRD header as <code>.nrrd_header</code> attribute.</p> <code>navis.NeuronList</code> <p>If import of multiple NRRD will return NeuronList of Dotprops/VoxelNeurons.</p> <code>(image, header)(np.ndarray, OrderedDict)</code> <p>If <code>output='raw'</code> return raw data contained in NRRD file.</p> Source code in <code>navis/io/nrrd_io.py</code> <pre><code>def read_nrrd(\n    f: Union[str, Iterable],\n    output: Union[Literal[\"voxels\"], Literal[\"dotprops\"], Literal[\"raw\"]] = \"voxels\",\n    threshold: Optional[Union[int, float]] = None,\n    thin: bool = False,\n    include_subdirs: bool = False,\n    parallel: Union[bool, int] = \"auto\",\n    fmt: str = \"{name}.nrrd\",\n    limit: Optional[int] = None,\n    errors: str = \"raise\",\n    **dotprops_kwargs,\n) -&gt; \"core.NeuronObject\":\n    \"\"\"Create Neuron/List from NRRD file.\n\n    See [here](http://teem.sourceforge.net/nrrd/format.html) for specs of\n    NRRD file format including description of the headers.\n\n    Parameters\n    ----------\n    f :                 str | list thereof\n                        Filename, folder or URL:\n                         - if folder, will import all `.nrrd` files\n                         - if a `.zip`, `.tar` or `.tar.gz` archive will read all\n                           NRRD files from the file\n                         - if a URL (http:// or https://), will download the\n                           file and import it\n                         - FTP address (ftp://) can point to a folder or a single\n                           file\n                        See also `limit` parameter to read only a subset of files.\n    output :            \"voxels\" | \"dotprops\" | \"raw\"\n                        Determines function's output. See Returns for details.\n    threshold :         int | float | None\n                        For `output='dotprops'` only: a threshold to filter\n                        low intensity voxels.\n                          - if `None`, all values &gt; 0 are converted to points\n                          - if &gt;=1, all values &gt;= threshold are converted to points\n                          - if &lt;1, all values &gt;= threshold * max(data) are converted\n    thin :              bool\n                        For `output='dotprops'` only: if True, will thin the\n                        point cloud using `skimage.morphology.skeletonize`\n                        after thresholding. Requires `scikit-image`.\n    include_subdirs :   bool, optional\n                        If True and `f` is a folder, will also search\n                        subdirectories for `.nrrd` files.\n    parallel :          \"auto\" | bool | int,\n                        Defaults to `auto` which means only use parallel\n                        processing if more than 10 NRRD files are imported.\n                        Spawning and joining processes causes overhead and is\n                        considerably slower for imports of small numbers of\n                        neurons. Integer will be interpreted as the number of\n                        cores (otherwise defaults to `os.cpu_count() - 2`).\n    fmt :               str\n                        Formatter to specify how filenames are parsed into neuron\n                        attributes. Some illustrative examples:\n                          - `{name}` (default) uses the filename\n                            (minus the suffix) as the neuron's name property\n                          - `{id}` (default) uses the filename as the neuron's ID\n                            property\n                          - `{name,id}` uses the filename as the neuron's\n                            name and ID properties\n                          - `{name}.{id}` splits the filename at a \".\"\n                            and uses the first part as name and the second as ID\n                          - `{name,id:int}` same as above but converts\n                            into integer for the ID\n                          - `{name}_{myproperty}` splits the filename at\n                            \"_\" and uses the first part as name and as a\n                            generic \"myproperty\" property\n                          - `{name}_{}_{id}` splits the filename at\n                            \"_\" and uses the first part as name and the last as\n                            ID. The middle part is ignored.\n\n                        Throws a ValueError if pattern can't be found in\n                        filename.\n    limit :             int | str | slice | list, optional\n                        When reading from a folder or archive you can use this parameter to\n                        restrict the which files read:\n                         - if an integer, will read only the first `limit` NMX files\n                           (useful to get a sample from a large library of skeletons)\n                         - if a string, will interpret it as filename (regex) pattern\n                           and only read files that match the pattern; e.g. `limit='.*_R.*'`\n                           will only read files that contain `_R` in their filename\n                         - if a slice (e.g. `slice(10, 20)`) will read only the files in\n                           that range\n                         - a list is expected to be a list of filenames to read from\n                           the folder/archive\n    errors :            \"raise\" | \"log\" | \"ignore\"\n                        If \"log\" or \"ignore\", errors will not be raised and the\n                        mesh will be skipped. Can result in empty output.\n    **dotprops_kwargs\n                        Keyword arguments passed to [`navis.make_dotprops`][]\n                        if `output='dotprops'`. Use this to adjust e.g. the\n                        number of nearest neighbors used for calculating the\n                        tangent vector by passing e.g. `k=5`.\n\n    Returns\n    -------\n    navis.VoxelNeuron\n                        If `output=\"voxels\"` (default): requires NRRD data to\n                        be 3-dimensional voxels. VoxelNeuron will have NRRD file\n                        header as `.nrrd_header` attribute.\n    navis.Dotprops\n                        If `output=\"dotprops\"`: requires NRRD data to be\n                        either:\n                          - `(N, M, K)` (i.e. 3D) in which case we will turn\n                            voxels into a point cloud (see also `threshold`\n                            parameter)\n                          - `(N, 3)` = x/y/z points\n                          - `(N, 6)` = x/y/z points + x/y/z vectors\n                          - `(N, 7)` = x/y/z points + x/y/z vectors + alpha\n\n                        Dotprops will contain NRRD header as `.nrrd_header`\n                        attribute.\n    navis.NeuronList\n                        If import of multiple NRRD will return NeuronList of\n                        Dotprops/VoxelNeurons.\n    (image, header)     (np.ndarray, OrderedDict)\n                        If `output='raw'` return raw data contained in NRRD\n                        file.\n\n    \"\"\"\n    if thin:\n        try:\n            from skimage.morphology import skeletonize\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                \"The 'thin' option requires 'scikit-image' to be installed:\\n\"\n                \"    pip install scikit-image -U\"\n            )\n\n    utils.eval_param(\n        output, name=\"output\", allowed_values=(\"raw\", \"dotprops\", \"voxels\")\n    )\n\n    if parallel == \"auto\":\n        # Set a lower threshold of 10 on parallel processing for NRRDs (default is 200)\n        parallel = (\"auto\", 10)\n\n    reader = NrrdReader(\n        output=output, threshold=threshold, thin=thin, fmt=fmt, errors=errors, dotprop_kwargs=dotprops_kwargs\n    )\n    return reader.read_any(f, include_subdirs, parallel, limit=limit)\n</code></pre>"},{"location":"reference/navis/#navis.read_parquet","title":"<code>navis.read_parquet</code>","text":"<p>Read parquet file into Neuron/List.</p> <p>See here for format specifications.</p> PARAMETER DESCRIPTION <code>f</code> <pre><code>            File to be read.\n</code></pre> <p> TYPE: <code>                str</code> </p> <code>read_meta</code> <pre><code>            Whether to read neuron meta data stored in the parquet\n            file (e.g. name or units). Defaults to True but can be\n            switched off in case there are any issues.\n</code></pre> <p> TYPE: <code>        bool</code> DEFAULT: <code>True</code> </p> <code>limit</code> <pre><code>            If reading from a file containing multiple neurons you\n            can use this parameter to read only the first `limit`\n            neurons. Useful if wanting to get a sample from a large\n            library of neurons.\n</code></pre> <p> TYPE: <code>            int</code> DEFAULT: <code>None</code> </p> <code>subset</code> <pre><code>            If the parquet file contains multiple neurons you can\n            use this to select the IDs of the neurons to load. Only\n            works if the parquet file actually contains multiple\n            neurons.\n</code></pre> <p> TYPE: <code>           str | int | list thereof</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>navis.TreeNeuron / Dotprops</code> <p>If parquet file contains a single neuron.</p> <code>navis.NeuronList</code> <p>If parquet file contains multiple neurons.</p> See Also <p><code>navis.write_parquet</code>                     Export neurons as parquet files. <code>navis.scan_parquet</code>                     Scan parquet file for its contents.</p> <p>Examples:</p> <p>See <code>navis.write_parquet</code> for examples.</p> Source code in <code>navis/io/pq_io.py</code> <pre><code>def read_parquet(\n    f: Union[str, Path],\n    read_meta: bool = True,\n    limit: Optional[int] = None,\n    subset: Optional[List[Union[str, int]]] = None,\n    progress=True,\n) -&gt; \"core.NeuronObject\":\n    \"\"\"Read parquet file into Neuron/List.\n\n    See [here](https://github.com/navis-org/navis/blob/master/navis/io/pq_io.md)\n    for format specifications.\n\n    Parameters\n    ----------\n    f :                 str\n                        File to be read.\n    read_meta :         bool\n                        Whether to read neuron meta data stored in the parquet\n                        file (e.g. name or units). Defaults to True but can be\n                        switched off in case there are any issues.\n    limit :             int, optional\n                        If reading from a file containing multiple neurons you\n                        can use this parameter to read only the first `limit`\n                        neurons. Useful if wanting to get a sample from a large\n                        library of neurons.\n    subset :            str | int | list thereof\n                        If the parquet file contains multiple neurons you can\n                        use this to select the IDs of the neurons to load. Only\n                        works if the parquet file actually contains multiple\n                        neurons.\n\n    Returns\n    -------\n    navis.TreeNeuron/Dotprops\n                        If parquet file contains a single neuron.\n    navis.NeuronList\n                        If parquet file contains multiple neurons.\n\n    See Also\n    --------\n    [`navis.write_parquet`][]\n                        Export neurons as parquet files.\n    [`navis.scan_parquet`][]\n                        Scan parquet file for its contents.\n\n    Examples\n    --------\n    See [`navis.write_parquet`][] for examples.\n\n    \"\"\"\n    f = Path(f).expanduser()\n    if not f.is_file():\n        raise FileNotFoundError(f'File \"{f}\" does not exist.')\n\n    try:\n        import pyarrow.parquet as pq\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"Reading parquet files requires the pyarrow library:\\n pip3 install pyarrow\"\n        )\n\n    if limit is not None:\n        if subset not in (None, False):\n            raise ValueError(\n                \"You can provide either a `subset` or a `limit` but not both.\"\n            )\n        scan = scan_parquet(f)\n        if scan.empty:\n            raise ValueError(\n                f\"Parquet file {f} either does not contain any neurons or meta data could not be read.\"\n            )\n        subset = scan.id.values[:limit]\n\n    if isinstance(subset, (pd.Series)):\n        subset = subset.values\n\n    # Read the table\n    if subset is None or subset is False:\n        table = pq.read_table(f)\n    elif isinstance(subset, (str, int)):\n        table = pq.read_table(f, filters=[(\"neuron\", \"=\", subset)])\n    elif isinstance(subset, (list, np.ndarray)):\n        table = pq.read_table(f, filters=[(\"neuron\", \"in\", subset)])\n    else:\n        raise TypeError(f'`subset` must be int, str or iterable, got \"{type(subset)}')\n\n    # Extract meta data (will be byte encoded)\n    if read_meta:\n        metadata = {k.decode(): v.decode() for k, v in table.schema.metadata.items()}\n    else:\n        metadata = {}\n\n    # Extract neuron meta data once here instead of for every neuron individually\n    # Meta data is encoded as {\"{ID}_{PROPERTY}\": VALUE}\n    # Here we pre-emptively turn this into {(ID, PROPERTY): VALUE}\n    # Note that we're dropping \"private\" properties where the key starts with \"_\"\n    neuron_meta = {tuple(k.split(':')): v for k, v in metadata.items() if not k.startswith('_')}\n\n    # Convert to pandas\n    table = table.to_pandas()\n\n    # Check if we're doing skeletons or dotprops\n    if 'node_id' in table.columns:\n        _extract_neuron = _extract_skeleton\n    elif 'vect_x' in table.columns:\n        _extract_neuron = _extract_dotprops\n    else:\n        raise TypeError('Unable to extract neuron from parquet file with '\n                        f'columns {table.columns}')\n\n    # If this is a single neuron\n    if 'neuron' not in table.columns:\n        if metadata:\n            id = [v for k, v in metadata.items() if k[1] == 'id'][0]\n        else:\n            id = '0'  # &lt;-- generic ID as fallback if we don't have metadata\n        return _extract_neuron(table, id, neuron_meta)\n    else:\n        neurons = []\n        # Note: this could be done in threads\n        for i, (id, this_table) in enumerate(config.tqdm(table.groupby('neuron'),\n                                             disable=not progress,\n                                             leave=False,\n                                             desc='Making nrn')):\n            this_table = this_table.drop(\"neuron\", axis=1)\n            neurons.append(_extract_neuron(this_table, id, neuron_meta))\n        return core.NeuronList(neurons)\n</code></pre>"},{"location":"reference/navis/#navis.read_precomputed","title":"<code>navis.read_precomputed</code>","text":"<p>Read skeletons and meshes from neuroglancer's precomputed format.</p> <p>Follows the formats specified here.</p> PARAMETER DESCRIPTION <code>f</code> <pre><code>            Filename, folder or bytes. If folder, will import all\n            files. If a `.zip`, `.tar` or `.tar.gz` file will\n            read all files in the archive. See also `limit` parameter.\n</code></pre> <p> TYPE: <code>                filepath | folder | zip file | bytes</code> </p> <code>datatype</code> <pre><code>            Which data type we expect to read from the files. If\n            \"auto\", we require a \"info\" file in the same directory\n            as `f`.\n</code></pre> <p> TYPE: <code>         \"auto\" | \"skeleton\" | \"mesh\"</code> DEFAULT: <code>'auto'</code> </p> <code>include_subdirs</code> <pre><code>            If True and `f` is a folder, will also search\n            subdirectories for binary files.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> <code>fmt</code> <pre><code>            Formatter to specify how filenames are parsed into neuron\n            attributes. Some illustrative examples:\n              - `{name}` (default) uses the filename\n                (minus the suffix) as the neuron's name property\n              - `{id}` (default) uses the filename as the neuron's ID\n                property\n              - `{name,id}` uses the filename as the neuron's\n                name and ID properties\n              - `{name}.{id}` splits the filename at a \".\"\n                and uses the first part as name and the second as ID\n              - `{name,id:int}` same as above but converts\n                into integer for the ID\n              - `{name}_{myproperty}` splits the filename at\n                \"_\" and uses the first part as name and as a\n                generic \"myproperty\" property\n              - `{name}_{}_{id}` splits the filename at\n                \"_\" and uses the first part as name and the last as\n                ID. The middle part is ignored.\n\n            Throws a ValueError if pattern can't be found in\n            filename.\n</code></pre> <p> TYPE: <code>              str</code> DEFAULT: <code>'{id}'</code> </p> <code>info</code> <pre><code>            An info file describing the data:\n              - `True` = will look for `info` file in base folder\n              - `False` = do not use/look for `info` file\n              - `str` = filepath to `info` file\n              - `dict` = already parsed info file\n</code></pre> <p> TYPE: <code>             bool | str | dict</code> DEFAULT: <code>True</code> </p> <code>limit</code> <pre><code>            When reading from a folder or archive you can use this parameter to\n            restrict the which files read:\n             - if an integer, will read only the first `limit` files\n               (useful to get a sample from a large library of neurons)\n             - if a string, will interpret it as filename (regex) pattern\n               and only read files that match the pattern; e.g. `limit='.*_R.*'`\n               will only read files that contain `_R` in their filename\n             - if a slice (e.g. `slice(10, 20)`) will read only the files in\n               that range\n             - a list is expected to be a list of filenames to read from\n               the folder/archive\n</code></pre> <p> TYPE: <code>            int | str | slice | list</code> DEFAULT: <code>None</code> </p> <code>parallel</code> <pre><code>            Defaults to `auto` which means only use parallel\n            processing if more than 200 files are imported. Spawning\n            and joining processes causes overhead and is\n            considerably slower for imports of small numbers of\n            neurons. Integer will be interpreted as the\n            number of cores (otherwise defaults to\n            `os.cpu_count() // 2`).\n</code></pre> <p> TYPE: <code>         \"auto\" | bool | int</code> DEFAULT: <code>'auto'</code> </p> <code>errors</code> <pre><code>            If \"log\" or \"ignore\", errors will not be raised and the\n            mesh will be skipped. Can result in empty output.\n</code></pre> <p> TYPE: <code>           \"raise\" | \"log\" | \"ignore\"</code> DEFAULT: <code>'raise'</code> </p> <code>**kwargs</code> <pre><code>            Keyword arguments passed to the construction of the\n            neurons. You can use this to e.g. set meta data such\n            as `units`.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>navis.MeshNeuron</code> <code>navis.NeuronList</code> See Also <p><code>navis.write_precomputed</code>                     Export neurons/volumes to precomputed format. <code>navis.read_mesh</code>                     Read common mesh formats (obj, stl, etc).</p> Source code in <code>navis/io/precomputed_io.py</code> <pre><code>def read_precomputed(\n    f: Union[str, io.BytesIO],\n    datatype: Union[Literal[\"auto\"], Literal[\"mesh\"], Literal[\"skeleton\"]] = \"auto\",\n    include_subdirs: bool = False,\n    fmt: str = \"{id}\",\n    info: Union[bool, str, dict] = True,\n    limit: Optional[int] = None,\n    parallel: Union[bool, int] = \"auto\",\n    errors: Literal[\"raise\", \"log\", \"ignore\"] = \"raise\",\n    **kwargs,\n) -&gt; \"core.NeuronObject\":\n    \"\"\"Read skeletons and meshes from neuroglancer's precomputed format.\n\n    Follows the formats specified\n    [here](https://github.com/google/neuroglancer/tree/master/src/neuroglancer/datasource/precomputed).\n\n    Parameters\n    ----------\n    f :                 filepath | folder | zip file | bytes\n                        Filename, folder or bytes. If folder, will import all\n                        files. If a `.zip`, `.tar` or `.tar.gz` file will\n                        read all files in the archive. See also `limit` parameter.\n    datatype :          \"auto\" | \"skeleton\" | \"mesh\"\n                        Which data type we expect to read from the files. If\n                        \"auto\", we require a \"info\" file in the same directory\n                        as `f`.\n    include_subdirs :   bool, optional\n                        If True and `f` is a folder, will also search\n                        subdirectories for binary files.\n    fmt :               str\n                        Formatter to specify how filenames are parsed into neuron\n                        attributes. Some illustrative examples:\n                          - `{name}` (default) uses the filename\n                            (minus the suffix) as the neuron's name property\n                          - `{id}` (default) uses the filename as the neuron's ID\n                            property\n                          - `{name,id}` uses the filename as the neuron's\n                            name and ID properties\n                          - `{name}.{id}` splits the filename at a \".\"\n                            and uses the first part as name and the second as ID\n                          - `{name,id:int}` same as above but converts\n                            into integer for the ID\n                          - `{name}_{myproperty}` splits the filename at\n                            \"_\" and uses the first part as name and as a\n                            generic \"myproperty\" property\n                          - `{name}_{}_{id}` splits the filename at\n                            \"_\" and uses the first part as name and the last as\n                            ID. The middle part is ignored.\n\n                        Throws a ValueError if pattern can't be found in\n                        filename.\n    info :              bool | str | dict\n                        An info file describing the data:\n                          - `True` = will look for `info` file in base folder\n                          - `False` = do not use/look for `info` file\n                          - `str` = filepath to `info` file\n                          - `dict` = already parsed info file\n    limit :             int | str | slice | list, optional\n                        When reading from a folder or archive you can use this parameter to\n                        restrict the which files read:\n                         - if an integer, will read only the first `limit` files\n                           (useful to get a sample from a large library of neurons)\n                         - if a string, will interpret it as filename (regex) pattern\n                           and only read files that match the pattern; e.g. `limit='.*_R.*'`\n                           will only read files that contain `_R` in their filename\n                         - if a slice (e.g. `slice(10, 20)`) will read only the files in\n                           that range\n                         - a list is expected to be a list of filenames to read from\n                           the folder/archive\n    parallel :          \"auto\" | bool | int\n                        Defaults to `auto` which means only use parallel\n                        processing if more than 200 files are imported. Spawning\n                        and joining processes causes overhead and is\n                        considerably slower for imports of small numbers of\n                        neurons. Integer will be interpreted as the\n                        number of cores (otherwise defaults to\n                        `os.cpu_count() // 2`).\n    errors :            \"raise\" | \"log\" | \"ignore\"\n                        If \"log\" or \"ignore\", errors will not be raised and the\n                        mesh will be skipped. Can result in empty output.\n    **kwargs\n                        Keyword arguments passed to the construction of the\n                        neurons. You can use this to e.g. set meta data such\n                        as `units`.\n\n    Returns\n    -------\n    navis.MeshNeuron\n    navis.NeuronList\n\n    See Also\n    --------\n    [`navis.write_precomputed`][]\n                        Export neurons/volumes to precomputed format.\n    [`navis.read_mesh`][]\n                        Read common mesh formats (obj, stl, etc).\n\n    \"\"\"\n    utils.eval_param(\n        datatype, name=\"datatype\", allowed_values=(\"skeleton\", \"mesh\", \"auto\")\n    )\n\n    # See if we can get the info file from somewhere\n    if info is True and not isinstance(f, bytes):\n        f_ = f\n        # If iterable, assume list of files or URLs\n        if utils.is_iterable(f_):\n            f_ = f_[0]\n\n        # Find info in zip archive\n        if str(f_).endswith(\".zip\"):\n            with ZipFile(Path(f_).expanduser(), \"r\") as zip:\n                if \"info\" in [f_.filename for f_ in zip.filelist]:\n                    info = json.loads(zip.read(\"info\").decode())\n                elif datatype == \"auto\":\n                    raise ValueError(\n                        \"No `info` file found in zip file. Please \"\n                        \"specify data type using the `datatype` \"\n                        \"parameter.\"\n                    )\n        # Try loading info from URL\n        elif utils.is_url(str(f_)):\n            base_url = \"/\".join(str(f_).split(\"/\")[:-1])\n            info = _fetch_info_file(base_url, raise_missing=False)\n        # Try loading info from parent path\n        else:\n            fp = Path(str(f_))\n            # Find first existing root\n            while not fp.is_dir():\n                fp = fp.parent\n            fp = fp / \"info\"\n            if fp.is_file():\n                with open(fp, \"r\") as info_file:\n                    info = json.load(info_file)\n\n    # At this point we should have a dictionary - even if it's empty\n    if not isinstance(info, dict):\n        info = {}\n\n    # Parse data type from info file (if required)\n    if datatype == \"auto\":\n        if \"@type\" not in info:\n            raise ValueError(\n                \"Either no `info` file found or it does not specify \"\n                \"a data type. Please provide data type using the \"\n                \"`datatype` parameter.\"\n            )\n\n        if info.get(\"@type\", None) == \"neuroglancer_legacy_mesh\":\n            datatype = \"mesh\"\n        elif info.get(\"@type\", None) == \"neuroglancer_skeletons\":\n            datatype = \"skeleton\"\n        else:\n            raise ValueError(\n                'Data type specified in `info` file unknown: '\n                f'{info.get(\"@type\", None)}. Please provide data '\n                'type using the `datatype` parameter.'\n            )\n\n    if isinstance(f, bytes):\n        f = io.BytesIO(f)\n\n    if datatype == \"skeleton\":\n        if not isinstance(info, dict):\n            info = {}\n        reader = PrecomputedSkeletonReader(\n            fmt=fmt, errors=errors, attrs=kwargs, info=info\n        )\n    else:\n        reader = PrecomputedMeshReader(fmt=fmt, errors=errors, attrs=kwargs)\n\n    return reader.read_any(f, include_subdirs, parallel, limit=limit)\n</code></pre>"},{"location":"reference/navis/#navis.read_rda","title":"<code>navis.read_rda</code>","text":"<p>Read objects from nat R data (.rda) file.</p> <p>Currently supports parsing neurons, dotprops and mesh3d. Note that this is rather slow and I do not recommend doing this for large collections of neurons. For large scale conversion I recommend using the R interface (<code>navis.interfaces.r</code>, see online tutorials) via <code>rpy2</code>.</p> PARAMETER DESCRIPTION <code>f</code> <pre><code>            Filepath.\n</code></pre> <p> TYPE: <code>                str</code> </p> <code>combined</code> <pre><code>            What to do if there are multiple neuronlists contained\n            in the RDA files. By default, we will combine them into\n            a single NeuronList but you can also choose to keep them\n            as separate neuronlists.\n</code></pre> <p> TYPE: <code>         bool</code> </p> <code>neurons_only</code> <pre><code>            Whether to only parse and return neurons and dotprops\n            found in the RDA file.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> <code>**kwargs</code> <pre><code>            Keyword arguments passed to the construction of\n            `Tree/MeshNeuron/Dotprops`. You can use this to e.g. set\n            meta data.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>navis.NeuronList</code> <p>If <code>combine=True</code> and <code>neurons_only=True</code> returns a single NeuronList with the parsed neurons.</p> <code>dict</code> <p>If <code>combine=False</code> or <code>neurons_only=False</code> returns a dictionary with the original R object name as key and the parsed object as value.</p> Source code in <code>navis/io/rda_io.py</code> <pre><code>def read_rda(f: str,\n             combine: bool = True,\n             neurons_only: bool = True,\n             **kwargs) -&gt; 'core.NeuronList':\n    \"\"\"Read objects from nat R data (.rda) file.\n\n    Currently supports parsing neurons, dotprops and mesh3d. Note that this is\n    rather slow and I do not recommend doing this for large collections of\n    neurons. For large scale conversion I recommend using the R interface\n    (`navis.interfaces.r`, see online tutorials) via `rpy2`.\n\n    Parameters\n    ----------\n    f :                 str\n                        Filepath.\n    combined :          bool\n                        What to do if there are multiple neuronlists contained\n                        in the RDA files. By default, we will combine them into\n                        a single NeuronList but you can also choose to keep them\n                        as separate neuronlists.\n    neurons_only :      bool\n                        Whether to only parse and return neurons and dotprops\n                        found in the RDA file.\n    **kwargs\n                        Keyword arguments passed to the construction of\n                        `Tree/MeshNeuron/Dotprops`. You can use this to e.g. set\n                        meta data.\n\n    Returns\n    -------\n    navis.NeuronList\n                        If `combine=True` and `neurons_only=True` returns\n                        a single NeuronList with the parsed neurons.\n    dict\n                        If `combine=False` or `neurons_only=False` returns\n                        a dictionary with the original R object name as key and\n                        the parsed object as value.\n\n    \"\"\"\n    # Parse the file\n    parsed = rdata.parser.parse_file(f)\n\n    # Now convert to Python objects\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        converted = rdata.conversion.convert(parsed, CLASS_MAP_EXT)\n\n    # Some clean-up\n    for k, v in converted.items():\n        # Convert single neurons to neuronlist\n        if isinstance(v, core.BaseNeuron):\n            converted[k] = core.NeuronList(v)\n        # Give volumes a name\n        elif isinstance(v, core.Volume):\n            converted[k].name = k\n\n    if combine:\n        nl = core.NeuronList([n for n in converted.values() if isinstance(n, core.NeuronList)])\n        if nl:\n            converted = {k: v for k, v in converted.items() if not isinstance(v, core.NeuronList)}\n            converted['neurons'] = nl\n\n    if neurons_only:\n        if combine:\n            converted = converted['neurons']\n        else:\n            converted = {k: v for k, v in converted.items() if isinstance(v, core.NeuronList)}\n\n    return converted\n</code></pre>"},{"location":"reference/navis/#navis.read_swc","title":"<code>navis.read_swc</code>","text":"<p>Create Neuron/List from SWC file.</p> <p>This import is following format specified here.</p> PARAMETER DESCRIPTION <code>f</code> <pre><code>            Filename, folder, SWC string, URL or DataFrame:\n             - if folder, will import all `.swc` files\n             - if a `.zip`, `.tar` or `.tar.gz` archive will read all\n               SWC files from the file\n             - if a URL (http:// or https://), will download the\n               file and import it\n             - FTP address (ftp://) can point to a folder or a single\n               file\n             - Google Storage URLs (gs://) can point to a folder (in which\n               case all SWC files are read) or a single SWC file\n             - DataFrames are interpreted as a SWC tables\n            See also `limit` parameter to read only a subset of files.\n</code></pre> <p> TYPE: <code>                str | pandas.DataFrame | list thereof</code> </p> <code>connector_labels</code> <pre><code>            If provided will extract connectors from SWC.\n            Dictionary must map types to labels:\n            `{'presynapse': 7, 'postsynapse': 8}`\n</code></pre> <p> TYPE: <code> dict</code> DEFAULT: <code>{}</code> </p> <code>include_subdirs</code> <pre><code>            If True and `f` is a folder, will also search\n            subdirectories for `.swc` files.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> <code>delimiter</code> <pre><code>            Delimiter to use. Passed to `pandas.read_csv`.\n</code></pre> <p> TYPE: <code>        str</code> DEFAULT: <code>' '</code> </p> <code>parallel</code> <pre><code>            Whether to use parallel processes for reading:\n             - \"auto\" (default): will use parallel processing if\n                more than 200 SWCs are imported.\n              - Integers will be interpreted as the number of\n                processes to use. Defaults to `os.cpu_count() // 2`.\n              - False will use a single process.\n            Ignored for tar archives. Please note that spawning\n            processes incurs an overhead and might not be faster\n            for small numbers of files.\n</code></pre> <p> TYPE: <code>         \"auto\" | bool | int</code> DEFAULT: <code>'auto'</code> </p> <code>precision</code> <pre><code>            Precision for data. Defaults to 32 bit integers/floats.\n            If `None` will let pandas infer data types - this\n            typically leads to higher than necessary precision.\n</code></pre> <p> TYPE: <code>        int [8, 16, 32, 64] | None</code> DEFAULT: <code>32</code> </p> <code>fmt</code> <pre><code>            Formatter to specify how filenames are parsed into\n            neuron attributes. Some illustrative examples:\n\n              - `{name}.swc` (default) uses the filename\n                (minus the suffix) as the neuron's name property\n              - `{id}.swc` uses the filename as the neuron's ID\n                property\n              - `{name,id}.swc` uses the filename as the neuron's\n                name and ID properties\n              - `{name}.{id}.swc` splits the filename at a \".\"\n                and uses the first part as name and the second as ID\n              - `{name,id:int}.swc` same as above but converts\n                into integer for the ID\n              - `{name}_{myproperty}.swc` splits the filename at\n                \"_\" and uses the first part as name and as a\n                generic \"myproperty\" property\n              - `{name}_{}_{id}.swc` splits the filename at\n                \"_\" and uses the first part as name and the last as\n                ID. The middle part is ignored.\n\n            Throws a ValueError if pattern can't be found in\n            filename. Ignored for DataFrames.\n</code></pre> <p> TYPE: <code>              str</code> DEFAULT: <code>'{name}.swc'</code> </p> <code>read_meta</code> <pre><code>            If True and SWC header contains a line with JSON-encoded\n            meta data e.g. (`# Meta: {'id': 123}`), these data\n            will be read as neuron properties. `fmt` still takes\n            precedence. Will try to assign meta data directly as\n            neuron attribute (e.g. `neuron.id`). Failing that\n            (can happen for properties intrinsic to `TreeNeurons`),\n            will add a `.meta` dictionary to the neuron.\n</code></pre> <p> TYPE: <code>        bool</code> DEFAULT: <code>True</code> </p> <code>limit</code> <pre><code>            When reading from a folder or archive you can use this parameter to\n            restrict the which files read:\n             - if an integer, will read only the first `limit` SWC files\n              (useful to get a sample from a large library of skeletons)\n             - if a string, will interpret it as filename (regex) pattern\n               and only read files that match the pattern; e.g. `limit='.*_R.*'`\n               will only read files that contain `_R` in their filename\n             - if a slice (e.g. `slice(10, 20)`) will read only the files in\n               that range\n             - a list is expected to be a list of filenames to read from\n               the folder/archive\n</code></pre> <p> TYPE: <code>            int | str | slice | list</code> DEFAULT: <code>None</code> </p> <code>errors</code> <pre><code>            If \"log\" or \"ignore\", errors will not be raised and the\n            mesh will be skipped. Can result in empty output.\n</code></pre> <p> TYPE: <code>           \"raise\" | \"log\" | \"ignore\"</code> DEFAULT: <code>'raise'</code> </p> <code>**kwargs</code> <pre><code>            Keyword arguments passed to the construction of\n            `navis.TreeNeuron`. You can use this to e.g. set\n            meta data.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>navis.TreeNeuron</code> <p>Contains SWC file header as <code>.swc_header</code> attribute.</p> <code>navis.NeuronList</code> <p>If import of multiple SWCs will return NeuronList of TreeNeurons.</p> See Also <p><code>navis.write_swc</code>                     Export neurons as SWC files.</p> <p>Examples:</p> <p>Read a single file:</p> <pre><code>&gt;&gt;&gt; s = navis.read_swc('skeleton.swc')\n</code></pre> <p>Read all .swc files in a directory:</p> <pre><code>&gt;&gt;&gt; s = navis.read_swc('/some/directory/')\n</code></pre> <p>Read all .swc files in a zip archive:</p> <pre><code>&gt;&gt;&gt; s = navis.read_swc('skeletons.zip')\n</code></pre> <p>Sample the first 100 SWC files in a zip archive:</p> <pre><code>&gt;&gt;&gt; s = navis.read_swc('skeletons.zip', limit=100)\n</code></pre> <p>Read first all SWC files an ftp folder:</p> <pre><code>&gt;&gt;&gt; s = navis.read_swc('ftp://server:port/path/to/swc/')\n</code></pre> Source code in <code>navis/io/swc_io.py</code> <pre><code>def read_swc(\n    f: Union[str, pd.DataFrame, Iterable],\n    connector_labels: Optional[Dict[str, Union[str, int]]] = {},\n    soma_label: Union[str, int] = 1,\n    include_subdirs: bool = False,\n    delimiter: str = \" \",\n    parallel: Union[bool, int] = \"auto\",\n    precision: int = 32,\n    fmt: str = \"{name}.swc\",\n    read_meta: bool = True,\n    limit: Optional[int] = None,\n    errors: str = \"raise\",\n    **kwargs,\n) -&gt; \"core.NeuronObject\":\n    \"\"\"Create Neuron/List from SWC file.\n\n    This import is following format specified\n    [here](http://www.neuronland.org/NLMorphologyConverter/MorphologyFormats/SWC/Spec.html).\n\n    Parameters\n    ----------\n    f :                 str | pandas.DataFrame | list thereof\n                        Filename, folder, SWC string, URL or DataFrame:\n                         - if folder, will import all `.swc` files\n                         - if a `.zip`, `.tar` or `.tar.gz` archive will read all\n                           SWC files from the file\n                         - if a URL (http:// or https://), will download the\n                           file and import it\n                         - FTP address (ftp://) can point to a folder or a single\n                           file\n                         - Google Storage URLs (gs://) can point to a folder (in which\n                           case all SWC files are read) or a single SWC file\n                         - DataFrames are interpreted as a SWC tables\n                        See also `limit` parameter to read only a subset of files.\n    connector_labels :  dict, optional\n                        If provided will extract connectors from SWC.\n                        Dictionary must map types to labels:\n                        `{'presynapse': 7, 'postsynapse': 8}`\n    include_subdirs :   bool, optional\n                        If True and `f` is a folder, will also search\n                        subdirectories for `.swc` files.\n    delimiter :         str\n                        Delimiter to use. Passed to `pandas.read_csv`.\n    parallel :          \"auto\" | bool | int\n                        Whether to use parallel processes for reading:\n                         - \"auto\" (default): will use parallel processing if\n                            more than 200 SWCs are imported.\n                          - Integers will be interpreted as the number of\n                            processes to use. Defaults to `os.cpu_count() // 2`.\n                          - False will use a single process.\n                        Ignored for tar archives. Please note that spawning\n                        processes incurs an overhead and might not be faster\n                        for small numbers of files.\n    precision :         int [8, 16, 32, 64] | None\n                        Precision for data. Defaults to 32 bit integers/floats.\n                        If `None` will let pandas infer data types - this\n                        typically leads to higher than necessary precision.\n    fmt :               str\n                        Formatter to specify how filenames are parsed into\n                        neuron attributes. Some illustrative examples:\n\n                          - `{name}.swc` (default) uses the filename\n                            (minus the suffix) as the neuron's name property\n                          - `{id}.swc` uses the filename as the neuron's ID\n                            property\n                          - `{name,id}.swc` uses the filename as the neuron's\n                            name and ID properties\n                          - `{name}.{id}.swc` splits the filename at a \".\"\n                            and uses the first part as name and the second as ID\n                          - `{name,id:int}.swc` same as above but converts\n                            into integer for the ID\n                          - `{name}_{myproperty}.swc` splits the filename at\n                            \"_\" and uses the first part as name and as a\n                            generic \"myproperty\" property\n                          - `{name}_{}_{id}.swc` splits the filename at\n                            \"_\" and uses the first part as name and the last as\n                            ID. The middle part is ignored.\n\n                        Throws a ValueError if pattern can't be found in\n                        filename. Ignored for DataFrames.\n    read_meta :         bool\n                        If True and SWC header contains a line with JSON-encoded\n                        meta data e.g. (`# Meta: {'id': 123}`), these data\n                        will be read as neuron properties. `fmt` still takes\n                        precedence. Will try to assign meta data directly as\n                        neuron attribute (e.g. `neuron.id`). Failing that\n                        (can happen for properties intrinsic to `TreeNeurons`),\n                        will add a `.meta` dictionary to the neuron.\n    limit :             int | str | slice | list, optional\n                        When reading from a folder or archive you can use this parameter to\n                        restrict the which files read:\n                         - if an integer, will read only the first `limit` SWC files\n                          (useful to get a sample from a large library of skeletons)\n                         - if a string, will interpret it as filename (regex) pattern\n                           and only read files that match the pattern; e.g. `limit='.*_R.*'`\n                           will only read files that contain `_R` in their filename\n                         - if a slice (e.g. `slice(10, 20)`) will read only the files in\n                           that range\n                         - a list is expected to be a list of filenames to read from\n                           the folder/archive\n    errors :            \"raise\" | \"log\" | \"ignore\"\n                        If \"log\" or \"ignore\", errors will not be raised and the\n                        mesh will be skipped. Can result in empty output.\n    **kwargs\n                        Keyword arguments passed to the construction of\n                        `navis.TreeNeuron`. You can use this to e.g. set\n                        meta data.\n\n    Returns\n    -------\n    navis.TreeNeuron\n                        Contains SWC file header as `.swc_header` attribute.\n    navis.NeuronList\n                        If import of multiple SWCs will return NeuronList of\n                        TreeNeurons.\n\n    See Also\n    --------\n    [`navis.write_swc`][]\n                        Export neurons as SWC files.\n\n    Examples\n    --------\n\n    Read a single file:\n\n    &gt;&gt;&gt; s = navis.read_swc('skeleton.swc')                      # doctest: +SKIP\n\n    Read all .swc files in a directory:\n\n    &gt;&gt;&gt; s = navis.read_swc('/some/directory/')                  # doctest: +SKIP\n\n    Read all .swc files in a zip archive:\n\n    &gt;&gt;&gt; s = navis.read_swc('skeletons.zip')                     # doctest: +SKIP\n\n    Sample the first 100 SWC files in a zip archive:\n\n    &gt;&gt;&gt; s = navis.read_swc('skeletons.zip', limit=100)          # doctest: +SKIP\n\n    Read first all SWC files an ftp folder:\n\n    &gt;&gt;&gt; s = navis.read_swc('ftp://server:port/path/to/swc/')    # doctest: +SKIP\n\n    \"\"\"\n    # SwcReader will try its best to read whatever you throw at it - with limited\n    # sanity checks. For example: if you misspell a filepath, it will assume\n    # that it's a SWC string (because anything that's a string but doesn't\n    # point to an existing file or a folder MUST be a SWC) which will lead to\n    # strange error messages.\n    # The easiest fix is to implement a small sanity check here:\n    if isinstance(f, str) and \"\\n\" not in f and not utils.is_url(f):\n        # If this looks like a path\n        p = Path(f).expanduser()\n        if not p.is_dir() and not p.is_file():\n            raise FileNotFoundError(\n                f'\"{f}\" looks like a directory or filepath '\n                \"but does not appear to exist.\"\n            )\n\n    reader = SwcReader(\n        connector_labels=connector_labels,\n        soma_label=soma_label,\n        delimiter=delimiter,\n        precision=precision,\n        read_meta=read_meta,\n        fmt=fmt,\n        errors=errors,\n        attrs=kwargs,\n    )\n    res = reader.read_any(f, include_subdirs, parallel, limit=limit)\n\n    failed = []\n    for n in core.NeuronList(res):\n        if not hasattr(n, \"meta\"):\n            continue\n        failed += list(n.meta.keys())\n\n    if failed:\n        failed = list(set(failed))\n        logger.warning(\n            \"Some meta data could not be directly attached to the \"\n            \"neuron(s) - probably some clash with intrinsic \"\n            \"properties. You can find these data attached as \"\n            \"`.meta` dictionary.\"\n        )\n\n    return res\n</code></pre>"},{"location":"reference/navis/#navis.read_tiff","title":"<code>navis.read_tiff</code>","text":"<p>Create Neuron/List from TIFF file.</p> <p>Requires <code>tifffile</code> library which is not automatically installed!</p> PARAMETER DESCRIPTION <code>f</code> <pre><code>            Filename(s) or folder. If folder, will import all\n            `.tif` files.\n</code></pre> <p> TYPE: <code>                str | iterable</code> </p> <code>output</code> <pre><code>            Determines function's output. See Returns for details.\n</code></pre> <p> TYPE: <code>           \"voxels\" | \"dotprops\" | \"raw\"</code> DEFAULT: <code>'voxels'</code> </p> <code>channel</code> <pre><code>            Which channel to import. Ignored if file has only one\n            channel or when `output=\"raw\". Can use e.g. -1 to\n            get the last channel.\n</code></pre> <p> TYPE: <code>          int</code> DEFAULT: <code>0</code> </p> <code>threshold</code> <pre><code>            For `output='dotprops'` only: a threshold to filter\n            low intensity voxels. If `None`, no threshold is\n            applied and all values &gt; 0 are converted to points.\n</code></pre> <p> TYPE: <code>        int | float | None</code> DEFAULT: <code>None</code> </p> <code>thin</code> <pre><code>            For `output='dotprops'` only: if True, will thin the\n            point cloud using `skimage.morphology.skeletonize`\n            after thresholding. Requires `scikit-image`.\n</code></pre> <p> TYPE: <code>             bool</code> DEFAULT: <code>False</code> </p> <code>include_subdirs</code> <pre><code>            If True and `f` is a folder, will also search\n            subdirectories for `.tif` files.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> <code>parallel</code> <pre><code>            Defaults to `auto` which means only use parallel\n            processing if more than 10 TIFF files are imported.\n            Spawning and joining processes causes overhead and is\n            considerably slower for imports of small numbers of\n            neurons. Integer will be interpreted as the number of\n            cores (otherwise defaults to `os.cpu_count() - 2`).\n</code></pre> <p> TYPE: <code>         \"auto\" | bool | int,</code> DEFAULT: <code>'auto'</code> </p> <code>fmt</code> <pre><code>            Formatter to specify how filenames are parsed into neuron\n            attributes. Some illustrative examples:\n              - `{name}` (default) uses the filename\n                (minus the suffix) as the neuron's name property\n              - `{id}` (default) uses the filename as the neuron's ID\n                property\n              - `{name,id}` uses the filename as the neuron's\n                name and ID properties\n              - `{name}.{id}` splits the filename at a \".\"\n                and uses the first part as name and the second as ID\n              - `{name,id:int}` same as above but converts\n                into integer for the ID\n              - `{name}_{myproperty}` splits the filename at\n                \"_\" and uses the first part as name and as a\n                generic \"myproperty\" property\n              - `{name}_{}_{id}` splits the filename at\n                \"_\" and uses the first part as name and the last as\n                ID. The middle part is ignored.\n\n            Throws a ValueError if pattern can't be found in\n            filename.\n</code></pre> <p> TYPE: <code>              str</code> DEFAULT: <code>'{name}.tif'</code> </p> <code>limit</code> <pre><code>            When reading from a folder or archive you can use this parameter to\n            restrict the which files read:\n             - if an integer, will read only the first `limit` NMX files\n               (useful to get a sample from a large library of skeletons)\n             - if a string, will interpret it as filename (regex) pattern\n               and only read files that match the pattern; e.g. `limit='.*_R.*'`\n               will only read files that contain `_R` in their filename\n             - if a slice (e.g. `slice(10, 20)`) will read only the files in\n               that range\n             - a list is expected to be a list of filenames to read from\n               the folder/archive\n</code></pre> <p> TYPE: <code>            int | str | slice | list</code> DEFAULT: <code>None</code> </p> <code>errors</code> <pre><code>            If \"log\" or \"ignore\", errors will not be raised and the\n            mesh will be skipped. Can result in empty output.\n</code></pre> <p> TYPE: <code>           \"raise\" | \"log\" | \"ignore\"</code> DEFAULT: <code>'raise'</code> </p> <code>**dotprops_kwargs</code> <pre><code>            Keyword arguments passed to [`navis.make_dotprops`][]\n            if `output='dotprops'`. Use this to adjust e.g. the\n            number of nearest neighbors used for calculating the\n            tangent vector by passing e.g. `k=5`.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>navis.VoxelNeuron</code> <p>If <code>output=\"voxels\"</code> (default): requires TIFF data to be 3-dimensional voxels. VoxelNeuron will have TIFF file info as <code>.tiff_header</code> attribute.</p> <code>navis.Dotprops</code> <p>If <code>output=\"dotprops\"</code>. Dotprops will contain TIFF header as <code>.tiff_header</code> attribute.</p> <code>navis.NeuronList</code> <p>If import of multiple TIFF will return NeuronList of Dotprops/VoxelNeurons.</p> <code>(image, header)(np.ndarray, OrderedDict)</code> <p>If <code>output='raw'</code> return raw data contained in TIFF file.</p> Source code in <code>navis/io/tiff_io.py</code> <pre><code>def read_tiff(\n    f: Union[str, Iterable],\n    output: Union[Literal[\"voxels\"], Literal[\"dotprops\"], Literal[\"raw\"]] = \"voxels\",\n    channel: int = 0,\n    threshold: Optional[Union[int, float]] = None,\n    thin: bool = False,\n    include_subdirs: bool = False,\n    parallel: Union[bool, int] = \"auto\",\n    fmt: str = \"{name}.tif\",\n    limit: Optional[int] = None,\n    errors: str = \"raise\",\n    **dotprops_kwargs,\n) -&gt; \"core.NeuronObject\":\n    \"\"\"Create Neuron/List from TIFF file.\n\n    Requires `tifffile` library which is not automatically installed!\n\n    Parameters\n    ----------\n    f :                 str | iterable\n                        Filename(s) or folder. If folder, will import all\n                        `.tif` files.\n    output :            \"voxels\" | \"dotprops\" | \"raw\"\n                        Determines function's output. See Returns for details.\n    channel :           int\n                        Which channel to import. Ignored if file has only one\n                        channel or when `output=\"raw\". Can use e.g. -1 to\n                        get the last channel.\n    threshold :         int | float | None\n                        For `output='dotprops'` only: a threshold to filter\n                        low intensity voxels. If `None`, no threshold is\n                        applied and all values &gt; 0 are converted to points.\n    thin :              bool\n                        For `output='dotprops'` only: if True, will thin the\n                        point cloud using `skimage.morphology.skeletonize`\n                        after thresholding. Requires `scikit-image`.\n    include_subdirs :   bool, optional\n                        If True and `f` is a folder, will also search\n                        subdirectories for `.tif` files.\n    parallel :          \"auto\" | bool | int,\n                        Defaults to `auto` which means only use parallel\n                        processing if more than 10 TIFF files are imported.\n                        Spawning and joining processes causes overhead and is\n                        considerably slower for imports of small numbers of\n                        neurons. Integer will be interpreted as the number of\n                        cores (otherwise defaults to `os.cpu_count() - 2`).\n    fmt :               str\n                        Formatter to specify how filenames are parsed into neuron\n                        attributes. Some illustrative examples:\n                          - `{name}` (default) uses the filename\n                            (minus the suffix) as the neuron's name property\n                          - `{id}` (default) uses the filename as the neuron's ID\n                            property\n                          - `{name,id}` uses the filename as the neuron's\n                            name and ID properties\n                          - `{name}.{id}` splits the filename at a \".\"\n                            and uses the first part as name and the second as ID\n                          - `{name,id:int}` same as above but converts\n                            into integer for the ID\n                          - `{name}_{myproperty}` splits the filename at\n                            \"_\" and uses the first part as name and as a\n                            generic \"myproperty\" property\n                          - `{name}_{}_{id}` splits the filename at\n                            \"_\" and uses the first part as name and the last as\n                            ID. The middle part is ignored.\n\n                        Throws a ValueError if pattern can't be found in\n                        filename.\n    limit :             int | str | slice | list, optional\n                        When reading from a folder or archive you can use this parameter to\n                        restrict the which files read:\n                         - if an integer, will read only the first `limit` NMX files\n                           (useful to get a sample from a large library of skeletons)\n                         - if a string, will interpret it as filename (regex) pattern\n                           and only read files that match the pattern; e.g. `limit='.*_R.*'`\n                           will only read files that contain `_R` in their filename\n                         - if a slice (e.g. `slice(10, 20)`) will read only the files in\n                           that range\n                         - a list is expected to be a list of filenames to read from\n                           the folder/archive\n    errors :            \"raise\" | \"log\" | \"ignore\"\n                        If \"log\" or \"ignore\", errors will not be raised and the\n                        mesh will be skipped. Can result in empty output.\n\n    **dotprops_kwargs\n                        Keyword arguments passed to [`navis.make_dotprops`][]\n                        if `output='dotprops'`. Use this to adjust e.g. the\n                        number of nearest neighbors used for calculating the\n                        tangent vector by passing e.g. `k=5`.\n\n    Returns\n    -------\n    navis.VoxelNeuron\n                        If `output=\"voxels\"` (default): requires TIFF data to\n                        be 3-dimensional voxels. VoxelNeuron will have TIFF file\n                        info as `.tiff_header` attribute.\n    navis.Dotprops\n                        If `output=\"dotprops\"`. Dotprops will contain TIFF\n                        header as `.tiff_header` attribute.\n    navis.NeuronList\n                        If import of multiple TIFF will return NeuronList of\n                        Dotprops/VoxelNeurons.\n    (image, header)     (np.ndarray, OrderedDict)\n                        If `output='raw'` return raw data contained in TIFF\n                        file.\n\n    \"\"\"\n    try:\n        import tifffile\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"`navis.read_tiff` requires the `tifffile` library:\\n\"\n            \"  pip3 install tifffile -U\"\n        )\n\n    utils.eval_param(\n        output, name=\"output\", allowed_values=(\"raw\", \"dotprops\", \"voxels\")\n    )\n\n    if parallel == \"auto\":\n        # Set a lower threshold of 10 on parallel processing for TIFFs (default is 200)\n        parallel = (\"auto\", 10)\n\n    reader = TiffReader(\n        channel=channel,\n        output=output,\n        threshold=threshold,\n        thin=thin,\n        fmt=fmt,\n        dotprop_kwargs=dotprops_kwargs,\n        errors=errors,\n    )\n    return reader.read_any(f, include_subdirs, parallel, limit=limit)\n</code></pre>"},{"location":"reference/navis/#navis.remove_nodes","title":"<code>navis.remove_nodes</code>","text":"<p>Drop nodes from neuron without disconnecting it.</p> <p>Dropping node 2 from 1-&gt;2-&gt;3 will lead to connectivity 1-&gt;3.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Neuron to remove nodes from.\n</code></pre> <p> TYPE: <code>        TreeNeuron</code> </p> <code>which</code> <pre><code>    IDs of nodes to remove.\n</code></pre> <p> TYPE: <code>    list of node IDs</code> </p> <code>inplace</code> <pre><code>    If True, will rewire the neuron inplace. If False, will return\n    a rewired copy of the neuron.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>TreeNeuron</code> <p>Examples:</p> <p>Drop points from a neuron</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; n.n_nodes\n4465\n&gt;&gt;&gt; # Drop a hundred nodes\n&gt;&gt;&gt; n2 = navis.remove_nodes(n, n.nodes.node_id.values[100:200])\n&gt;&gt;&gt; n2.n_nodes\n4365\n</code></pre> Source code in <code>navis/graph/graph_utils.py</code> <pre><code>def remove_nodes(\n    x: \"core.TreeNeuron\", which: List[int], inplace: bool = False\n) -&gt; Optional[\"core.TreeNeuron\"]:\n    \"\"\"Drop nodes from neuron without disconnecting it.\n\n    Dropping node 2 from 1-&gt;2-&gt;3 will lead to connectivity 1-&gt;3.\n\n    Parameters\n    ----------\n    x :         TreeNeuron\n                Neuron to remove nodes from.\n    which :     list of node IDs\n                IDs of nodes to remove.\n    inplace :   bool\n                If True, will rewire the neuron inplace. If False, will return\n                a rewired copy of the neuron.\n\n    Returns\n    -------\n    TreeNeuron\n\n    Examples\n    --------\n    Drop points from a neuron\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; n.n_nodes\n    4465\n    &gt;&gt;&gt; # Drop a hundred nodes\n    &gt;&gt;&gt; n2 = navis.remove_nodes(n, n.nodes.node_id.values[100:200])\n    &gt;&gt;&gt; n2.n_nodes\n    4365\n\n    \"\"\"\n    utils.eval_param(x, name=\"x\", allowed_types=(core.TreeNeuron,))\n\n    if not utils.is_iterable(which):\n        which = [which]\n    which = np.asarray(which)\n\n    miss = ~np.isin(which, x.nodes.node_id.values)\n    if np.any(miss):\n        raise ValueError(f\"{len(miss)} node IDs not found in neuron\")\n\n    if not inplace:\n        x = x.copy()\n\n    # Generate new list of parents\n    lop = dict(zip(x.nodes.node_id.values, x.nodes.parent_id.values))\n\n    # Rewire to skip the to-be-removed nodes\n    for n in which:\n        lop.update({c: lop[n] for c, p in lop.items() if p == n})\n\n    # Rewire neuron\n    x.nodes[\"parent_id\"] = x.nodes.node_id.map(lop)\n\n    # Drop nodes\n    x.nodes = x.nodes[~x.nodes.node_id.isin(which)].copy()\n\n    # Clear temporary attributes\n    x._clear_temp_attr()\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.reroot_skeleton","title":"<code>navis.reroot_skeleton</code>","text":"<p>Reroot neuron to new root.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>   List must contain only a SINGLE neuron.\n</code></pre> <p> TYPE: <code>       TreeNeuron | NeuronList</code> </p> <code>new_root</code> <pre><code>   Node ID(s) of node(s) to reroot to. If multiple new roots are\n   provided, they will be rerooted in sequence.\n</code></pre> <p> TYPE: <code>int | iterable</code> </p> <code>inplace</code> <pre><code>   If True the input neuron will be rerooted in place. If False will\n   reroot and return a copy of the original.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>TreeNeuron</code> <p>Rerooted neuron.</p> See Also <p><code>navis.TreeNeuron.reroot</code>             Quick access to reroot directly from TreeNeuron/List             objects.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1, kind='skeleton')\n&gt;&gt;&gt; # Reroot neuron to its soma\n&gt;&gt;&gt; n2 = navis.reroot_skeleton(n, n.soma)\n</code></pre> Source code in <code>navis/graph/graph_utils.py</code> <pre><code>@utils.lock_neuron\ndef reroot_skeleton(\n    x: \"core.NeuronObject\", new_root: Union[int, str], inplace: bool = False\n) -&gt; \"core.TreeNeuron\":\n    \"\"\"Reroot neuron to new root.\n\n    Parameters\n    ----------\n    x :        TreeNeuron | NeuronList\n               List must contain only a SINGLE neuron.\n    new_root : int | iterable\n               Node ID(s) of node(s) to reroot to. If multiple new roots are\n               provided, they will be rerooted in sequence.\n    inplace :  bool, optional\n               If True the input neuron will be rerooted in place. If False will\n               reroot and return a copy of the original.\n\n    Returns\n    -------\n    TreeNeuron\n               Rerooted neuron.\n\n    See Also\n    --------\n    [`navis.TreeNeuron.reroot`][]\n                Quick access to reroot directly from TreeNeuron/List\n                objects.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1, kind='skeleton')\n    &gt;&gt;&gt; # Reroot neuron to its soma\n    &gt;&gt;&gt; n2 = navis.reroot_skeleton(n, n.soma)\n\n    \"\"\"\n    if isinstance(x, core.NeuronList):\n        if len(x) == 1:\n            x = x[0]\n        else:\n            raise ValueError(f\"Expected a single neuron, got {len(x)}\")\n\n    if not isinstance(x, core.TreeNeuron):\n        raise ValueError(f'Unable to reroot object of type \"{type(x)}\"')\n\n    # Make new root an iterable\n    new_roots = utils.make_iterable(new_root)\n\n    # Parse new roots\n    for i, root in enumerate(new_roots):\n        if root is None:\n            raise ValueError(\"New root can not be &lt;None&gt;\")\n\n        # If new root is a tag, rather than a ID, try finding that node\n        if isinstance(root, str):\n            if x.tags is None:\n                raise ValueError(\"Neuron does not have tags\")\n\n            if root not in x.tags:\n                raise ValueError(\n                    f\"#{x.id}: Found no nodes with tag {root}\" \" - please double check!\"\n                )\n\n            elif len(x.tags[root]) &gt; 1:\n                raise ValueError(\n                    f\"#{x.id}: Found multiple node with tag \"\n                    f\"{root} - please double check!\"\n                )\n            else:\n                new_roots[i] = x.tags[root][0]\n\n    # At this point x is TreeNeuron\n    x: core.TreeNeuron\n    # At this point new_roots is list of int\n    new_roots: Iterable[int]\n\n    if not inplace:\n        # Make a copy\n        x = x.copy()\n        # Run this in a separate function so that the lock is applied to copy\n        _ = reroot_skeleton(x, new_root=new_roots, inplace=True)\n        return x\n\n    # Keep track of node ID dtype\n    nodeid_dtype = x.nodes.node_id.dtype\n\n    # Go over each new root\n    for new_root in new_roots:\n        # Skip if new root is old root\n        if any(x.root == new_root):\n            continue\n\n        if x.igraph and config.use_igraph:\n            # Grab graph once to avoid overhead from stale checks\n            g = x.igraph\n\n            # Prevent warnings in the following code - querying paths between\n            # unreachable nodes will otherwise generate a runtime warning\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n\n                # Find paths to all roots\n                path = g.get_shortest_paths(\n                    g.vs.find(node_id=new_root), [g.vs.find(node_id=r) for r in x.root]\n                )\n                epath = g.get_shortest_paths(\n                    g.vs.find(node_id=new_root),\n                    [g.vs.find(node_id=r) for r in x.root],\n                    output=\"epath\",\n                )\n\n            # Extract paths that actually worked (i.e. within a continuous fragment)\n            path = [p for p in path if p][0]\n            epath = [p for p in epath if p][0]\n\n            edges = [(s, t) for s, t in zip(path[:-1], path[1:])]\n\n            weights = [g.es[e][\"weight\"] for e in epath]\n\n            # Get all weights and append inversed new weights\n            all_weights = g.es[\"weight\"] + weights\n\n            # Add inverse edges: old_root-&gt;new_root\n            g.add_edges([(e[1], e[0]) for e in edges])\n\n            # Re-set weights\n            g.es[\"weight\"] = all_weights\n\n            # Remove new_root-&gt;old_root\n            g.delete_edges(edges)\n\n            # Get degree of old root for later categorisation\n            old_root_deg = len(g.es.select(_target=path[-1]))\n\n            # Translate path indices to node IDs\n            ix2id = {\n                ix: n\n                for ix, n in zip(g.vs.indices, g.vs.get_attribute_values(\"node_id\"))\n            }\n            path = [ix2id[i] for i in path]\n        else:\n            # Grab graph once to avoid overhead from stale checks\n            g = x.graph\n            # If this NetworkX graph is just an (immutable) view, turn it into a\n            # full, independent graph\n            nx_main_version = \".\".join(nx.__version__.split(\".\")[:2])\n            if float(nx_main_version) &lt; 2.2:\n                if isinstance(g, nx.classes.graphviews.ReadOnlyGraph):\n                    x._graph_nx = g = nx.DiGraph(g)\n            elif hasattr(g, \"_NODE_OK\"):\n                x._graph_nx = g = nx.DiGraph(g)\n            elif nx.is_frozen(g):\n                x._graph_nx = g = nx.DiGraph(g)\n\n            # Walk from new root to old root and remove edges along the way\n            parent = next(g.successors(new_root), None)\n            if not parent:\n                # new_root is already the root\n                continue\n\n            path = [new_root]\n            weights = []\n            while parent is not None:\n                weights.append(g[path[-1]][parent][\"weight\"])\n                g.remove_edge(path[-1], parent)\n                path.append(parent)\n                parent = next(g.successors(parent), None)\n\n            # Invert path and add weights\n            new_edges = [\n                (path[i + 1], path[i], {\"weight\": weights[i]})\n                for i in range(len(path) - 1)\n            ]\n\n            # Add inverted path between old and new root\n            g.add_edges_from(new_edges)\n\n            # Get degree of old root for later categorisation\n            old_root_deg = g.in_degree(path[-1])\n\n        # Set index to node ID for later\n        x.nodes.set_index(\"node_id\", inplace=True)\n\n        # Propagate changes in graph back to node table\n        # Assign new node type to old root\n        x.nodes.loc[path[1:], \"parent_id\"] = path[:-1]\n        if old_root_deg == 1:\n            x.nodes.loc[path[-1], \"type\"] = \"slab\"\n        elif old_root_deg &gt; 1:\n            x.nodes.loc[path[-1], \"type\"] = \"branch\"\n        else:\n            x.nodes.loc[path[-1], \"type\"] = \"end\"\n        # Make new root node type \"root\"\n        x.nodes.loc[path[0], \"type\"] = \"root\"\n\n        # Set new root's parent to None\n        x.nodes.loc[new_root, \"parent_id\"] = -1\n\n        # Reset index\n        x.nodes.reset_index(drop=False, inplace=True)\n\n    # Make sure node ID has the same datatype as before\n    if x.nodes.node_id.dtype != nodeid_dtype:\n        x.nodes[\"node_id\"] = x.nodes.node_id.astype(nodeid_dtype, copy=False)\n\n    # Finally: only reset non-graph related attributes\n    if x.igraph and config.use_igraph:\n        x._clear_temp_attr(exclude=[\"igraph\", \"classify_nodes\"])\n    else:\n        x._clear_temp_attr(exclude=[\"graph\", \"classify_nodes\"])\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.resample_along_axis","title":"<code>navis.resample_along_axis</code>","text":"<p>Resample neuron such that nodes lie exactly on given 1d grid.</p> <p>This function does not simply snap nodes to the closest grid line but instead adds new nodes where edges between existing nodes intersect with the planes defined by the grid.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Neuron(s) to resample.\n</code></pre> <p> TYPE: <code>            TreeNeuron | NeuronList</code> </p> <code>interval</code> <pre><code>        Intervals defining a 1-dimensional grid along given axes\n        (see examples). If neuron(s) have `.units` set, you can also\n        pass a string such as \"50 nm\".\n</code></pre> <p> TYPE: <code>     float | int | str</code> </p> <code>axis</code> <pre><code>        Along which axes (x/y/z) to resample.\n</code></pre> <p> TYPE: <code>          0 | 1 | 2</code> DEFAULT: <code>2</code> </p> <code>old_nodes</code> <pre><code>        Existing nodes are unlikely to intersect with the planes as\n        defined by the grid interval. There are three possible ways\n        to deal with them:\n         - \"remove\" (default) will simply drop old nodes: this\n           guarantees all remaining nodes will lie on a plane\n         - \"keep\" will keep old nodes without changing them\n         - \"snap\" will snap those nodes to the closest coordinate\n           on the grid without interpolation\n</code></pre> <p> TYPE: <code>    \"remove\" | \"keep\" | \"snap\"</code> DEFAULT: <code>'remove'</code> </p> <code>inplace</code> <pre><code>        If False, will resample and return a copy of the original. If\n        True, will resample input neuron in place.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>TreeNeuron / List</code> <p>The resampled neuron(s).</p> See Also <p><code>navis.resample_skeleton</code>                     Resample neuron such that edges between nodes have a                     given length. <code>navis.downsample_neuron</code>                     This function reduces the number of nodes instead of                     resample to certain resolution. Useful if you are                     just after some simplification e.g. for speeding up                     your calculations or you want to preserve node IDs.</p> <p>Examples:</p> <p>Resample neuron such that we have one node in every 40nm slice along z axis</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; n.n_nodes\n4465\n&gt;&gt;&gt; res = navis.resample_along_axis(n, interval='40 nm',\n...                                 axis=2, old_nodes='remove')\n&gt;&gt;&gt; res.n_nodes &lt; n.n_nodes\nTrue\n</code></pre> Source code in <code>navis/sampling/resampling.py</code> <pre><code>@utils.map_neuronlist(desc='Binning', allow_parallel=True)\ndef resample_along_axis(x: 'core.TreeNeuron',\n                        interval: Union[int, float, str],\n                        axis: int = 2,\n                        old_nodes: Union[Literal['remove'],\n                                         Literal['keep'],\n                                         Literal['snap']] = 'remove',\n                        inplace: bool = False\n                        ) -&gt; Optional['core.TreeNeuron']:\n    \"\"\"Resample neuron such that nodes lie exactly on given 1d grid.\n\n    This function does not simply snap nodes to the closest grid line but\n    instead adds new nodes where edges between existing nodes intersect\n    with the planes defined by the grid.\n\n    Parameters\n    ----------\n    x :             TreeNeuron | NeuronList\n                    Neuron(s) to resample.\n    interval :      float | int | str\n                    Intervals defining a 1-dimensional grid along given axes\n                    (see examples). If neuron(s) have `.units` set, you can also\n                    pass a string such as \"50 nm\".\n    axis :           0 | 1 | 2\n                    Along which axes (x/y/z) to resample.\n    old_nodes :     \"remove\" | \"keep\" | \"snap\"\n                    Existing nodes are unlikely to intersect with the planes as\n                    defined by the grid interval. There are three possible ways\n                    to deal with them:\n                     - \"remove\" (default) will simply drop old nodes: this\n                       guarantees all remaining nodes will lie on a plane\n                     - \"keep\" will keep old nodes without changing them\n                     - \"snap\" will snap those nodes to the closest coordinate\n                       on the grid without interpolation\n\n    inplace :       bool\n                    If False, will resample and return a copy of the original. If\n                    True, will resample input neuron in place.\n\n    Returns\n    -------\n    TreeNeuron/List\n                    The resampled neuron(s).\n\n    See Also\n    --------\n    [`navis.resample_skeleton`][]\n                        Resample neuron such that edges between nodes have a\n                        given length.\n    [`navis.downsample_neuron`][]\n                        This function reduces the number of nodes instead of\n                        resample to certain resolution. Useful if you are\n                        just after some simplification e.g. for speeding up\n                        your calculations or you want to preserve node IDs.\n\n    Examples\n    --------\n    Resample neuron such that we have one node in every 40nm slice along z axis\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; n.n_nodes\n    4465\n    &gt;&gt;&gt; res = navis.resample_along_axis(n, interval='40 nm',\n    ...                                 axis=2, old_nodes='remove')\n    &gt;&gt;&gt; res.n_nodes &lt; n.n_nodes\n    True\n\n    \"\"\"\n    utils.eval_param(axis, name='axis', allowed_values=(0, 1, 2))\n    utils.eval_param(old_nodes, name='old_nodes',\n                     allowed_values=(\"remove\", \"keep\", \"snap\"))\n    utils.eval_param(x, name='x', allowed_types=(core.TreeNeuron, ))\n\n    interval = x.map_units(interval, on_error='raise')\n\n    if not inplace:\n        x = x.copy()\n\n    # Collect coordinates of nodes and their parents\n    nodes = x.nodes\n    not_root = nodes.loc[nodes.parent_id &gt;= 0]\n    node_locs = not_root[['x', 'y', 'z']].values\n    parent_locs = nodes.set_index('node_id').loc[not_root.parent_id.values,\n                                                 ['x', 'y', 'z']].values\n\n    # Get all vectors\n    vecs = parent_locs - node_locs\n\n    # Get coordinates along this axis\n    loc1 = node_locs[:, axis]\n    loc2 = parent_locs[:, axis]\n\n    # This prevents runtime warnings e.g. from division by zero\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        # Find out which grid interval these are on\n        int1 = (loc1 / interval).astype(int)\n        int2 = (loc2 / interval).astype(int)\n\n        # Difference in bin between both locs\n        diff = int2 - int1\n        sign = diff / np.abs(diff)\n\n        # Figure out by how far we are from the gridline\n        dist = np.zeros(diff.shape[0])\n        dist[diff &lt; 0] = loc1[diff &lt; 0] % interval\n        dist[diff &gt; 0] = -loc1[diff &gt; 0] % interval\n\n        # Now we need to calculate the new position\n        # Get other axes\n        other_axes = list({0, 1, 2} - {axis})\n        # Normalize other vectors by this vector\n        other_vecs_norm = vecs[:, other_axes] / vecs[:, [axis]]\n\n        # Get offset for other axis\n        other_offset = other_vecs_norm * dist.reshape(dist.shape[0], 1)\n\n        # Offset for this axis\n        this_offset = dist * sign\n\n    # Apply offsets\n    new_coords = node_locs.copy()\n    new_coords[:, other_axes] += other_offset * sign.reshape(sign.shape[0], 1)\n    new_coords[:, [axis]] += this_offset.reshape(this_offset.shape[0], 1)\n\n    # Now extract nodes that need to be inserted\n    insert_between = not_root.loc[diff != 0, ['node_id', 'parent_id']].values\n    new_coords = new_coords[diff != 0]\n\n    # Insert nodes\n    graph.insert_nodes(x, where=insert_between, coords=new_coords, inplace=True)\n\n    # Figure out what to do with nodes that are not on the grid\n    if old_nodes == 'remove':\n        mod = x.nodes[['x', 'y', 'z'][axis]].values % interval\n        not_lined_up = mod != 0\n        to_remove = x.nodes.loc[not_lined_up, 'node_id'].values\n    elif old_nodes == 'keep':\n        to_remove = insert_between[:, 0]\n    elif old_nodes == 'snap':\n        not_lined_up = x.nodes[['x', 'y', 'z']].values[:, axis] % interval != 0\n        to_snap = x.nodes.loc[not_lined_up, ['x', 'y', 'z'][axis]].values\n        snapped = (to_snap / interval).round() * interval\n        x.nodes.loc[not_lined_up, ['x', 'y', 'z'][axis]] = snapped\n        to_remove = []\n\n    if np.any(to_remove):\n        graph.remove_nodes(x, which=to_remove, inplace=True)\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.resample_skeleton","title":"<code>navis.resample_skeleton</code>","text":"<p>Resample skeleton(s) to given resolution.</p> <p>Preserves root, leafs and branchpoints. Soma, connectors and node tags (if present) are mapped onto the closest node in the resampled neuron.</p> Important <p>A few things to keep in mind:   - This generates an entirely new set of node IDs! They will be unique     within a neuron, but you may encounter duplicates across neurons.   - Any non-standard node table columns (e.g. \"labels\") will be lost.   - Soma(s) will be pinned to the closest node in the resampled neuron.   - We may end up upcasting the data type for node and parent IDs to     accommodate the new node IDs.</p> <p>Also: be aware that high-resolution neurons will use A LOT of memory.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            Neuron(s) to resample.\n</code></pre> <p> TYPE: <code>                TreeNeuron | NeuronList</code> </p> <code>resample_to</code> <pre><code>            Target sampling resolution, i.e. one node every\n            N units of cable. Note that hitting the exact\n            sampling resolution might not be possible e.g. if\n            a branch is shorter than the target resolution. If\n            neuron(s) have their `.units` parameter, you can also\n            pass a string such as \"1 micron\".\n</code></pre> <p> TYPE: <code>      int | float | str</code> </p> <code>method</code> <pre><code>            See `scipy.interpolate.interp1d` for possible\n            options. By default, we're using linear interpolation.\n</code></pre> <p> TYPE: <code>           str</code> DEFAULT: <code>'linear'</code> </p> <code>map_columns</code> <pre><code>            Names of additional columns to carry over to the resampled\n            neuron. Numerical columns will be interpolated according to\n            `method`. Non-numerical columns will be interpolated\n            using nearest neighbour interpolation.\n</code></pre> <p> TYPE: <code>      list of str</code> DEFAULT: <code>None</code> </p> <code>inplace</code> <pre><code>            If True, will modify original neuron. If False, a\n            resampled copy is returned.\n</code></pre> <p> TYPE: <code>          bool</code> DEFAULT: <code>False</code> </p> <code>skip_errors</code> <pre><code>            If True, will skip errors during interpolation and\n            only print summary.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>TreeNeuron / List</code> <p>Downsampled neuron(s).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; # Check sampling resolution (nodes/cable)\n&gt;&gt;&gt; round(n.sampling_resolution)\n60\n&gt;&gt;&gt; # Resample to 1 micron (example neurons are in 8x8x8nm)\n&gt;&gt;&gt; n_rs = navis.resample_skeleton(n,\n...                                resample_to=1000 / 8,\n...                                inplace=False)\n&gt;&gt;&gt; round(n_rs.sampling_resolution)\n134\n</code></pre> See Also <p><code>navis.downsample_neuron</code>                     This function reduces the number of nodes instead of                     resample to certain resolution. Useful if you are                     just after some simplification - e.g. for speeding up                     your calculations or you want to preserve node IDs. <code>navis.resample_along_axis</code>                     Resample neuron along a single axis such that nodes                     align with given 1-dimensional grid.</p> Source code in <code>navis/sampling/resampling.py</code> <pre><code>@utils.map_neuronlist(desc='Resampling', allow_parallel=True)\ndef resample_skeleton(x: 'core.NeuronObject',\n                      resample_to: Union[int, str],\n                      inplace: bool = False,\n                      method: str = 'linear',\n                      map_columns: Optional[list] = None,\n                      skip_errors: bool = True\n                      ) -&gt; Optional['core.NeuronObject']:\n    \"\"\"Resample skeleton(s) to given resolution.\n\n    Preserves root, leafs and branchpoints. Soma, connectors and node tags\n    (if present) are mapped onto the closest node in the resampled neuron.\n\n    Important\n    ---------\n    A few things to keep in mind:\n      - This generates an entirely new set of node IDs! They will be unique\n        within a neuron, but you may encounter duplicates across neurons.\n      - Any non-standard node table columns (e.g. \"labels\") will be lost.\n      - Soma(s) will be pinned to the closest node in the resampled neuron.\n      - We may end up upcasting the data type for node and parent IDs to\n        accommodate the new node IDs.\n\n    Also: be aware that high-resolution neurons will use A LOT of memory.\n\n    Parameters\n    ----------\n    x :                 TreeNeuron | NeuronList\n                        Neuron(s) to resample.\n    resample_to :       int | float | str\n                        Target sampling resolution, i.e. one node every\n                        N units of cable. Note that hitting the exact\n                        sampling resolution might not be possible e.g. if\n                        a branch is shorter than the target resolution. If\n                        neuron(s) have their `.units` parameter, you can also\n                        pass a string such as \"1 micron\".\n    method :            str, optional\n                        See `scipy.interpolate.interp1d` for possible\n                        options. By default, we're using linear interpolation.\n    map_columns :       list of str, optional\n                        Names of additional columns to carry over to the resampled\n                        neuron. Numerical columns will be interpolated according to\n                        `method`. Non-numerical columns will be interpolated\n                        using nearest neighbour interpolation.\n    inplace :           bool, optional\n                        If True, will modify original neuron. If False, a\n                        resampled copy is returned.\n    skip_errors :       bool, optional\n                        If True, will skip errors during interpolation and\n                        only print summary.\n\n    Returns\n    -------\n    TreeNeuron/List\n                        Downsampled neuron(s).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; # Check sampling resolution (nodes/cable)\n    &gt;&gt;&gt; round(n.sampling_resolution)\n    60\n    &gt;&gt;&gt; # Resample to 1 micron (example neurons are in 8x8x8nm)\n    &gt;&gt;&gt; n_rs = navis.resample_skeleton(n,\n    ...                                resample_to=1000 / 8,\n    ...                                inplace=False)\n    &gt;&gt;&gt; round(n_rs.sampling_resolution)\n    134\n\n    See Also\n    --------\n    [`navis.downsample_neuron`][]\n                        This function reduces the number of nodes instead of\n                        resample to certain resolution. Useful if you are\n                        just after some simplification - e.g. for speeding up\n                        your calculations or you want to preserve node IDs.\n    [`navis.resample_along_axis`][]\n                        Resample neuron along a single axis such that nodes\n                        align with given 1-dimensional grid.\n\n    \"\"\"\n    if not isinstance(x, core.TreeNeuron):\n        raise TypeError(f'Unable to resample data of type \"{type(x)}\"')\n\n    # Map units (non-str are just passed through)\n    resample_to = x.map_units(resample_to, on_error=\"raise\")\n\n    if not inplace:\n        x = x.copy()\n\n    num_cols = [\"x\", \"y\", \"z\", \"radius\"]\n    non_num_cols = []\n\n    if map_columns:\n        if isinstance(map_columns, str):\n            map_columns = [map_columns]\n\n        for col in map_columns:\n            if col in num_cols or col in non_num_cols:\n                continue\n            if col not in x.nodes.columns:\n                raise ValueError(f'Column \"{col}\" not found in node table')\n            if pd.api.types.is_numeric_dtype(x.nodes[col].dtype):\n                num_cols.append(col)\n            else:\n                non_num_cols.append(col)\n\n    # Collect coordinates\n    locs = dict(zip(x.nodes.node_id.values, x.nodes[[\"x\", \"y\", \"z\"]].values))\n\n    # Collect values for all columns\n    values = {\n        col: dict(zip(x.nodes.node_id.values, x.nodes[col].values))\n        for col in num_cols + non_num_cols\n    }\n\n    # For categorical columns, we need to translate them to numerical values\n    cat2num = {}\n    num2cat = {}\n    for col in non_num_cols:\n        cat2num[col] = {c: i for i, c in enumerate(x.nodes[col].unique())}\n        num2cat[col] = {i: c for c, i in cat2num[col].items()}\n\n    new_nodes: List = []\n    max_tn_id = x.nodes.node_id.max() + 1\n\n    errors = 0\n\n    # Iterate over segments\n    for i, seg in enumerate(x.small_segments):\n        # Get coordinates\n        coords = np.vstack([locs[n] for n in seg])\n        # Get radii\n        # rad = [radii[tn] for tn in seg]\n\n        # Vecs between subsequently measured points\n        vecs = np.diff(coords.T)\n\n        # path: cum distance along points (norm from first to Nth point)\n        dist = np.cumsum(np.linalg.norm(vecs, axis=0))\n        dist = np.insert(dist, 0, 0)\n\n        # If path is too short, just keep the first and last node\n        if dist[-1] &lt; resample_to or (method == \"cubic\" and len(seg) &lt;= 3):\n            new_nodes += [\n                [seg[0], seg[-1]] + [values[c][seg[0]] for c in num_cols + non_num_cols]\n            ]\n            continue\n\n        # Distances (i.e. resolution) of interpolation\n        n_nodes = np.round(dist[-1] / resample_to)\n        new_dist = np.linspace(dist[0], dist[-1], int(n_nodes))\n\n        samples = {}\n        # Interpolate numerical columns\n        for col in num_cols:\n            try:\n                samples[col] = scipy.interpolate.interp1d(\n                    dist, [values[col][n] for n in seg], kind=method\n                )\n            except ValueError as e:\n                if skip_errors:\n                    errors += 1\n                    new_nodes += x.nodes.loc[\n                        x.nodes.node_id.isin(seg[:-1]),\n                        [\"node_id\", \"parent_id\"] + num_cols + non_num_cols,\n                    ].values.tolist()\n                    continue\n                else:\n                    raise e\n        # Interpolate non-numerical columns\n        for col in non_num_cols:\n            try:\n                samples[col] = scipy.interpolate.interp1d(\n                    dist, [cat2num[col][values[col][n]] for n in seg], kind=\"nearest\"\n                )\n            except ValueError as e:\n                if skip_errors:\n                    errors += 1\n                    new_nodes += x.nodes.loc[\n                        x.nodes.node_id.isin(seg[:-1]),\n                        [\"node_id\", \"parent_id\"] + num_cols + non_num_cols,\n                    ].values.tolist()\n                    continue\n                else:\n                    raise e\n\n        # Sample each column\n        new_values = {}\n        for col in num_cols:\n            new_values[col] = samples[col](new_dist)\n        for col in non_num_cols:\n            new_values[col] = [num2cat[col][int(samples[col](d))] for d in new_dist]\n\n        # Generate new ids (start and end node IDs of this segment are kept)\n        new_ids = np.concatenate(\n            (seg[:1], [max_tn_id + i for i in range(len(new_dist) - 2)], seg[-1:])\n        )\n\n        # Increase max index\n        max_tn_id += len(new_ids)\n\n        # Keep track of new nodes\n        new_nodes += [\n            [tn, pn] + [new_values[c][i] for c in num_cols + non_num_cols]\n            for i, (tn, pn) in enumerate(zip(new_ids[:-1], new_ids[1:]))\n        ]\n\n    if errors:\n        logger.warning(f\"{errors} ({errors/i:.0%}) segments skipped due to \" \"errors\")\n\n    # Add root node(s)\n    root = x.nodes.loc[\n        x.nodes.node_id.isin(utils.make_iterable(x.root)),\n        [\"node_id\", \"parent_id\"] + num_cols + non_num_cols,\n    ]\n    new_nodes += [list(r) for r in root.values]\n\n    # Generate new nodes dataframe\n    new_nodes = pd.DataFrame(\n        data=new_nodes, columns=[\"node_id\", \"parent_id\"] + num_cols + non_num_cols\n    )\n\n    # At this point, new node and parent IDs will be 64 bit integers and x/y/z columns will\n    # be float 64. We will convert them back to the original dtypes but we have to\n    # be careful with node &amp; parent IDs to avoid overflows if the original datatype\n    # can't accommodate the new IDs.\n\n    # Gather the original dtypes\n    dtypes = {\n        k: x.nodes[k].dtype for k in [\"node_id\", \"parent_id\"] + num_cols + non_num_cols\n    }\n\n    # Check for overflow\n    for col in (\"node_id\", \"parent_id\"):\n        # No need for checks if we're not changing the dtype\n        if new_nodes[col].dtype == dtypes[col]:\n            continue\n\n        # If there is an overflow downcast to smallest possible dtype\n        # N.B. we could also check for underflow but that's less likely\n        if new_nodes[col].max() &gt;= np.iinfo(np.int32).max:\n            new_nodes[col] = pd.to_nunmeric(new_nodes[col], downcast=\"integer\")\n            dtypes[col] = new_nodes[col].dtype  # Update dtype\n\n    # Now cast the rest\n    new_nodes = new_nodes.astype(dtypes, errors=\"ignore\")\n\n    # Remove duplicate nodes (branch points)\n    new_nodes = new_nodes[~new_nodes.node_id.duplicated()]\n\n    # Generate KDTree\n    tree = scipy.spatial.cKDTree(new_nodes[[\"x\", \"y\", \"z\"]].values)\n    # Map soma onto new nodes if required\n    # Note that if `._soma` is a soma detection function we can't tell\n    # how to deal with it. Ideally the new soma node will\n    # be automatically detected but it is possible, for example, that\n    # the radii of nodes have changed due to interpolation such that more\n    # than one soma is detected now. Also a \"label\" column in the node\n    # table would be lost at this point.\n    # We will go for the easy option which is to pin the soma at this point.\n    nodes = x.nodes.set_index(\"node_id\", inplace=False)\n    if np.any(getattr(x, \"soma\")):\n        soma_nodes = utils.make_iterable(x.soma)\n        old_pos = nodes.loc[soma_nodes, [\"x\", \"y\", \"z\"]].values\n\n        # Get nearest neighbours\n        dist, ix = tree.query(old_pos)\n        node_map = dict(zip(soma_nodes, new_nodes.node_id.values[ix]))\n\n        # Map back onto neuron\n        if utils.is_iterable(x.soma):\n            # Use _soma to avoid checks - the new nodes have not yet been\n            # assigned to the neuron!\n            x._soma = [node_map[n] for n in x.soma]\n        else:\n            x._soma = node_map[x.soma]\n    else:\n        # If `._soma` was (read: is) a function but it didn't detect anything in\n        # the original neurons, this makes sure that the resampled neuron\n        # doesn't have a soma either:\n        x.soma = None\n\n    # Map connectors back if necessary\n    if x.has_connectors:\n        # Get position of old synapse-bearing nodes\n        old_tn_position = nodes.loc[x.connectors.node_id, [\"x\", \"y\", \"z\"]].values\n\n        # Get nearest neighbours\n        dist, ix = tree.query(old_tn_position)\n\n        # Map back onto neuron\n        x.connectors[\"node_id\"] = new_nodes.node_id.values[ix]\n\n    # Map tags back if necessary\n    # Expects `tags` to be a dictionary {'tag': [node_id1, node_id2, ...]}\n    if x.has_tags and isinstance(x.tags, dict):\n        # Get nodes that need remapping\n        nodes_to_remap = list({n for l in x.tags.values() for n in l})\n\n        # Get position of old tag-bearing nodes\n        old_tn_position = nodes.loc[nodes_to_remap, [\"x\", \"y\", \"z\"]].values\n\n        # Get nearest neighbours\n        dist, ix = tree.query(old_tn_position)\n\n        # Map back onto tags\n        node_map = dict(zip(nodes_to_remap, new_nodes.node_id.values[ix]))\n        x.tags = {k: [node_map[n] for n in v] for k, v in x.tags.items()}\n\n    # Set nodes (avoid setting on copy warning)\n    x.nodes = new_nodes.copy()\n\n    # Clear and regenerate temporary attributes\n    x._clear_temp_attr()\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.rewire_skeleton","title":"<code>navis.rewire_skeleton</code>","text":"<p>Rewire neuron from graph.</p> <p>This function takes a graph representation of a neuron and rewires its node table accordingly. This is useful if we made changes to the graph (i.e. adding or removing edges) and want those to propagate to the node table.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Neuron to be rewired.\n</code></pre> <p> TYPE: <code>        TreeNeuron</code> </p> <code>g</code> <pre><code>    Graph to use for rewiring. Please note that directionality (if\n    present) is not taken into account. Nodes not included in the\n    graph will be disconnected (i.e. won't have a parent). Nodes\n    in the graph but not in the table are ignored!\n</code></pre> <p> TYPE: <code>        networkx.Graph</code> </p> <code>root</code> <pre><code>    Node ID for the new root. If not given, will try to use the\n    current root.\n</code></pre> <p> TYPE: <code>     int</code> DEFAULT: <code>None</code> </p> <code>inplace</code> <pre><code>    If True, will rewire the neuron inplace. If False, will return\n    a rewired copy of the neuron.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>TreeNeuron</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; n.n_trees\n1\n&gt;&gt;&gt; # Drop one edge from graph\n&gt;&gt;&gt; g = n.graph.copy()\n&gt;&gt;&gt; g.remove_edge(310, 309)\n&gt;&gt;&gt; # Rewire neuron\n&gt;&gt;&gt; n2 = navis.rewire_skeleton(n, g, inplace=False)\n&gt;&gt;&gt; n2.n_trees\n2\n</code></pre> Source code in <code>navis/graph/graph_utils.py</code> <pre><code>def rewire_skeleton(\n    x: \"core.TreeNeuron\", g: nx.Graph, root: Optional[id] = None, inplace: bool = False\n) -&gt; Optional[\"core.TreeNeuron\"]:\n    \"\"\"Rewire neuron from graph.\n\n    This function takes a graph representation of a neuron and rewires its\n    node table accordingly. This is useful if we made changes to the graph\n    (i.e. adding or removing edges) and want those to propagate to the node\n    table.\n\n    Parameters\n    ----------\n    x :         TreeNeuron\n                Neuron to be rewired.\n    g :         networkx.Graph\n                Graph to use for rewiring. Please note that directionality (if\n                present) is not taken into account. Nodes not included in the\n                graph will be disconnected (i.e. won't have a parent). Nodes\n                in the graph but not in the table are ignored!\n    root :      int\n                Node ID for the new root. If not given, will try to use the\n                current root.\n    inplace :   bool\n                If True, will rewire the neuron inplace. If False, will return\n                a rewired copy of the neuron.\n\n    Returns\n    -------\n    TreeNeuron\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; n.n_trees\n    1\n    &gt;&gt;&gt; # Drop one edge from graph\n    &gt;&gt;&gt; g = n.graph.copy()\n    &gt;&gt;&gt; g.remove_edge(310, 309)\n    &gt;&gt;&gt; # Rewire neuron\n    &gt;&gt;&gt; n2 = navis.rewire_skeleton(n, g, inplace=False)\n    &gt;&gt;&gt; n2.n_trees\n    2\n\n    \"\"\"\n    assert isinstance(x, core.TreeNeuron), f\"Expected TreeNeuron, got {type(x)}\"\n    assert isinstance(g, nx.Graph), f\"Expected networkx graph, got {type(g)}\"\n\n    if not inplace:\n        x = x.copy()\n\n    if g.is_directed():\n        g = g.to_undirected()\n\n    g = nx.minimum_spanning_tree(g, weight=\"weight\")\n\n    if not root:\n        root = x.root[0] if x.root[0] in g.nodes else next(iter(g.nodes))\n\n    # Generate tree for the main component\n    tree = nx.dfs_tree(g, source=root)\n\n    # Generate list of parents\n    lop = {e[1]: e[0] for e in tree.edges}\n\n    # If the graph has more than one connected component,\n    # the remaining components have arbitrary roots\n    if len(tree.edges) != len(g.edges):\n        for cc in nx.connected_components(g):\n            if root not in cc:\n                tree = nx.dfs_tree(g, source=cc.pop())\n                lop.update({e[1]: e[0] for e in tree.edges})\n\n    # Update parent IDs\n    x.nodes[\"parent_id\"] = x.nodes.node_id.map(lambda x: lop.get(x, -1))\n\n    x._clear_temp_attr()\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.scan_parquet","title":"<code>navis.scan_parquet</code>","text":"<p>Scan parquet file.</p> PARAMETER DESCRIPTION <code>file</code> <pre><code>            File to be scan.\n</code></pre> <p> TYPE: <code>             str</code> </p> RETURNS DESCRIPTION <code>pd.DataFrame</code> <p>Summary of file's content.</p> See Also <p><code>navis.write_parquet</code>                     Export neurons as parquet files. <code>navis.read_parquet</code>                     Read parquet file into neurons.</p> <p>Examples:</p> <p>See <code>navis.write_parquet</code> for examples.</p> Source code in <code>navis/io/pq_io.py</code> <pre><code>def scan_parquet(file: Union[str, Path]):\n    \"\"\"Scan parquet file.\n\n    Parameters\n    ----------\n    file :              str\n                        File to be scan.\n\n    Returns\n    -------\n    pd.DataFrame\n                        Summary of file's content.\n\n    See Also\n    --------\n    [`navis.write_parquet`][]\n                        Export neurons as parquet files.\n    [`navis.read_parquet`][]\n                        Read parquet file into neurons.\n\n    Examples\n    --------\n    See [`navis.write_parquet`][] for examples.\n\n    \"\"\"\n    try:\n        import pyarrow.parquet as pq\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"Reading parquet files requires the pyarrow library:\\n pip3 install pyarrow\"\n        )\n\n    f = Path(file).expanduser()\n    if not f.is_file():\n        raise FileNotFoundError(f'File \"{f}\" does not exist.')\n\n    metadata = pq.read_metadata(f)\n\n    try:\n        meta = {k.decode(): v.decode() for k, v in metadata.metadata.items()}\n    except BaseException:\n        logger.warning(f\"Unable to decode meta data for parquet file {f}\")\n        meta = {}\n\n    # Parse meta data\n    # The metadata is encoded as {\"{ID}_{PROPERTY}\": \"VALUE\"}\n    ids = [v for k, v in meta.items() if k.endswith(\":id\") and not k.startswith(\"_\")]\n    records = {i: {} for i in ids}\n    for k, v in meta.items():\n        # Skip private properties\n        if k.startswith(\"_\"):\n            continue\n        # Skip properties without a key\n        if \":\" not in k:\n            continue\n\n        id, prop = k.split(\":\")\n\n        if id not in records:  # there might be an \"ARROW:schema\" entry\n            continue\n\n        records[id][prop] = v\n\n    # Turn into DataFrame\n    df = pd.DataFrame.from_records(list(records.values()))\n\n    # Move ID column to front\n    ids = df[\"id\"]\n    df.drop(labels=[\"id\"], axis=1, inplace=True)\n    df.insert(0, \"id\", ids)\n\n    # The IDs are always stored as strings but the column might be integers\n    if \"neuron\" in metadata.schema.names:\n        schema = metadata.schema.column(metadata.schema.names.index(\"neuron\"))\n        if schema.physical_type.lower() in (\"int\", \"int64\", \"int32\", \"int16\", \"int8\"):\n            df[\"id\"] = df[\"id\"].astype(int)\n\n    return df\n</code></pre>"},{"location":"reference/navis/#navis.segment_analysis","title":"<code>navis.segment_analysis</code>","text":"<p>Calculate morphometric properties a neuron's segments.</p> <p>This currently includes Strahler index, length, distance to root and tortuosity. If the neuron has a radius will also calculate radius-based metrics such as volume.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            Neuron(s) to produce segment analysis for.\n</code></pre> <p> TYPE: <code>                TreeNeuron | MeshNeuron</code> </p> RETURNS DESCRIPTION <code>pandas.DataFrame</code> <p>Each row represents one linear segment between leafs/branch nodes (corresponds to <code>x.small_segments</code>):   - <code>strahler_index</code> is the Strahler Index of this segment   - <code>length</code> is the geodesic length of the segment   - <code>tortuosity</code> is the arc-chord ratio, i.e. the     ratio of <code>length</code> to the distance between its ends   - <code>root_dist</code> is the geodesic distance from the base     of the segment to the root If neuron node table has a <code>radius</code> column will also compute the following properties:   - <code>radius_mean</code>   - <code>radius_max</code>   - <code>radius_min</code>   - <code>volume</code></p> See Also <p><code>navis.strahler_index</code>                     This function calculates the Strahler index for every                     nodes/vertex in the neuron. <code>navis.tortuosity</code>                     This function calculates a tortuosity for the entire                     neuron.</p> <p>Examples:</p> <p>Run analysis on a single neuron:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1, kind='skeleton')\n&gt;&gt;&gt; n.reroot(n.soma, inplace=True)\n&gt;&gt;&gt; sa = navis.segment_analysis(n)\n&gt;&gt;&gt; sa.head()\n        length  tortuosity     root_dist  strahler_index  ...        volume\n0  1073.535053    1.151022    229.448586               1  ...  4.159788e+07\n1   112.682839    1.092659  10279.037511               1  ...  1.153095e+05\n2   214.124934    1.013030   9557.521377               1  ...  8.618440e+05\n3   159.585328    1.074575   9747.866968               1  ...  9.088157e+05\n4   229.448586    1.000000      0.000000               6  ...  3.206231e+07\n&gt;&gt;&gt; # Get per Strahler index means\n&gt;&gt;&gt; sa.groupby('strahler_index').mean()\n                    length  tortuosity     root_dist  ...        volume\nstrahler_index\n1               200.957415    1.111979  13889.593659  ...  8.363172e+05\n2               171.283617    1.047736  14167.056400  ...  1.061405e+06\n3               134.788019    1.023672  13409.920288  ...  9.212662e+05\n4               711.063734    1.016606  15768.886051  ...  7.304981e+06\n5               146.350195    1.000996   8443.345668  ...  2.262917e+06\n6               685.852990    1.056258   1881.594266  ...  1.067976e+07\n</code></pre> <p>Compare across neurons:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(5, kind='skeleton')\n&gt;&gt;&gt; sa = navis.segment_analysis(nl)\n&gt;&gt;&gt; # Note the `neuron` column when running the analysis on NeuronLists\n&gt;&gt;&gt; sa.head()\n       neuron       length  tortuosity     root_dist  ...        volume\n0  1734350788   112.682839    1.092659  11123.123978  ...  1.153095e+05\n1  1734350788   214.124934    1.013030  10401.607843  ...  8.618440e+05\n2  1734350788   159.585328    1.074575  10591.953435  ...  9.088157e+05\n3  1734350788  1073.535053    1.151022      0.000000  ...  4.159788e+07\n4  1734350788   260.538727    1.000000   1073.535053  ...  3.593405e+07\n&gt;&gt;&gt; # Get Strahler index counts for each neuron\n&gt;&gt;&gt; si_counts = sa.groupby(['neuron', 'strahler_index']).size().unstack()\n&gt;&gt;&gt; si_counts\nstrahler_index      1      2      3      4     5     6     7\nneuron\n722817260       656.0  336.0  167.0   74.0  32.0  24.0   NaN\n754534424       726.0  345.0  176.0  111.0  37.0   9.0  18.0\n754538881       642.0  344.0  149.0   88.0  21.0  24.0   NaN\n1734350788      618.0  338.0  138.0   74.0  38.0  11.0   NaN\n1734350908      761.0  363.0  203.0  116.0  20.0  33.0   NaN\n</code></pre> Source code in <code>navis/morpho/mmetrics.py</code> <pre><code>@utils.map_neuronlist_df(desc=\"Analyzing\", allow_parallel=True, reset_index=True)\n@utils.meshneuron_skeleton(method=\"pass_through\", reroot_soma=True)\ndef segment_analysis(x: \"core.NeuronObject\") -&gt; \"core.NeuronObject\":\n    \"\"\"Calculate morphometric properties a neuron's segments.\n\n    This currently includes Strahler index, length, distance to root and\n    tortuosity. If the neuron has a radius will also calculate radius-based\n    metrics such as volume.\n\n    Parameters\n    ----------\n    x :                 TreeNeuron | MeshNeuron\n                        Neuron(s) to produce segment analysis for.\n\n    Returns\n    -------\n    pandas.DataFrame\n                        Each row represents one linear segment between\n                        leafs/branch nodes (corresponds to `x.small_segments`):\n                          - `strahler_index` is the Strahler Index of this segment\n                          - `length` is the geodesic length of the segment\n                          - `tortuosity` is the arc-chord ratio, i.e. the\n                            ratio of `length` to the distance between its ends\n                          - `root_dist` is the geodesic distance from the base\n                            of the segment to the root\n                        If neuron node table has a `radius` column will also\n                        compute the following properties:\n                          - `radius_mean`\n                          - `radius_max`\n                          - `radius_min`\n                          - `volume`\n\n    See Also\n    --------\n    [`navis.strahler_index`][]\n                        This function calculates the Strahler index for every\n                        nodes/vertex in the neuron.\n    [`navis.tortuosity`][]\n                        This function calculates a tortuosity for the entire\n                        neuron.\n\n    Examples\n    --------\n\n    Run analysis on a single neuron:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1, kind='skeleton')\n    &gt;&gt;&gt; n.reroot(n.soma, inplace=True)\n    &gt;&gt;&gt; sa = navis.segment_analysis(n)\n    &gt;&gt;&gt; sa.head()                                               # doctest: +SKIP\n            length  tortuosity     root_dist  strahler_index  ...        volume\n    0  1073.535053    1.151022    229.448586               1  ...  4.159788e+07\n    1   112.682839    1.092659  10279.037511               1  ...  1.153095e+05\n    2   214.124934    1.013030   9557.521377               1  ...  8.618440e+05\n    3   159.585328    1.074575   9747.866968               1  ...  9.088157e+05\n    4   229.448586    1.000000      0.000000               6  ...  3.206231e+07\n    &gt;&gt;&gt; # Get per Strahler index means\n    &gt;&gt;&gt; sa.groupby('strahler_index').mean()                     # doctest: +SKIP\n                        length  tortuosity     root_dist  ...        volume\n    strahler_index\n    1               200.957415    1.111979  13889.593659  ...  8.363172e+05\n    2               171.283617    1.047736  14167.056400  ...  1.061405e+06\n    3               134.788019    1.023672  13409.920288  ...  9.212662e+05\n    4               711.063734    1.016606  15768.886051  ...  7.304981e+06\n    5               146.350195    1.000996   8443.345668  ...  2.262917e+06\n    6               685.852990    1.056258   1881.594266  ...  1.067976e+07\n\n    Compare across neurons:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(5, kind='skeleton')\n    &gt;&gt;&gt; sa = navis.segment_analysis(nl)\n    &gt;&gt;&gt; # Note the `neuron` column when running the analysis on NeuronLists\n    &gt;&gt;&gt; sa.head()                                               # doctest: +SKIP\n           neuron       length  tortuosity     root_dist  ...        volume\n    0  1734350788   112.682839    1.092659  11123.123978  ...  1.153095e+05\n    1  1734350788   214.124934    1.013030  10401.607843  ...  8.618440e+05\n    2  1734350788   159.585328    1.074575  10591.953435  ...  9.088157e+05\n    3  1734350788  1073.535053    1.151022      0.000000  ...  4.159788e+07\n    4  1734350788   260.538727    1.000000   1073.535053  ...  3.593405e+07\n    &gt;&gt;&gt; # Get Strahler index counts for each neuron\n    &gt;&gt;&gt; si_counts = sa.groupby(['neuron', 'strahler_index']).size().unstack()\n    &gt;&gt;&gt; si_counts                                               # doctest: +SKIP\n    strahler_index      1      2      3      4     5     6     7\n    neuron\n    722817260       656.0  336.0  167.0   74.0  32.0  24.0   NaN\n    754534424       726.0  345.0  176.0  111.0  37.0   9.0  18.0\n    754538881       642.0  344.0  149.0   88.0  21.0  24.0   NaN\n    1734350788      618.0  338.0  138.0   74.0  38.0  11.0   NaN\n    1734350908      761.0  363.0  203.0  116.0  20.0  33.0   NaN\n\n    \"\"\"\n    utils.eval_param(x, name=\"x\", allowed_types=(core.TreeNeuron,))\n\n    if \"strahler_index\" not in x.nodes:\n        strahler_index(x)\n\n    # Get small segments for this neuron\n    segs = graph._break_segments(x)\n\n    # For each segment get the SI\n    nodes = x.nodes.set_index(\"node_id\")\n    SI = nodes.loc[[s[0] for s in segs], \"strahler_index\"].values\n\n    # Get segment lengths\n    seg_lengths = np.array([graph.segment_length(x, s) for s in segs])\n\n    # Get tortuosity\n    start = nodes.loc[[s[0] for s in segs], [\"x\", \"y\", \"z\"]].values\n    end = nodes.loc[[s[-1] for s in segs], [\"x\", \"y\", \"z\"]].values\n    L = np.sqrt(((start - end) ** 2).sum(axis=1))\n    tort = seg_lengths / L\n\n    # Get distance from root\n    root_dists_dict = graph.dist_to_root(x, weight=\"weight\")\n    root_dists = np.array([root_dists_dict[s[-1]] for s in segs])\n\n    # Compile results\n    res = pd.DataFrame()\n    res[\"length\"] = seg_lengths\n    res[\"tortuosity\"] = tort\n    res[\"root_dist\"] = root_dists\n    res[\"strahler_index\"] = SI\n\n    if \"radius\" in nodes:\n        # Generate radius dict\n        radii = nodes.radius.to_dict()\n\n        seg_radii = [[radii.get(n, 0) for n in s] for s in segs]\n        res[\"radius_mean\"] = [np.nanmean(s) for s in seg_radii]\n        res[\"radius_min\"] = [np.nanmin(s) for s in seg_radii]\n        res[\"radius_max\"] = [np.nanmax(s) for s in seg_radii]\n\n        # Get radii for each cylinder\n        r1 = nodes.index.map(radii).values\n        r2 = nodes.parent_id.map(radii)  # Note we keep the Series here to avoid \"read-only\" errors when trying to replace NaNs\n        r2.iloc[np.isnan(r2)] = 0\n        r2 = r2.values\n\n        # Get the height for each node -&gt; parent cylinder\n        h = parent_dist(x, root_dist=0)\n\n        # Radii for top and bottom of tapered cylinder\n        vols = 1 / 3 * np.pi * (r1**2 + r1 * r2 + r2**2) * h\n        vols_dict = dict(zip(nodes.index.values, vols))\n\n        # For each segment get the volume\n        res[\"volume\"] = [np.nansum([vols_dict.get(n, 0) for n in s[:-1]]) for s in segs]\n\n    return res\n</code></pre>"},{"location":"reference/navis/#navis.segment_length","title":"<code>navis.segment_length</code>","text":"<p>Get length of a linear segment.</p> <p>This function is superfast but has no checks - you must provide a valid segment.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Neuron to which this segment belongs.\n</code></pre> <p> TYPE: <code>        TreeNeuron</code> </p> <code>segment</code> <pre><code>    Linear segment as list of node IDs ordered child-&gt;parent.\n</code></pre> <p> TYPE: <code>  list of ints</code> </p> RETURNS DESCRIPTION <code>length</code> <p> TYPE: <code>float</code> </p> See Also <p><code>navis.dist_between</code>     If you only know start and end points of the segment.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; l = navis.segment_length(n, n.segments[0])\n&gt;&gt;&gt; round(l)\n56356\n</code></pre> Source code in <code>navis/graph/graph_utils.py</code> <pre><code>@utils.lock_neuron\ndef segment_length(x: \"core.TreeNeuron\", segment: List[int]) -&gt; float:\n    \"\"\"Get length of a linear segment.\n\n    This function is superfast but has no checks - you must provide a\n    valid segment.\n\n    Parameters\n    ----------\n    x :         TreeNeuron\n                Neuron to which this segment belongs.\n    segment :   list of ints\n                Linear segment as list of node IDs ordered child-&gt;parent.\n\n    Returns\n    -------\n    length :    float\n\n    See Also\n    --------\n    [`navis.dist_between`][]\n        If you only know start and end points of the segment.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; l = navis.segment_length(n, n.segments[0])\n    &gt;&gt;&gt; round(l)\n    56356\n\n    \"\"\"\n    if not isinstance(x, core.TreeNeuron):\n        raise ValueError(f'Unable to process data of type \"{type(x)}\"')\n\n    # Get graph once to avoid overhead from validation - do NOT change this\n    graph = x.graph\n    dist = np.array(\n        [graph.edges[(c, p)][\"weight\"] for c, p in zip(segment[:-1], segment[1:])]\n    )\n    return sum(dist)\n</code></pre>"},{"location":"reference/navis/#navis.segregation_index","title":"<code>navis.segregation_index</code>","text":"<p>Calculate segregation index (SI).</p> <p>The segregation index as established by Schneider-Mizell et al. (eLife, 2016) is a measure for how polarized a neuron is. SI of 1 indicates total segregation of inputs and outputs into dendrites and axon, respectively. SI of 0 indicates homogeneous distribution.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            Neuron to calculate segregation index (SI) for. If a\n            NeuronList, will assume that it contains\n            fragments (e.g. from axon/ dendrite splits) of a\n            single neuron. If list, must be records containing\n            number of pre- and postsynapses for each fragment::\n\n                [{'presynapses': 10, 'postsynapses': 320},\n                 {'presynapses': 103, 'postsynapses': 21}]\n</code></pre> <p> TYPE: <code>                NeuronList | list</code> </p> Notes <p>From Schneider-Mizell et al. (2016): \"Note that even a modest amount of mixture (e.g. axo-axonic inputs) corresponds to values near H = 0.5\u20130.6 (Figure 7\u2014figure supplement 1). We consider an unsegregated neuron (H \u00a1 0.05) to be purely dendritic due to their anatomical similarity with the dendritic domains of those segregated neurons that have dendritic outputs.\"</p> RETURNS DESCRIPTION <code>H</code> <p>Segregation Index (SI).</p> <p> TYPE: <code>float</code> </p> Source code in <code>navis/morpho/mmetrics.py</code> <pre><code>def segregation_index(x: Union[\"core.NeuronObject\", dict]) -&gt; float:\n    \"\"\"Calculate segregation index (SI).\n\n    The segregation index as established by Schneider-Mizell et al. (eLife,\n    2016) is a measure for how polarized a neuron is. SI of 1 indicates total\n    segregation of inputs and outputs into dendrites and axon, respectively.\n    SI of 0 indicates homogeneous distribution.\n\n    Parameters\n    ----------\n    x :                 NeuronList | list\n                        Neuron to calculate segregation index (SI) for. If a\n                        NeuronList, will assume that it contains\n                        fragments (e.g. from axon/ dendrite splits) of a\n                        single neuron. If list, must be records containing\n                        number of pre- and postsynapses for each fragment::\n\n                            [{'presynapses': 10, 'postsynapses': 320},\n                             {'presynapses': 103, 'postsynapses': 21}]\n\n    Notes\n    -----\n    From Schneider-Mizell et al. (2016): \"Note that even a modest amount of\n    mixture (e.g. axo-axonic inputs) corresponds to values near H = 0.5\u20130.6\n    (Figure 7\u2014figure supplement 1). We consider an unsegregated neuron\n    (H \u00a1 0.05) to be purely dendritic due to their anatomical similarity with\n    the dendritic domains of those segregated neurons that have dendritic\n    outputs.\"\n\n    Returns\n    -------\n    H :                 float\n                        Segregation Index (SI).\n\n    \"\"\"\n    if not isinstance(x, (core.NeuronList, list)):\n        raise ValueError(f'Expected NeuronList or list got \"{type(x)}\"')\n\n    if isinstance(x, core.NeuronList) and len(x) &lt;= 1:\n        raise ValueError(f\"Expected multiple neurons, got {len(x)}\")\n\n    # Turn NeuronList into records\n    if isinstance(x, core.NeuronList):\n        x = [\n            {\"presynapses\": n.n_presynapses, \"postsynapses\": n.n_postsynapses}\n            for n in x\n        ]\n\n    # Extract the total number of pre- and postsynapses\n    total_pre = sum([n[\"presynapses\"] for n in x])\n    total_post = sum([n[\"postsynapses\"] for n in x])\n    total_syn = total_pre + total_post\n\n    # Calculate entropy for each fragment\n    entropy = []\n    for n in x:\n        n[\"total_syn\"] = n[\"postsynapses\"] + n[\"presynapses\"]\n\n        # This is to avoid warnings\n        if n[\"total_syn\"]:\n            p = n[\"postsynapses\"] / n[\"total_syn\"]\n        else:\n            p = float(\"inf\")\n\n        if 0 &lt; p &lt; 1:\n            S = -(p * math.log(p) + (1 - p) * math.log(1 - p))\n        else:\n            S = 0\n\n        entropy.append(S)\n\n    # Calc entropy between fragments\n    S = 1 / total_syn * sum([e * n[\"total_syn\"] for n, e in zip(x, entropy)])\n\n    # Normalize to entropy in whole neuron\n    p_norm = total_post / total_syn\n    if 0 &lt; p_norm &lt; 1:\n        S_norm = -(p_norm * math.log(p_norm) + (1 - p_norm) * math.log(1 - p_norm))\n        H = 1 - S / S_norm\n    else:\n        S_norm = 0\n        H = 0\n\n    return H\n</code></pre>"},{"location":"reference/navis/#navis.set_default_connector_colors","title":"<code>navis.set_default_connector_colors</code>","text":"<p>Set/update default connector colors.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    New default connector colors. Can be:\n\n       {'cn_label': (r, g, b), ..}\n       {'cn_label': {'color': (r, g, b)}, ..}\n</code></pre> <p> TYPE: <code>        dict</code> </p> Source code in <code>navis/utils/misc.py</code> <pre><code>def set_default_connector_colors(x: Union[List[tuple], Dict[str, tuple]]\n                                 ) -&gt; None:\n    \"\"\"Set/update default connector colors.\n\n    Parameters\n    ----------\n    x :         dict\n                New default connector colors. Can be:\n\n                   {'cn_label': (r, g, b), ..}\n                   {'cn_label': {'color': (r, g, b)}, ..}\n\n    \"\"\"\n    if not isinstance(x, dict):\n        raise TypeError(f'Expect dict, got \"{type(x)}\"')\n\n    for k, v in x.items():\n        if isinstance(v, dict):\n            config.default_connector_colors[k].update(v)\n        else:\n            config.default_connector_colors[k]['color'] = v\n\n    return\n</code></pre>"},{"location":"reference/navis/#navis.set_loggers","title":"<code>navis.set_loggers</code>","text":"<p>Set levels for all associated module loggers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from navis.utils import set_loggers\n&gt;&gt;&gt; from navis import config\n&gt;&gt;&gt; # Get current level\n&gt;&gt;&gt; lvl = config.logger.level\n&gt;&gt;&gt; # Set new level\n&gt;&gt;&gt; set_loggers('INFO')\n&gt;&gt;&gt; # Revert to old level\n&gt;&gt;&gt; set_loggers(lvl)\n</code></pre> Source code in <code>navis/utils/misc.py</code> <pre><code>def set_loggers(level: str = 'INFO'):\n    \"\"\"Set levels for all associated module loggers.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from navis.utils import set_loggers\n    &gt;&gt;&gt; from navis import config\n    &gt;&gt;&gt; # Get current level\n    &gt;&gt;&gt; lvl = config.logger.level\n    &gt;&gt;&gt; # Set new level\n    &gt;&gt;&gt; set_loggers('INFO')\n    &gt;&gt;&gt; # Revert to old level\n    &gt;&gt;&gt; set_loggers(lvl)\n\n    \"\"\"\n    config.logger.setLevel(level)\n</code></pre>"},{"location":"reference/navis/#navis.set_pbars","title":"<code>navis.set_pbars</code>","text":"<p>Set global progress bar behaviors.</p> PARAMETER DESCRIPTION <code>hide</code> <pre><code>    Set to True to hide all progress bars.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>None</code> </p> <code>leave</code> <pre><code>    Set to False to clear progress bars after they have finished.\n</code></pre> <p> TYPE: <code>    bool</code> DEFAULT: <code>None</code> </p> <code>jupyter</code> <pre><code>    Set to False to force using of classic tqdm even if in\n    Jupyter environment.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Nothing</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from navis.utils import set_pbars\n&gt;&gt;&gt; # Hide progress bars after finishing\n&gt;&gt;&gt; set_pbars(leave=False)\n&gt;&gt;&gt; # Never show progress bars\n&gt;&gt;&gt; set_pbars(hide=True)\n&gt;&gt;&gt; # Never use Jupyter widget progress bars\n&gt;&gt;&gt; set_pbars(jupyter=False)\n</code></pre> Source code in <code>navis/utils/misc.py</code> <pre><code>def set_pbars(hide: Optional[bool] = None,\n              leave: Optional[bool] = None,\n              jupyter: Optional[bool] = None) -&gt; None:\n    \"\"\"Set global progress bar behaviors.\n\n    Parameters\n    ----------\n    hide :      bool, optional\n                Set to True to hide all progress bars.\n    leave :     bool, optional\n                Set to False to clear progress bars after they have finished.\n    jupyter :   bool, optional\n                Set to False to force using of classic tqdm even if in\n                Jupyter environment.\n\n    Returns\n    -------\n    Nothing\n\n    Examples\n    --------\n    &gt;&gt;&gt; from navis.utils import set_pbars\n    &gt;&gt;&gt; # Hide progress bars after finishing\n    &gt;&gt;&gt; set_pbars(leave=False)\n    &gt;&gt;&gt; # Never show progress bars\n    &gt;&gt;&gt; set_pbars(hide=True)\n    &gt;&gt;&gt; # Never use Jupyter widget progress bars\n    &gt;&gt;&gt; set_pbars(jupyter=False)\n\n    \"\"\"\n    if isinstance(hide, bool):\n        config.pbar_hide = hide\n\n    if isinstance(leave, bool):\n        config.pbar_leave = leave\n\n    if isinstance(jupyter, bool):\n        if jupyter:\n            if not is_jupyter():\n                logger.error('No Jupyter environment detected.')\n            else:\n                config.tqdm = config.tqdm_notebook\n                config.trange = config.trange_notebook\n        else:\n            config.tqdm = config.tqdm_classic\n            config.trange = config.trange_classic\n\n    return\n</code></pre>"},{"location":"reference/navis/#navis.sholl_analysis","title":"<code>navis.sholl_analysis</code>","text":"<p>Run Sholl analysis for given neuron(s).</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Neuron to analyze. If MeshNeuron, will generate and\n    use a skeleton representation.\n</code></pre> <p> TYPE: <code>        TreeNeuron | MeshNeuron | NeuronList</code> </p> <code>radii</code> <pre><code>    If integer, will produce N evenly space radii covering the\n    distance between the center and the most distal node.\n    Alternatively, you can also provide a list of radii to check.\n    If `x` is multiple neurons, must provide a list of `radii`!\n</code></pre> <p> TYPE: <code>    int | list-like</code> DEFAULT: <code>10</code> </p> <code>center</code> <pre><code>    The center to use for Sholl analysis:\n        - \"centermass\" (default) uses the mean across nodes positions\n        - \"root\" uses the current root of the skeleton\n        - \"soma\" uses the neuron's soma (will raise error if no soma)\n        - int is interpreted as a node ID\n        - (3, ) list-like is interpreted as x/y/z coordinate\n</code></pre> <p> TYPE: <code>   \"centermass\" | \"root\" | \"soma\" | int | list-like</code> DEFAULT: <code>'centermass'</code> </p> <code>geodesic</code> <pre><code>    If True, will use geodesic (along-the-arbor) instead of\n    Euclidean distances. This does not work if center is an x/y/z\n    coordinate.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>results</code> <p>Results contain, for each spherical bin, the number of intersections, cable length and number of branch points.</p> <p> TYPE: <code>pd.DataFrame</code> </p> References <p>See the Wikipedia article for a brief explanation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1, kind='skeleton')\n&gt;&gt;&gt; # Sholl analysis\n&gt;&gt;&gt; sha = navis.sholl_analysis(n, radii=100, center='root')\n&gt;&gt;&gt; # Plot distributions\n&gt;&gt;&gt; ax = sha.plot()\n&gt;&gt;&gt; # Sholl analysis but using geodesic distance\n&gt;&gt;&gt; sha = navis.sholl_analysis(n, radii=100, center='root', geodesic=True)\n</code></pre> Source code in <code>navis/morpho/mmetrics.py</code> <pre><code>@utils.map_neuronlist(desc=\"Sholl analysis\", allow_parallel=True)\ndef sholl_analysis(\n    x: \"core.NeuronObject\",\n    radii: Union[int, list] = 10,\n    center: Union[Literal[\"root\"], Literal[\"soma\"], list, int] = \"centermass\",\n    geodesic=False,\n) -&gt; Union[float, Sequence[float], pd.DataFrame]:\n    \"\"\"Run Sholl analysis for given neuron(s).\n\n    Parameters\n    ----------\n    x :         TreeNeuron | MeshNeuron | NeuronList\n                Neuron to analyze. If MeshNeuron, will generate and\n                use a skeleton representation.\n    radii :     int | list-like\n                If integer, will produce N evenly space radii covering the\n                distance between the center and the most distal node.\n                Alternatively, you can also provide a list of radii to check.\n                If `x` is multiple neurons, must provide a list of `radii`!\n    center :    \"centermass\" | \"root\" | \"soma\" | int | list-like\n                The center to use for Sholl analysis:\n                    - \"centermass\" (default) uses the mean across nodes positions\n                    - \"root\" uses the current root of the skeleton\n                    - \"soma\" uses the neuron's soma (will raise error if no soma)\n                    - int is interpreted as a node ID\n                    - (3, ) list-like is interpreted as x/y/z coordinate\n    geodesic :  bool\n                If True, will use geodesic (along-the-arbor) instead of\n                Euclidean distances. This does not work if center is an x/y/z\n                coordinate.\n\n    Returns\n    -------\n    results :   pd.DataFrame\n                Results contain, for each spherical bin, the number of\n                intersections, cable length and number of branch points.\n\n    References\n    ----------\n    See the [Wikipedia article](https://en.wikipedia.org/wiki/Sholl_analysis)\n    for a brief explanation.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1, kind='skeleton')\n    &gt;&gt;&gt; # Sholl analysis\n    &gt;&gt;&gt; sha = navis.sholl_analysis(n, radii=100, center='root')\n    &gt;&gt;&gt; # Plot distributions\n    &gt;&gt;&gt; ax = sha.plot()                                         # doctest: +SKIP\n    &gt;&gt;&gt; # Sholl analysis but using geodesic distance\n    &gt;&gt;&gt; sha = navis.sholl_analysis(n, radii=100, center='root', geodesic=True)\n\n    \"\"\"\n    # Use MeshNeuron's skeleton\n    if isinstance(x, core.MeshNeuron):\n        x = x.skeleton\n\n    if not isinstance(x, core.TreeNeuron):\n        raise TypeError(f\"Expected TreeNeuron or MeshNeuron(s), got {type(x)}\")\n\n    if geodesic and len(x.root) &gt; 1:\n        raise ValueError(\n            \"Unable to use `geodesic=True` with fragmented \"\n            \"neurons. Use `navis.heal_fragmented_neuron` first.\"\n        )\n\n    if center == \"soma\" and not x.has_soma:\n        raise ValueError(f\"Neuron {x.id} has no soma.\")\n    elif utils.is_iterable(center):\n        center = np.asarray(center)\n        if center.ndim != 1 or len(center) != 3:\n            raise ValueError(\n                \"`center` must be (3, ) list-like when providing \"\n                f\"a coordinate. Got {center.shape}\"\n            )\n        if geodesic:\n            raise ValueError(\n                \"Must not provide a `center` as coordinate when \" \"geodesic=True\"\n            )\n    elif center == \"root\" and len(x.root) &gt; 1:\n        raise ValueError(\n            f\"Neuron {x.id} has multiple roots. Please specify \"\n            \"which node/coordinate to use as center.\"\n        )\n\n    if center == \"centermass\":\n        center = x.nodes[[\"x\", \"y\", \"z\"]].mean(axis=0).values\n\n    # Calculate distances for each node\n    nodes = x.nodes.set_index(\"node_id\").copy()\n    if not geodesic:\n        if isinstance(center, int):\n            if center not in nodes.index.values:\n                raise ValueError(f\"{center} is not a valid node ID.\")\n\n            center = nodes.loc[center, [\"x\", \"y\", \"z\"]].values\n        elif center == \"soma\":\n            center = nodes.loc[utils.make_iterable(x.soma)[0], [\"x\", \"y\", \"z\"]].values\n        elif center == \"root\":\n            center = nodes.loc[utils.make_iterable(x.root)[0], [\"x\", \"y\", \"z\"]].values\n        center = center.astype(float)\n\n        nodes[\"dist\"] = np.sqrt(\n            ((x.nodes[[\"x\", \"y\", \"z\"]].values - center) ** 2).sum(axis=1)\n        )\n    else:\n        if center == \"soma\":\n            center = x.soma[0]\n        elif center == \"root\":\n            center = x.root[0]\n\n        nodes[\"dist\"] = graph.geodesic_matrix(x, from_=center)[\n            x.nodes.node_id.values\n        ].values[0]\n\n    not_root = nodes.parent_id &gt;= 0\n    dists = nodes.loc[not_root, \"dist\"].values\n    pdists = nodes.loc[nodes[not_root].parent_id.values, \"dist\"].values\n    le = parent_dist(x)[not_root]\n    ty = nodes.loc[not_root, \"type\"].values\n\n    # Generate radii for the Sholl spheres\n    if isinstance(radii, int):\n        radii = np.linspace(0, dists.max(), radii + 1)\n    else:\n        if radii[0] != 0:\n            radii = np.insert(radii, 0, 0)\n\n    data = []\n    for i in range(1, len(radii)):\n        # Find the number of crossings\n        crossings = ((dists &lt;= radii[i]) &amp; (pdists &gt; radii[i])).sum()\n\n        # Get the (approximate) cable length in this sphere\n        this_sphere = (dists &gt; radii[i - 1]) &amp; (dists &lt; radii[i])\n        cable = le[this_sphere].sum()\n\n        # The number of branch points in this sphere\n        n_branchpoints = (ty[this_sphere] == \"branch\").sum()\n\n        data.append([radii[i], crossings, cable, n_branchpoints])\n\n    return pd.DataFrame(\n        data, columns=[\"radius\", \"intersections\", \"cable_length\", \"branch_points\"]\n    ).set_index(\"radius\")\n</code></pre>"},{"location":"reference/navis/#navis.simplify_mesh","title":"<code>navis.simplify_mesh</code>","text":"<p>Simplify meshes (TriMesh, MeshNeuron, Volume).</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Mesh(es) to simplify.\n</code></pre> <p> TYPE: <code>        navis.MeshNeuron/List | navis.Volume | trimesh.Trimesh</code> </p> <code>F</code> <pre><code>    Determines how much the mesh is simplified:\n    Floats (0-1) are interpreted as ratio. For example, an F of\n    0.5 will reduce the number of faces to 50%.\n    Integers (&gt;1) are intepreted as target face count. For example,\n    an F of 5000 will attempt to reduce the number of faces to 5000.\n</code></pre> <p> TYPE: <code>        float | int</code> </p> <code>backend</code> <pre><code>    Which backend to use. Currenly we support `pyfqmr`, `open3d`,\n    Blender 3D and `pymeshlab`.\n</code></pre> <p> TYPE: <code>  \"auto\" | \"pyfqmr\" | \"open3d\" | \"blender\" | \"pymeshlab\"</code> DEFAULT: <code>'auto'</code> </p> <code>inplace</code> <pre><code>    If True, will perform simplication on `x`. If False, will\n    simplify and return a copy.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <pre><code>    Keyword arguments are passed through to the respective backend's\n    functions (see below).\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>simplified</code> <p>Simplified object.</p> See Also <p><code>navis.downsample_neuron</code>             Downsample all kinds of neurons. <code>navis.meshes.simplify_mesh_fqmr</code>             pyfqmr implementation for mesh simplification. <code>navis.meshes.simplify_mesh_open3d</code>             Open3D implementation for mesh simplification. <code>navis.meshes.simplify_mesh_pyml</code>             PyMeshLab implementation for mesh simplification. <code>navis.meshes.simplify_mesh_blender</code>             Blender 3D implementation for mesh simplification.</p> Source code in <code>navis/meshes/operations.py</code> <pre><code>@utils.map_neuronlist(desc='Simplifying', allow_parallel=True)\ndef simplify_mesh(x, F, backend='auto', inplace=False, **kwargs):\n    \"\"\"Simplify meshes (TriMesh, MeshNeuron, Volume).\n\n    Parameters\n    ----------\n    x :         navis.MeshNeuron/List | navis.Volume | trimesh.Trimesh\n                Mesh(es) to simplify.\n    F :         float | int\n                Determines how much the mesh is simplified:\n                Floats (0-1) are interpreted as ratio. For example, an F of\n                0.5 will reduce the number of faces to 50%.\n                Integers (&gt;1) are intepreted as target face count. For example,\n                an F of 5000 will attempt to reduce the number of faces to 5000.\n    backend :   \"auto\" | \"pyfqmr\" | \"open3d\" | \"blender\" | \"pymeshlab\"\n                Which backend to use. Currenly we support `pyfqmr`, `open3d`,\n                Blender 3D and `pymeshlab`.\n    inplace :   bool\n                If True, will perform simplication on `x`. If False, will\n                simplify and return a copy.\n    **kwargs\n                Keyword arguments are passed through to the respective backend's\n                functions (see below).\n\n    Returns\n    -------\n    simplified\n                Simplified object.\n\n    See Also\n    --------\n    [`navis.downsample_neuron`][]\n                Downsample all kinds of neurons.\n    [`navis.meshes.simplify_mesh_fqmr`][]\n                pyfqmr implementation for mesh simplification.\n    [`navis.meshes.simplify_mesh_open3d`][]\n                Open3D implementation for mesh simplification.\n    [`navis.meshes.simplify_mesh_pyml`][]\n                PyMeshLab implementation for mesh simplification.\n    [`navis.meshes.simplify_mesh_blender`][]\n                Blender 3D implementation for mesh simplification.\n\n    \"\"\"\n    if not isinstance(backend, str):\n        raise TypeError(f'`backend` must be string, got \"{type(backend)}\"')\n\n    backend = backend.lower()\n    backends = available_backends(only_first=backend == 'auto')\n\n    if not backends:\n        raise BaseException(\"None of the supported backends appear to be \"\n                            \"available. Please install either `pyfqmr`, `open3d` \"\n                            \"or `pymeshlab` via `pip`, or install Blender 3D.\")\n    elif backend == 'auto':\n        backend = backends[0]\n    elif backend not in backends:\n        raise ValueError(f'Backend \"{backend}\" appears to not be available. '\n                         'Please choose one of the available backends: '\n                         f'{\", \".join(backends)}')\n\n    if not inplace:\n        x = x.copy()\n\n    if backend == 'pyfqmr':\n        # This expects a target face count\n        if F &lt; 1:\n            F = F * len(x.faces)\n        _ = simplify_mesh_fqmr(x, F=F, inplace=True, **kwargs)\n    elif backend == 'open3d':\n        # This expects a target face count\n        if F &lt; 1:\n            F = F * len(x.faces)\n        _ = simplify_mesh_open3d(x, F=F, inplace=True, **kwargs)\n    elif backend == 'blender':\n        # This expects a ratio\n        if F &gt; 1:\n            F = F / len(x.faces)\n        _ = simplify_mesh_blender(x, F=F, inplace=True)\n    elif backend == 'pymeshlab':\n        # This expects a ratio\n        if F &gt; 1:\n            F = F / len(x.faces)\n        _ = simplify_mesh_pyml(x, F=F, inplace=True, **kwargs)\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.skeletonize","title":"<code>navis.skeletonize</code>","text":"<p>Turn neuron into skeleton.</p> <p>Currently, we can only skeletonize meshes, dotprops and point clouds but are looking into ways to also do it for <code>VoxelNeurons</code>.</p> <p>For meshes, this function is a thin-wrapper for <code>skeletor</code>. It uses sensible defaults for neurons but if you want to fine-tune your skeletons you should look into using <code>skeletor</code> directly.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Mesh(es) to skeletonize. Note that the quality of the results\n    very much depends on the mesh, so it might be worth doing some\n    pre-processing (see below).\n</code></pre> <p> TYPE: <code>        MeshNeuron | trimesh.Trimesh | Dotprops</code> </p> <code>**kwargs</code> <pre><code>    Keyword arguments are passed through to the respective\n    converters:\n        - meshes: [`navis.conversion.mesh2skeleton`][]\n        - dotprops and point clouds: [`navis.conversion.points2skeleton`][]\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>skeleton</code> <p>For meshes, this has a <code>.vertex_map</code> attribute that maps each vertex in the input mesh to a skeleton node ID.</p> <p> TYPE: <code>navis.TreeNeuron</code> </p> See Also <p><code>navis.drop_fluff</code>             Use this if your mesh has lots of tiny free floating bits to             reduce noise and speed up skeletonization.</p> <p>Examples:</p>"},{"location":"reference/navis/#navis.skeletonize--skeletonize-a-mesh","title":"Skeletonize a mesh","text":"<pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; # Get a mesh neuron\n&gt;&gt;&gt; n = navis.example_neurons(1, kind='mesh')\n&gt;&gt;&gt; # Convert to skeleton\n&gt;&gt;&gt; sk = navis.skeletonize(n)\n&gt;&gt;&gt; # Mesh vertex indices to node IDs map\n&gt;&gt;&gt; sk.vertex_map\narray([938, 990, 990, ...,  39, 234, 234])\n</code></pre>"},{"location":"reference/navis/#navis.skeletonize--skeletonize-dotprops-ie-point-clouds","title":"Skeletonize dotprops (i.e. point-clouds)","text":"<pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; # Get a skeleton and turn into dotprops\n&gt;&gt;&gt; dp = navis.make_dotprops(navis.example_neurons(1))\n&gt;&gt;&gt; # Turn back into a skeleton\n&gt;&gt;&gt; sk = navis.skeletonize(dp)\n</code></pre> Source code in <code>navis/conversion/wrappers.py</code> <pre><code>@utils.map_neuronlist(desc='Skeletonizing', allow_parallel=True)\ndef skeletonize(x: Union['core.MeshNeuron', 'core.Dotprops', np.ndarray],\n                **kwargs):\n    \"\"\"Turn neuron into skeleton.\n\n    Currently, we can only skeletonize meshes, dotprops and point clouds but\n    are looking into ways to also do it for `VoxelNeurons`.\n\n    For meshes, this function is a thin-wrapper for `skeletor`. It uses sensible\n    defaults for neurons but if you want to fine-tune your skeletons you should\n    look into using `skeletor` directly.\n\n    Parameters\n    ----------\n    x :         MeshNeuron | trimesh.Trimesh | Dotprops\n                Mesh(es) to skeletonize. Note that the quality of the results\n                very much depends on the mesh, so it might be worth doing some\n                pre-processing (see below).\n    **kwargs\n                Keyword arguments are passed through to the respective\n                converters:\n                    - meshes: [`navis.conversion.mesh2skeleton`][]\n                    - dotprops and point clouds: [`navis.conversion.points2skeleton`][]\n\n    Returns\n    -------\n    skeleton :  navis.TreeNeuron\n                For meshes, this has a `.vertex_map` attribute that maps each\n                vertex in the input mesh to a skeleton node ID.\n\n    See Also\n    --------\n    [`navis.drop_fluff`][]\n                Use this if your mesh has lots of tiny free floating bits to\n                reduce noise and speed up skeletonization.\n\n    Examples\n    --------\n    # Skeletonize a mesh\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; # Get a mesh neuron\n    &gt;&gt;&gt; n = navis.example_neurons(1, kind='mesh')\n    &gt;&gt;&gt; # Convert to skeleton\n    &gt;&gt;&gt; sk = navis.skeletonize(n)\n    &gt;&gt;&gt; # Mesh vertex indices to node IDs map\n    &gt;&gt;&gt; sk.vertex_map                                           # doctest: +SKIP\n    array([938, 990, 990, ...,  39, 234, 234])\n\n    # Skeletonize dotprops (i.e. point-clouds)\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; # Get a skeleton and turn into dotprops\n    &gt;&gt;&gt; dp = navis.make_dotprops(navis.example_neurons(1))\n    &gt;&gt;&gt; # Turn back into a skeleton\n    &gt;&gt;&gt; sk = navis.skeletonize(dp)\n\n    \"\"\"\n    if isinstance(x, (core.MeshNeuron, tm.Trimesh)):\n        return mesh2skeleton(x, **kwargs)\n    elif isinstance(x, (core.Dotprops, )):\n        sk = points2skeleton(x.points, **kwargs)\n        for attr in ('id', 'units', 'name'):\n            if hasattr(x, attr):\n                setattr(sk, attr, getattr(x, attr))\n        return sk\n    elif isinstance(x, np.ndarray):\n        return points2skeleton(x.points, **kwargs)\n\n    raise TypeError(f'Unable to skeletonize data of type {type(x)}')\n</code></pre>"},{"location":"reference/navis/#navis.smooth_mesh","title":"<code>navis.smooth_mesh</code>","text":"<p>Smooth meshes (TriMesh, MeshNeuron, Volume).</p> <p>Uses Laplacian smoothing. Not necessarily because that is always the best approach but because there are three backends (see below) that offer similar interfaces.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Mesh(es) to simplify.\n</code></pre> <p> TYPE: <code>            navis.MeshNeuron/List | navis.Volume | trimesh.Trimesh</code> </p> <code>iterations</code> <pre><code>        Round of smoothing to apply.\n</code></pre> <p> TYPE: <code>   int</code> DEFAULT: <code>5</code> </p> <code>L</code> <pre><code>        Diffusion speed constant lambda. Larger = more aggressive\n        smoothing.\n</code></pre> <p> TYPE: <code>            float [0-1]</code> DEFAULT: <code>0.5</code> </p> <code>backend</code> <pre><code>        Which backend to use. Currenly we support `open3d`,\n        Blender 3D or `trimesh`.\n</code></pre> <p> TYPE: <code>      \"auto\" | \"open3d\" | \"blender\" | \"trimesh\"</code> DEFAULT: <code>'auto'</code> </p> <code>inplace</code> <pre><code>        If True, will perform simplication on `x`. If False, will\n        simplify and return a copy.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>smoothed</code> <p>Smoothed object.</p> Source code in <code>navis/meshes/operations.py</code> <pre><code>@utils.map_neuronlist(desc='Smoothing', allow_parallel=True)\ndef smooth_mesh(x, iterations=5, L=.5, backend='auto', inplace=False):\n    \"\"\"Smooth meshes (TriMesh, MeshNeuron, Volume).\n\n    Uses Laplacian smoothing. Not necessarily because that is always the best\n    approach but because there are three backends (see below) that offer similar\n    interfaces.\n\n    Parameters\n    ----------\n    x :             navis.MeshNeuron/List | navis.Volume | trimesh.Trimesh\n                    Mesh(es) to simplify.\n    iterations :    int\n                    Round of smoothing to apply.\n    L :             float [0-1]\n                    Diffusion speed constant lambda. Larger = more aggressive\n                    smoothing.\n    backend :       \"auto\" | \"open3d\" | \"blender\" | \"trimesh\"\n                    Which backend to use. Currenly we support `open3d`,\n                    Blender 3D or `trimesh`.\n    inplace :       bool\n                    If True, will perform simplication on `x`. If False, will\n                    simplify and return a copy.\n\n    Returns\n    -------\n    smoothed\n                    Smoothed object.\n\n    \"\"\"\n    if not isinstance(backend, str):\n        raise TypeError(f'`backend` must be string, got \"{type(backend)}\"')\n\n    backend = backend.lower()\n    backends = available_backends() + ['trimesh']\n\n    # Drop pymeshlab from backend\n    if 'pymeshlab' in backends:\n        backends.remove('pymeshlab')\n\n    if backend == 'auto':\n        backend = backends[0]\n    elif backend not in backends:\n        raise ValueError(f'Backend \"{backend}\" appears to not be available. '\n                         'Please choose one of the available backends: '\n                         f'{\", \".join(backends)}')\n\n    if not inplace:\n        x = x.copy()\n\n    if backend == 'open3d':\n        _ = smooth_mesh_open3d(x, iterations=iterations, L=L, inplace=True)\n    elif backend == 'blender':\n        _ = smooth_mesh_blender(x, iterations=iterations, L=L, inplace=True)\n    elif backend == 'trimesh':\n        _ = smooth_mesh_trimesh(x, iterations=iterations, L=L, inplace=True)\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.smooth_skeleton","title":"<code>navis.smooth_skeleton</code>","text":"<p>Smooth skeleton(s) using rolling windows.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Neuron(s) to be processed.\n</code></pre> <p> TYPE: <code>            TreeNeuron | NeuronList</code> </p> <code>window</code> <pre><code>        Size (N observations) of the rolling window in number of\n        nodes.\n</code></pre> <p> TYPE: <code>       int</code> DEFAULT: <code>5</code> </p> <code>to_smooth</code> <pre><code>        Columns of the node table to smooth. Should work with any\n        numeric column (e.g. 'radius').\n</code></pre> <p> TYPE: <code>    list</code> DEFAULT: <code>['x', 'y', 'z']</code> </p> <code>inplace</code> <pre><code>        If False, will use and return copy of original neuron(s).\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>TreeNeuron / List</code> <p>Smoothed neuron(s).</p> <p>Examples:</p> <p>Smooth x/y/z locations (default):</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(2)\n&gt;&gt;&gt; smoothed = navis.smooth_skeleton(nl, window=5)\n</code></pre> <p>Smooth only radii:</p> <pre><code>&gt;&gt;&gt; rad_smoothed = navis.smooth_skeleton(nl, to_smooth='radius')\n</code></pre> See Also <p><code>navis.smooth_mesh</code>                 For smoothing MeshNeurons and other mesh-likes. <code>navis.smooth_voxels</code>                 For smoothing VoxelNeurons.</p> Source code in <code>navis/morpho/manipulation.py</code> <pre><code>@utils.map_neuronlist(desc=\"Smoothing\", allow_parallel=True)\ndef smooth_skeleton(\n    x: NeuronObject,\n    window: int = 5,\n    to_smooth: list = [\"x\", \"y\", \"z\"],\n    inplace: bool = False,\n) -&gt; NeuronObject:\n    \"\"\"Smooth skeleton(s) using rolling windows.\n\n    Parameters\n    ----------\n    x :             TreeNeuron | NeuronList\n                    Neuron(s) to be processed.\n    window :        int, optional\n                    Size (N observations) of the rolling window in number of\n                    nodes.\n    to_smooth :     list\n                    Columns of the node table to smooth. Should work with any\n                    numeric column (e.g. 'radius').\n    inplace :       bool, optional\n                    If False, will use and return copy of original neuron(s).\n\n    Returns\n    -------\n    TreeNeuron/List\n                    Smoothed neuron(s).\n\n    Examples\n    --------\n    Smooth x/y/z locations (default):\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(2)\n    &gt;&gt;&gt; smoothed = navis.smooth_skeleton(nl, window=5)\n\n    Smooth only radii:\n\n    &gt;&gt;&gt; rad_smoothed = navis.smooth_skeleton(nl, to_smooth='radius')\n\n    See Also\n    --------\n    [`navis.smooth_mesh`][]\n                    For smoothing MeshNeurons and other mesh-likes.\n    [`navis.smooth_voxels`][]\n                    For smoothing VoxelNeurons.\n\n    \"\"\"\n    # The decorator makes sure that at this point we have single neurons\n    if not isinstance(x, core.TreeNeuron):\n        raise TypeError(f\"Can only process TreeNeurons, not {type(x)}\")\n\n    if not inplace:\n        x = x.copy()\n\n    # Prepare nodes (add parent_dist for later, set index)\n    # mmetrics.parent_dist(x, root_dist=0)\n    nodes = x.nodes.set_index(\"node_id\", inplace=False).copy()\n\n    to_smooth = utils.make_iterable(to_smooth)\n\n    miss = to_smooth[~np.isin(to_smooth, nodes.columns)]\n    if len(miss):\n        raise ValueError(f\"Column(s) not found in node table: {miss}\")\n\n    # Go over each segment and smooth\n    for s in config.tqdm(\n        x.segments[::-1],\n        desc=\"Smoothing\",\n        disable=config.pbar_hide,\n        leave=config.pbar_leave,\n    ):\n        # Get this segment's parent distances and get cumsum\n        this_co = nodes.loc[s, to_smooth]\n\n        interp = this_co.rolling(window, min_periods=1).mean()\n\n        for i, c in enumerate(to_smooth):\n            nodes.loc[s, c] = interp.iloc[:, i].values.astype(\n                nodes[c].dtype, copy=False\n            )\n\n    # Reassign nodes\n    x.nodes = nodes.reset_index(drop=False, inplace=False)\n\n    x._clear_temp_attr()\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.smooth_voxels","title":"<code>navis.smooth_voxels</code>","text":"<p>Smooth voxel(s) using a Gaussian filter.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Neuron(s) to be processed.\n</code></pre> <p> TYPE: <code>            TreeNeuron | NeuronList</code> </p> <code>sigma</code> <pre><code>        Standard deviation for Gaussian kernel. The standard\n        deviations of the Gaussian filter are given for each axis\n        as a sequence, or as a single number, in which case it is\n        equal for all axes.\n</code></pre> <p> TYPE: <code>        int | (3, ) ints</code> DEFAULT: <code>1</code> </p> <code>inplace</code> <pre><code>        If False, will use and return copy of original neuron(s).\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>VoxelNeuron / List</code> <p>Smoothed neuron(s).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1, kind='mesh')\n&gt;&gt;&gt; vx = navis.voxelize(n, pitch='1 micron')\n&gt;&gt;&gt; smoothed = navis.smooth_voxels(vx, sigma=2)\n</code></pre> See Also <p><code>navis.smooth_mesh</code>                 For smoothing MeshNeurons and other mesh-likes. <code>navis.smooth_skeleton</code>                 For smoothing TreeNeurons.</p> Source code in <code>navis/morpho/images.py</code> <pre><code>@utils.map_neuronlist(desc=\"Smoothing\", allow_parallel=True)\ndef smooth_voxels(\n    x: NeuronObject, sigma: int = 1, inplace: bool = False\n) -&gt; NeuronObject:\n    \"\"\"Smooth voxel(s) using a Gaussian filter.\n\n    Parameters\n    ----------\n    x :             TreeNeuron | NeuronList\n                    Neuron(s) to be processed.\n    sigma :         int | (3, ) ints, optional\n                    Standard deviation for Gaussian kernel. The standard\n                    deviations of the Gaussian filter are given for each axis\n                    as a sequence, or as a single number, in which case it is\n                    equal for all axes.\n    inplace :       bool, optional\n                    If False, will use and return copy of original neuron(s).\n\n    Returns\n    -------\n    VoxelNeuron/List\n                    Smoothed neuron(s).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1, kind='mesh')\n    &gt;&gt;&gt; vx = navis.voxelize(n, pitch='1 micron')\n    &gt;&gt;&gt; smoothed = navis.smooth_voxels(vx, sigma=2)\n\n    See Also\n    --------\n    [`navis.smooth_mesh`][]\n                    For smoothing MeshNeurons and other mesh-likes.\n    [`navis.smooth_skeleton`][]\n                    For smoothing TreeNeurons.\n\n    \"\"\"\n    # The decorator makes sure that at this point we have single neurons\n    if not isinstance(x, core.VoxelNeuron):\n        raise TypeError(f\"Can only process VoxelNeurons, not {type(x)}\")\n\n    if not inplace:\n        x = x.copy()\n\n    # Apply gaussian\n    x._data = gaussian_filter(x.grid.astype(np.float32), sigma=sigma)\n    x._clear_temp_attr()\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.split_axon_dendrite","title":"<code>navis.split_axon_dendrite</code>","text":"<p>Split a neuron into axon and dendrite.</p> <p>The result is highly dependent on the method and on your neuron's morphology and works best for \"typical\" neurons.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            Neuron(s) to split into axon, dendrite (and cell body\n            fiber if possible).\n</code></pre> <p> TYPE: <code>                TreeNeuron | MeshNeuron | NeuronList</code> </p> <code>metric</code> <pre><code>            Defines which flow metric we will try to maximize when\n            splitting the neuron(s). There are four flavors:\n\n             - 'synapse_flow_centrality' via [`navis.synapse_flow_centrality`][]\n               (note that this metric was previously called just \"flow_centrality\")\n             - 'bending_flow' via [`navis.bending_flow`][]\n             - 'segregation_index' via [`navis.arbor_segregation_index`][]\n             - 'flow_centrality' via [`navis.flow_centrality`][]\n\n            Will try using existing columns in the node table. If\n            not present, will invoke the respective functions with\n            default parameters. All but `flow_centrality` require\n            the neuron to have connectors.\n</code></pre> <p> TYPE: <code>           'synapse_flow_centrality' | 'bending_flow' | 'segregation_index' | \"flow_centrality\"</code> DEFAULT: <code>'synapse_flow_centrality'</code> </p> <code>flow_thresh</code> <pre><code>            The \"linker\" between axon and dendrites will be the part\n            of the neuron with the highest flow (see metric). We\n            define it by `max(flow) * flow_thresh`. You might have\n            to decrease this value for atypical or not well\n            segregated neurons.\n</code></pre> <p> TYPE: <code>      float [0-1]</code> DEFAULT: <code>0.9</code> </p> <code>split</code> <pre><code>            Method for determining which compartment is axon and\n            which is the dendrites:\n\n                - 'prepost' uses number of in- vs. outputs. By default,\n                  a ratio of &gt;1 (more out- than inputs) is considered\n                  axon and vice versa. You can provide a custom threshold\n                  by setting `split='prepost:0.5'` for example. Values\n                  above 1.0 will bias towards dendrites and below 1.0\n                  towards axon.\n                - 'distance' assumes the compartment proximal to the\n                  soma is the dendrites.\n</code></pre> <p> TYPE: <code>            'prepost' | 'distance'</code> DEFAULT: <code>'prepost'</code> </p> <code>cellbodyfiber</code> <pre><code>            Determines whether we will try to find a cell body\n            fiber (CBF).\n\n                - \"soma\" will try finding the CBF only if the neuron\n                  has a soma\n                - \"root\" will consider the root to be the source\n                  of the CBF as fallback if there is no soma\n                - `False` will not attempt to extract the CBF\n\n            A CBF is something typically found in insect neurons\n            which are not bipolar unlike most vertebrate neurons but\n            rather have a passive soma some distance away from\n            axon/dendrites.\n</code></pre> <p> TYPE: <code>    \"soma\" | \"root\" | False</code> DEFAULT: <code>False</code> </p> <code>reroot_soma</code> <pre><code>            If True and neuron has a soma, will make sure the neuron\n            is rooted to its soma.\n</code></pre> <p> TYPE: <code>      bool,</code> DEFAULT: <code>True</code> </p> <code>label_only</code> <pre><code>            If True, will not split the neuron but rather add a\n            \"compartment\" column to the node and connector table of\n            the input neuron.\n</code></pre> <p> TYPE: <code>       bool,</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>NeuronList</code> <p>Axon, dendrite, linker and CBF (the latter two aren't guaranteed). Fragments will have a new property <code>compartment</code> (see example).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; x = navis.example_neurons(1)\n&gt;&gt;&gt; split = navis.split_axon_dendrite(x, metric='synapse_flow_centrality',\n...                                   reroot_soma=True)\n&gt;&gt;&gt; split\n&lt;class 'navis.NeuronList'&gt; of 3 neurons\n                      neuron_name  id  n_nodes  n_connectors  compartment\n0                  neuron 123457   16      148             0         axon\n1                  neuron 123457   16     9682          1766       linker\n2                  neuron 123457   16     2892           113     dendrite\n&gt;&gt;&gt; # For convenience, split_axon_dendrite assigns colors to the resulting\n&gt;&gt;&gt; # fragments: axon = red, dendrites = blue, CBF = green\n&gt;&gt;&gt; _ = split.plot3d(color=split.color)\n</code></pre> <p>Alternatively just label the compartments</p> <pre><code>&gt;&gt;&gt; x = navis.split_axon_dendrite(x, label_only=True)\n&gt;&gt;&gt; x.nodes[~x.nodes.compartment.isnull()].head()\n         node_id label        x        y        z     radius  parent_id  type compartment\n110      111     0  17024.0  33790.0  26602.0  72.462097        110  slab      linker\n111      112     0  17104.0  33670.0  26682.0  72.462097        111  slab      linker\n112      113     0  17184.0  33450.0  26782.0  70.000000        112  slab      linker\n113      114     0  17244.0  33270.0  26822.0  70.000000        113  slab      linker\n114      115     0  17324.0  33150.0  26882.0  74.852798        114  slab      linker\n</code></pre> See Also <p><code>navis.heal_skeleton</code>         Axon/dendrite split works only on neurons consisting of a single         tree. Use this function to heal fragmented neurons before trying         the axon/dendrite split.</p> Source code in <code>navis/morpho/manipulation.py</code> <pre><code>@utils.map_neuronlist(desc=\"Splitting\", allow_parallel=True)\n@utils.meshneuron_skeleton(\n    method=\"split\",\n    include_connectors=True,\n    copy_properties=[\"color\", \"compartment\"],\n    disallowed_kwargs={\"label_only\": True},\n    heal=True,\n)\ndef split_axon_dendrite(\n    x: NeuronObject,\n    metric: Union[\n        Literal[\"synapse_flow_centrality\"],\n        Literal[\"flow_centrality\"],\n        Literal[\"bending_flow\"],\n        Literal[\"segregation_index\"],\n    ] = \"synapse_flow_centrality\",\n    flow_thresh: float = 0.9,\n    split: Union[Literal[\"prepost\"], Literal[\"distance\"]] = \"prepost\",\n    cellbodyfiber: Union[Literal[\"soma\"], Literal[\"root\"], bool] = False,\n    reroot_soma: bool = True,\n    label_only: bool = False,\n) -&gt; \"core.NeuronList\":\n    \"\"\"Split a neuron into axon and dendrite.\n\n    The result is highly dependent on the method and on your neuron's\n    morphology and works best for \"typical\" neurons.\n\n    Parameters\n    ----------\n    x :                 TreeNeuron | MeshNeuron | NeuronList\n                        Neuron(s) to split into axon, dendrite (and cell body\n                        fiber if possible).\n    metric :            'synapse_flow_centrality' | 'bending_flow' | 'segregation_index' | \"flow_centrality\", optional\n                        Defines which flow metric we will try to maximize when\n                        splitting the neuron(s). There are four flavors:\n\n                         - 'synapse_flow_centrality' via [`navis.synapse_flow_centrality`][]\n                           (note that this metric was previously called just \"flow_centrality\")\n                         - 'bending_flow' via [`navis.bending_flow`][]\n                         - 'segregation_index' via [`navis.arbor_segregation_index`][]\n                         - 'flow_centrality' via [`navis.flow_centrality`][]\n\n                        Will try using existing columns in the node table. If\n                        not present, will invoke the respective functions with\n                        default parameters. All but `flow_centrality` require\n                        the neuron to have connectors.\n    flow_thresh :       float [0-1]\n                        The \"linker\" between axon and dendrites will be the part\n                        of the neuron with the highest flow (see metric). We\n                        define it by `max(flow) * flow_thresh`. You might have\n                        to decrease this value for atypical or not well\n                        segregated neurons.\n    split :             'prepost' | 'distance'\n                        Method for determining which compartment is axon and\n                        which is the dendrites:\n\n                            - 'prepost' uses number of in- vs. outputs. By default,\n                              a ratio of &gt;1 (more out- than inputs) is considered\n                              axon and vice versa. You can provide a custom threshold\n                              by setting `split='prepost:0.5'` for example. Values\n                              above 1.0 will bias towards dendrites and below 1.0\n                              towards axon.\n                            - 'distance' assumes the compartment proximal to the\n                              soma is the dendrites.\n\n    cellbodyfiber :     \"soma\" | \"root\" | False\n                        Determines whether we will try to find a cell body\n                        fiber (CBF).\n\n                            - \"soma\" will try finding the CBF only if the neuron\n                              has a soma\n                            - \"root\" will consider the root to be the source\n                              of the CBF as fallback if there is no soma\n                            - `False` will not attempt to extract the CBF\n\n                        A CBF is something typically found in insect neurons\n                        which are not bipolar unlike most vertebrate neurons but\n                        rather have a passive soma some distance away from\n                        axon/dendrites.\n    reroot_soma :       bool,\n                        If True and neuron has a soma, will make sure the neuron\n                        is rooted to its soma.\n    label_only :        bool,\n                        If True, will not split the neuron but rather add a\n                        \"compartment\" column to the node and connector table of\n                        the input neuron.\n\n    Returns\n    -------\n    NeuronList\n                        Axon, dendrite, linker and CBF (the latter two aren't\n                        guaranteed). Fragments will have a new property\n                        `compartment` (see example).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; x = navis.example_neurons(1)\n    &gt;&gt;&gt; split = navis.split_axon_dendrite(x, metric='synapse_flow_centrality',\n    ...                                   reroot_soma=True)\n    &gt;&gt;&gt; split                                                   # doctest: +SKIP\n    &lt;class 'navis.NeuronList'&gt; of 3 neurons\n                          neuron_name  id  n_nodes  n_connectors  compartment\n    0                  neuron 123457   16      148             0         axon\n    1                  neuron 123457   16     9682          1766       linker\n    2                  neuron 123457   16     2892           113     dendrite\n    &gt;&gt;&gt; # For convenience, split_axon_dendrite assigns colors to the resulting\n    &gt;&gt;&gt; # fragments: axon = red, dendrites = blue, CBF = green\n    &gt;&gt;&gt; _ = split.plot3d(color=split.color)\n\n    Alternatively just label the compartments\n\n    &gt;&gt;&gt; x = navis.split_axon_dendrite(x, label_only=True)\n    &gt;&gt;&gt; x.nodes[~x.nodes.compartment.isnull()].head()           # doctest: +SKIP\n             node_id label        x        y        z     radius  parent_id  type compartment\n    110      111     0  17024.0  33790.0  26602.0  72.462097        110  slab      linker\n    111      112     0  17104.0  33670.0  26682.0  72.462097        111  slab      linker\n    112      113     0  17184.0  33450.0  26782.0  70.000000        112  slab      linker\n    113      114     0  17244.0  33270.0  26822.0  70.000000        113  slab      linker\n    114      115     0  17324.0  33150.0  26882.0  74.852798        114  slab      linker\n\n    See Also\n    --------\n    [`navis.heal_skeleton`][]\n            Axon/dendrite split works only on neurons consisting of a single\n            tree. Use this function to heal fragmented neurons before trying\n            the axon/dendrite split.\n\n    \"\"\"\n    COLORS = {\n        \"axon\": (178, 34, 34),\n        \"dendrite\": (0, 0, 255),\n        \"cellbodyfiber\": (50, 50, 50),\n        \"linker\": (150, 150, 150),\n    }\n\n    # The decorator makes sure that at this point we have single neurons\n    if not isinstance(x, core.TreeNeuron):\n        raise TypeError(f'Can only process TreeNeurons, got \"{type(x)}\"')\n\n    if not x.has_connectors:\n        if metric != \"flow_centrality\":\n            raise ValueError(\"Neuron must have connectors.\")\n        elif split == \"prepost\":\n            raise ValueError(\n                'Set `split=\"distance\"` when trying to split neurons '\n                \"without connectors.\"\n            )\n\n    split_val = 1\n    if isinstance(split, str) and \":\" in split:\n        split, split_val = split.split(\":\")\n        split_val = float(split_val)\n\n    _METRIC = (\n        \"synapse_flow_centrality\",\n        \"bending_flow\",\n        \"segregation_index\",\n        \"flow_centrality\",\n    )\n    utils.eval_param(metric, \"metric\", allowed_values=_METRIC)\n    utils.eval_param(split, \"split\", allowed_values=(\"prepost\", \"distance\"))\n    utils.eval_param(\n        cellbodyfiber, \"cellbodyfiber\", allowed_values=(\"soma\", \"root\", False)\n    )\n\n    if metric == \"flow_centrality\":\n        msg = (\n            \"As of navis version 1.4.0, `method='flow_centrality'` \"\n            \"uses synapse-independent, morphology-only flow to generate splits.\"\n            \"Please use `method='synapse_flow_centrality' for \"\n            \"synapse-based axon-dendrite splits. \"\n            \"This warning will be removed in a future version of navis.\"\n        )\n        warnings.warn(msg, DeprecationWarning)\n        logger.warning(msg)\n\n    if len(x.root) &gt; 1:\n        raise ValueError(\n            f\"Unable to split neuron {x.id}: multiple roots. \"\n            \"Try `navis.heal_skeleton(x)` to merged \"\n            \"disconnected fragments.\"\n        )\n\n    # Make copy, so that we don't screw things up\n    original = x\n    x = x.copy()\n\n    if np.any(x.soma) and not np.all(np.isin(x.soma, x.root)) and reroot_soma:\n        x.reroot(x.soma, inplace=True)\n\n    FUNCS = {\n        \"bending_flow\": mmetrics.bending_flow,\n        \"synapse_flow_centrality\": mmetrics.synapse_flow_centrality,\n        \"flow_centrality\": mmetrics.flow_centrality,\n        \"segregation_index\": mmetrics.arbor_segregation_index,\n    }\n\n    if metric not in FUNCS:\n        raise ValueError(f'Unknown `metric`: \"{metric}\"')\n\n    # Add metric if not already present\n    if metric not in x.nodes.columns:\n        _ = FUNCS[metric](x)\n\n    # We can lock this neuron indefinitely since we are not returning it\n    x._lock = 1\n\n    # Make sure we have a metric for every single node\n    if np.any(np.isnan(x.nodes[metric].values)):\n        raise ValueError(f'NaN values encountered in \"{metric}\"')\n\n    # The first step is to remove the linker -&gt; that's the bit that connects\n    # the axon and dendrite\n    is_linker = x.nodes[metric] &gt;= x.nodes[metric].max() * flow_thresh\n    linker = set(x.nodes.loc[is_linker, \"node_id\"].values)\n\n    # We try to perform processing on the graph to avoid overhead from\n    # (re-)generating neurons\n    g = x.graph.to_undirected()\n\n    # Drop linker nodes\n    g.remove_nodes_from(linker)\n\n    # Break into connected components\n    cc = list(nx.connected_components(g))\n\n    # Figure out which one is which\n    axon = set()\n    if split == \"prepost\":\n        # Collect # of pre- and postsynapses on each of the connected components\n        sm = pd.DataFrame()\n        sm[\"n_nodes\"] = [len(c) for c in cc]\n        pre = x.presynapses\n        post = x.postsynapses\n        sm[\"n_pre\"] = [pre[pre.node_id.isin(c)].shape[0] for c in cc]\n        sm[\"n_post\"] = [post[post.node_id.isin(c)].shape[0] for c in cc]\n        sm[\"prepost_ratio\"] = sm.n_pre / sm.n_post\n        sm[\"frac_post\"] = sm.n_post / sm.n_post.sum()\n        sm[\"frac_pre\"] = sm.n_pre / sm.n_pre.sum()\n\n        # In theory, we can encounter neurons with either no pre- or no\n        # postsynapses (e.g. sensory neurons).\n        # For those n_pre/post.sum() would cause a division by 0 which in turn\n        # causes frac_pre/post to be NaN. By filling, we make sure that the\n        # split doesn't fail further down but they might end up missing either\n        # an axon or a dendrite (which may actually be OK?).\n        sm[\"frac_post\"] = sm[\"frac_post\"].fillna(0)\n        sm[\"frac_pre\"] = sm[\"frac_pre\"].fillna(0)\n\n        # Produce the ratio of pre- to postsynapses\n        sm[\"frac_prepost\"] = sm.frac_pre / sm.frac_post\n\n        # Some small side branches might have either no pre- or no postsynapses.\n        # Even if they have synapses: if the total count is low they might be\n        # incorrectly assigned to a compartment. Here, we will make sure that\n        # they are disregarded for now to avoid introducing noise. Instead we\n        # will connect them onto their parent compartment later.\n        sm.loc[\n            sm[[\"frac_pre\", \"frac_post\"]].max(axis=1) &lt; 0.01,\n            [\"prepost_ratio\", \"frac_prepost\"],\n        ] = np.nan\n\n        # Each fragment is considered separately as either giver or recipient\n        # of flow:\n        # - prepost &lt; 1 = dendritic\n        # - prepost &gt; 1 = axonic\n        dendrite = [cc[i] for i in sm[sm.frac_prepost &lt; split_val].index.values]\n        if len(dendrite):\n            dendrite = set.union(*dendrite)\n        axon = [cc[i] for i in sm[sm.frac_prepost &gt;= split_val].index.values]\n        if len(axon):\n            axon = set.union(*axon)\n    else:\n        for c in cc:\n            # If original root present assume it's the proximal dendrites\n            if x.root[0] in c:\n                dendrite = c\n            else:\n                axon = axon | c\n\n    # Now that we have in principle figured out what's what we need to do some\n    # clean-up\n    # First: it is quite likely that the axon(s) and/or the dendrites fragmented\n    # and we need to stitch them back together using linker but not dendrites!\n    g = x.graph.subgraph(np.append(list(axon), list(linker)))\n    axon = set(graph.connected_subgraph(g, axon)[0])\n\n    # Remove nodes that were re-assigned to axon from linker\n    linker = linker - axon\n\n    g = x.graph.subgraph(np.append(list(dendrite), list(linker)))\n    dendrite = set(graph.connected_subgraph(g, dendrite)[0])\n\n    # Remove nodes that were re-assigned to axon from linker\n    linker = linker - set(dendrite)\n\n    # Next up: finding the CBF\n    # The CBF is defined as the part of the neuron between the soma (or root)\n    # and the first branch point with sizeable synapse flow\n    cbf = set()\n    if cellbodyfiber and (np.any(x.soma) or cellbodyfiber == \"root\"):\n        # To excise the CBF, we subset the neuron to those parts with\n        # no/hardly any flow and find the part that contains the soma\n        no_flow = x.nodes[x.nodes[metric] &lt;= x.nodes[metric].max() * 0.05]\n        g = x.graph.subgraph(no_flow.node_id.values)\n\n        # Find the connected component containing the soma\n        for c in nx.connected_components(g.to_undirected()):\n            if x.root[0] in c:\n                cbf = set(c)\n                dendrite = dendrite - cbf\n                axon = axon - cbf\n                linker = linker - cbf\n                break\n\n    # See if we lost any nodes on the way\n    miss = set(original.nodes.node_id.values) - linker - axon - dendrite - cbf\n    miss = np.array(list(miss))\n\n    # From hereon we can use lists\n    linker = list(linker)\n    axon = list(axon)\n    cbf = list(cbf)\n    dendrite = list(dendrite)\n\n    # If we have, assign these nodes to the closest node with a compartment\n    if any(miss):\n        # Find the closest nodes with a compartment\n        m = graph.geodesic_matrix(original, directed=False, weight=None, from_=miss)\n\n        # Subset geodesic matrix to nodes that have a compartment\n        nodes_w_comp = original.nodes.node_id.values[\n            ~np.isin(original.nodes.node_id.values, miss)\n        ]\n        closest = np.argmin(m.loc[:, nodes_w_comp].values, axis=1)\n        closest_id = nodes_w_comp[closest]\n\n        linker += m.index.values[np.isin(closest_id, linker)].tolist()\n        axon += m.index.values[np.isin(closest_id, axon)].tolist()\n        dendrite += m.index.values[np.isin(closest_id, dendrite)].tolist()\n        cbf += m.index.values[np.isin(closest_id, cbf)].tolist()\n\n    # Add labels\n    if label_only:\n        nodes = original.nodes\n        nodes[\"compartment\"] = None\n        is_linker = nodes.node_id.isin(linker)\n        is_axon = nodes.node_id.isin(axon)\n        is_dend = nodes.node_id.isin(dendrite)\n        is_cbf = nodes.node_id.isin(cbf)\n        nodes.loc[is_linker, \"compartment\"] = \"linker\"\n        nodes.loc[is_dend, \"compartment\"] = \"dendrite\"\n        nodes.loc[is_axon, \"compartment\"] = \"axon\"\n        nodes.loc[is_cbf, \"compartment\"] = \"cellbodyfiber\"\n\n        # Set connector compartments\n        cmp_map = original.nodes.set_index(\"node_id\").compartment.to_dict()\n        original.connectors[\"compartment\"] = original.connectors.node_id.map(cmp_map)\n\n        # Turn into categorical data\n        original.nodes[\"compartment\"] = original.nodes.compartment.astype(\"category\")\n        original.connectors[\"compartment\"] = original.connectors.compartment.astype(\n            \"category\"\n        )\n\n        return original\n\n    # Generate the actual splits\n    nl = []\n    for label, nodes in zip(\n        [\"cellbodyfiber\", \"dendrite\", \"linker\", \"axon\"], [cbf, dendrite, linker, axon]\n    ):\n        if not len(nodes):\n            continue\n        n = subset.subset_neuron(original, nodes)\n        n.color = COLORS.get(label, (100, 100, 100))\n        n._register_attr(\"compartment\", label)\n        nl.append(n)\n\n    return core.NeuronList(nl)\n</code></pre>"},{"location":"reference/navis/#navis.split_into_fragments","title":"<code>navis.split_into_fragments</code>","text":"<p>Split neuron into fragments.</p> <p>Cuts are based on longest neurites: the first cut is made where the second largest neurite merges onto the largest neurite, the second cut is made where the third largest neurite merges into either of the first fragments and so on.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            Must be a single neuron.\n</code></pre> <p> TYPE: <code>                TreeNeuron | MeshNeuron | NeuronList</code> </p> <code>n</code> <pre><code>            Number of fragments to split into. Must be &gt;1.\n</code></pre> <p> TYPE: <code>                int</code> DEFAULT: <code>2</code> </p> <code>min_size</code> <pre><code>            Minimum size of fragment to be cut off. If too\n            small, will stop cutting. This takes only the longest\n            path in each fragment into account! If the neuron(s),\n            has its `.units` set, you can also pass this as a string\n            such as \"10 microns\".\n</code></pre> <p> TYPE: <code>         int | str</code> DEFAULT: <code>None</code> </p> <code>reroot_soma</code> <pre><code>            If True, neuron will be rerooted to soma.\n</code></pre> <p> TYPE: <code>       bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>NeuronList</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; x = navis.example_neurons(1)\n&gt;&gt;&gt; # Cut into two fragments\n&gt;&gt;&gt; cut1 = navis.split_into_fragments(x, n=2)\n&gt;&gt;&gt; # Cut into fragments of &gt;10 um size\n&gt;&gt;&gt; cut2 = navis.split_into_fragments(x, n=float('inf'), min_size=10e3)\n</code></pre> Source code in <code>navis/graph/graph_utils.py</code> <pre><code>@utils.meshneuron_skeleton(method=\"split\")\ndef split_into_fragments(\n    x: \"core.NeuronObject\",\n    n: int = 2,\n    min_size: Optional[Union[float, str]] = None,\n    reroot_soma: bool = False,\n) -&gt; \"core.NeuronList\":\n    \"\"\"Split neuron into fragments.\n\n    Cuts are based on longest neurites: the first cut is made where the second\n    largest neurite merges onto the largest neurite, the second cut is made\n    where the third largest neurite merges into either of the first fragments\n    and so on.\n\n    Parameters\n    ----------\n    x :                 TreeNeuron | MeshNeuron | NeuronList\n                        Must be a single neuron.\n    n :                 int, optional\n                        Number of fragments to split into. Must be &gt;1.\n    min_size :          int | str, optional\n                        Minimum size of fragment to be cut off. If too\n                        small, will stop cutting. This takes only the longest\n                        path in each fragment into account! If the neuron(s),\n                        has its `.units` set, you can also pass this as a string\n                        such as \"10 microns\".\n    reroot_soma :        bool, optional\n                        If True, neuron will be rerooted to soma.\n\n    Returns\n    -------\n    NeuronList\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; x = navis.example_neurons(1)\n    &gt;&gt;&gt; # Cut into two fragments\n    &gt;&gt;&gt; cut1 = navis.split_into_fragments(x, n=2)\n    &gt;&gt;&gt; # Cut into fragments of &gt;10 um size\n    &gt;&gt;&gt; cut2 = navis.split_into_fragments(x, n=float('inf'), min_size=10e3)\n\n    \"\"\"\n    if isinstance(x, core.NeuronList):\n        if len(x) == 1:\n            x = x[0]\n        else:\n            raise Exception(\n                f\"{x.shape[0]} neurons provided. Please provide \"\n                \"only a single neuron!\"\n            )\n\n    if not isinstance(x, core.TreeNeuron):\n        raise TypeError(f'Expected a single TreeNeuron, got \"{type(x)}\"')\n\n    if n &lt; 2:\n        raise ValueError(\"Number of fragments must be at least 2.\")\n\n    # At this point x is TreeNeuron\n    x: core.TreeNeuron\n\n    min_size = x.map_units(min_size, on_error=\"raise\")\n\n    if reroot_soma and not isinstance(x.soma, type(None)):\n        x.reroot(x.soma, inplace=True)\n\n    # Collect nodes of the n longest neurites\n    tn_to_preserve: List[int] = []\n    fragments = []\n    i = 0\n    while i &lt; n:\n        if tn_to_preserve:\n            # Generate fresh graph\n            g = graph.neuron2nx(x)\n\n            # Remove nodes that we have already preserved\n            g.remove_nodes_from(tn_to_preserve)\n        else:\n            g = x.graph\n\n        # Get path\n        longest_path = nx.dag_longest_path(g)\n\n        # Check if fragment is still long enough\n        if min_size:\n            this_length = sum(\n                [\n                    v\n                    for k, v in nx.get_edge_attributes(g, \"weight\").items()\n                    if k[1] in longest_path\n                ]\n            )\n            if this_length &lt;= min_size:\n                break\n\n        tn_to_preserve += longest_path\n        fragments.append(longest_path)\n\n        i += 1\n\n    # Next, make some virtual cuts and get the complement of nodes for\n    # each fragment\n    graphs = [x.graph.copy()]\n    # Grab graph once to avoide overhead from stale checking\n    g = x.graph\n    for fr in fragments[1:]:\n        this_g = nx.bfs_tree(g, fr[-1], reverse=True)\n\n        graphs.append(this_g)\n\n    # Next, we need to remove nodes that are in subsequent graphs from\n    # those graphs\n    for i, g in enumerate(graphs):\n        for g2 in graphs[i + 1 :]:\n            g.remove_nodes_from(g2.nodes)\n\n    # Now make neurons\n    nl = core.NeuronList([morpho.subset_neuron(x, g) for g in graphs])\n\n    return nl\n</code></pre>"},{"location":"reference/navis/#navis.stitch_skeletons","title":"<code>navis.stitch_skeletons</code>","text":"<p>Stitch multiple skeletons together.</p> <p>Uses minimum spanning tree to determine a way to connect all fragments while minimizing length (Euclidean distance) of the new edges. Nodes that have been stitched will get a \"stitched\" tag.</p> Important <p>If duplicate node IDs are found across the fragments to stitch they will be remapped to new unique values!</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            Neurons to stitch (see examples).\n</code></pre> <p> TYPE: <code>                NeuronList | list of TreeNeuron/List</code> DEFAULT: <code>()</code> </p> <code>method</code> <pre><code>            Set stitching method:\n                (1) 'LEAFS': Only leaf (including root) nodes will\n                    be allowed to make new edges.\n                (2) 'ALL': All nodes are considered.\n                (3) 'NONE': Node and connector tables will simply\n                    be combined without generating any new edges.\n                    The resulting neuron will have multiple roots.\n                (4) List of node IDs that are allowed to be used.\n                    Note that if these nodes are insufficient\n                    the resulting neuron will not be fully\n                    connected.\n</code></pre> <p> TYPE: <code>           'LEAFS' | 'ALL' | 'NONE' | list of node IDs</code> DEFAULT: <code>'ALL'</code> </p> <code>master</code> <pre><code>            Sets the master neuron:\n                (1) 'SOMA': The largest fragment with a soma\n                    becomes the master neuron. If no neuron with\n                    soma, will pick the largest (option 2).\n                (2) 'LARGEST': The largest (by number of nodes)\n                    fragment becomes the master neuron.\n                (3) 'FIRST': The first fragment provided becomes\n                    the master neuron.\n</code></pre> <p> TYPE: <code>           'SOMA' | 'LARGEST' | 'FIRST'</code> DEFAULT: <code>'SOMA'</code> </p> <code>max_dist</code> <pre><code>            Max distance at which to stitch nodes. This can result\n            in a neuron with multiple roots.\n</code></pre> <p> TYPE: <code>         float,  optional</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>TreeNeuron</code> <p>Stitched neuron.</p> See Also <p><code>navis.combine_neurons</code>                     Combines multiple neurons of the same type into one                     without stitching. Works on TreeNeurons, MeshNeurons                     and Dotprops.</p> <p>Examples:</p> <p>Stitching neuronlist by simply combining data tables:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(2)\n&gt;&gt;&gt; stitched = navis.stitch_skeletons(nl, method='NONE')\n</code></pre> <p>Stitching fragmented neurons:</p> <pre><code>&gt;&gt;&gt; a = navis.example_neurons(1)\n&gt;&gt;&gt; fragments = navis.cut_skeleton(a, 100)\n&gt;&gt;&gt; stitched = navis.stitch_skeletons(fragments, method='LEAFS')\n</code></pre> Source code in <code>navis/morpho/manipulation.py</code> <pre><code>def stitch_skeletons(\n    *x: Union[Sequence[NeuronObject], \"core.NeuronList\"],\n    method: Union[\n        Literal[\"LEAFS\"], Literal[\"ALL\"], Literal[\"NONE\"], Sequence[int]\n    ] = \"ALL\",\n    master: Union[Literal[\"SOMA\"], Literal[\"LARGEST\"], Literal[\"FIRST\"]] = \"SOMA\",\n    max_dist: Optional[float] = None,\n) -&gt; \"core.TreeNeuron\":\n    \"\"\"Stitch multiple skeletons together.\n\n    Uses minimum spanning tree to determine a way to connect all fragments\n    while minimizing length (Euclidean distance) of the new edges. Nodes\n    that have been stitched will get a \"stitched\" tag.\n\n    Important\n    ---------\n    If duplicate node IDs are found across the fragments to stitch they will\n    be remapped to new unique values!\n\n    Parameters\n    ----------\n    x :                 NeuronList | list of TreeNeuron/List\n                        Neurons to stitch (see examples).\n    method :            'LEAFS' | 'ALL' | 'NONE' | list of node IDs\n                        Set stitching method:\n                            (1) 'LEAFS': Only leaf (including root) nodes will\n                                be allowed to make new edges.\n                            (2) 'ALL': All nodes are considered.\n                            (3) 'NONE': Node and connector tables will simply\n                                be combined without generating any new edges.\n                                The resulting neuron will have multiple roots.\n                            (4) List of node IDs that are allowed to be used.\n                                Note that if these nodes are insufficient\n                                the resulting neuron will not be fully\n                                connected.\n\n    master :            'SOMA' | 'LARGEST' | 'FIRST', optional\n                        Sets the master neuron:\n                            (1) 'SOMA': The largest fragment with a soma\n                                becomes the master neuron. If no neuron with\n                                soma, will pick the largest (option 2).\n                            (2) 'LARGEST': The largest (by number of nodes)\n                                fragment becomes the master neuron.\n                            (3) 'FIRST': The first fragment provided becomes\n                                the master neuron.\n    max_dist :          float,  optional\n                        Max distance at which to stitch nodes. This can result\n                        in a neuron with multiple roots.\n\n    Returns\n    -------\n    TreeNeuron\n                        Stitched neuron.\n\n    See Also\n    --------\n    [`navis.combine_neurons`][]\n                        Combines multiple neurons of the same type into one\n                        without stitching. Works on TreeNeurons, MeshNeurons\n                        and Dotprops.\n\n    Examples\n    --------\n    Stitching neuronlist by simply combining data tables:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(2)\n    &gt;&gt;&gt; stitched = navis.stitch_skeletons(nl, method='NONE')\n\n    Stitching fragmented neurons:\n\n    &gt;&gt;&gt; a = navis.example_neurons(1)\n    &gt;&gt;&gt; fragments = navis.cut_skeleton(a, 100)\n    &gt;&gt;&gt; stitched = navis.stitch_skeletons(fragments, method='LEAFS')\n\n    \"\"\"\n    master = str(master).upper()\n    ALLOWED_MASTER = (\"SOMA\", \"LARGEST\", \"FIRST\")\n    utils.eval_param(master, \"master\", allowed_values=ALLOWED_MASTER)\n\n    # Compile list of individual neurons\n    neurons = utils.unpack_neurons(x)\n\n    # Use copies of the original neurons!\n    nl = core.NeuronList(neurons).copy()\n\n    if len(nl) &lt; 2:\n        logger.warning(f\"Need at least 2 neurons to stitch, found {len(nl)}\")\n        return nl[0]\n\n    # If no soma, switch to largest\n    if master == \"SOMA\" and not any(nl.has_soma):\n        master = \"LARGEST\"\n\n    # First find master\n    if master == \"SOMA\":\n        # Pick the first neuron with a soma\n        m_ix = [i for i, n in enumerate(nl) if n.has_soma][0]\n    elif master == \"LARGEST\":\n        # Pick the largest neuron\n        m_ix = sorted(list(range(len(nl))), key=lambda x: nl[x].n_nodes, reverse=True)[\n            0\n        ]\n    else:\n        # Pick the first neuron\n        m_ix = 0\n    m = nl[m_ix]\n\n    # Check if we need to make any node IDs unique\n    if nl.nodes.duplicated(subset=\"node_id\").sum() &gt; 0:\n        # Master neuron will not be changed\n        seen_tn: Set[int] = set(m.nodes.node_id)\n        for i, n in enumerate(nl):\n            # Skip the master neuron\n            # Note we're using the index in case we have two neurons that are\n            # equal (by our definition) - happens e.g. if a neuron has been\n            # mirrored\n            if i == m_ix:\n                continue\n\n            # Grab nodes\n            this_tn = set(n.nodes.node_id)\n\n            # Get duplicate node IDs\n            non_unique = seen_tn &amp; this_tn\n\n            # Add this neuron's existing nodes to seen\n            seen_tn = seen_tn | this_tn\n            if non_unique:\n                # Generate new, unique node IDs\n                new_tn = np.arange(0, len(non_unique)) + max(seen_tn) + 1\n\n                # Generate new map\n                new_map = dict(zip(non_unique, new_tn))\n\n                # Remap node IDs - if no new value, keep the old\n                n.nodes[\"node_id\"] = n.nodes.node_id.map(lambda x: new_map.get(x, x))\n\n                if n.has_connectors:\n                    n.connectors[\"node_id\"] = n.connectors.node_id.map(\n                        lambda x: new_map.get(x, x)\n                    )\n\n                if getattr(n, \"tags\", None) is not None:\n                    n.tags = {new_map.get(k, k): v for k, v in n.tags.items()}  # type: ignore\n\n                # Remap parent IDs\n                new_map[None] = -1  # type: ignore\n                n.nodes[\"parent_id\"] = n.nodes.parent_id.map(\n                    lambda x: new_map.get(x, x)\n                ).astype(int)\n\n                # Add new nodes to seen\n                seen_tn = seen_tn | set(new_tn)\n\n                # Make sure the graph is updated\n                n._clear_temp_attr()\n\n    # We will start by simply merging all neurons into one\n    m._nodes = pd.concat(\n        [n.nodes for n in nl],  # type: ignore  # no stubs for concat\n        ignore_index=True,\n    )\n\n    if any(nl.has_connectors):\n        m._connectors = pd.concat(\n            [n.connectors for n in nl],  # type: ignore  # no stubs for concat\n            ignore_index=True,\n        )\n\n    if not m.has_tags or not isinstance(m.tags, dict):\n        m.tags = {}  # type: ignore  # TreeNeuron has no tags\n\n    for n in nl:\n        for k, v in (getattr(n, \"tags\", None) or {}).items():\n            m.tags[k] = m.tags.get(k, []) + list(utils.make_iterable(v))\n\n    # Reset temporary attributes of our final neuron\n    m._clear_temp_attr()\n\n    # If this is all we meant to do, return this neuron\n    if not utils.is_iterable(method) and (method == \"NONE\" or method is None):\n        return m\n\n    return _stitch_mst(m, nodes=method, inplace=False, max_dist=max_dist)\n</code></pre>"},{"location":"reference/navis/#navis.strahler_index","title":"<code>navis.strahler_index</code>","text":"<p>Calculate Strahler Index (SI).</p> <p>Starts with SI of 1 at each leaf and walks to root. At forks with different incoming SIs, the highest index is continued. At forks with the same incoming SI, highest index + 1 is continued.</p> PARAMETER DESCRIPTION <code>x</code> <p> TYPE: <code>                TreeNeuron | MeshNeuron | NeuronList</code> </p> <code>method</code> <pre><code>            Method used to calculate Strahler indices: 'standard'\n            will use the method described above; 'greedy' will\n            always increase the index at converging branches\n            whether these branches have the same index or not.\n</code></pre> <p> TYPE: <code>           'standard' | 'greedy'</code> DEFAULT: <code>'standard'</code> </p> <code>to_ignore</code> <pre><code>            List of node IDs to ignore. Must be the FIRST node\n            of the branch. Excluded branches will not contribute\n            to Strahler index calculations and instead be assigned\n            the SI of their parent branch.\n</code></pre> <p> TYPE: <code>        iterable</code> DEFAULT: <code>[]</code> </p> <code>min_twig_size</code> <pre><code>            If provided, will ignore twigs with fewer nodes than\n            this. Instead, they will be assigned the SI of their\n            parent branch.\n</code></pre> <p> TYPE: <code>    int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>neuron</code> <p>Adds \"strahler_index\" as column in the node table (for TreeNeurons) or as <code>.\"strahler_index</code> property (for MeshNeurons).</p> See Also <p><code>navis.segment_analysis</code>             This function provides by-segment morphometrics, including             Strahler indices.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(2, kind='skeleton')\n&gt;&gt;&gt; n.reroot(n.soma, inplace=True)\n&gt;&gt;&gt; _ = navis.strahler_index(n)\n&gt;&gt;&gt; n[0].nodes.strahler_index.max()\n6\n&gt;&gt;&gt; m = navis.example_neurons(1, kind='mesh')\n&gt;&gt;&gt; _ = navis.strahler_index(m)\n&gt;&gt;&gt; m.strahler_index.max()\n5\n</code></pre> Source code in <code>navis/morpho/mmetrics.py</code> <pre><code>@utils.map_neuronlist(desc=\"Calc. SI\", allow_parallel=True)\n@utils.meshneuron_skeleton(\n    method=\"node_properties\", reroot_soma=True, node_props=[\"strahler_index\"]\n)\ndef strahler_index(\n    x: \"core.NeuronObject\",\n    method: Union[Literal[\"standard\"], Literal[\"greedy\"]] = \"standard\",\n    to_ignore: list = [],\n    min_twig_size: Optional[int] = None,\n) -&gt; \"core.NeuronObject\":\n    \"\"\"Calculate Strahler Index (SI).\n\n    Starts with SI of 1 at each leaf and walks to root. At forks with different\n    incoming SIs, the highest index is continued. At forks with the same\n    incoming SI, highest index + 1 is continued.\n\n    Parameters\n    ----------\n    x :                 TreeNeuron | MeshNeuron | NeuronList\n    method :            'standard' | 'greedy', optional\n                        Method used to calculate Strahler indices: 'standard'\n                        will use the method described above; 'greedy' will\n                        always increase the index at converging branches\n                        whether these branches have the same index or not.\n    to_ignore :         iterable, optional\n                        List of node IDs to ignore. Must be the FIRST node\n                        of the branch. Excluded branches will not contribute\n                        to Strahler index calculations and instead be assigned\n                        the SI of their parent branch.\n    min_twig_size :     int, optional\n                        If provided, will ignore twigs with fewer nodes than\n                        this. Instead, they will be assigned the SI of their\n                        parent branch.\n\n    Returns\n    -------\n    neuron\n                Adds \"strahler_index\" as column in the node table (for\n                TreeNeurons) or as `.\"strahler_index` property\n                (for MeshNeurons).\n\n    See Also\n    --------\n    [`navis.segment_analysis`][]\n                This function provides by-segment morphometrics, including\n                Strahler indices.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(2, kind='skeleton')\n    &gt;&gt;&gt; n.reroot(n.soma, inplace=True)\n    &gt;&gt;&gt; _ = navis.strahler_index(n)\n    &gt;&gt;&gt; n[0].nodes.strahler_index.max()\n    6\n    &gt;&gt;&gt; m = navis.example_neurons(1, kind='mesh')\n    &gt;&gt;&gt; _ = navis.strahler_index(m)\n    &gt;&gt;&gt; m.strahler_index.max()\n    5\n\n    \"\"\"\n    utils.eval_param(x, name=\"x\", allowed_types=(core.TreeNeuron,))\n\n    if method not in [\"standard\", \"greedy\"]:\n        raise ValueError(f'`method` must be \"standard\" or \"greedy\", got \"{method}\"')\n\n    if utils.fastcore:\n        x.nodes[\"strahler_index\"] = utils.fastcore.strahler_index(\n            x.nodes.node_id.values,\n            x.nodes.parent_id.values,\n            method=method,\n            to_ignore=to_ignore,\n            min_twig_size=min_twig_size,\n        ).astype(np.int16)\n        x.nodes[\"strahler_index\"] = x.nodes.strahler_index.fillna(1)\n        return x\n\n    # Find branch, root and end nodes\n    if \"type\" not in x.nodes:\n        graph.classify_nodes(x)\n\n    end_nodes = x.nodes[x.nodes.type == \"end\"].node_id.values\n    branch_nodes = x.nodes[x.nodes.type == \"branch\"].node_id.values\n    root = x.nodes[x.nodes.type == \"root\"].node_id.values\n\n    end_nodes = set(end_nodes)\n    branch_nodes = set(branch_nodes)\n    root = set(root)\n\n    if min_twig_size:\n        to_ignore = np.append(\n            to_ignore,\n            [\n                seg[0]\n                for seg in x.small_segments\n                if seg[0] in end_nodes and len(seg) &lt; min_twig_size\n            ],\n        ).astype(int)\n\n    # Generate dicts for childs and parents\n    list_of_childs = graph.generate_list_of_childs(x)\n\n    # Get a node ID -&gt; parent ID dictionary for fast lookups\n    parents = x.nodes.set_index(\"node_id\").parent_id.to_dict()\n\n    # Do NOT name any parameter `strahler_index` - this overwrites the function!\n    SI: Dict[int, int] = {}\n\n    starting_points = end_nodes\n    seen = set()\n    while starting_points:\n        logger.debug(f\"New starting point. Remaining: {len(starting_points)}\")\n        this_node = starting_points.pop()\n\n        # Get upstream indices for this branch\n        previous_indices = [SI[c] for c in list_of_childs[this_node]]\n\n        # If this is a not-a-branch branch\n        if this_node in to_ignore:\n            this_branch_index = 0\n        # If this is an end node: start at 1\n        elif not len(previous_indices):\n            this_branch_index = 1\n        # If this is a slab: assign SI of predecessor\n        elif len(previous_indices) == 1:\n            this_branch_index = previous_indices[0]\n        # If this is a branch point and we're using the greedy method\n        elif method == \"greedy\":\n            this_branch_index = sum(previous_indices)\n        # If this is a branch point at which similar indices collide: +1\n        elif previous_indices.count(max(previous_indices)) &gt;= 2:\n            this_branch_index = max(previous_indices) + 1\n        # If just a branch point: continue max SI\n        else:\n            this_branch_index = max(previous_indices)\n\n        # Keep track of that this node has been processed\n        seen.add(this_node)\n\n        # Now walk down this segment\n        # Find parent\n        segment = [this_node]\n        parent_node = parents[this_node]\n        while parent_node &gt;= 0 and parent_node not in branch_nodes:\n            this_node = parent_node\n            parent_node = parents[this_node]\n            segment.append(this_node)\n            seen.add(this_node)\n\n        # Update indices for the entire segment\n        SI.update({n: this_branch_index for n in segment})\n\n        # The last `this_node` is either a branch node or the root\n        # If a branch point: check, if all its childs have already been\n        # processed\n        if parent_node &gt; 0:\n            node_ready = True\n            for child in list_of_childs[parent_node]:\n                if child not in seen:\n                    node_ready = False\n                    break\n\n            if node_ready is True:\n                starting_points.add(parent_node)\n\n    # Fix branches that were ignored\n    if len(to_ignore):\n        # Go over all terminal branches with the tag\n        for tn in x.nodes[\n            (x.nodes.type == \"end\") &amp; x.nodes.node_id.isin(to_ignore)\n        ].node_id.values:\n            # Get this terminal's segment\n            this_seg = [s for s in x.small_segments if s[0] == tn][0]\n            # Get strahler index of parent branch\n            this_SI = SI.get(this_seg[-1], 1)\n            SI.update({n: this_SI for n in this_seg})\n\n    # Disconnected single nodes (e.g. after pruning) will end up w/o an entry\n    # --&gt; we will give them an SI of 1\n    x.nodes[\"strahler_index\"] = x.nodes.node_id.map(lambda x: SI.get(x, 1))\n\n    # Set correct data type\n    x.nodes[\"strahler_index\"] = x.nodes.strahler_index.astype(np.int16)\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.subset_neuron","title":"<code>navis.subset_neuron</code>","text":"<p>Subset a neuron to a given set of nodes/vertices.</p> <p>Note that for <code>MeshNeurons</code> it is not guaranteed that all vertices in <code>subset</code> survive because we will also drop degenerate vertices that do not participate in any faces.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>              Neuron to subset. When passing a NeuronList, it's advised\n              to use a function for `subset` (see below).\n</code></pre> <p> TYPE: <code>                  TreeNeuron | MeshNeuron | Dotprops | NeuronList</code> </p> <code>subset</code> <pre><code>              Subset of the neuron to keep. Depending on the neuron:\n                For TreeNeurons:\n                 - node IDs\n                 - a boolean mask matching the number of nodes\n                 - DataFrame with `node_id` column\n                For MeshNeurons:\n                 - vertex indices\n                 - a boolean mask matching either the number of\n                   vertices or faces\n                For Dotprops:\n                 - point indices\n                 - a boolean mask matching the number of points\n              Alternatively, you can pass a function that accepts\n              a neuron and returns a suitable `subset` as described\n              above. This is useful e.g. when wanting to subset a\n              list of neurons.\n</code></pre> <p> TYPE: <code>             list-like | set | NetworkX.Graph | pandas.DataFrame | Callable</code> </p> <code>keep_disc_cn</code> <pre><code>              If False, will remove disconnected connectors that\n              have \"lost\" their parent node/vertex.\n</code></pre> <p> TYPE: <code>       bool</code> DEFAULT: <code>False</code> </p> <code>prevent_fragments</code> <pre><code>              If True, will add nodes/vertices to `subset`\n              required to keep neuron from fragmenting. Ignored for\n              `Dotprops`.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> <code>inplace</code> <pre><code>              If False, a copy of the neuron is returned.\n</code></pre> <p> TYPE: <code>            bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>TreeNeuron | MeshNeuron | Dotprops | NeuronList</code> <p>Examples:</p> <p>Subset skeleton to all branches with less than 10 nodes</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; # Get neuron\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; # Get all linear segments\n&gt;&gt;&gt; segs = n.segments\n&gt;&gt;&gt; # Get short segments\n&gt;&gt;&gt; short_segs = [s for s in segs if len(s) &lt;= 10]\n&gt;&gt;&gt; # Flatten segments into list of nodes\n&gt;&gt;&gt; nodes_to_keep = [n for s in short_segs for n in s]\n&gt;&gt;&gt; # Subset neuron\n&gt;&gt;&gt; n_short = navis.subset_neuron(n, subset=nodes_to_keep)\n</code></pre> <p>Subset multiple neurons using a callable</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(2)\n&gt;&gt;&gt; # Subset neurons to all leaf nodes\n&gt;&gt;&gt; nl_end = navis.subset_neuron(\n...     nl,\n...     subset=lambda x: x.leafs.node_id\n... )\n</code></pre> See Also <p><code>navis.cut_skeleton</code>         Cut neuron at specific points. <code>navis.in_volume</code>         To intersect a neuron with a volume (mesh).</p> Source code in <code>navis/morpho/subset.py</code> <pre><code>@utils.map_neuronlist(desc=\"Subsetting\", allow_parallel=True)\n@utils.lock_neuron\ndef subset_neuron(\n    x: Union[\"core.TreeNeuron\", \"core.MeshNeuron\"],\n    subset: Union[Sequence[Union[int, str]], nx.DiGraph, pd.DataFrame, Callable],\n    inplace: bool = False,\n    keep_disc_cn: bool = False,\n    prevent_fragments: bool = False,\n) -&gt; \"core.NeuronObject\":\n    \"\"\"Subset a neuron to a given set of nodes/vertices.\n\n    Note that for `MeshNeurons` it is not guaranteed that all vertices in\n    `subset` survive because we will also drop degenerate vertices that do\n    not participate in any faces.\n\n    Parameters\n    ----------\n    x :                   TreeNeuron | MeshNeuron | Dotprops | NeuronList\n                          Neuron to subset. When passing a NeuronList, it's advised\n                          to use a function for `subset` (see below).\n    subset :              list-like | set | NetworkX.Graph | pandas.DataFrame | Callable\n                          Subset of the neuron to keep. Depending on the neuron:\n                            For TreeNeurons:\n                             - node IDs\n                             - a boolean mask matching the number of nodes\n                             - DataFrame with `node_id` column\n                            For MeshNeurons:\n                             - vertex indices\n                             - a boolean mask matching either the number of\n                               vertices or faces\n                            For Dotprops:\n                             - point indices\n                             - a boolean mask matching the number of points\n                          Alternatively, you can pass a function that accepts\n                          a neuron and returns a suitable `subset` as described\n                          above. This is useful e.g. when wanting to subset a\n                          list of neurons.\n    keep_disc_cn :        bool, optional\n                          If False, will remove disconnected connectors that\n                          have \"lost\" their parent node/vertex.\n    prevent_fragments :   bool, optional\n                          If True, will add nodes/vertices to `subset`\n                          required to keep neuron from fragmenting. Ignored for\n                          `Dotprops`.\n    inplace :             bool, optional\n                          If False, a copy of the neuron is returned.\n\n    Returns\n    -------\n    TreeNeuron | MeshNeuron | Dotprops | NeuronList\n\n    Examples\n    --------\n    Subset skeleton to all branches with less than 10 nodes\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; # Get neuron\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; # Get all linear segments\n    &gt;&gt;&gt; segs = n.segments\n    &gt;&gt;&gt; # Get short segments\n    &gt;&gt;&gt; short_segs = [s for s in segs if len(s) &lt;= 10]\n    &gt;&gt;&gt; # Flatten segments into list of nodes\n    &gt;&gt;&gt; nodes_to_keep = [n for s in short_segs for n in s]\n    &gt;&gt;&gt; # Subset neuron\n    &gt;&gt;&gt; n_short = navis.subset_neuron(n, subset=nodes_to_keep)\n\n    Subset multiple neurons using a callable\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(2)\n    &gt;&gt;&gt; # Subset neurons to all leaf nodes\n    &gt;&gt;&gt; nl_end = navis.subset_neuron(\n    ...     nl,\n    ...     subset=lambda x: x.leafs.node_id\n    ... )\n\n    See Also\n    --------\n    [`navis.cut_skeleton`][]\n            Cut neuron at specific points.\n    [`navis.in_volume`][]\n            To intersect a neuron with a volume (mesh).\n\n    \"\"\"\n    if isinstance(x, core.NeuronList) and len(x) == 1:\n        x = x[0]\n\n    utils.eval_param(\n        x, name=\"x\", allowed_types=(core.TreeNeuron, core.MeshNeuron, core.Dotprops)\n    )\n\n    if callable(subset):\n        subset = subset(x)\n\n    # Make a copy of the neuron\n    if not inplace:\n        x = x.copy()\n        # We have to run this in a separate function so that the lock is applied\n        # to the copy\n        subset_neuron(\n            x,\n            subset=subset,\n            inplace=True,\n            keep_disc_cn=keep_disc_cn,\n            prevent_fragments=prevent_fragments,\n        )\n        return x\n\n    if isinstance(x, core.TreeNeuron):\n        x = _subset_treeneuron(\n            x,\n            subset=subset,\n            keep_disc_cn=keep_disc_cn,\n            prevent_fragments=prevent_fragments,\n        )\n    elif isinstance(x, core.MeshNeuron):\n        x = _subset_meshneuron(\n            x,\n            subset=subset,\n            keep_disc_cn=keep_disc_cn,\n            prevent_fragments=prevent_fragments,\n        )\n    elif isinstance(x, core.Dotprops):\n        x = _subset_dotprops(x, subset=subset, keep_disc_cn=keep_disc_cn)\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.symmetrize_brain","title":"<code>navis.symmetrize_brain</code>","text":"<p>Symmetrize 3D object (neuron, coordinates).</p> <p>The way this works is by:  1. Finding the closest mirror transform (unless provided)  2. Mirror data on the left-hand-side to the right-hand-side using the     proper (warp) mirror transform to offset deformations  3. Simply flip that data back to the left-hand-side</p> <p>This works reasonably well but may produce odd results around the midline. For high quality symmetrization you are better off generating dedicated transform (see <code>navis-flybrains</code> for an example).</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Data to transform. Dataframe must contain `['x', 'y', 'z']`\n        columns. Numpy array must be shape `(N, 3)`.\n</code></pre> <p> TYPE: <code>            Neuron/List | Volume/trimesh | numpy.ndarray | pandas.DataFrame</code> </p> <code>template</code> <pre><code>        Source template brain space that the data is in. If string\n        will be searched against registered template brains.\n</code></pre> <p> TYPE: <code>     str | TemplateBrain</code> </p> <code>via</code> <pre><code>        By default (\"auto\") it will find and apply the closest\n        mirror transform. You can also specify a template that\n        should be used. That template must have a mirror transform!\n</code></pre> <p> TYPE: <code>          \"auto\" | str</code> DEFAULT: <code>'auto'</code> </p> <code>progress</code> <pre><code>        Whether to show a progress bar when symmetrizing multiple\n        neurons.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> <code>verbose</code> <pre><code>        If True, will print some useful info on the transform(s).\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>xs</code> <p>Same object type as input (array, neurons, etc) but hopefully symmetrical.</p> <p>Examples:</p> <p>This example requires the flybrains library to be installed: <code>pip3 install flybrains</code></p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; import flybrains\n&gt;&gt;&gt; # Get the FAFB14 neuropil mesh\n&gt;&gt;&gt; m = flybrains.FAFB14.mesh\n&gt;&gt;&gt; # Symmetrize the mesh\n&gt;&gt;&gt; s = navis.symmetrize_brain(m, template='FAFB14')\n&gt;&gt;&gt; # Plot side-by-side for comparison\n&gt;&gt;&gt; m.plot3d()\n&gt;&gt;&gt; s.plot3d(color=(1, 0, 0))\n</code></pre> Source code in <code>navis/transforms/templates.py</code> <pre><code>def symmetrize_brain(\n    x: Union[\"core.NeuronObject\", \"pd.DataFrame\", \"np.ndarray\"],\n    template: Union[str, \"TemplateBrain\"],\n    via: Optional[str] = \"auto\",\n    progress: bool = True,\n    verbose: bool = False,\n) -&gt; Union[\"core.NeuronObject\", \"pd.DataFrame\", \"np.ndarray\"]:\n    \"\"\"Symmetrize 3D object (neuron, coordinates).\n\n    The way this works is by:\n     1. Finding the closest mirror transform (unless provided)\n     2. Mirror data on the left-hand-side to the right-hand-side using the\n        proper (warp) mirror transform to offset deformations\n     3. Simply flip that data back to the left-hand-side\n\n    This works reasonably well but may produce odd results around the midline.\n    For high quality symmetrization you are better off generating dedicated\n    transform (see `navis-flybrains` for an example).\n\n    Parameters\n    ----------\n    x :             Neuron/List | Volume/trimesh | numpy.ndarray | pandas.DataFrame\n                    Data to transform. Dataframe must contain `['x', 'y', 'z']`\n                    columns. Numpy array must be shape `(N, 3)`.\n    template :      str | TemplateBrain\n                    Source template brain space that the data is in. If string\n                    will be searched against registered template brains.\n    via :           \"auto\" | str\n                    By default (\"auto\") it will find and apply the closest\n                    mirror transform. You can also specify a template that\n                    should be used. That template must have a mirror transform!\n    progress :      bool\n                    Whether to show a progress bar when symmetrizing multiple\n                    neurons.\n    verbose :       bool\n                    If True, will print some useful info on the transform(s).\n\n    Returns\n    -------\n    xs\n                    Same object type as input (array, neurons, etc) but\n                    hopefully symmetrical.\n\n    Examples\n    --------\n    This example requires the\n    [flybrains](https://github.com/navis-org/navis-flybrains)\n    library to be installed: `pip3 install flybrains`\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; import flybrains\n    &gt;&gt;&gt; # Get the FAFB14 neuropil mesh\n    &gt;&gt;&gt; m = flybrains.FAFB14.mesh\n    &gt;&gt;&gt; # Symmetrize the mesh\n    &gt;&gt;&gt; s = navis.symmetrize_brain(m, template='FAFB14')\n    &gt;&gt;&gt; # Plot side-by-side for comparison\n    &gt;&gt;&gt; m.plot3d()                                              # doctest: +SKIP\n    &gt;&gt;&gt; s.plot3d(color=(1, 0, 0))                               # doctest: +SKIP\n\n    \"\"\"\n    if not isinstance(template, str):\n        TypeError(f'Expected template of type str, got \"{type(template)}\"')\n\n    if via == \"auto\":\n        # Find closest mirror transform\n        via = registry.find_closest_mirror_reg(template)\n\n    if isinstance(x, core.NeuronList):\n        if len(x) == 1:\n            x = x[0]\n        else:\n            xf = []\n            for n in config.tqdm(\n                x,\n                desc=\"Mirroring\",\n                disable=config.pbar_hide or not progress,\n                leave=config.pbar_leave,\n            ):\n                xf.append(symmetrize_brain(n, template=template, via=via))\n            return core.NeuronList(xf)\n\n    if isinstance(x, core.BaseNeuron):\n        x = x.copy()\n        if isinstance(x, core.TreeNeuron):\n            x.nodes = symmetrize_brain(x.nodes, template=template, via=via)\n        elif isinstance(x, core.Dotprops):\n            x.points = symmetrize_brain(x.points, template=template, via=via)\n            # Set tangent vectors and alpha to None so they will be regenerated\n            x._vect = x._alpha = None\n        elif isinstance(x, core.MeshNeuron):\n            x.vertices = symmetrize_brain(x.vertices, template=template, via=via)\n        else:\n            raise TypeError(f\"Don't know how to transform neuron of type '{type(x)}'\")\n\n        if x.has_connectors:\n            x.connectors = symmetrize_brain(x.connectors, template=template, via=via)\n        return x\n    elif isinstance(x, tm.Trimesh):\n        x = x.copy()\n        x.vertices = symmetrize_brain(x.vertices, template=template, via=via)\n        return x\n    elif isinstance(x, pd.DataFrame):\n        if any([c not in x.columns for c in [\"x\", \"y\", \"z\"]]):\n            raise ValueError(\"DataFrame must have x, y and z columns.\")\n        x = x.copy()\n        x[[\"x\", \"y\", \"z\"]] = symmetrize_brain(\n            x[[\"x\", \"y\", \"z\"]].values.astype(float), template=template, via=via\n        )\n        return x\n    else:\n        try:\n            # At this point we expect numpy arrays\n            x = np.asarray(x)\n        except BaseException:\n            raise TypeError(f'Unable to transform data of type \"{type(x)}\"')\n\n        if not x.ndim == 2 or x.shape[1] != 3:\n            raise ValueError(\"Array must be of shape (N, 3).\")\n\n    # Now find the meta info for this template brain\n    if isinstance(template, TemplateBrain):\n        tb = template\n    else:\n        tb = registry.find_template(template, non_found=\"raise\")\n\n    # Get the bounding box\n    if not hasattr(tb, \"boundingbox\"):\n        raise ValueError(f'Template \"{tb.label}\" has no bounding box info.')\n\n    if not isinstance(tb.boundingbox, (list, tuple, np.ndarray)):\n        raise TypeError(\n            \"Expected the template brain's bounding box to be a \"\n            f\"list, tuple or array - got '{type(tb.boundingbox)}'\"\n        )\n\n    # Get bounding box of template brain\n    bbox = np.asarray(tb.boundingbox)\n\n    # Reshape if flat array\n    if bbox.ndim == 1:\n        bbox = bbox.reshape(3, 2)\n\n    # Find points on the left\n    center = bbox[0][0] + (bbox[0][1] - bbox[0][0]) / 2\n    is_left = x[:, 0] &gt; center\n\n    # Make a copy of the original data\n    x = x.copy()\n\n    # If nothing to symmetrize - return\n    if is_left.sum() == 0:\n        return x\n\n    # Mirror with compensation for deformations\n    xm = mirror_brain(\n        x[is_left], template=template, via=via, mirror_axis=\"x\", verbose=verbose\n    )\n\n    # And now flip them back without compensation for deformations\n    xmf = mirror_brain(xm, template=template, warp=False, mirror_axis=\"x\")\n\n    # Replace values\n    x[is_left] = xmf\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.synapse_flow_centrality","title":"<code>navis.synapse_flow_centrality</code>","text":"<p>Calculate synapse flow centrality (SFC).</p> <p>From Schneider-Mizell et al. (2016): \"We use flow centrality for four purposes. First, to split an arbor into axon and dendrite at the maximum centrifugal SFC, which is a preliminary step for computing the segregation index, for expressing all kinds of connectivity edges (e.g. axo-axonic, dendro-dendritic) in the wiring diagram, or for rendering the arbor in 3d with differently colored regions. Second, to quantitatively estimate the cable distance between the axon terminals and dendritic arbor by measuring the amount of cable with the maximum centrifugal SFC value. Third, to measure the cable length of the main dendritic shafts using centripetal SFC, which applies only to insect neurons with at least one output synapse in their dendritic arbor. And fourth, to weigh the color of each skeleton node in a 3d view, providing a characteristic signature of the arbor that enables subjective evaluation of its identity.\"</p> <p>Uses navis-fastcore if available.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Neuron(s) to calculate synapse flow centrality for. Must have\n    connectors!\n</code></pre> <p> TYPE: <code>        TreeNeuron | MeshNeuron | NeuronList</code> </p> <code>mode</code> <pre><code>    Type of flow centrality to calculate. There are three flavors::\n    (1) centrifugal counts paths from proximal inputs to distal outputs\n    (2) centripetal counts paths from distal inputs to proximal outputs\n    (3) the sum of both - this is the original implementation\n</code></pre> <p> TYPE: <code>     'centrifugal' | 'centripetal' | 'sum'</code> DEFAULT: <code>'sum'</code> </p> RETURNS DESCRIPTION <code>neuron</code> <p>Adds \"synapse_flow_centrality\" as column in the node table (for TreeNeurons) or as <code>.synapse_flow_centrality</code> property (for MeshNeurons).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(2)\n&gt;&gt;&gt; n.reroot(n.soma, inplace=True)\n&gt;&gt;&gt; _ = navis.synapse_flow_centrality(n)\n&gt;&gt;&gt; n[0].nodes.synapse_flow_centrality.max()\n786969\n</code></pre> See Also <p><code>navis.bending_flow</code>         Variation of synapse flow centrality: calculates bending flow. <code>navis.arbor_segregation_index</code>         By-arbor segregation index. <code>navis.segregation_index</code>         Calculates segregation score (polarity) of a neuron. <code>navis.split_axon_dendrite</code>         Tries splitting a neuron into axon and dendrite. <code>navis.flow_centrality</code>         Leaf-based version of flow centrality.</p> Source code in <code>navis/morpho/mmetrics.py</code> <pre><code>@utils.map_neuronlist(desc=\"Calc. flow\", allow_parallel=True)\n@utils.meshneuron_skeleton(\n    method=\"node_properties\",\n    include_connectors=True,\n    heal=True,\n    node_props=[\"synapse_flow_centrality\"],\n)\ndef synapse_flow_centrality(\n    x: \"core.NeuronObject\",\n    mode: Union[Literal[\"centrifugal\"], Literal[\"centripetal\"], Literal[\"sum\"]] = \"sum\",\n) -&gt; \"core.NeuronObject\":\n    \"\"\"Calculate synapse flow centrality (SFC).\n\n    From Schneider-Mizell et al. (2016): \"We use flow centrality for\n    four purposes. First, to split an arbor into axon and dendrite at the\n    maximum centrifugal SFC, which is a preliminary step for computing the\n    segregation index, for expressing all kinds of connectivity edges (e.g.\n    axo-axonic, dendro-dendritic) in the wiring diagram, or for rendering the\n    arbor in 3d with differently colored regions. Second, to quantitatively\n    estimate the cable distance between the axon terminals and dendritic arbor\n    by measuring the amount of cable with the maximum centrifugal SFC value.\n    Third, to measure the cable length of the main dendritic shafts using\n    centripetal SFC, which applies only to insect neurons with at least one\n    output synapse in their dendritic arbor. And fourth, to weigh the color\n    of each skeleton node in a 3d view, providing a characteristic signature of\n    the arbor that enables subjective evaluation of its identity.\"\n\n    Uses navis-fastcore if available.\n\n    Parameters\n    ----------\n    x :         TreeNeuron | MeshNeuron | NeuronList\n                Neuron(s) to calculate synapse flow centrality for. Must have\n                connectors!\n    mode :      'centrifugal' | 'centripetal' | 'sum', optional\n                Type of flow centrality to calculate. There are three flavors::\n                (1) centrifugal counts paths from proximal inputs to distal outputs\n                (2) centripetal counts paths from distal inputs to proximal outputs\n                (3) the sum of both - this is the original implementation\n\n    Returns\n    -------\n    neuron\n                Adds \"synapse_flow_centrality\" as column in the node table (for\n                TreeNeurons) or as `.synapse_flow_centrality` property\n                (for MeshNeurons).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(2)\n    &gt;&gt;&gt; n.reroot(n.soma, inplace=True)\n    &gt;&gt;&gt; _ = navis.synapse_flow_centrality(n)\n    &gt;&gt;&gt; n[0].nodes.synapse_flow_centrality.max()\n    786969\n\n    See Also\n    --------\n    [`navis.bending_flow`][]\n            Variation of synapse flow centrality: calculates bending flow.\n    [`navis.arbor_segregation_index`][]\n            By-arbor segregation index.\n    [`navis.segregation_index`][]\n            Calculates segregation score (polarity) of a neuron.\n    [`navis.split_axon_dendrite`][]\n            Tries splitting a neuron into axon and dendrite.\n    [`navis.flow_centrality`][]\n            Leaf-based version of flow centrality.\n\n    \"\"\"\n    # Quick disclaimer:\n    # This function may look unnecessarily complicated. I did also try out an\n    # implementation using igraph + shortest paths which works like a charm and\n    # causes less headaches. It is, however, about &gt;10X slower than this version!\n    # Note to self: do not go down that rabbit hole again!\n\n    if mode not in [\"centrifugal\", \"centripetal\", \"sum\"]:\n        raise ValueError(f'Unknown \"mode\" parameter: {mode}')\n\n    if not isinstance(x, core.TreeNeuron):\n        raise ValueError(f'Expected TreeNeuron(s), got \"{type(x)}\"')\n\n    if not x.has_connectors:\n        raise ValueError(\"Neuron must have connectors.\")\n\n    if np.any(x.soma) and not np.all(np.isin(x.soma, x.root)):\n        logger.warning(f\"Neuron {x.id} is not rooted to its soma!\")\n\n    # Figure out how connector types are labeled\n    cn_types = x.connectors.type.unique()\n    if any(np.isin([\"pre\", \"post\"], cn_types)):\n        pre, post = \"pre\", \"post\"\n    elif any(np.isin([0, 1], cn_types)):\n        pre, post = 0, 1\n    else:\n        raise ValueError(\n            f'Unable to parse connector types \"{cn_types}\" for neuron {x.id}'\n        )\n\n    if utils.fastcore:\n        x.nodes[\"synapse_flow_centrality\"] = utils.fastcore.synapse_flow_centrality(\n            node_ids=x.nodes.node_id.values,\n            parent_ids=x.nodes.parent_id.values,\n            presynapses=x.nodes.node_id.map(\n                x.connectors[(x.connectors.type == pre)].node_id.value_counts()\n            )\n            .fillna(0)\n            .astype(int)\n            .values,\n            postsynapses=x.nodes.node_id.map(\n                x.connectors[(x.connectors.type == post)].node_id.value_counts()\n            )\n            .fillna(0)\n            .astype(int)\n            .values,\n            mode=mode,\n        )\n        # Add info on method/mode used for flow centrality\n        x.centrality_method = mode  # type: ignore\n\n        # Need to add a restriction, that a branchpoint cannot have a lower\n        # flow than its highest child -&gt; this happens at the main branch point to\n        # the cell body fiber because the flow doesn't go \"through\" it in\n        # child -&gt; parent direction but rather \"across\" it from one child to the\n        # other\n        is_bp = x.nodes[\"type\"] == \"branch\"\n        bp = x.nodes.loc[is_bp, \"node_id\"].values\n        bp_childs = x.nodes[x.nodes.parent_id.isin(bp)]\n        max_flow = bp_childs.groupby(\"parent_id\").synapse_flow_centrality.max()\n        x.nodes.loc[is_bp, \"synapse_flow_centrality\"] = max_flow.loc[bp].values\n        x.nodes[\"synapse_flow_centrality\"] = x.nodes.synapse_flow_centrality.astype(int)\n        return x\n\n    # Get list of nodes with pre/postsynapses\n    pre_node_ids = x.connectors[x.connectors.type == pre].node_id.values\n    post_node_ids = x.connectors[x.connectors.type == post].node_id.values\n    total_post = len(post_node_ids)\n    total_pre = len(pre_node_ids)\n\n    # Get list of points to calculate flow centrality for:\n    # branches and and their children\n    is_bp = x.nodes[\"type\"] == \"branch\"\n    is_cn = x.nodes.node_id.isin(x.connectors.node_id)\n    calc_node_ids = x.nodes[is_bp | is_cn].node_id.values\n\n    # We will be processing a super downsampled version of the neuron to\n    # speed up calculations\n    current_level = logger.level\n    current_state = config.pbar_hide\n    logger.setLevel(\"ERROR\")\n    config.pbar_hide = True\n    y = sampling.downsample_neuron(\n        x=x,\n        downsampling_factor=float(\"inf\"),\n        inplace=False,\n        preserve_nodes=calc_node_ids,\n    )\n    logger.setLevel(current_level)\n    config.pbar_hide = current_state\n\n    # Get number of pre/postsynapses distal to each branch's childs\n    # Note that we're using geodesic matrix here because it is much more\n    # efficient than for `distal_to` for larger queries/neurons\n    dists = graph.geodesic_matrix(\n        y, from_=np.append(pre_node_ids, post_node_ids), directed=True, weight=None\n    )\n    distal = dists[calc_node_ids] &lt; np.inf\n\n    # Since nodes can have multiple pre-/postsynapses but they show up only\n    # once in distal, we have to reindex to reflect the correct number of synapes\n    distal_pre = distal.loc[pre_node_ids]\n    distal_post = distal.loc[post_node_ids]\n\n    # Sum up axis - now each row represents the number of pre/postsynapses\n    # that are distal to that node\n    distal_pre = distal_pre.sum(axis=0)\n    distal_post = distal_post.sum(axis=0)\n\n    if mode != \"centripetal\":\n        # Centrifugal is the flow from all proximal posts- to all distal presynapses\n        centrifugal = {\n            n: (total_post - distal_post[n]) * distal_pre[n] for n in calc_node_ids\n        }\n\n    if mode != \"centrifugal\":\n        # Centripetal is the flow from all distal post- to all non-distal presynapses\n        centripetal = {\n            n: distal_post[n] * (total_pre - distal_pre[n]) for n in calc_node_ids\n        }\n\n    # Now map this onto our neuron\n    if mode == \"centrifugal\":\n        flow = centrifugal\n    elif mode == \"centripetal\":\n        flow = centripetal\n    elif mode == \"sum\":\n        flow = {n: centrifugal[n] + centripetal[n] for n in centrifugal}\n\n    # At this point there is only flow for branch points and connectors nodes.\n    # Let's complete that mapping by adding flow for the nodes between branch points.\n    for s in x.small_segments:\n        # Segments' orientation goes from distal -&gt; proximal\n\n        # If first node in the segment has no flow, set to 0\n        flow[s[0]] = flow.get(s[0], 0)\n\n        # For each node get the flow of its child\n        for i in range(1, len(s)):\n            if s[i] not in flow:\n                flow[s[i]] = flow[s[i - 1]]\n\n    x.nodes[\"synapse_flow_centrality\"] = x.nodes.node_id.map(flow).fillna(0).astype(int)\n\n    # Need to add a restriction, that a branchpoint cannot have a lower\n    # flow than its highest child -&gt; this happens at the main branch point to\n    # the cell body fiber because the flow doesn't go \"through\" it in\n    # child -&gt; parent direction but rather \"across\" it from one child to the\n    # other\n    bp = x.nodes.loc[is_bp, \"node_id\"].values\n    bp_childs = x.nodes[x.nodes.parent_id.isin(bp)]\n    max_flow = bp_childs.groupby(\"parent_id\").synapse_flow_centrality.max()\n    x.nodes.loc[is_bp, \"synapse_flow_centrality\"] = max_flow.loc[bp].values\n    x.nodes[\"synapse_flow_centrality\"] = x.nodes.synapse_flow_centrality.astype(int)\n\n    # Add info on method/mode used for flow centrality\n    x.centrality_method = mode  # type: ignore\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.synapse_similarity","title":"<code>navis.synapse_similarity</code>","text":"<p>Cluster neurons based on their synapse placement.</p> <p>Distances score is calculated by calculating for each synapse of neuron A: (1) the (Euclidean) distance to the closest synapse in neuron B and (2) comparing the synapse density around synapse A and B. This is type-sensitive: presynapses will only be matched with presynapses, post with post, etc. The formula is described in Schlegel et al., eLife (2017):</p> \\[ f(i_{s},j_{k}) = \\exp(\\frac{-d^{2}_{sk}}{2\\sigma^{2}}) \\exp(\\frac{|n(i_{s})-n(j_{k})|}{n(i_{s})+n(j_{k})}) \\] <p>The synapse similarity score for neurons i and j being the average of \\(f(i_{s},j_{k})\\) over all synapses s of i. Synapse k is the closest synapse of the same sign (pre/post) in neuron j to synapse s. \\(d^{2}_{sk}\\) is the Euclidean distance between these distances. Variable \\(\\sigma\\) (<code>sigma</code>) determines what distance between s and k is considered \"close\". \\(n(i_{s})\\) and \\(n(j_{k})\\) are defined as the number of synapses of neuron i/j that are within given radius \\(\\omega\\) (<code>omega</code>) of synapse s and j, respectively (same sign only). This esnures that in cases of a strong disparity between \\(n(i_{s})\\) and \\(n(j_{k})\\), the synapse similarity will be close to zero even if the distance between s and k is very small.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            Neurons to compare. Must have connectors.\n</code></pre> <p> TYPE: <code>                NeuronList</code> </p> <code>sigma</code> <pre><code>            Distance between synapses that is considered to be\n            \"close\".\n</code></pre> <p> TYPE: <code>            int | float</code> </p> <code>omega</code> <pre><code>            Radius over which to calculate synapse density.\n</code></pre> <p> TYPE: <code>            int | float</code> </p> <code>mu_score</code> <pre><code>            If True, score is calculated as mean between A-&gt;B and\n            B-&gt;A comparison.\n</code></pre> <p> TYPE: <code>         bool</code> DEFAULT: <code>True</code> </p> <code>restrict_cn</code> <pre><code>            Restrict to given connector types. Must map to\n            a `type`, `relation` or `label` column in the\n            connector tables.\n            If None, will use all connector types. Use either\n            single integer or list. E.g. `restrict_cn=[0, 1]`\n            to use only pre- and postsynapses.\n</code></pre> <p> TYPE: <code>      int | list | None</code> DEFAULT: <code>None</code> </p> <code>n_cores</code> <pre><code>            Number of parallel processes to use. Defaults to half\n            the available cores.\n</code></pre> <p> TYPE: <code>          int</code> DEFAULT: <code>max(1, os.cpu_count() // 2)</code> </p> RETURNS DESCRIPTION <code>pandas.DataFrame</code> See Also <p><code>navis.synblast</code>                     NBLAST variant using synapses.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(5)\n&gt;&gt;&gt; scores = navis.synapse_similarity(nl, omega=5000/8, sigma=2000/8)\n</code></pre> Source code in <code>navis/connectivity/similarity.py</code> <pre><code>def synapse_similarity(x: 'core.NeuronList',\n                       sigma: Union[float, int],\n                       omega: Union[float, int],\n                       mu_score: bool = True,\n                       restrict_cn: Optional[List[str]] = None,\n                       n_cores: int = max(1, os.cpu_count() // 2)\n                       ) -&gt; pd.DataFrame:\n    r\"\"\"Cluster neurons based on their synapse placement.\n\n    Distances score is calculated by calculating for each synapse of\n    neuron A: (1) the (Euclidean) distance to the closest synapse in neuron B\n    and (2) comparing the synapse density around synapse A and B.\n    This is type-sensitive: presynapses will only be matched with presynapses,\n    post with post, etc. The formula is described in\n    [Schlegel et al., eLife (2017)](https://elifesciences.org/articles/16799):\n\n    $$\n    f(i_{s},j_{k}) = \\exp(\\frac{-d^{2}_{sk}}{2\\sigma^{2}}) \\exp(\\frac{|n(i_{s})-n(j_{k})|}{n(i_{s})+n(j_{k})})\n    $$\n\n    The synapse similarity score for neurons i and j being the average\n    of $f(i_{s},j_{k})$ over all synapses s of i. Synapse k is the\n    closest synapse of the same sign (pre/post) in neuron j to synapse s.\n    $d^{2}_{sk}$ is the Euclidean distance between these distances.\n    Variable $\\sigma$ (`sigma`) determines what distance between\n    s and k is considered \"close\". $n(i_{s})$ and $n(j_{k})$ are\n    defined as the number of synapses of neuron i/j that are within given\n    radius $\\omega$ (`omega`) of synapse s and j, respectively (same\n    sign only). This esnures that in cases of a strong disparity between\n    $n(i_{s})$ and $n(j_{k})$, the synapse similarity will be\n    close to zero even if the distance between s and k is very small.\n\n\n    Parameters\n    ----------\n    x :                 NeuronList\n                        Neurons to compare. Must have connectors.\n    sigma :             int | float\n                        Distance between synapses that is considered to be\n                        \"close\".\n    omega :             int | float\n                        Radius over which to calculate synapse density.\n    mu_score :          bool\n                        If True, score is calculated as mean between A-&gt;B and\n                        B-&gt;A comparison.\n    restrict_cn :       int | list | None\n                        Restrict to given connector types. Must map to\n                        a `type`, `relation` or `label` column in the\n                        connector tables.\n                        If None, will use all connector types. Use either\n                        single integer or list. E.g. `restrict_cn=[0, 1]`\n                        to use only pre- and postsynapses.\n    n_cores :           int\n                        Number of parallel processes to use. Defaults to half\n                        the available cores.\n\n    Returns\n    -------\n    pandas.DataFrame\n\n    See Also\n    --------\n    [`navis.synblast`][]\n                        NBLAST variant using synapses.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(5)\n    &gt;&gt;&gt; scores = navis.synapse_similarity(nl, omega=5000/8, sigma=2000/8)\n\n    \"\"\"\n    if not isinstance(x, core.NeuronList):\n        raise TypeError(f'Expected Neuronlist got {type(x)}')\n\n    if any([not n.has_connectors for n in x]):\n        raise ValueError('All neurons must have connector tables as .connectors property.')\n\n    # If single value, turn into list\n    if not isinstance(restrict_cn, type(None)):\n        restrict_cn = utils.make_iterable(restrict_cn)\n\n    combinations = [(nA.connectors, nB.connectors, sigma, omega, restrict_cn)\n                    for nA in x for nB in x]\n\n    with ProcessPoolExecutor(max_workers=n_cores) as e:\n        futures = e.map(_unpack_synapse_helper, combinations, chunksize=1000)\n\n        scores = [n for n in config.tqdm(futures, total=len(combinations),\n                                         desc='Processing',\n                                         disable=config.pbar_hide,\n                                         leave=config.pbar_leave)]\n\n    # Create empty score matrix\n    sim_matrix = pd.DataFrame(np.zeros((len(x), len(x))),\n                              index=x.id,\n                              columns=x.id)\n    # Populate matrix\n    comb_names = [(nA.id, nB.id) for nA in x for nB in x]\n    for c, v in zip(comb_names, scores):\n        sim_matrix.loc[c[0], c[1]] = v\n\n    if mu_score:\n        sim_matrix = (sim_matrix + sim_matrix.T) / 2\n\n    return sim_matrix\n</code></pre>"},{"location":"reference/navis/#navis.synblast","title":"<code>navis.synblast</code>","text":"<p>Synapsed-based variant of NBLAST.</p> <p>The gist is this: for each synapse in the query neuron, we find the closest synapse in the target neuron (can be restricted by synapse types). Those distances are then scored similar to nearest-neighbor pairs in NBLAST but without the vector component.</p> PARAMETER DESCRIPTION <code>query</code> <pre><code>        Query neuron(s) to SynBLAST against the targets. Units should\n        be in microns as NBLAST is optimized for that and have\n        similar sampling resolutions. Neurons must have (non-empty)\n        connector tables.\n</code></pre> <p> TYPE: <code>Union[BaseNeuron, NeuronList]</code> </p> <code>by_type</code> <pre><code>        If True, will use the \"type\" column in the connector tables\n        to only compare e.g. pre- with pre- and post- with\n        postsynapses.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> <code>cn_types</code> <pre><code>        Use this to restrict synblast to specific types of\n        connectors (e.g. \"pre\"synapses only).\n</code></pre> <p> TYPE: <code>     str | list</code> DEFAULT: <code>None</code> </p> <code>scores</code> <pre><code>        Determines the final scores:\n\n            - 'forward' (default) returns query-&gt;target scores\n            - 'mean' returns the mean of query-&gt;target and target-&gt;query scores\n            - 'min' returns the minium between query-&gt;target and target-&gt;query scores\n            - 'max' returns the maximum between query-&gt;target and target-&gt;query scores\n</code></pre> <p> TYPE: <code>       'forward' | 'mean' | 'min' | 'max'</code> DEFAULT: <code>'forward'</code> </p> <code>n_cores</code> <pre><code>        Max number of cores to use for nblasting. Default is\n        `os.cpu_count() // 2`. This should ideally be an even\n        number as that allows optimally splitting queries onto\n        individual processes.\n</code></pre> <p> TYPE: <code>      int</code> DEFAULT: <code>os.cpu_count() // 2</code> </p> <code>normalized</code> <pre><code>        Whether to return normalized SynBLAST scores.\n</code></pre> <p> TYPE: <code>   bool</code> DEFAULT: <code>True</code> </p> <code>smat</code> <pre><code>        Score matrix. If 'auto' (default), will use scoring matrices\n        from FCWB. Same behaviour as in R's nat.nblast\n        implementation. If `smat=None` the scores will be\n        generated as the product of the distances and the dotproduct\n        of the vectors of nearest-neighbor pairs.\n</code></pre> <p> TYPE: <code>         str | pd.DataFrame</code> DEFAULT: <code>'auto'</code> </p> <code>progress</code> <pre><code>        Whether to show progress bars. This may cause some overhead,\n        so switch off if you don't really need it.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>scores</code> <p>Matrix with SynBLAST scores. Rows are query neurons, columns are targets.</p> <p> TYPE: <code>pandas.DataFrame</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(n=5)\n&gt;&gt;&gt; nl.units\n&lt;Quantity([8 8 8 8 8], 'nanometer')&gt;\n&gt;&gt;&gt; # Convert to microns\n&gt;&gt;&gt; nl_um = nl * (8 / 1000)\n&gt;&gt;&gt; # Run type-agnostic SyNBLAST\n&gt;&gt;&gt; scores = navis.synblast(nl_um[:3], nl_um[3:], progress=False)\n&gt;&gt;&gt; # Run type-sensitive (i.e. pre vs pre and post vs post) SyNBLAST\n&gt;&gt;&gt; scores = navis.synblast(nl_um[:3], nl_um[3:], by_type=True, progress=False)\n</code></pre> See Also <p><code>navis.nblast</code>             The original morphology-based NBLAST.</p> Source code in <code>navis/nbl/synblast_funcs.py</code> <pre><code>def synblast(query: Union['BaseNeuron', 'NeuronList'],\n             target: Union['BaseNeuron', 'NeuronList'],\n             by_type: bool = False,\n             cn_types: Optional[list] = None,\n             scores: Union[Literal['forward'],\n                           Literal['mean'],\n                           Literal['min'],\n                           Literal['max']] = 'forward',\n             normalized: bool = True,\n             smat: Optional[Union[str, pd.DataFrame]] = 'auto',\n             n_cores: int = os.cpu_count() // 2,\n             progress: bool = True) -&gt; pd.DataFrame:\n    \"\"\"Synapsed-based variant of NBLAST.\n\n    The gist is this: for each synapse in the query neuron, we find the closest\n    synapse in the target neuron (can be restricted by synapse types). Those\n    distances are then scored similar to nearest-neighbor pairs in NBLAST but\n    without the vector component.\n\n    Parameters\n    ----------\n    query,target :  Neuron/List\n                    Query neuron(s) to SynBLAST against the targets. Units should\n                    be in microns as NBLAST is optimized for that and have\n                    similar sampling resolutions. Neurons must have (non-empty)\n                    connector tables.\n    by_type :       bool\n                    If True, will use the \"type\" column in the connector tables\n                    to only compare e.g. pre- with pre- and post- with\n                    postsynapses.\n    cn_types :      str | list, optional\n                    Use this to restrict synblast to specific types of\n                    connectors (e.g. \"pre\"synapses only).\n    scores :        'forward' | 'mean' | 'min' | 'max'\n                    Determines the final scores:\n\n                        - 'forward' (default) returns query-&gt;target scores\n                        - 'mean' returns the mean of query-&gt;target and target-&gt;query scores\n                        - 'min' returns the minium between query-&gt;target and target-&gt;query scores\n                        - 'max' returns the maximum between query-&gt;target and target-&gt;query scores\n\n    n_cores :       int, optional\n                    Max number of cores to use for nblasting. Default is\n                    `os.cpu_count() // 2`. This should ideally be an even\n                    number as that allows optimally splitting queries onto\n                    individual processes.\n    normalized :    bool, optional\n                    Whether to return normalized SynBLAST scores.\n    smat :          str | pd.DataFrame, optional\n                    Score matrix. If 'auto' (default), will use scoring matrices\n                    from FCWB. Same behaviour as in R's nat.nblast\n                    implementation. If `smat=None` the scores will be\n                    generated as the product of the distances and the dotproduct\n                    of the vectors of nearest-neighbor pairs.\n    progress :      bool\n                    Whether to show progress bars. This may cause some overhead,\n                    so switch off if you don't really need it.\n\n    Returns\n    -------\n    scores :        pandas.DataFrame\n                    Matrix with SynBLAST scores. Rows are query neurons, columns\n                    are targets.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(n=5)\n    &gt;&gt;&gt; nl.units\n    &lt;Quantity([8 8 8 8 8], 'nanometer')&gt;\n    &gt;&gt;&gt; # Convert to microns\n    &gt;&gt;&gt; nl_um = nl * (8 / 1000)\n    &gt;&gt;&gt; # Run type-agnostic SyNBLAST\n    &gt;&gt;&gt; scores = navis.synblast(nl_um[:3], nl_um[3:], progress=False)\n    &gt;&gt;&gt; # Run type-sensitive (i.e. pre vs pre and post vs post) SyNBLAST\n    &gt;&gt;&gt; scores = navis.synblast(nl_um[:3], nl_um[3:], by_type=True, progress=False)\n\n    See Also\n    --------\n    [`navis.nblast`][]\n                The original morphology-based NBLAST.\n\n    \"\"\"\n    # Make sure we're working on NeuronList\n    query = NeuronList(query)\n    target = NeuronList(target)\n\n    # Run pre-flight checks\n    nblast_preflight(query, target, n_cores,\n                     req_unique_ids=True, req_dotprops=False,\n                     req_microns=isinstance(smat, str) and smat=='auto')\n\n    # Make sure all neurons have connectors\n    if not all(query.has_connectors):\n        raise ValueError('Some query neurons appear to not have a connector table.')\n    if not all(target.has_connectors):\n        raise ValueError('Some target neurons appear to not have a connector table.')\n\n    if not isinstance(cn_types, type(None)):\n        cn_types = utils.make_iterable(cn_types)\n\n    if not isinstance(cn_types, type(None)) or by_type:\n        if any(['type' not in n.connectors.columns for n in query]):\n            raise ValueError('Connector tables must have a \"type\" column if '\n                             '`by_type=True` or `cn_types` is not `None`.')\n\n    # Find a partition that produces batches that each run in approximately\n    # 10 seconds\n    if n_cores and n_cores &gt; 1:\n        if progress:\n            # If progress bar, we need to make smaller mini batches.\n            # These mini jobs must not be too small - otherwise the overhead\n            # from spawning and sending results between processes slows things\n            # down dramatically. Hence we want to make sure that each job runs\n            # for &gt;10s. The run time depends on the system and how big the neurons\n            # are. Here, we run a quick test and try to extrapolate from there\n            n_rows, n_cols = find_batch_partition(query, target,\n                                                  T=10 * JOB_SIZE_MULTIPLIER)\n        else:\n            # If no progress bar needed, we can just split neurons evenly across\n            # all available cores\n            n_rows, n_cols = find_optimal_partition(n_cores, query, target)\n    else:\n        n_rows = n_cols = 1\n\n    # Calculate self-hits once for all neurons\n    nb = SynBlaster(normalized=normalized,\n                    by_type=by_type,\n                    smat=smat,\n                    progress=progress)\n\n    def get_connectors(n):\n        \"\"\"Gets the required connectors from a neuron.\"\"\"\n        if not isinstance(cn_types, type(None)):\n            return n.connectors[n.connectors['type'].isin(cn_types)]\n        else:\n            return n.connectors\n\n    query_self_hits = np.array([nb.calc_self_hit(get_connectors(n)) for n in query])\n    target_self_hits = np.array([nb.calc_self_hit(get_connectors(n)) for n in target])\n\n    # Initialize a pool of workers\n    # Note that we're forcing \"spawn\" instead of \"fork\" (default on linux)!\n    # This is to reduce the memory footprint since \"fork\" appears to inherit all\n    # variables (including all neurons) while \"spawn\" appears to get only\n    # what's required to run the job?\n    with ProcessPoolExecutor(max_workers=n_cores,\n                             mp_context=mp.get_context('spawn')) as pool:\n        with config.tqdm(desc='Preparing',\n                         total=n_rows * n_cols,\n                         leave=False,\n                         disable=not progress) as pbar:\n            futures = {}\n            blasters = []\n            for qix in np.array_split(np.arange(len(query)), n_rows):\n                for tix in np.array_split(np.arange(len(target)), n_cols):\n                    # Initialize NBlaster\n                    this = SynBlaster(normalized=normalized,\n                                      by_type=by_type,\n                                      smat=smat,\n                                      progress=progress)\n\n                    # Add queries and targets\n                    for i, ix in enumerate(qix):\n                        n = query[ix]\n                        this.append(get_connectors(n), id=n.id, self_hit=query_self_hits[ix])\n                    for i, ix in enumerate(tix):\n                        n = target[ix]\n                        this.append(get_connectors(n), id=n.id, self_hit=target_self_hits[ix])\n\n                    # Keep track of indices of queries and targets\n                    this.queries = np.arange(len(qix))\n                    this.targets = np.arange(len(tix)) + len(qix)\n                    this.queries_ix = qix  # this facilitates filling in the big matrix later\n                    this.targets_ix = tix  # this facilitates filling in the big matrix later\n                    this.pbar_position = len(blasters) if not utils.is_jupyter() else None\n\n                    blasters.append(this)\n                    pbar.update()\n\n                    # If multiple cores requested, submit job to the pool right away\n                    if n_cores and n_cores &gt; 1 and (n_cols &gt; 1 or n_rows &gt; 1):\n                        this.progress=False  # no progress bar for individual NBLASTERs\n                        futures[pool.submit(this.multi_query_target,\n                                            q_idx=this.queries,\n                                            t_idx=this.targets,\n                                            scores=scores)] = this\n\n        # Collect results\n        if futures and len(futures) &gt; 1:\n            # Prepare empty score matrix\n            scores = pd.DataFrame(np.empty((len(query), len(target)),\n                                           dtype=this.dtype),\n                                  index=query.id, columns=target.id)\n            scores.index.name = 'query'\n            scores.columns.name = 'target'\n\n            # Collect results\n            # We're dropping the \"N / N_total\" bit from the progress bar because\n            # it's not helpful here\n            fmt = ('{desc}: {percentage:3.0f}%|{bar}| [{elapsed}&lt;{remaining}]')\n            for f in config.tqdm(as_completed(futures),\n                                 desc='NBLASTing',\n                                 bar_format=fmt,\n                                 total=len(futures),\n                                 smoothing=0,\n                                 disable=not progress,\n                                 leave=False):\n                res = f.result()\n                this = futures[f]\n                # Fill-in big score matrix\n                scores.iloc[this.queries_ix, this.targets_ix] = res.values\n        else:\n            scores = this.multi_query_target(this.queries,\n                                             this.targets,\n                                             scores=scores)\n\n    return scores\n\n    # Find an optimal partition that minimizes the number of neurons\n    # we have to send to each process\n    n_rows, n_cols = find_optimal_partition(n_cores, query, target)\n\n    blasters = []\n    for q in np.array_split(query, n_rows):\n        for t in np.array_split(target, n_cols):\n            # Initialize SynNBlaster\n            this = SynBlaster(normalized=normalized,\n                              by_type=by_type,\n                              smat=smat,\n                              progress=progress)\n            # Add queries and targets\n            for nl in [q, t]:\n                for n in nl:\n                    if not isinstance(cn_types, type(None)):\n                        cn = n.connectors[n.connectors['type'].isin(cn_types)]\n                    else:\n                        cn = n.connectors\n\n                    this.append(cn, id=n.id)\n\n            # Keep track of indices of queries and targets\n            this.queries = np.arange(len(q))\n            this.targets = np.arange(len(t)) + len(q)\n            this.pbar_position = len(blasters) if not utils.is_jupyter() else None\n\n            blasters.append(this)\n\n    # If only one core, we don't need to break out the multiprocessing\n    if n_cores == 1:\n        return this.multi_query_target(this.queries,\n                                       this.targets,\n                                       scores=scores)\n\n    with ProcessPoolExecutor(max_workers=len(blasters)) as pool:\n        # Each nblaster is passed to its own process\n        futures = [pool.submit(this.multi_query_target,\n                               q_idx=this.queries,\n                               t_idx=this.targets,\n                               scores=scores) for this in blasters]\n\n        results = [f.result() for f in futures]\n\n    scores = pd.DataFrame(np.zeros((len(query), len(target))),\n                          index=query.id, columns=target.id)\n\n    for res in results:\n        scores.loc[res.index, res.columns] = res.values\n\n    return scores\n</code></pre>"},{"location":"reference/navis/#navis.thin_voxels","title":"<code>navis.thin_voxels</code>","text":"<p>Skeletonize image data to single voxel width.</p> <p>This is a simple thin wrapper around scikit-learn's <code>skeletonize</code>.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    The image to thin.\n</code></pre> <p> TYPE: <code>        VoxelNeuron | numpy array</code> </p> <code>inplace</code> <pre><code>    For VoxelNeurons only: Whether to manipulate the neuron\n    in place.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>thin</code> <p>Thinned VoxelNeuron or numpy array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1, kind='mesh')\n&gt;&gt;&gt; vx = navis.voxelize(n, pitch='1 micron')\n&gt;&gt;&gt; thinned = navis.thin_voxels(vx)\n</code></pre> Source code in <code>navis/morpho/images.py</code> <pre><code>@utils.map_neuronlist(desc=\"Thinning\", allow_parallel=True)\ndef thin_voxels(x, inplace=False):\n    \"\"\"Skeletonize image data to single voxel width.\n\n    This is a simple thin wrapper around scikit-learn's `skeletonize`.\n\n    Parameters\n    ----------\n    x :         VoxelNeuron | numpy array\n                The image to thin.\n    inplace :   bool\n                For VoxelNeurons only: Whether to manipulate the neuron\n                in place.\n\n    Returns\n    -------\n    thin\n                Thinned VoxelNeuron or numpy array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1, kind='mesh')\n    &gt;&gt;&gt; vx = navis.voxelize(n, pitch='1 micron')\n    &gt;&gt;&gt; thinned = navis.thin_voxels(vx)\n\n    \"\"\"\n    try:\n        from skimage.morphology import skeletonize\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"`thin_image` requires the scikit-image packge:\\n\"\n            \"  pip install scikit-image\"\n        )\n\n    if isinstance(x, core.VoxelNeuron):\n        if not inplace:\n            x = x.copy()\n\n        x.grid = skeletonize(x.grid)\n    elif isinstance(x, np.ndarray):\n        x = skeletonize(x)\n    else:\n        raise TypeError(f\"Unable to thin data of type {type(x)}\")\n\n    return x\n</code></pre>"},{"location":"reference/navis/#navis.tortuosity","title":"<code>navis.tortuosity</code>","text":"<p>Calculate tortuosity of a neuron.</p> <p>See Stepanyants et al., Neuron (2004) for detailed explanation. Briefly, tortuosity index <code>T</code> is defined as the ratio of the branch segment length <code>L</code> (<code>seg_length</code>) to the Euclidean distance <code>R</code> between its ends.</p> <p>The way this is implemented in <code>navis</code>: For each linear stretch (i.e. segments between branch points, leafs or roots) we calculate its geodesic length <code>L</code> and the Euclidean distance <code>R</code> between its ends. The final tortuosity is the mean of <code>L / R</code> across all segments.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            Neuron to analyze. If MeshNeuron, will generate and\n            use a skeleton representation.\n</code></pre> <p> TYPE: <code>                TreeNeuron | MeshNeuron | NeuronList</code> </p> <code>seg_length</code> <pre><code>            Target segment length(s) `L`. If `seg_length` is\n            provided, each linear segment is further divided into\n            segments of exactly `seg_length` (geodesic) length\n            and the tortuosity is calculated for each of these\n            sub-segments. If `seg_length` is not provided, the\n            tortuosity is calculated for each linear segment as is.\n\n            If neuron(s) have their  `.units` set, you can also\n            pass a string such as \"1 micron\". `seg_length` must\n            be larger than the current sampling resolution of the\n            neuron. If you want to make sure that segments are as\n            close to length `L` as possible, consider resampling the\n            neuron using [`navis.resample_skeleton`][].\n</code></pre> <p> TYPE: <code>       int | float | str | list thereof</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tortuosity</code> <p>If x is NeuronList, will return DataFrame. If x is single TreeNeuron, will return either a single float (if no or a single seg_length is queried) or a DataFrame (if multiple seg_lengths are queried).</p> <p> TYPE: <code>float | np.array | pandas.DataFrame</code> </p> See Also <p><code>navis.segment_analysis</code>             This function provides by-segment morphometrics, including             tortuosity.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; # Calculate tortuosity as-is\n&gt;&gt;&gt; T = navis.tortuosity(n)\n&gt;&gt;&gt; round(T, 3)\n1.074\n&gt;&gt;&gt; # Calculate tortuosity with 1 micron segment lengths\n&gt;&gt;&gt; T = navis.tortuosity(n, seg_length='1 micron')\n&gt;&gt;&gt; round(T, 3)\n1.054\n</code></pre> Source code in <code>navis/morpho/mmetrics.py</code> <pre><code>def tortuosity(\n    x: \"core.NeuronObject\",\n    seg_length: Optional[Union[int, float, str, Sequence[Union[int, float, str]]]] = None,\n) -&gt; Union[float, Sequence[float], pd.DataFrame]:\n    \"\"\"Calculate tortuosity of a neuron.\n\n    See Stepanyants et al., Neuron (2004) for detailed explanation. Briefly,\n    tortuosity index `T` is defined as the ratio of the branch segment length\n    `L` (`seg_length`) to the Euclidean distance `R` between its ends.\n\n    The way this is implemented in `navis`:\n    For each linear stretch (i.e. segments between branch points, leafs or roots)\n    we calculate its geodesic length `L` and the Euclidean distance `R` between\n    its ends. The final tortuosity is the mean of `L / R` across all segments.\n\n\n    Parameters\n    ----------\n    x :                 TreeNeuron | MeshNeuron | NeuronList\n                        Neuron to analyze. If MeshNeuron, will generate and\n                        use a skeleton representation.\n    seg_length :        int | float | str | list thereof, optional\n                        Target segment length(s) `L`. If `seg_length` is\n                        provided, each linear segment is further divided into\n                        segments of exactly `seg_length` (geodesic) length\n                        and the tortuosity is calculated for each of these\n                        sub-segments. If `seg_length` is not provided, the\n                        tortuosity is calculated for each linear segment as is.\n\n                        If neuron(s) have their  `.units` set, you can also\n                        pass a string such as \"1 micron\". `seg_length` must\n                        be larger than the current sampling resolution of the\n                        neuron. If you want to make sure that segments are as\n                        close to length `L` as possible, consider resampling the\n                        neuron using [`navis.resample_skeleton`][].\n\n    Returns\n    -------\n    tortuosity :        float | np.array | pandas.DataFrame\n                        If x is NeuronList, will return DataFrame.\n                        If x is single TreeNeuron, will return either a\n                        single float (if no or a single seg_length is queried)\n                        or a DataFrame (if multiple seg_lengths are queried).\n\n    See Also\n    --------\n    [`navis.segment_analysis`][]\n                This function provides by-segment morphometrics, including\n                tortuosity.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; # Calculate tortuosity as-is\n    &gt;&gt;&gt; T = navis.tortuosity(n)\n    &gt;&gt;&gt; round(T, 3)\n    1.074\n    &gt;&gt;&gt; # Calculate tortuosity with 1 micron segment lengths\n    &gt;&gt;&gt; T = navis.tortuosity(n, seg_length='1 micron')\n    &gt;&gt;&gt; round(T, 3)\n    1.054\n\n    \"\"\"\n    if isinstance(x, core.NeuronList):\n        if not isinstance(seg_length, (list, np.ndarray, tuple)):\n            seg_length = [seg_length]  # type: ignore\n        df = pd.DataFrame(\n            [\n                tortuosity(n, seg_length=seg_length)\n                for n in config.tqdm(\n                    x,\n                    desc=\"Tortuosity\",\n                    disable=config.pbar_hide,\n                    leave=config.pbar_leave,\n                )\n            ],\n            index=x.id,\n            columns=seg_length,\n        ).T\n        df.index.name = \"seg_length\"\n        return df\n\n    if isinstance(x, core.MeshNeuron):\n        x = x.skeleton\n    elif not isinstance(x, core.TreeNeuron):\n        raise TypeError(f\"Expected TreeNeuron(s), got {type(x)}\")\n\n    if isinstance(seg_length, (list, np.ndarray)):\n        return [tortuosity(x, l) for l in seg_length]\n\n    if seg_length is None:\n        return _tortuosity_simple(x)\n    else:\n        return _tortuosity_segmented(x, seg_length)\n</code></pre>"},{"location":"reference/navis/#navis.vary_colors","title":"<code>navis.vary_colors</code>","text":"<p>Add small variance to color.</p> Source code in <code>navis/plotting/colors.py</code> <pre><code>def vary_colors(color: AnyColor,\n                by_max: float = .1) -&gt; np.ndarray:\n    \"\"\"Add small variance to color.\"\"\"\n    if isinstance(color, str):\n        color = mcl.to_rgb(color)\n\n    if not isinstance(color, np.ndarray):\n        color = np.array(color)\n\n    if color.ndim == 1:\n        color = color.reshape(1, color.shape[0])\n\n    variance = (np.random.randint(0, 100, color.shape) / 100) * by_max\n    variance = variance - by_max / 2\n\n    # We need to make sure color is array of floats\n    color = color.astype(float)\n    color[:, :3] = color[:, :3] + variance[:, :3]\n\n    return np.clip(color, 0, 1)\n</code></pre>"},{"location":"reference/navis/#navis.voxelize","title":"<code>navis.voxelize</code>","text":"<p>Turn neuron into voxels.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Neuron(s) to voxelize. Uses the neurons' nodes, vertices and\n        points, respectively.\n</code></pre> <p> TYPE: <code>            TreeNeuron | MeshNeuron | Dotprops</code> </p> <code>pitch</code> <pre><code>        Side length(s) voxels. Can be isometric (float) or an\n        iterable of dimensions in (x, y, z).\n</code></pre> <p> TYPE: <code>        float | iterable thereof</code> </p> <code>bounds</code> <pre><code>        Boundaries [in units of `x`] for the voxel grid. If not\n        provided, will use `x.bbox`.\n</code></pre> <p> TYPE: <code>       (3, 2)  or (2, 3) array</code> DEFAULT: <code>None</code> </p> <code>counts</code> <pre><code>        If True, voxel grid will have point counts for values\n        instead of just True/False.\n</code></pre> <p> TYPE: <code>       bool</code> DEFAULT: <code>False</code> </p> <code>vectors</code> <pre><code>        If True, will also attach a vector field as `.vectors`\n        property.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> <code>alphas</code> <pre><code>        If True, will also return a grid with alpha values as\n        `.alpha` property.\n</code></pre> <p> TYPE: <code>       bool</code> DEFAULT: <code>False</code> </p> <code>smooth</code> <pre><code>        If non-zero, will apply a Gaussian filter with `smooth`\n        as `sigma`.\n</code></pre> <p> TYPE: <code>       int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>VoxelNeuron</code> <p>Has the voxel grid as <code>.grid</code> and (optionally) <code>.vectors</code> and <code>.alphas</code> properties. <code>.grid</code> data type depends on settings:  - default = bool (i.e. True/False)  - if <code>counts=True</code> = integer  - if <code>smooth=True</code> = float Empty voxels will have vector (0, 0, 0) and alpha 0. Also note that data tables (e.g. <code>connectors</code>) are not carried over from the input neuron.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; # Get a skeleton\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; # Convert to voxel neuron\n&gt;&gt;&gt; vx = navis.voxelize(n, pitch='5 microns')\n</code></pre> Source code in <code>navis/conversion/wrappers.py</code> <pre><code>@utils.map_neuronlist(desc='Voxelizing', allow_parallel=True)\ndef voxelize(x: 'core.BaseNeuron',\n             pitch: Union[list, tuple, float],\n             bounds: Optional[list] = None,\n             counts: bool = False,\n             vectors: bool = False,\n             alphas: bool = False,\n             smooth: int = 0) -&gt; 'core.VoxelNeuron':\n    \"\"\"Turn neuron into voxels.\n\n    Parameters\n    ----------\n    x :             TreeNeuron | MeshNeuron | Dotprops\n                    Neuron(s) to voxelize. Uses the neurons' nodes, vertices and\n                    points, respectively.\n    pitch :         float | iterable thereof\n                    Side length(s) voxels. Can be isometric (float) or an\n                    iterable of dimensions in (x, y, z).\n    bounds :        (3, 2)  or (2, 3) array, optional\n                    Boundaries [in units of `x`] for the voxel grid. If not\n                    provided, will use `x.bbox`.\n    counts :        bool\n                    If True, voxel grid will have point counts for values\n                    instead of just True/False.\n    vectors :       bool\n                    If True, will also attach a vector field as `.vectors`\n                    property.\n    alphas :        bool\n                    If True, will also return a grid with alpha values as\n                    `.alpha` property.\n    smooth :        int\n                    If non-zero, will apply a Gaussian filter with `smooth`\n                    as `sigma`.\n\n    Returns\n    -------\n    VoxelNeuron\n                    Has the voxel grid as `.grid` and (optionally) `.vectors`\n                    and `.alphas` properties. `.grid` data type depends\n                    on settings:\n                     - default = bool (i.e. True/False)\n                     - if `counts=True` = integer\n                     - if `smooth=True` = float\n                    Empty voxels will have vector (0, 0, 0) and alpha 0. Also\n                    note that data tables (e.g. `connectors`) are not carried\n                    over from the input neuron.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; # Get a skeleton\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; # Convert to voxel neuron\n    &gt;&gt;&gt; vx = navis.voxelize(n, pitch='5 microns')\n\n    \"\"\"\n    if isinstance(x, (core.TreeNeuron, core.MeshNeuron, core.Dotprops)):\n        return neuron2voxels(x,\n                             pitch=pitch,\n                             bounds=bounds,\n                             counts=counts,\n                             vectors=vectors,\n                             alphas=alphas,\n                             smooth=smooth)\n\n    raise TypeError(f'Unable to voxelize data of type {type(x)}')\n</code></pre>"},{"location":"reference/navis/#navis.write_h5","title":"<code>navis.write_h5</code>","text":"<p>Write Neuron/List to Hdf5 file.</p> PARAMETER DESCRIPTION <code>n</code> <pre><code>            Neuron(s) to write to file.\n</code></pre> <p> TYPE: <code>                Neuron | NeuronList</code> </p> <code>filepath</code> <pre><code>            Path to HDF5 file. Will be created if it does not\n            exist. If it does exist, we will add data to it.\n</code></pre> <p> TYPE: <code>         str</code> </p> <code>serialized</code> <pre><code>            Whether to write a serialized (pickled) version of the\n            neuron to file.\n</code></pre> <p> TYPE: <code>       bool</code> DEFAULT: <code>True</code> </p> <code>raw</code> <pre><code>            Whether to write the neurons' raw data to file. This\n            is required to re-generate neurons from tools other\n            than `navis` (e.g. R's `nat`). This follows the schema\n            specified [here](https://github.com/flyconnectome/hnf).\n</code></pre> <p> TYPE: <code>              bool</code> DEFAULT: <code>False</code> </p> <code>append</code> <pre><code>            If file already exists, whether to append data or to\n            overwrite the entire file.\n</code></pre> <p> TYPE: <code>           bool</code> DEFAULT: <code>True</code> </p> <code>overwrite_neurons</code> <pre><code>            If a given neuron already exists in the h5 file whether\n            to overwrite it or throw an exception.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>Only</code> <p> </p> <code>annotations</code> <pre><code>            Whether to write annotations (e.g. \"connectors\")\n            associated with the neuron(s) to file. Annotations\n            must be pandas DataFrames. If a neuron does not contain\n            a given annotation, it is silently skipped.\n</code></pre> <p> TYPE: <code>      str | list thereof</code> DEFAULT: <code>None</code> </p> <code>format</code> <pre><code>            Which version of the format specs to use. By default\n            use latest. Note that we don't allow mixing format\n            specs in the same HDF5 file. So if you want to write\n            to a file which already contains data in a given\n            format, you have to use that format.\n</code></pre> <p> TYPE: <code>           \"latest\" | \"v1\"</code> DEFAULT: <code>'latest'</code> </p> RETURNS DESCRIPTION <code>Nothing</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; # First get mesh, skeleton and dotprop representations for some neurons\n&gt;&gt;&gt; sk = navis.example_neurons(5, kind='skeleton')\n&gt;&gt;&gt; me = navis.example_neurons(5, kind='mesh')\n&gt;&gt;&gt; dp = navis.make_dotprops(sk, k=5)\n&gt;&gt;&gt; # Write them to a file\n&gt;&gt;&gt; navis.write_h5(sk + me + dp, '~/test.h5', overwrite_neurons=True)\n&gt;&gt;&gt; # Read back from file\n&gt;&gt;&gt; nl = navis.read_h5('~/test.h5')\n</code></pre> See Also <p><code>navis.read_h5</code>                     Read neurons from h5 file.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def write_h5(n: 'core.NeuronObject',\n             filepath: str,\n             serialized: bool = True,\n             raw: bool = False,\n             annotations: Optional[Union[str, list]] = None,\n             format: str = 'latest',\n             append: bool = True,\n             overwrite_neurons: bool = False) -&gt; 'core.NeuronObject':\n    \"\"\"Write Neuron/List to Hdf5 file.\n\n    Parameters\n    ----------\n    n :                 Neuron | NeuronList\n                        Neuron(s) to write to file.\n    filepath :          str\n                        Path to HDF5 file. Will be created if it does not\n                        exist. If it does exist, we will add data to it.\n    serialized :        bool\n                        Whether to write a serialized (pickled) version of the\n                        neuron to file.\n    raw :               bool\n                        Whether to write the neurons' raw data to file. This\n                        is required to re-generate neurons from tools other\n                        than `navis` (e.g. R's `nat`). This follows the schema\n                        specified [here](https://github.com/flyconnectome/hnf).\n    append :            bool\n                        If file already exists, whether to append data or to\n                        overwrite the entire file.\n    overwrite_neurons : bool\n                        If a given neuron already exists in the h5 file whether\n                        to overwrite it or throw an exception.\n\n    Only relevant if `raw=True`:\n\n    annotations :       str | list thereof, optional\n                        Whether to write annotations (e.g. \"connectors\")\n                        associated with the neuron(s) to file. Annotations\n                        must be pandas DataFrames. If a neuron does not contain\n                        a given annotation, it is silently skipped.\n    format :            \"latest\" | \"v1\"\n                        Which version of the format specs to use. By default\n                        use latest. Note that we don't allow mixing format\n                        specs in the same HDF5 file. So if you want to write\n                        to a file which already contains data in a given\n                        format, you have to use that format.\n\n    Returns\n    -------\n    Nothing\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; # First get mesh, skeleton and dotprop representations for some neurons\n    &gt;&gt;&gt; sk = navis.example_neurons(5, kind='skeleton')\n    &gt;&gt;&gt; me = navis.example_neurons(5, kind='mesh')\n    &gt;&gt;&gt; dp = navis.make_dotprops(sk, k=5)\n    &gt;&gt;&gt; # Write them to a file\n    &gt;&gt;&gt; navis.write_h5(sk + me + dp, '~/test.h5', overwrite_neurons=True)\n    &gt;&gt;&gt; # Read back from file\n    &gt;&gt;&gt; nl = navis.read_h5('~/test.h5')\n\n    See Also\n    --------\n    [`navis.read_h5`][]\n                        Read neurons from h5 file.\n\n    \"\"\"\n    if not serialized and not raw:\n        raise ValueError('`serialized` and `raw` must not both be False.')\n\n    utils.eval_param(format, name='format',\n                     allowed_values=tuple(WRITERS.keys()))\n\n    filepath = os.path.expanduser(filepath)\n\n    # Get the writer for the specified format\n    writer = WRITERS[format]\n\n    # This opens the file\n    with writer(filepath, mode='a' if append else 'w') as w:\n        w.write_base_info()\n        w.write_neurons(n,\n                        raw=raw,\n                        serialized=serialized,\n                        overwrite=overwrite_neurons,\n                        annotations=annotations)\n</code></pre>"},{"location":"reference/navis/#navis.write_json","title":"<code>navis.write_json</code>","text":"<p>Save neuron(s) to json-formatted file.</p> <p>Nodes and connectors are serialised using pandas' <code>to_json()</code>. Most other items in the neuron's dict are serialised using <code>json.dumps()</code>. Properties not serialised: <code>.graph</code>, <code>.igraph</code>.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Neuron(s) to save.\n</code></pre> <p> TYPE: <code>        TreeNeuron | NeuronList</code> </p> <code>filepath</code> <pre><code>    File to save data to. If `None` will return a json-formatted\n    string.\n</code></pre> <p> TYPE: <code> str</code> </p> <code>**kwargs</code> <pre><code>    Parameters passed to `json.dumps()` and\n    `pandas.DataFrame.to_json()`.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Only if <code>filepath=None</code>.</p> See Also <p><code>navis.read_json</code>             Read json back into navis neurons.</p> Source code in <code>navis/io/json_io.py</code> <pre><code>def write_json(x: 'core.NeuronObject', filepath, **kwargs) -&gt; str:\n    \"\"\"Save neuron(s) to json-formatted file.\n\n    Nodes and connectors are serialised using pandas' `to_json()`. Most\n    other items in the neuron's __dict__ are serialised using\n    `json.dumps()`. Properties not serialised: `.graph`, `.igraph`.\n\n    Parameters\n    ----------\n    x :         TreeNeuron | NeuronList\n                Neuron(s) to save.\n    filepath :  str, optional\n                File to save data to. If `None` will return a json-formatted\n                string.\n    **kwargs\n                Parameters passed to `json.dumps()` and\n                `pandas.DataFrame.to_json()`.\n\n    Returns\n    -------\n    str\n                Only if `filepath=None`.\n\n    See Also\n    --------\n    [`navis.read_json`][]\n                Read json back into navis neurons.\n\n    \"\"\"\n    if not isinstance(x, (core.TreeNeuron, core.NeuronList)):\n        raise TypeError(f'Unable to convert data of type \"{type(x)}\"')\n\n    if isinstance(x, core.BaseNeuron):\n        x = core.NeuronList([x])\n\n    data = []\n    for n in x:\n        this_data = {'id': n.id}\n        for k, v in n.__dict__.items():\n            if not isinstance(k, str):\n                continue\n            if k.startswith('_') and k not in ['_nodes', '_connectors']:\n                continue\n\n            if isinstance(v, pd.DataFrame):\n                this_data[k] = v.to_json()\n            elif isinstance(v, np.ndarray):\n                this_data[k] = v.tolist()\n            else:\n                this_data[k] = v\n\n        data.append(this_data)\n\n    if not isinstance(filepath, type(None)):\n        with open(filepath, 'w') as f:\n            json.dump(data, f, **kwargs)\n    else:\n        return json.dumps(data, **kwargs)\n</code></pre>"},{"location":"reference/navis/#navis.write_mesh","title":"<code>navis.write_mesh</code>","text":"<p>Export meshes (MeshNeurons, Volumes, Trimeshes) to disk.</p> <p>Under the hood this is using trimesh to export meshes.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            If multiple objects, will generate a file for each\n            neuron (see also `filepath`).\n</code></pre> <p> TYPE: <code>                MeshNeuron | Volume | Trimesh | NeuronList</code> </p> <code>filepath</code> <pre><code>            If `None`, will return byte string or list of\n            thereof. If filepath will save to this file. If path\n            will save neuron(s) in that path using `{x.id}`\n            as filename(s). If list, input must be NeuronList and\n            a filepath must be provided for each neuron.\n</code></pre> <p> TYPE: <code>         None | str | list</code> DEFAULT: <code>None</code> </p> <code>filetype</code> <pre><code>            If `filepath` does not include the file extension,\n            you need to provide it as `filetype`.\n</code></pre> <p> TYPE: <code>         stl | ply | obj</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>None</code> <p>If filepath is not <code>None</code>.</p> <code>bytes</code> <p>If filepath is <code>None</code>.</p> See Also <p><code>navis.read_mesh</code>                     Import neurons. <code>navis.write_precomputed</code>                     Write meshes to Neuroglancer's precomputed format.</p> <p>Examples:</p> <p>Write <code>MeshNeurons</code> to folder:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(3, kind='mesh')\n&gt;&gt;&gt; navis.write_mesh(nl, tmp_dir, filetype='obj')\n</code></pre> <p>Specify the filenames:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(3, kind='mesh')\n&gt;&gt;&gt; navis.write_mesh(nl, tmp_dir / '{neuron.name}.obj')\n</code></pre> <p>Write directly to zip archive:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(3, kind='mesh')\n&gt;&gt;&gt; navis.write_mesh(nl, tmp_dir / 'meshes.zip', filetype='obj')\n</code></pre> Source code in <code>navis/io/mesh_io.py</code> <pre><code>def write_mesh(\n    x: Union[\"core.NeuronList\", \"core.MeshNeuron\", \"core.Volume\", \"tm.Trimesh\"],\n    filepath: Optional[str] = None,\n    filetype: str = None,\n) -&gt; None:\n    \"\"\"Export meshes (MeshNeurons, Volumes, Trimeshes) to disk.\n\n    Under the hood this is using trimesh to export meshes.\n\n    Parameters\n    ----------\n    x :                 MeshNeuron | Volume | Trimesh | NeuronList\n                        If multiple objects, will generate a file for each\n                        neuron (see also `filepath`).\n    filepath :          None | str | list, optional\n                        If `None`, will return byte string or list of\n                        thereof. If filepath will save to this file. If path\n                        will save neuron(s) in that path using `{x.id}`\n                        as filename(s). If list, input must be NeuronList and\n                        a filepath must be provided for each neuron.\n    filetype :          stl | ply | obj, optional\n                        If `filepath` does not include the file extension,\n                        you need to provide it as `filetype`.\n\n    Returns\n    -------\n    None\n                        If filepath is not `None`.\n    bytes\n                        If filepath is `None`.\n\n    See Also\n    --------\n    [`navis.read_mesh`][]\n                        Import neurons.\n    [`navis.write_precomputed`][]\n                        Write meshes to Neuroglancer's precomputed format.\n\n    Examples\n    --------\n\n    Write `MeshNeurons` to folder:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(3, kind='mesh')\n    &gt;&gt;&gt; navis.write_mesh(nl, tmp_dir, filetype='obj')\n\n    Specify the filenames:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(3, kind='mesh')\n    &gt;&gt;&gt; navis.write_mesh(nl, tmp_dir / '{neuron.name}.obj')\n\n    Write directly to zip archive:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(3, kind='mesh')\n    &gt;&gt;&gt; navis.write_mesh(nl, tmp_dir / 'meshes.zip', filetype='obj')\n\n    \"\"\"\n    if filetype is not None:\n        utils.eval_param(filetype, name=\"filetype\", allowed_values=MESH_WRITE_EXT)\n    else:\n        # See if we can get filetype from filepath\n        if filepath is not None:\n            for f in MESH_WRITE_EXT:\n                if str(filepath).endswith(f\".{f}\"):\n                    filetype = f\n                    break\n\n        if not filetype:\n            raise ValueError(\n                \"Must provide mesh type either explicitly via \"\n                \"`filetype` variable or implicitly via the \"\n                \"file extension in `filepath`\"\n            )\n\n    writer = base.Writer(_write_mesh, ext=f\".{filetype}\")\n\n    return writer.write_any(x, filepath=filepath)\n</code></pre>"},{"location":"reference/navis/#navis.write_nrrd","title":"<code>navis.write_nrrd</code>","text":"<p>Write VoxelNeurons or Dotprops to NRRD file(s).</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            If multiple neurons, will generate a NRRD file\n            for each neuron (see also `filepath`).\n</code></pre> <p> TYPE: <code>                VoxelNeuron | Dotprops | NeuronList</code> </p> <code>filepath</code> <pre><code>            Destination for the NRRD files. See examples for options.\n            If `x` is multiple neurons, `filepath` must either\n            be a folder, a \"formattable\" filename (see Examples) or\n            a list of filenames (one for each neuron in `x`).\n            Existing files will be overwritten!\n</code></pre> <p> TYPE: <code>         str | pathlib.Path | list thereof</code> </p> <code>compression_level</code> <pre><code>            Lower = faster writing but larger files. Higher = slower\n            writing but smaller files.\n</code></pre> <p> TYPE: <code>int 1-9</code> DEFAULT: <code>3</code> </p> <code>attrs</code> <pre><code>            Any additional attributes will be written to NRRD header.\n</code></pre> <p> TYPE: <code>            dict</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Nothing</code> <p>Examples:</p> <p>Save a single neuron to a specific file:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1, kind='skeleton')\n&gt;&gt;&gt; vx = navis.voxelize(n, pitch='2 microns')\n&gt;&gt;&gt; navis.write_nrrd(vx, tmp_dir / 'my_neuron.nrrd')\n</code></pre> <p>Save multiple neurons to a folder (must exist). Filenames will be autogenerated as \"{neuron.id}.nrrd\":</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(5, kind='skeleton')\n&gt;&gt;&gt; dp = navis.make_dotprops(nl, k=5)\n&gt;&gt;&gt; navis.write_nrrd(dp, tmp_dir)\n</code></pre> <p>Save multiple neurons to a folder but modify the pattern for the autogenerated filenames:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(5, kind='skeleton')\n&gt;&gt;&gt; vx = navis.voxelize(nl, pitch='2 microns')\n&gt;&gt;&gt; navis.write_nrrd(vx, tmp_dir / 'voxels-{neuron.name}.nrrd')\n</code></pre> <p>Save multiple neurons to a zip file:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(5, kind='skeleton')\n&gt;&gt;&gt; vx = navis.voxelize(nl, pitch='2 microns')\n&gt;&gt;&gt; navis.write_nrrd(vx, tmp_dir / 'neuronlist.zip')\n</code></pre> <p>Save multiple neurons to a zip file but modify the filenames:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(5, kind='skeleton')\n&gt;&gt;&gt; vx = navis.voxelize(nl, pitch='2 microns')\n&gt;&gt;&gt; navis.write_nrrd(vx, tmp_dir / 'voxels-{neuron.name}.nrrd@neuronlist.zip')\n</code></pre> See Also <p><code>navis.read_nrrd</code>                     Import VoxelNeuron from NRRD files.</p> Source code in <code>navis/io/nrrd_io.py</code> <pre><code>def write_nrrd(\n    x: \"core.NeuronObject\",\n    filepath: Union[str, Path],\n    compression_level: int = 3,\n    attrs: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"Write VoxelNeurons or Dotprops to NRRD file(s).\n\n    Parameters\n    ----------\n    x :                 VoxelNeuron | Dotprops | NeuronList\n                        If multiple neurons, will generate a NRRD file\n                        for each neuron (see also `filepath`).\n    filepath :          str | pathlib.Path | list thereof\n                        Destination for the NRRD files. See examples for options.\n                        If `x` is multiple neurons, `filepath` must either\n                        be a folder, a \"formattable\" filename (see Examples) or\n                        a list of filenames (one for each neuron in `x`).\n                        Existing files will be overwritten!\n    compression_level : int 1-9\n                        Lower = faster writing but larger files. Higher = slower\n                        writing but smaller files.\n    attrs :             dict\n                        Any additional attributes will be written to NRRD header.\n\n    Returns\n    -------\n    Nothing\n\n    Examples\n    --------\n    Save a single neuron to a specific file:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1, kind='skeleton')\n    &gt;&gt;&gt; vx = navis.voxelize(n, pitch='2 microns')\n    &gt;&gt;&gt; navis.write_nrrd(vx, tmp_dir / 'my_neuron.nrrd')\n\n    Save multiple neurons to a folder (must exist). Filenames will be\n    autogenerated as \"{neuron.id}.nrrd\":\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(5, kind='skeleton')\n    &gt;&gt;&gt; dp = navis.make_dotprops(nl, k=5)\n    &gt;&gt;&gt; navis.write_nrrd(dp, tmp_dir)\n\n    Save multiple neurons to a folder but modify the pattern for the\n    autogenerated filenames:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(5, kind='skeleton')\n    &gt;&gt;&gt; vx = navis.voxelize(nl, pitch='2 microns')\n    &gt;&gt;&gt; navis.write_nrrd(vx, tmp_dir / 'voxels-{neuron.name}.nrrd')\n\n    Save multiple neurons to a zip file:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(5, kind='skeleton')\n    &gt;&gt;&gt; vx = navis.voxelize(nl, pitch='2 microns')\n    &gt;&gt;&gt; navis.write_nrrd(vx, tmp_dir / 'neuronlist.zip')\n\n    Save multiple neurons to a zip file but modify the filenames:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(5, kind='skeleton')\n    &gt;&gt;&gt; vx = navis.voxelize(nl, pitch='2 microns')\n    &gt;&gt;&gt; navis.write_nrrd(vx, tmp_dir / 'voxels-{neuron.name}.nrrd@neuronlist.zip')\n\n    See Also\n    --------\n    [`navis.read_nrrd`][]\n                        Import VoxelNeuron from NRRD files.\n\n    \"\"\"\n    compression_level = int(compression_level)\n\n    if (compression_level &lt; 1) or (compression_level &gt; 9):\n        raise ValueError(\"`compression_level` must be 1-9, got \" f\"{compression_level}\")\n\n    writer = base.Writer(_write_nrrd, ext=\".nrrd\")\n\n    return writer.write_any(\n        x, filepath=filepath, compression_level=compression_level, **(attrs or {})\n    )\n</code></pre>"},{"location":"reference/navis/#navis.write_parquet","title":"<code>navis.write_parquet</code>","text":"<p>Write TreeNeuron(s) or Dotprops to parquet file.</p> <p>See here for format specifications.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            Neuron(s) to save. If NeuronList must contain either\n            only TreeNeurons or only Dotprops.\n</code></pre> <p> TYPE: <code>                TreeNeuron | Dotprop | NeuronList thereof</code> </p> <code>filepath</code> <pre><code>            Destination for the file.\n</code></pre> <p> TYPE: <code>         str | pathlib.Path</code> </p> <code>write_meta</code> <pre><code>            Whether to also write neuron properties to file. By\n            default this is `.name`, `.units` and `.soma`. You can\n            change which properties are written by providing them as\n            list of strings.\n</code></pre> <p> TYPE: <code>       bool | list of str</code> DEFAULT: <code>True</code> </p> See Also <p><code>navis.read_parquet</code>                     Import skeleton from parquet file. <code>navis.scan_parquet</code>                     Scan parquet file for its contents.</p> <p>Examples:</p> <p>Save a bunch of skeletons:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(3, kind='skeleton')\n&gt;&gt;&gt; navis.write_parquet(nl, tmp_dir / 'skeletons.parquet')\n</code></pre> <p>Inspect that file's content</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; contents = navis.scan_parquet(tmp_dir / 'skeletons.parquet')\n&gt;&gt;&gt; contents\n           id        units       name    soma\n0   722817260  8 nanometer  DA1_lPN_R     NaN\n1  1734350908  8 nanometer  DA1_lPN_R     [6]\n2  1734350788  8 nanometer  DA1_lPN_R  [4177]\n</code></pre> <p>Read the skeletons back in</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.read_parquet(tmp_dir / 'skeletons.parquet')\n&gt;&gt;&gt; len(nl)\n3\n</code></pre> Source code in <code>navis/io/pq_io.py</code> <pre><code>def write_parquet(x: 'core.NeuronObject',\n                  filepath: Union[str, Path],\n                  write_meta: bool = True) -&gt; None:\n    \"\"\"Write TreeNeuron(s) or Dotprops to parquet file.\n\n    See [here](https://github.com/navis-org/navis/blob/master/navis/io/pq_io.md)\n    for format specifications.\n\n    Parameters\n    ----------\n    x :                 TreeNeuron | Dotprop | NeuronList thereof\n                        Neuron(s) to save. If NeuronList must contain either\n                        only TreeNeurons or only Dotprops.\n    filepath :          str | pathlib.Path\n                        Destination for the file.\n    write_meta :        bool | list of str\n                        Whether to also write neuron properties to file. By\n                        default this is `.name`, `.units` and `.soma`. You can\n                        change which properties are written by providing them as\n                        list of strings.\n\n    See Also\n    --------\n    [`navis.read_parquet`][]\n                        Import skeleton from parquet file.\n    [`navis.scan_parquet`][]\n                        Scan parquet file for its contents.\n\n    Examples\n    --------\n    Save a bunch of skeletons:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(3, kind='skeleton')\n    &gt;&gt;&gt; navis.write_parquet(nl, tmp_dir / 'skeletons.parquet')\n\n    Inspect that file's content\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; contents = navis.scan_parquet(tmp_dir / 'skeletons.parquet')\n    &gt;&gt;&gt; contents                                                # doctest: +SKIP\n               id        units       name    soma\n    0   722817260  8 nanometer  DA1_lPN_R     NaN\n    1  1734350908  8 nanometer  DA1_lPN_R     [6]\n    2  1734350788  8 nanometer  DA1_lPN_R  [4177]\n\n    Read the skeletons back in\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.read_parquet(tmp_dir / 'skeletons.parquet')\n    &gt;&gt;&gt; len(nl)\n    3\n\n    \"\"\"\n    filepath = Path(filepath).expanduser()\n\n    # Make sure inputs are only TreeNeurons or Dotprops\n    if isinstance(x, core.NeuronList):\n        types = x.types\n        if types == (core.TreeNeuron,):\n            _write_parquet = _write_parquet_skeletons\n        elif types == (core.Dotprops, ):\n            _write_parquet = _write_parquet_dotprops\n        else:\n            raise TypeError('Can only write either TreeNeurons or Dotprops to '\n                            f'parquet but NeuronList contains {types}')\n        if x.is_degenerated:\n            raise ValueError('NeuronList must not contain non-unique IDs')\n    else:\n        if isinstance(x, (core.TreeNeuron, )):\n            _write_parquet = _write_parquet_skeletons\n        elif isinstance(x, (core.Dotprops, )):\n            _write_parquet = _write_parquet_dotprops\n        else:\n            raise TypeError('Can only write TreeNeurons or Dotprops to parquet, '\n                            f'got \"{type(x)}\"')\n\n    return _write_parquet(x, filepath=filepath, write_meta=write_meta)\n</code></pre>"},{"location":"reference/navis/#navis.write_precomputed","title":"<code>navis.write_precomputed</code>","text":"<p>Export skeletons or meshes to neuroglancer's (legacy) precomputed format.</p> <p>Note that you should not mix meshes and skeletons in the same folder!</p> <p>Follows the formats specified here.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            If multiple neurons, will generate a file for each\n            neuron (see also `filepath`). For use in neuroglancer\n            coordinates should generally be in nanometers.\n</code></pre> <p> TYPE: <code>                TreeNeuron | MeshNeuron | Volume | Trimesh | NeuronList</code> </p> <code>filepath</code> <pre><code>            If `None`, will return byte string or list of\n            thereof. If filepath will save to this file. If path\n            will save neuron(s) in that path using `{x.id}`\n            as filename(s). If list, input must be NeuronList and\n            a filepath must be provided for each neuron.\n</code></pre> <p> TYPE: <code>         None | str | list</code> DEFAULT: <code>None</code> </p> <code>write_info</code> <pre><code>            Whether to also write a JSON-formatted `info` file that\n            can be parsed by e.g. neuroglancer. This only works if\n            inputs are either only skeletons or only meshes!\n</code></pre> <p> TYPE: <code>       bool</code> DEFAULT: <code>True</code> </p> <code>write_manifest</code> <pre><code>            For meshes only: whether to also write manifests. For\n            each mesh we will create a JSON-encoded `{id}:0` file\n            that contains a \"fragments\" entry that maps to the\n            actual filename. Note that this will not work on Windows\n            because colons aren't allowed in file names and on OSX\n            the colon will show up as a `/` in the Finder.\n</code></pre> <p> TYPE: <code>   bool</code> DEFAULT: <code>False</code> </p> <code>radius</code> <pre><code>            For TreeNeurons only: whether to write radius as\n            additional vertex property.\n</code></pre> <p> TYPE: <code>           bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>None</code> <p>If filepath is not <code>None</code>.</p> <code>bytes</code> <p>If filepath is <code>None</code>.</p> See Also <p><code>navis.read_precomputed</code>                     Import neurons from neuroglancer's precomputed format. <code>navis.write_mesh</code>                     Write meshes to generic mesh formats (obj, stl, etc).</p> <p>Examples:</p> <p>Write skeletons:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(3, kind='skeleton')\n&gt;&gt;&gt; navis.write_precomputed(n, tmp_dir)\n</code></pre> <p>Write meshes:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(3, kind='mesh')\n&gt;&gt;&gt; navis.write_precomputed(n, tmp_dir)\n</code></pre> <p>Write directly to zip archive:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(3, kind='skeleton')\n&gt;&gt;&gt; navis.write_precomputed(n, tmp_dir / 'precomputed.zip')\n</code></pre> Source code in <code>navis/io/precomputed_io.py</code> <pre><code>def write_precomputed(\n    x: Union[\"core.NeuronList\", \"core.TreeNeuron\", \"core.MeshNeuron\", \"core.Volume\"],\n    filepath: Optional[str] = None,\n    write_info: bool = True,\n    write_manifest: bool = False,\n    radius: bool = False,\n) -&gt; None:\n    \"\"\"Export skeletons or meshes to neuroglancer's (legacy) precomputed format.\n\n    Note that you should not mix meshes and skeletons in the same folder!\n\n    Follows the formats specified\n    [here](https://github.com/google/neuroglancer/tree/master/src/neuroglancer/datasource/precomputed).\n\n    Parameters\n    ----------\n    x :                 TreeNeuron | MeshNeuron | Volume | Trimesh | NeuronList\n                        If multiple neurons, will generate a file for each\n                        neuron (see also `filepath`). For use in neuroglancer\n                        coordinates should generally be in nanometers.\n    filepath :          None | str | list, optional\n                        If `None`, will return byte string or list of\n                        thereof. If filepath will save to this file. If path\n                        will save neuron(s) in that path using `{x.id}`\n                        as filename(s). If list, input must be NeuronList and\n                        a filepath must be provided for each neuron.\n    write_info :        bool\n                        Whether to also write a JSON-formatted `info` file that\n                        can be parsed by e.g. neuroglancer. This only works if\n                        inputs are either only skeletons or only meshes!\n    write_manifest :    bool\n                        For meshes only: whether to also write manifests. For\n                        each mesh we will create a JSON-encoded `{id}:0` file\n                        that contains a \"fragments\" entry that maps to the\n                        actual filename. Note that this will not work on Windows\n                        because colons aren't allowed in file names and on OSX\n                        the colon will show up as a `/` in the Finder.\n    radius :            bool\n                        For TreeNeurons only: whether to write radius as\n                        additional vertex property.\n\n    Returns\n    -------\n    None\n                        If filepath is not `None`.\n    bytes\n                        If filepath is `None`.\n\n    See Also\n    --------\n    [`navis.read_precomputed`][]\n                        Import neurons from neuroglancer's precomputed format.\n    [`navis.write_mesh`][]\n                        Write meshes to generic mesh formats (obj, stl, etc).\n\n    Examples\n    --------\n\n    Write skeletons:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(3, kind='skeleton')\n    &gt;&gt;&gt; navis.write_precomputed(n, tmp_dir)\n\n    Write meshes:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(3, kind='mesh')\n    &gt;&gt;&gt; navis.write_precomputed(n, tmp_dir)\n\n    Write directly to zip archive:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(3, kind='skeleton')\n    &gt;&gt;&gt; navis.write_precomputed(n, tmp_dir / 'precomputed.zip')\n\n    \"\"\"\n    writer = PrecomputedWriter(_write_precomputed, ext=None)\n\n    return writer.write_any(\n        x,\n        filepath=filepath,\n        write_info=write_info,\n        write_manifest=write_manifest,\n        radius=radius,\n    )\n</code></pre>"},{"location":"reference/navis/#navis.write_swc","title":"<code>navis.write_swc</code>","text":"<p>Write TreeNeuron(s) to SWC.</p> <p>Follows the format specified here.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            If multiple neurons, will generate a single SWC file\n            for each neuron (see also `filepath`).\n</code></pre> <p> TYPE: <code>                TreeNeuron | NeuronList</code> </p> <code>filepath</code> <pre><code>            Destination for the SWC files. See examples for options.\n            If `x` is multiple neurons, `filepath` must either\n            be a folder, a \"formattable\" filename, a filename ending\n            in `.zip` or a list of filenames (one for each neuron\n            in `x`). Existing files will be overwritten!\n</code></pre> <p> TYPE: <code>         str | pathlib.Path | list thereof</code> </p> <code>header</code> <pre><code>            Header for SWC file. If not provided, will use generic\n            header.\n</code></pre> <p> TYPE: <code>           str | None</code> DEFAULT: <code>None</code> </p> <code>write_meta</code> <pre><code>            If not False, will add meta data as JSON-formatted\n            string to the header::\n\n               True: adds neuron `id`, `name` and `units`\n               list: use to set which properties, e.g. ['id', 'units']\n               dict: use to set meta data, e.g. {'template': 'JRC2018F'}\n\n            This parameter is ignored if custom header is provided.\n</code></pre> <p> TYPE: <code>       bool | list | dict</code> DEFAULT: <code>True</code> </p> <code>labels</code> <pre><code>            Node labels. Can be::\n\n                str : column name in node table\n                dict: must be of format {node_id: 'label', ...}.\n                bool: if True, will generate automatic labels, if False all nodes have label \"0\".\n</code></pre> <p> TYPE: <code>           str | dict | bool</code> DEFAULT: <code>True</code> </p> <code>export_connectors</code> <pre><code>            If True, will label nodes with pre- (\"7\") and\n            postsynapse (\"8\"). Because only one label can be given\n            this might drop synapses (i.e. in case of multiple\n            pre- and/or postsynapses on a single node)! `labels`\n            must be `True` for this to have any effect.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>return_node_map</code> <pre><code>            If True, will return a dictionary mapping the old node\n            ID to the new reindexed node IDs in the file.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>node_map</code> <p>Only if <code>return_node_map=True</code>.</p> <p> TYPE: <code>dict</code> </p> See Also <p><code>navis.read_swc</code>                     Import skeleton from SWC files.</p> <p>Examples:</p> <p>Save a single neuron to a specific file:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1, kind='skeleton')\n&gt;&gt;&gt; navis.write_swc(n, tmp_dir / 'my_neuron.swc')\n</code></pre> <p>Save two neurons to specific files:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(2, kind='skeleton')\n&gt;&gt;&gt; navis.write_swc(nl, [tmp_dir / 'my_neuron1.swc', tmp_dir / 'my_neuron2.swc'])\n</code></pre> <p>Save multiple neurons to a folder (must exist). Filenames will be autogenerated as \"{neuron.id}.swc\":</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(5, kind='skeleton')\n&gt;&gt;&gt; navis.write_swc(nl, tmp_dir)\n</code></pre> <p>Save multiple neurons to a folder but modify the pattern for the autogenerated filenames:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(5, kind='skeleton')\n&gt;&gt;&gt; navis.write_swc(nl, tmp_dir / 'skel-{neuron.name}.swc')\n</code></pre> <p>Save multiple neurons to a zip file:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(5, kind='skeleton')\n&gt;&gt;&gt; navis.write_swc(nl, tmp_dir / 'neuronlist.zip')\n</code></pre> <p>Save multiple neurons to a zip file but modify the filenames:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(5, kind='skeleton')\n&gt;&gt;&gt; navis.write_swc(nl, tmp_dir / 'skel-{neuron.name}.swc@neuronlist.zip')\n</code></pre> Source code in <code>navis/io/swc_io.py</code> <pre><code>def write_swc(\n    x: \"core.NeuronObject\",\n    filepath: Union[str, Path],\n    header: Optional[str] = None,\n    write_meta: Union[bool, List[str], dict] = True,\n    labels: Union[str, dict, bool] = True,\n    export_connectors: bool = False,\n    return_node_map: bool = False,\n) -&gt; None:\n    \"\"\"Write TreeNeuron(s) to SWC.\n\n    Follows the format specified\n    [here](http://www.neuronland.org/NLMorphologyConverter/MorphologyFormats/SWC/Spec.html).\n\n    Parameters\n    ----------\n    x :                 TreeNeuron | NeuronList\n                        If multiple neurons, will generate a single SWC file\n                        for each neuron (see also `filepath`).\n    filepath :          str | pathlib.Path | list thereof\n                        Destination for the SWC files. See examples for options.\n                        If `x` is multiple neurons, `filepath` must either\n                        be a folder, a \"formattable\" filename, a filename ending\n                        in `.zip` or a list of filenames (one for each neuron\n                        in `x`). Existing files will be overwritten!\n    header :            str | None, optional\n                        Header for SWC file. If not provided, will use generic\n                        header.\n    write_meta :        bool | list | dict\n                        If not False, will add meta data as JSON-formatted\n                        string to the header::\n\n                           True: adds neuron `id`, `name` and `units`\n                           list: use to set which properties, e.g. ['id', 'units']\n                           dict: use to set meta data, e.g. {'template': 'JRC2018F'}\n\n                        This parameter is ignored if custom header is provided.\n    labels :            str | dict | bool, optional\n                        Node labels. Can be::\n\n                            str : column name in node table\n                            dict: must be of format {node_id: 'label', ...}.\n                            bool: if True, will generate automatic labels, if False all nodes have label \"0\".\n\n    export_connectors : bool, optional\n                        If True, will label nodes with pre- (\"7\") and\n                        postsynapse (\"8\"). Because only one label can be given\n                        this might drop synapses (i.e. in case of multiple\n                        pre- and/or postsynapses on a single node)! `labels`\n                        must be `True` for this to have any effect.\n    return_node_map :   bool\n                        If True, will return a dictionary mapping the old node\n                        ID to the new reindexed node IDs in the file.\n\n    Returns\n    -------\n    node_map :          dict\n                        Only if `return_node_map=True`.\n\n    See Also\n    --------\n    [`navis.read_swc`][]\n                        Import skeleton from SWC files.\n\n    Examples\n    --------\n    Save a single neuron to a specific file:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1, kind='skeleton')\n    &gt;&gt;&gt; navis.write_swc(n, tmp_dir / 'my_neuron.swc')\n\n    Save two neurons to specific files:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(2, kind='skeleton')\n    &gt;&gt;&gt; navis.write_swc(nl, [tmp_dir / 'my_neuron1.swc', tmp_dir / 'my_neuron2.swc'])\n\n    Save multiple neurons to a folder (must exist). Filenames will be\n    autogenerated as \"{neuron.id}.swc\":\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(5, kind='skeleton')\n    &gt;&gt;&gt; navis.write_swc(nl, tmp_dir)\n\n    Save multiple neurons to a folder but modify the pattern for the\n    autogenerated filenames:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(5, kind='skeleton')\n    &gt;&gt;&gt; navis.write_swc(nl, tmp_dir / 'skel-{neuron.name}.swc')\n\n    Save multiple neurons to a zip file:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(5, kind='skeleton')\n    &gt;&gt;&gt; navis.write_swc(nl, tmp_dir / 'neuronlist.zip')\n\n    Save multiple neurons to a zip file but modify the filenames:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(5, kind='skeleton')\n    &gt;&gt;&gt; navis.write_swc(nl, tmp_dir / 'skel-{neuron.name}.swc@neuronlist.zip')\n\n    \"\"\"\n    # Make sure inputs are only TreeNeurons\n    if isinstance(x, core.NeuronList):\n        for n in x:\n            if not isinstance(n, core.TreeNeuron):\n                msg = f'Can only write TreeNeurons to SWC, not \"{type(n)}\"'\n                if isinstance(n, core.Dotprops):\n                    msg += (\n                        \". For Dotprops, you can use either `navis.write_nrrd`\"\n                        \" or `navis.write_parquet`.\"\n                    )\n                raise TypeError(msg)\n    elif not isinstance(x, core.TreeNeuron):\n        msg = f'Can only write TreeNeurons to SWC, not \"{type(n)}\"'\n        if isinstance(n, core.Dotprops):\n            msg += (\n                \". For Dotprops, you can use either `navis.write_nrrd`\"\n                \" or `navis.write_parquet`.\"\n            )\n        raise TypeError(msg)\n\n    writer = base.Writer(write_func=_write_swc, ext=\".swc\")\n\n    return writer.write_any(\n        x,\n        filepath=filepath,\n        header=header,\n        write_meta=write_meta,\n        labels=labels,\n        export_connectors=export_connectors,\n        return_node_map=return_node_map,\n    )\n</code></pre>"},{"location":"reference/navis/#navis.xform","title":"<code>navis.xform</code>","text":"<p>Apply transform(s) to data.</p> Notes <p>For Neurons only: whether there is a change in units during transformation (e.g. nm -&gt; um) is inferred by comparing distances between x/y/z coordinates before and after transform. This guesstimate is then used to convert <code>.units</code> and node/soma radii. This works reasonably well with base 10 increments (e.g. nm -&gt; um) but is off with odd changes in units.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            Data to transform. Dataframe must contain `['x', 'y', 'z']`\n            columns. Numpy array must be shape `(N, 3)`.\n</code></pre> <p> TYPE: <code>                Neuron/List | Volume/Trimesh | numpy.ndarray | pandas.DataFrame</code> </p> <code>transform</code> <pre><code>            Either a single transform or a transform sequence.\n</code></pre> <p> TYPE: <code>        Transform/Sequence or list thereof</code> </p> <code>affine_fallback</code> <pre><code>            In same cases the non-rigid transformation of points\n            can fail - for example if points are outside the\n            deformation field. If that happens, they will be\n            returned as `NaN`. Unless `affine_fallback` is\n            `True`, in which case we will apply only the rigid\n            affine  part of the transformation to at least get close\n            to the correct coordinates.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>True</code> </p> <code>caching</code> <pre><code>            If True, will (pre-)cache data for transforms whenever\n            possible. Depending on the data and the type of\n            transforms this can tremendously speed things up at the\n            cost of increased memory usage:\n              - `False` = no upfront cost, lower memory footprint\n              - `True` = higher upfront cost, most definitely faster\n            Only applies if input is NeuronList and if transforms\n            include H5 transform.\n</code></pre> <p> TYPE: <code>          bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>same type as `x`</code> <p>Copy of input with transformed coordinates.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; # Example neurons are in 8nm voxel space\n&gt;&gt;&gt; nl = navis.example_neurons()\n&gt;&gt;&gt; # Make a simple Affine transform to go from voxel to nanometers\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; M = np.diag([8, 8, 8, 8])\n&gt;&gt;&gt; tr = navis.transforms.AffineTransform(M)\n&gt;&gt;&gt; # Apply the transform\n&gt;&gt;&gt; xf = navis.xform(nl, tr)\n</code></pre> See Also <p><code>navis.xform_brain</code>                 Higher level function that finds and applies a sequence of                 transforms to go from one template brain to another.</p> Source code in <code>navis/transforms/xfm_funcs.py</code> <pre><code>def xform(x: Union['core.NeuronObject', 'pd.DataFrame', 'np.ndarray'],\n          transform: Union[BaseTransform, TransformSequence],\n          affine_fallback: bool = True,\n          caching: bool = True) -&gt; Union['core.NeuronObject',\n                                         'pd.DataFrame',\n                                         'np.ndarray']:\n    \"\"\"Apply transform(s) to data.\n\n    Notes\n    -----\n    For Neurons only: whether there is a change in units during transformation\n    (e.g. nm -&gt; um) is inferred by comparing distances between x/y/z coordinates\n    before and after transform. This guesstimate is then used to convert\n    `.units` and node/soma radii. This works reasonably well with base 10\n    increments (e.g. nm -&gt; um) but is off with odd changes in units.\n\n    Parameters\n    ----------\n    x :                 Neuron/List | Volume/Trimesh | numpy.ndarray | pandas.DataFrame\n                        Data to transform. Dataframe must contain `['x', 'y', 'z']`\n                        columns. Numpy array must be shape `(N, 3)`.\n    transform :         Transform/Sequence or list thereof\n                        Either a single transform or a transform sequence.\n    affine_fallback :   bool\n                        In same cases the non-rigid transformation of points\n                        can fail - for example if points are outside the\n                        deformation field. If that happens, they will be\n                        returned as `NaN`. Unless `affine_fallback` is\n                        `True`, in which case we will apply only the rigid\n                        affine  part of the transformation to at least get close\n                        to the correct coordinates.\n    caching :           bool\n                        If True, will (pre-)cache data for transforms whenever\n                        possible. Depending on the data and the type of\n                        transforms this can tremendously speed things up at the\n                        cost of increased memory usage:\n                          - `False` = no upfront cost, lower memory footprint\n                          - `True` = higher upfront cost, most definitely faster\n                        Only applies if input is NeuronList and if transforms\n                        include H5 transform.\n\n    Returns\n    -------\n    same type as `x`\n                        Copy of input with transformed coordinates.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; # Example neurons are in 8nm voxel space\n    &gt;&gt;&gt; nl = navis.example_neurons()\n    &gt;&gt;&gt; # Make a simple Affine transform to go from voxel to nanometers\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; M = np.diag([8, 8, 8, 8])\n    &gt;&gt;&gt; tr = navis.transforms.AffineTransform(M)\n    &gt;&gt;&gt; # Apply the transform\n    &gt;&gt;&gt; xf = navis.xform(nl, tr)\n\n    See Also\n    --------\n    [`navis.xform_brain`][]\n                    Higher level function that finds and applies a sequence of\n                    transforms to go from one template brain to another.\n\n    \"\"\"\n    # We need to work with TransformSequence\n    if isinstance(transform, (list, np.ndarray)):\n        transform = TransformSequence(*transform)\n    elif isinstance(transform, BaseTransform):\n        transform = TransformSequence(transform)\n    elif not isinstance(transform, TransformSequence):\n        raise TypeError(f'Expected Transform or TransformSequence, got \"{type(transform)}\"')\n\n    if isinstance(x, core.NeuronList):\n        if len(x) == 1:\n            x = x[0]\n        else:\n            xf = []\n            # Get the transformation sequence\n            with TransOptimizer(transform, bbox=x.bbox, caching=caching):\n                try:\n                    for i, n in enumerate(config.tqdm(x, desc='Xforming',\n                                                      disable=config.pbar_hide,\n                                                      leave=config.pbar_leave)):\n                        xf.append(xform(n,\n                                        transform=transform,\n                                        caching=caching,\n                                        affine_fallback=affine_fallback))\n\n                        # If not caching we will clear the map cache after\n                        # each neuron to free memory\n                        if not caching:\n                            _get_coordinates_map.cache_clear()\n                except BaseException:\n                    raise\n                finally:\n                    # Make sure we clear the coordinate map cache when done\n                    _get_coordinates_map.cache_clear()\n\n            return x.__class__(xf)\n\n    if isinstance(x, core.BaseNeuron):\n        # VoxelNeurons are a special case and have hence their own function\n        if isinstance(x, core.VoxelNeuron):\n            return _xform_image(x, transform=transform)\n\n        xf = x.copy()\n        # We will collate spatial data to reduce overhead from calling\n        # R's xform_brain\n        if isinstance(xf, core.TreeNeuron):\n            xyz = xf.nodes[['x', 'y', 'z']].values\n        elif isinstance(xf, core.MeshNeuron):\n            xyz = xf.vertices\n        elif isinstance(xf, core.Dotprops):\n            xyz = xf.points\n            # If this dotprops has a `k`, we only need to transform points and\n            # can regenerate the rest. If not, we need to make helper points\n            # to carry over vectors\n            if isinstance(xf.k, type(None)) or xf.k &lt;= 0:\n                # To avoid problems with these helpers we need to make sure\n                # they aren't too close to their cognate points (otherwise we'll\n                # get NaNs later). We can fix this by scaling the vector by the\n                # sampling resolution which should also help make things less\n                # noisy.\n                hp = xf.points + xf.vect * xf.sampling_resolution\n                xyz = np.append(xyz, hp, axis=0)\n        else:\n            raise TypeError(f\"Don't know how to transform neuron of type '{type(xf)}'\")\n\n        # Add connectors if they exist\n        if xf.has_connectors:\n            xyz = np.vstack([xyz, xf.connectors[['x', 'y', 'z']].values])\n\n        # Do the xform of all spatial data\n        xyz_xf = xform(xyz,\n                       transform=transform,\n                       affine_fallback=affine_fallback)\n\n        # Guess change in spatial units\n        if xyz.shape[0] &gt; 1:\n            change, magnitude = _guess_change(xyz, xyz_xf, sample=1000)\n        else:\n            change, magnitude = 1, 0\n            logger.warning(f'Unable to assess change of units for neuron {x.id}: '\n                           'must have at least two nodes/points.')\n\n        # Round change -&gt; this rounds to the first non-zero digit\n        # change = np.around(change, decimals=-magnitude)\n\n        # Map xformed coordinates back\n        if isinstance(xf, core.TreeNeuron):\n            xf.nodes[['x', 'y', 'z']] = xyz_xf[:xf.n_nodes]\n            # Fix radius based on our best estimate\n            if 'radius' in xf.nodes.columns:\n                xf.nodes['radius'] *= 10**magnitude\n        elif isinstance(xf, core.Dotprops):\n            xf.points = xyz_xf[:xf.points.shape[0]]\n\n            # If this dotprops has a `k`, set tangent vectors and alpha to\n            # None so they will be regenerated\n            if not isinstance(xf.k, type(None)) and xf.k &gt; 0:\n                xf._vect = xf._alpha = None\n            else:\n                # Re-generate vectors\n                hp = xyz_xf[xf.points.shape[0]: xf.points.shape[0] * 2]\n                vect = xf.points - hp\n                vect = vect / np.linalg.norm(vect, axis=1).reshape(-1, 1)\n                xf._vect = vect\n        elif isinstance(xf, core.MeshNeuron):\n            xf.vertices = xyz_xf[:xf.vertices.shape[0]]\n\n        if xf.has_connectors:\n            xf.connectors[['x', 'y', 'z']] = xyz_xf[-xf.connectors.shape[0]:]\n\n        # Make an educated guess as to whether the units have changed\n        if hasattr(xf, 'units') and magnitude != 0:\n            if isinstance(xf.units, (config.ureg.Unit, config.ureg.Quantity)):\n                xf.units = (xf.units / 10**magnitude).to_compact()\n\n        # Fix soma radius if applicable\n        if hasattr(xf, 'soma_radius') and isinstance(xf.soma_radius, numbers.Number):\n            xf.soma_radius *= 10**magnitude\n\n        return xf\n    elif isinstance(x, pd.DataFrame):\n        if any([c not in x.columns for c in ['x', 'y', 'z']]):\n            raise ValueError('DataFrame must have x, y and z columns.')\n        x = x.copy()\n        x[['x', 'y', 'z']] = xform(x[['x', 'y', 'z']].values,\n                                   transform=transform,\n                                   affine_fallback=affine_fallback)\n        return x\n    elif isinstance(x, tm.Trimesh):\n        x = x.copy()\n        x.vertices = xform(x.vertices,\n                           transform=transform,\n                           affine_fallback=affine_fallback)\n        return x\n    else:\n        try:\n            # At this point we expect numpy arrays\n            x = np.asarray(x)\n        except BaseException:\n            raise TypeError(f'Unable to transform data of type \"{type(x)}\"')\n\n        if not x.ndim == 2 or x.shape[1] != 3:\n            raise ValueError('Array must be of shape (N, 3).')\n\n    # Apply transform and return xformed points\n    return transform.xform(x, affine_fallback=affine_fallback)\n</code></pre>"},{"location":"reference/navis/#navis.xform_brain","title":"<code>navis.xform_brain</code>","text":"<p>Transform 3D data between template brains.</p> <p>This requires the appropriate transforms to be registered with <code>navis</code>. See the docs/tutorials for details.</p> Notes <p>For Neurons only: transforms can introduce a change in the units (e.g. if the transform goes from micron to nanometer space). Some template brains have their units hard-coded in their meta data (as <code>_navis_units</code>). If that's not the case we fall-back to trying to infer any change in units by comparing distances between x/y/z coordinate before and after the transform. That approach works reasonably well with base 10 increments (e.g. nm -&gt; um) but may be off with odd changes in units (e.g. physical -&gt; voxel space). Regardless of whether hard-coded or inferred, any change in units is used to update the <code>.units</code> property and node/soma radii for TreeNeurons.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            Data to transform. Dataframe must contain `['x', 'y', 'z']`\n            columns. Numpy array must be shape `(N, 3)`.\n</code></pre> <p> TYPE: <code>                Neuron/List | numpy.ndarray | pandas.DataFrame</code> </p> <code>source</code> <pre><code>            Source template brain that the data currently is in.\n</code></pre> <p> TYPE: <code>           str</code> </p> <code>target</code> <pre><code>            Target template brain that the data should be\n            transformed into.\n</code></pre> <p> TYPE: <code>           str</code> </p> <code>via</code> <pre><code>            Optionally set intermediate template(s). This can be\n            helpful to force a specific transformation sequence.\n</code></pre> <p> TYPE: <code>              str | list thereof</code> DEFAULT: <code>None</code> </p> <code>avoid</code> <pre><code>            Prohibit going through specific intermediate template(s).\n</code></pre> <p> TYPE: <code>            str | list thereof</code> DEFAULT: <code>None</code> </p> <code>affine_fallback</code> <pre><code>            In some cases the non-rigid transformation of points\n            can fail - for example if points are outside the\n            deformation field. If that happens, they will be\n            returned as `NaN`. If `affine_fallback=True`\n            we will apply only the rigid affine part of the\n            transformation to those points to get as close as\n            possible to the correct coordinates.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>True</code> </p> <code>caching</code> <pre><code>            If True, will (pre-)cache data for transforms whenever\n            possible. Depending on the data and the type of\n            transforms this can speed things up significantly at the\n            cost of increased memory usage:\n              - `False` = no upfront cost, lower memory footprint\n              - `True` = higher upfront cost, most definitely faster\n            Only applies if input is NeuronList and if transforms\n            include H5 transform.\n</code></pre> <p> TYPE: <code>          bool</code> DEFAULT: <code>True</code> </p> <code>verbose</code> <pre><code>            If True, will print some useful info on transform.\n</code></pre> <p> TYPE: <code>          bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>same type as `x`</code> <p>Copy of input with transformed coordinates.</p> <p>Examples:</p> <p>This example requires the flybrains library to be installed: <code>pip3 install flybrains</code></p> <p>Also, if you haven't already, you will need to have the optional Saalfeld lab (Janelia Research Campus) transforms installed (this is a one-off):</p> <pre><code>&gt;&gt;&gt; import flybrains\n&gt;&gt;&gt; flybrains.download_jrc_transforms()\n</code></pre> <p>Once <code>flybrains</code> is installed and you have downloaded the registrations, you can run this:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; import flybrains\n&gt;&gt;&gt; # navis example neurons are in raw (8nm voxel) hemibrain (JRCFIB2018Fraw) space\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; # Transform to FAFB14 space\n&gt;&gt;&gt; xf = navis.xform_brain(n, source='JRCFIB2018Fraw', target='FAFB14')\n</code></pre> See Also <p><code>navis.xform</code>                 Lower level entry point that takes data and applies a given                 transform or sequence thereof. <code>navis.mirror_brain</code>                 Uses non-rigid transforms to mirror neurons from the left                 to the right side of given template brain and vice versa.</p> Source code in <code>navis/transforms/templates.py</code> <pre><code>def xform_brain(\n    x: Union[\"core.NeuronObject\", \"pd.DataFrame\", \"np.ndarray\"],\n    source: str,\n    target: str,\n    via: Optional[str] = None,\n    avoid: Optional[str] = None,\n    affine_fallback: bool = True,\n    caching: bool = True,\n    verbose: bool = True,\n) -&gt; Union[\"core.NeuronObject\", \"pd.DataFrame\", \"np.ndarray\"]:\n    \"\"\"Transform 3D data between template brains.\n\n    This requires the appropriate transforms to be registered with `navis`.\n    See the docs/tutorials for details.\n\n    Notes\n    -----\n    For Neurons only: transforms can introduce a change in the units (e.g. if\n    the transform goes from micron to nanometer space). Some template brains have\n    their units hard-coded in their meta data (as `_navis_units`). If that's\n    not the case we fall-back to trying to infer any change in units by comparing\n    distances between x/y/z coordinate before and after the transform. That\n    approach works reasonably well with base 10 increments (e.g. nm -&gt; um) but\n    may be off with odd changes in units (e.g. physical -&gt; voxel space).\n    Regardless of whether hard-coded or inferred, any change in units is used to\n    update the `.units` property and node/soma radii for TreeNeurons.\n\n    Parameters\n    ----------\n    x :                 Neuron/List | numpy.ndarray | pandas.DataFrame\n                        Data to transform. Dataframe must contain `['x', 'y', 'z']`\n                        columns. Numpy array must be shape `(N, 3)`.\n    source :            str\n                        Source template brain that the data currently is in.\n    target :            str\n                        Target template brain that the data should be\n                        transformed into.\n    via :               str | list thereof, optional\n                        Optionally set intermediate template(s). This can be\n                        helpful to force a specific transformation sequence.\n    avoid :             str | list thereof, optional\n                        Prohibit going through specific intermediate template(s).\n    affine_fallback :   bool\n                        In some cases the non-rigid transformation of points\n                        can fail - for example if points are outside the\n                        deformation field. If that happens, they will be\n                        returned as `NaN`. If `affine_fallback=True`\n                        we will apply only the rigid affine part of the\n                        transformation to those points to get as close as\n                        possible to the correct coordinates.\n    caching :           bool\n                        If True, will (pre-)cache data for transforms whenever\n                        possible. Depending on the data and the type of\n                        transforms this can speed things up significantly at the\n                        cost of increased memory usage:\n                          - `False` = no upfront cost, lower memory footprint\n                          - `True` = higher upfront cost, most definitely faster\n                        Only applies if input is NeuronList and if transforms\n                        include H5 transform.\n    verbose :           bool\n                        If True, will print some useful info on transform.\n\n    Returns\n    -------\n    same type as `x`\n                        Copy of input with transformed coordinates.\n\n    Examples\n    --------\n    This example requires the\n    [flybrains](https://github.com/navis-org/navis-flybrains)\n    library to be installed: `pip3 install flybrains`\n\n    Also, if you haven't already, you will need to have the optional Saalfeld\n    lab (Janelia Research Campus) transforms installed (this is a one-off):\n\n    &gt;&gt;&gt; import flybrains                                        # doctest: +SKIP\n    &gt;&gt;&gt; flybrains.download_jrc_transforms()                     # doctest: +SKIP\n\n    Once `flybrains` is installed and you have downloaded the registrations,\n    you can run this:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; import flybrains\n    &gt;&gt;&gt; # navis example neurons are in raw (8nm voxel) hemibrain (JRCFIB2018Fraw) space\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; # Transform to FAFB14 space\n    &gt;&gt;&gt; xf = navis.xform_brain(n, source='JRCFIB2018Fraw', target='FAFB14') # doctest: +SKIP\n\n    See Also\n    --------\n    [`navis.xform`][]\n                    Lower level entry point that takes data and applies a given\n                    transform or sequence thereof.\n    [`navis.mirror_brain`][]\n                    Uses non-rigid transforms to mirror neurons from the left\n                    to the right side of given template brain and vice versa.\n\n    \"\"\"\n    if not isinstance(source, str):\n        TypeError(f'Expected source of type str, got \"{type(source)}\"')\n\n    if not isinstance(target, str):\n        TypeError(f'Expected target of type str, got \"{type(target)}\"')\n\n    # Get the transformation sequence\n    path, transforms = registry.find_bridging_path(source, target, via=via, avoid=avoid)\n\n    if verbose:\n        path_str = path[0]\n        for p, tr in zip(path[1:], transforms):\n            if isinstance(tr, AliasTransform):\n                link = \"=\"\n            else:\n                link = \"-&gt;\"\n            path_str += f\" {link} {p}\"\n\n        print(\"Transform path:\", path_str)\n\n    # Combine into transform sequence\n    trans_seq = TransformSequence(*transforms)\n\n    # Apply transform and returned xformed points\n    xf = xform(x, transform=trans_seq, caching=caching, affine_fallback=affine_fallback)\n\n    # We might be able to set the correct units based on the target template's\n    # meta data (the \"guessed\" new units can be off if the transform is\n    # not base 10 which happens for e.g. voxels -&gt; physical space)\n    if isinstance(xf, (core.NeuronList, core.BaseNeuron)):\n        # First we need to find the last non-alias template space\n        for tmp, tr in zip(path[::-1], transforms[::-1]):\n            if not isinstance(tr, AliasTransform):\n                # There is a chance that there is no meta data for this template\n                try:\n                    last_temp = registry.find_template(tmp)\n                except ValueError:\n                    break\n                except BaseException:\n                    raise\n                # If this template brain has a property for navis units\n                if hasattr(last_temp, \"_navis_units\"):\n                    for n in core.NeuronList(xf):\n                        n.units = last_temp._navis_units\n                break\n\n    return xf\n</code></pre>"},{"location":"reference/navis/config/","title":"config","text":""},{"location":"reference/navis/config/#navis.config.default_logging","title":"<code>navis.config.default_logging</code>","text":"<p>Add a formatted stream handler to the <code>navis</code> logger.</p> <p>Called by default when navis is imported for the first time. To prevent this behaviour, set an environment variable: <code>NAVIS_SKIP_LOG_SETUP=True</code>.</p> Source code in <code>navis/config.py</code> <pre><code>def default_logging():\n    \"\"\"Add a formatted stream handler to the `navis` logger.\n\n    Called by default when navis is imported for the first time.\n    To prevent this behaviour, set an environment variable:\n    `NAVIS_SKIP_LOG_SETUP=True`.\n    \"\"\"\n    logger.setLevel(logging.INFO)\n    if len(logger.handlers) == 0:\n        sh = logging.StreamHandler()\n        sh.setLevel(logging.DEBUG)\n        # Create formatter and add it to the handlers\n        formatter = logging.Formatter(\n            '%(levelname)-5s : %(message)s (%(name)s)')\n        sh.setFormatter(formatter)\n        logger.addHandler(sh)\n</code></pre>"},{"location":"reference/navis/config/#navis.config.get_logger","title":"<code>navis.config.get_logger</code>","text":"Source code in <code>navis/config.py</code> <pre><code>def get_logger(name: str):\n    if skip_log_setup:\n        return logging.getLogger(name)\n    return logger\n</code></pre>"},{"location":"reference/navis/config/#navis.config.is_jupyter","title":"<code>navis.config.is_jupyter</code>","text":"<p>Test if navis is run in a Jupyter notebook.</p> Source code in <code>navis/config.py</code> <pre><code>def is_jupyter():\n    \"\"\"Test if navis is run in a Jupyter notebook.\"\"\"\n    return _type_of_script() == 'jupyter'\n</code></pre>"},{"location":"reference/navis/config/#navis.config.remove_log_handlers","title":"<code>navis.config.remove_log_handlers</code>","text":"<p>Remove all handlers from the <code>navis</code> logger.</p> <p>It may be preferable to skip navis' default log handler being added in the first place. Do this by setting an environment variable before the first import: <code>NAVIS_SKIP_LOG_SETUP=True</code>.</p> Source code in <code>navis/config.py</code> <pre><code>def remove_log_handlers():\n    \"\"\"Remove all handlers from the `navis` logger.\n\n    It may be preferable to skip navis' default log handler being added in the\n    first place.\n    Do this by setting an environment variable before the first import:\n    `NAVIS_SKIP_LOG_SETUP=True`.\n    \"\"\"\n    logger.handlers.clear()\n</code></pre>"},{"location":"reference/navis/connectivity/","title":"connectivity","text":""},{"location":"reference/navis/connectivity/#navis.connectivity.group_matrix","title":"<code>navis.connectivity.group_matrix</code>","text":"<p>Group adjacency matrix into neuron groups.</p> PARAMETER DESCRIPTION <code>mat</code> <pre><code>            Matrix to group.\n</code></pre> <p> TYPE: <code>              pandas.DataFrame | numpy.array</code> </p> <code>row_groups</code> <pre><code>            Row groups to be formed. Can be either:\n\n              1. `{group1: [neuron1, neuron2, ...], ...}`\n              2. `{neuron1: group1, neuron2:group2, ...}`\n\n            If grouping numpy arrays, use indices!\n</code></pre> <p> TYPE: <code>       dict</code> DEFAULT: <code>{}</code> </p> <code>col_groups</code> <pre><code>            Col groups. See `row_groups` for details.\n</code></pre> <p> TYPE: <code>       dict</code> DEFAULT: <code>{}</code> </p> <code>drop_ungrouped</code> <pre><code>            If ungrouped, neurons that are not part of a\n            row/col_group are dropped from the matrix.\n</code></pre> <p> TYPE: <code>   bool</code> DEFAULT: <code>False</code> </p> <code>method</code> <pre><code>            Method by which values are collapsed into groups.\n</code></pre> <p> TYPE: <code>           'AVERAGE' | 'MAX' | 'MIN' | 'SUM'</code> DEFAULT: <code>'SUM'</code> </p> RETURNS DESCRIPTION <code>pandas.DataFrame</code> Source code in <code>navis/connectivity/matrix_utils.py</code> <pre><code>def group_matrix(mat: Union[pd.DataFrame, np.ndarray],\n                 row_groups: Optional[dict] = {},\n                 col_groups: Optional[dict] = {},\n                 drop_ungrouped: bool = False,\n                 method: str = 'SUM') -&gt; pd.DataFrame:\n    \"\"\"Group adjacency matrix into neuron groups.\n\n    Parameters\n    ----------\n    mat :               pandas.DataFrame | numpy.array\n                        Matrix to group.\n    row_groups :        dict, optional\n                        Row groups to be formed. Can be either:\n\n                          1. `{group1: [neuron1, neuron2, ...], ...}`\n                          2. `{neuron1: group1, neuron2:group2, ...}`\n\n                        If grouping numpy arrays, use indices!\n    col_groups :        dict, optional\n                        Col groups. See `row_groups` for details.\n    drop_ungrouped :    bool, optional\n                        If ungrouped, neurons that are not part of a\n                        row/col_group are dropped from the matrix.\n    method :            'AVERAGE' | 'MAX' | 'MIN' | 'SUM', optional\n                        Method by which values are collapsed into groups.\n\n    Returns\n    -------\n    pandas.DataFrame\n\n    \"\"\"\n    PERMISSIBLE_METHODS = ['AVERAGE', 'MIN', 'MAX', 'SUM']\n    if method not in PERMISSIBLE_METHODS:\n        raise ValueError(f'Unknown method \"{method}\". Please use either '\n                         f'{\",\".join(PERMISSIBLE_METHODS)}')\n\n    if not row_groups and not col_groups:\n        logger.warning('No column/row groups provided - skipping.')\n        return mat\n\n    # Convert numpy array to DataFrame\n    if isinstance(mat, np.ndarray):\n        mat = pd.DataFrame(mat)\n    # Make copy of original DataFrame\n    elif isinstance(mat, pd.DataFrame):\n        mat = mat.copy()\n    else:\n        raise TypeError(f'Expected numpy array or pandas DataFrames, got \"{type(mat)}\"')\n\n    # Convert to neuron-&gt;group format if necessary\n    if col_groups and utils.is_iterable(list(col_groups.values())[0]):\n        col_groups = {n: g for g in col_groups for n in col_groups[g]}\n    if row_groups and utils.is_iterable(list(row_groups.values())[0]):\n        row_groups = {n: g for g in row_groups for n in row_groups[g]}\n\n    # Make sure everything is string\n    mat.index = mat.index.astype(str)\n    mat.columns = mat.columns.astype(str)\n    col_groups = {str(k): str(v) for k, v in col_groups.items()}  # type: ignore # redefinition error\n    row_groups = {str(k): str(v) for k, v in row_groups.items()}  # type: ignore # redefinition error\n\n    if row_groups:\n        # Drop non-grouped values if applicable\n        if drop_ungrouped:\n            mat = mat.loc[mat.index.isin(row_groups.keys())]\n\n        # Add temporary grouping column\n        mat['row_groups'] = [row_groups.get(s, s) for s in mat.index]\n\n        if method == 'AVERAGE':\n            mat = mat.groupby('row_groups').mean()\n        elif method == 'MAX':\n            mat = mat.groupby('row_groups').max()\n        elif method == 'MIN':\n            mat = mat.groupby('row_groups').min()\n        elif method == 'SUM':\n            mat = mat.groupby('row_groups').sum()\n\n    if col_groups:\n        # Transpose for grouping\n        mat = mat.T\n\n        # Drop non-grouped values if applicable\n        if drop_ungrouped:\n            mat = mat.loc[mat.index.isin(col_groups.keys())]\n\n        # Add temporary grouping column\n        mat['col_groups'] = [col_groups.get(s, s) for s in mat.index]\n\n        if method == 'AVERAGE':\n            mat = mat.groupby('col_groups').mean()\n        elif method == 'MAX':\n            mat = mat.groupby('col_groups').max()\n        elif method == 'MIN':\n            mat = mat.groupby('col_groups').min()\n        elif method == 'SUM':\n            mat = mat.groupby('col_groups').sum()\n\n        # Transpose back\n        mat = mat.T\n\n    # Preserve datatype\n    mat.datatype = 'adjacency_matrix'\n    # Add flag that this matrix has been grouped\n    mat.is_grouped = True\n\n    return mat\n</code></pre>"},{"location":"reference/navis/conversion/","title":"conversion","text":""},{"location":"reference/navis/conversion/#navis.conversion.mesh2skeleton","title":"<code>navis.conversion.mesh2skeleton</code>","text":"<p>Turn mesh neuron into skeleton.</p> <p>This function is a thin-wrapper for <code>skeletor</code>. It uses sensible defaults for neurons but if you want to fine-tune your skeletons you should look into using <code>skeletor</code> directly.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Mesh(es) to skeletonize. Note that the quality of the results\n    very much depends on the mesh, so it might be worth doing some\n    pre-processing (see below).\n</code></pre> <p> TYPE: <code>        MeshNeuron | trimesh.Trimesh</code> </p> <code>method</code> <pre><code>    Method to use for skeletonization:\n     - \"wavefront\": fast but noisier, skeletons will be ~centered\n       within the neuron\n     - \"teasar\": slower but smoother, skeletons follow the\n       surface of the mesh, requires the `inv_dist` parameter to be\n       set\n    \"wavefront\" also produces radii, \"teasar\" doesn't.\n</code></pre> <p> TYPE: <code>   'wavefront' | 'teasar'</code> DEFAULT: <code>'wavefront'</code> </p> <code>fix_mesh</code> <pre><code>    Whether to try to fix some common issues in the mesh before\n    skeletonization. Note that this might compromise the\n    vertex-to-node-ID mapping.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>False</code> </p> <code>shave</code> <pre><code>    Whether to \"shave\" the resulting skeleton to reduce bristles\n    on the backbone.\n</code></pre> <p> TYPE: <code>    bool</code> DEFAULT: <code>True</code> </p> <code>heal</code> <pre><code>    Whether to heal the resulting skeleton if it is fragmented.\n    For more control over the stitching set `heal=False` and use\n    [`navis.heal_skeleton`][] directly. Note that this\n    can be fairly costly if the mesh as many tiny fragments.\n</code></pre> <p> TYPE: <code>     bool | \"LEAFS\" | \"ALL\"</code> DEFAULT: <code>False</code> </p> <code>connectors</code> <pre><code>    Whether to carry over existing connector tables. This will\n    attach connectors by first snapping them to the closest mesh\n    vertex and then to the skeleton node corresponding to that\n    vertex.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>inv_dist</code> <pre><code>    Only required for method \"teasar\": invalidation distance for\n    the traversal. Smaller `inv_dist` captures smaller features\n    but is slower and more noisy, and vice versa. A good starting\n    value is around 2-5 microns. Can be a unit string - e.g.\n    \"5 microns\" - if your neuron has its units set.\n</code></pre> <p> TYPE: <code> int | float | str</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <pre><code>    Additional keyword arguments are passed through to the respective\n    function in `skeletor` - i.e. `by_wavefront` or `by_teasar`.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>skeleton</code> <p>Has a <code>.vertex_map</code> attribute that maps each vertex in the input mesh to a skeleton node ID.</p> <p> TYPE: <code>navis.TreeNeuron</code> </p> See Also <p><code>navis.drop_fluff</code>             Use this if your mesh has lots of tiny free floating bits to             reduce noise and speed up skeletonization.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; # Get a mesh neuron\n&gt;&gt;&gt; n = navis.example_neurons(1, kind='mesh')\n&gt;&gt;&gt; # Convert to skeleton\n&gt;&gt;&gt; sk = navis.conversion.mesh2skeleton(n)\n&gt;&gt;&gt; # Mesh vertex indices to node IDs map\n&gt;&gt;&gt; sk.vertex_map\narray([938, 990, 990, ...,  39, 234, 234])\n</code></pre> Source code in <code>navis/conversion/converters.py</code> <pre><code>@utils.map_neuronlist(desc='Skeletonizing', allow_parallel=True)\ndef mesh2skeleton(x: 'core.MeshNeuron',\n                  method: str = 'wavefront',\n                  fix_mesh: bool = False,\n                  shave: bool = True,\n                  heal: bool = False,\n                  connectors: bool = False,\n                  inv_dist: Union[int, float] = None,\n                  **kwargs):\n    \"\"\"Turn mesh neuron into skeleton.\n\n    This function is a thin-wrapper for `skeletor`. It uses sensible defaults\n    for neurons but if you want to fine-tune your skeletons you should look\n    into using `skeletor` directly.\n\n    Parameters\n    ----------\n    x :         MeshNeuron | trimesh.Trimesh\n                Mesh(es) to skeletonize. Note that the quality of the results\n                very much depends on the mesh, so it might be worth doing some\n                pre-processing (see below).\n    method :    'wavefront' | 'teasar'\n                Method to use for skeletonization:\n                 - \"wavefront\": fast but noisier, skeletons will be ~centered\n                   within the neuron\n                 - \"teasar\": slower but smoother, skeletons follow the\n                   surface of the mesh, requires the `inv_dist` parameter to be\n                   set\n                \"wavefront\" also produces radii, \"teasar\" doesn't.\n    fix_mesh :  bool\n                Whether to try to fix some common issues in the mesh before\n                skeletonization. Note that this might compromise the\n                vertex-to-node-ID mapping.\n    shave :     bool\n                Whether to \"shave\" the resulting skeleton to reduce bristles\n                on the backbone.\n    heal :      bool | \"LEAFS\" | \"ALL\"\n                Whether to heal the resulting skeleton if it is fragmented.\n                For more control over the stitching set `heal=False` and use\n                [`navis.heal_skeleton`][] directly. Note that this\n                can be fairly costly if the mesh as many tiny fragments.\n    connectors : bool\n                Whether to carry over existing connector tables. This will\n                attach connectors by first snapping them to the closest mesh\n                vertex and then to the skeleton node corresponding to that\n                vertex.\n    inv_dist :  int | float | str\n                Only required for method \"teasar\": invalidation distance for\n                the traversal. Smaller `inv_dist` captures smaller features\n                but is slower and more noisy, and vice versa. A good starting\n                value is around 2-5 microns. Can be a unit string - e.g.\n                \"5 microns\" - if your neuron has its units set.\n    **kwargs\n                Additional keyword arguments are passed through to the respective\n                function in `skeletor` - i.e. `by_wavefront` or `by_teasar`.\n\n    Returns\n    -------\n    skeleton :  navis.TreeNeuron\n                Has a `.vertex_map` attribute that maps each vertex in the\n                input mesh to a skeleton node ID.\n\n    See Also\n    --------\n    [`navis.drop_fluff`][]\n                Use this if your mesh has lots of tiny free floating bits to\n                reduce noise and speed up skeletonization.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; # Get a mesh neuron\n    &gt;&gt;&gt; n = navis.example_neurons(1, kind='mesh')\n    &gt;&gt;&gt; # Convert to skeleton\n    &gt;&gt;&gt; sk = navis.conversion.mesh2skeleton(n)\n    &gt;&gt;&gt; # Mesh vertex indices to node IDs map\n    &gt;&gt;&gt; sk.vertex_map                                           # doctest: +SKIP\n    array([938, 990, 990, ...,  39, 234, 234])\n\n    \"\"\"\n    utils.eval_param(x, name='x', allowed_types=(core.MeshNeuron, tm.Trimesh))\n    utils.eval_param(method, name='method', allowed_values=('wavefront', 'teasar'))\n\n    if method == 'teasar' and inv_dist is None:\n        raise ValueError('Must set `inv_dist` parameter when using method '\n                         '\"teasar\". A good starting value is around 2-5 microns.')\n\n    props = {'soma': None}\n    if isinstance(x, core.MeshNeuron):\n        props.update({'id': x.id, 'name': x.name, 'units': x.units})\n        if x.has_soma_pos:\n            props['soma_pos'] = x.soma_pos\n\n        if not isinstance(inv_dist, type(None)):\n            inv_dist = x.map_units(inv_dist)\n\n        mesh = x.trimesh\n    else:\n        mesh = x\n        x = core.MeshNeuron(x)\n\n    if fix_mesh:\n        mesh = sk.pre.fix_mesh(mesh, remove_disconnected=False)\n\n    kwargs['progress'] = False\n    if method == 'wavefront':\n        skeleton = sk.skeletonize.by_wavefront(mesh, **kwargs)\n    elif method == 'teasar':\n        skeleton = sk.skeletonize.by_teasar(x, inv_dist=inv_dist, **kwargs)\n\n    props['vertex_map'] = skeleton.mesh_map\n\n    s = core.TreeNeuron(skeleton.swc, **props)\n\n    if s.has_soma:\n        s.reroot(s.soma, inplace=True)\n\n    if heal:\n        _ = morpho.heal_skeleton(s, inplace=True, method='ALL')\n\n    if shave:\n        # Find single node bristles\n        leafs = s.leafs.node_id.values\n\n        # Make sure we keep the soma\n        if s.has_soma:\n            leafs = leafs[~np.isin(leafs, s.soma)]\n\n        bp = s.branch_points.node_id.values\n        bristles = s.nodes[s.nodes.node_id.isin(leafs)\n                           &amp; s.nodes.parent_id.isin(bp)]\n\n        # Subset neuron\n        keep = s.nodes[~s.nodes.node_id.isin(bristles.node_id)].node_id.values\n        s = morpho.subset_neuron(s, keep, inplace=True)\n\n        # Fix vertex map\n        for b, p in zip(bristles.node_id.values, bristles.parent_id.values):\n            s.vertex_map[s.vertex_map == b] = p\n\n    # In particular with method wavefront, some nodes (mostly leafs) can have\n    # a radius of 0. We will fix this here by giving them 1/2 the radius of\n    # their parent nodes'\n    to_fix = (s.nodes.radius == 0) &amp; (s.nodes.parent_id &gt;= 0)\n    if any(to_fix):\n        radii = s.nodes.set_index('node_id').radius\n        new_radii = radii.loc[s.nodes.loc[to_fix].parent_id].values / 2\n        s.nodes.loc[to_fix, 'radius'] = new_radii\n\n    # Last but not least: map connectors\n    if connectors and x.has_connectors:\n        cn_table = x.connectors.copy()\n\n        # A connector/id column is currently required for skeletons but not\n        # meshes\n        if not any(np.isin(('id', 'connector_id'), cn_table.columns)):\n            cn_table.insert(0, 'connector_id', np.arange(len(cn_table)))\n\n        cn_table['node_id'] = x.snap(cn_table[['x', 'y', 'z']].values)[0]\n        node_map = dict(zip(np.arange(len(s.vertex_map)), s.vertex_map))\n        cn_table['node_id'] = cn_table.node_id.map(node_map)\n        s.connectors = cn_table\n\n    return s\n</code></pre>"},{"location":"reference/navis/conversion/#navis.conversion.neuron2voxels","title":"<code>navis.conversion.neuron2voxels</code>","text":"<p>Turn neuron into voxels.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Neuron(s) to voxelize. Uses the neurons' nodes, vertices and\n        points, respectively.\n</code></pre> <p> TYPE: <code>            TreeNeuron | MeshNeuron | Dotprops</code> </p> <code>pitch</code> <pre><code>        Side length(s) voxels. Can be isometric (float) or an\n        iterable of dimensions in (x, y, z).\n</code></pre> <p> TYPE: <code>        float | iterable thereof</code> </p> <code>bounds</code> <pre><code>        Boundaries [in units of `x`] for the voxel grid. If not\n        provided, will use `x.bbox`.\n</code></pre> <p> TYPE: <code>       (3, 2)  or (2, 3) array</code> DEFAULT: <code>None</code> </p> <code>counts</code> <pre><code>        If True, voxel grid will have point counts for values\n        instead of just True/False.\n</code></pre> <p> TYPE: <code>       bool</code> DEFAULT: <code>False</code> </p> <code>vectors</code> <pre><code>        If True, will also attach a vector field as `.vectors`\n        property.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> <code>alphas</code> <pre><code>        If True, will also return a grid with alpha values as\n        `.alpha` property.\n</code></pre> <p> TYPE: <code>       bool</code> DEFAULT: <code>False</code> </p> <code>smooth</code> <pre><code>        If non-zero, will apply a Gaussian filter with `smooth`\n        as `sigma`.\n</code></pre> <p> TYPE: <code>       int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>VoxelNeuron</code> <p>Has the voxel grid as <code>.grid</code> and (optionally) <code>.vectors</code> and <code>.alphas</code> properties. <code>.grid</code> data type depends on settings:  - default = bool (i.e. True/False)  - if <code>counts=True</code> = integer  - if <code>smooth=True</code> = float Empty voxels will have vector (0, 0, 0) and alpha 0. Also note that data tables (e.g. <code>connectors</code>) are not carried over from the input neuron.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; # Get a skeleton\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; # Convert to voxel neuron\n&gt;&gt;&gt; vx = navis.conversion.neuron2voxels(n, pitch='5 microns')\n</code></pre> Source code in <code>navis/conversion/converters.py</code> <pre><code>@utils.map_neuronlist(desc='Voxelizing', allow_parallel=True)\ndef neuron2voxels(x: 'core.BaseNeuron',\n                  pitch: Union[list, tuple, float],\n                  bounds: Optional[list] = None,\n                  counts: bool = False,\n                  vectors: bool = False,\n                  alphas: bool = False,\n                  smooth: int = 0) -&gt; 'core.VoxelNeuron':\n    \"\"\"Turn neuron into voxels.\n\n    Parameters\n    ----------\n    x :             TreeNeuron | MeshNeuron | Dotprops\n                    Neuron(s) to voxelize. Uses the neurons' nodes, vertices and\n                    points, respectively.\n    pitch :         float | iterable thereof\n                    Side length(s) voxels. Can be isometric (float) or an\n                    iterable of dimensions in (x, y, z).\n    bounds :        (3, 2)  or (2, 3) array, optional\n                    Boundaries [in units of `x`] for the voxel grid. If not\n                    provided, will use `x.bbox`.\n    counts :        bool\n                    If True, voxel grid will have point counts for values\n                    instead of just True/False.\n    vectors :       bool\n                    If True, will also attach a vector field as `.vectors`\n                    property.\n    alphas :        bool\n                    If True, will also return a grid with alpha values as\n                    `.alpha` property.\n    smooth :        int\n                    If non-zero, will apply a Gaussian filter with `smooth`\n                    as `sigma`.\n\n    Returns\n    -------\n    VoxelNeuron\n                    Has the voxel grid as `.grid` and (optionally) `.vectors`\n                    and `.alphas` properties. `.grid` data type depends\n                    on settings:\n                     - default = bool (i.e. True/False)\n                     - if `counts=True` = integer\n                     - if `smooth=True` = float\n                    Empty voxels will have vector (0, 0, 0) and alpha 0. Also\n                    note that data tables (e.g. `connectors`) are not carried\n                    over from the input neuron.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; # Get a skeleton\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; # Convert to voxel neuron\n    &gt;&gt;&gt; vx = navis.conversion.neuron2voxels(n, pitch='5 microns')\n\n    \"\"\"\n    if not utils.is_iterable(pitch):\n        # Map units (non-str are just passed through)\n        pitch = x.map_units(pitch, on_error='raise')\n        if not isinstance(pitch, Number):\n            raise TypeError('Expected `pitch` to be a number (or list thereof)'\n                            f', got {type(pitch)}')\n        pitch = [pitch] * 3\n    elif len(pitch) != 3:\n        raise ValueError('`pitch` must be single number or a list of three')\n    else:\n        pitch = np.array([x.map_units(p, on_error='raise') for p in pitch])\n\n    # Convert to voxel indices\n    ix, _ = _make_voxels(x=x, pitch=pitch, strip=False)\n\n    if isinstance(bounds, type(None)):\n        bounds = x.bbox\n    else:\n        bounds = np.asarray(bounds)\n\n    if bounds.shape == (2, 3):\n        bounds = bounds.T\n\n    # Shape of grid\n    dim = np.ceil(bounds[:, 1] / pitch) - np.floor(bounds[:, 0] / pitch)\n    shape = np.ceil(dim).astype(int) + 1\n\n    # Get unique voxels\n    if not counts:\n        vxl = np.unique(ix, axis=0)\n    else:\n        vxl, cnt = np.unique(ix, axis=0, return_counts=True)\n\n    # Substract lower bounds\n    offset = (bounds[:, 0] / pitch)\n    vxl = vxl - offset.round().astype(int)\n    ix = ix - offset.round().astype(int)\n\n    # Drop voxels outside the defined bounds\n    vxl = vxl[vxl.min(axis=1) &gt;= 0]\n    vxl = vxl[np.all(vxl &lt; shape, axis=1)]\n\n    # Generate grid\n    grid = np.zeros(shape=shape, dtype=bool)\n\n    # Populate grid\n    if not counts:\n        grid[vxl[:, 0], vxl[:, 1], vxl[:, 2]] = True\n    else:\n        grid = grid.astype(int)\n        grid[vxl[:, 0], vxl[:, 1], vxl[:, 2]] = cnt\n\n    # Apply Gaussian filter\n    if smooth:\n        grid = gaussian_filter(grid.astype(np.float32), sigma=smooth)\n\n    # Generate neuron\n    units = [f'{p * u} {x.units.units}' for p, u in zip(utils.make_iterable(pitch),\n                                                        x.units_xyz.magnitude)]\n    offset = offset * pitch * x.units_xyz.magnitude\n    n = core.VoxelNeuron(grid, id=x.id, name=x.name, units=units, offset=offset)\n\n    # If no vectors required, we can just return now\n    if not vectors and not alphas:\n        return n\n\n    if isinstance(x, core.TreeNeuron):\n        pts = x.nodes[['x', 'y', 'z']].values\n    elif isinstance(x, core.Dotprops):\n        pts = x.points\n    elif isinstance(x, core.MeshNeuron):\n        pts = np.array(x.vertices)\n\n    # Generate an empty vector field\n    vects = np.zeros((grid.shape[0], grid.shape[1], grid.shape[2], 3),\n                     dtype=np.float32)\n    alph = np.zeros(grid.shape, dtype=np.float32)\n\n    # Get unique voxels\n    uni, inv = np.unique(ix, axis=0, return_inverse=True)\n\n    # Go over each voxel\n    for i in range(len(uni)):\n        # Get points in this voxel\n        pt = pts[inv == i]\n\n        # Reshape\n        pt = pt.reshape(1, -1, 3)\n\n        # Generate centers for each cloud of k nearest neighbors\n        centers = np.mean(pt, axis=1)\n\n        # Generate vector from center\n        cpt = pt - centers.reshape((pt.shape[0], 1, 3))\n\n        # Get inertia (N, 3, 3)\n        inertia = cpt.transpose((0, 2, 1)) @ cpt\n\n        # Extract vector and alpha\n        u, s, vh = np.linalg.svd(inertia)\n        vect = vh[:, 0, :]\n\n        # No alpha if only one point\n        if pt.shape[1] &gt; 1:\n            alpha = (s[:, 0] - s[:, 1]) / np.sum(s, axis=1)\n        else:\n            alpha = [0]\n\n        vects[uni[i][0], uni[i][1], uni[i][2]] = vect.flatten()\n        alph[uni[i][0], uni[i][1], uni[i][2]] = alpha[0]\n\n    if vectors:\n        n.vectors = vects\n    if alpha:\n        n.alphas = alpha\n\n    return n\n</code></pre>"},{"location":"reference/navis/conversion/#navis.conversion.points2skeleton","title":"<code>navis.conversion.points2skeleton</code>","text":"<p>Turn points into skeleton.</p> <p>This function works by:  1. Compute the <code>k</code> nearest neighbors for each point  2. Generate a graph from the nearest-neighbor edges  3. Extract a minimum-spanning tree (MST) from the graph  4. Process the MST into a skeleton</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Points to skeletonize.\n</code></pre> <p> TYPE: <code>        (N, 3) array | Dotprops</code> </p> <code>k</code> <pre><code>    Number of nearest neighbors to consider. Too low values of `k`\n    can lead to disconnected skeletons.\n</code></pre> <p> TYPE: <code>        int</code> DEFAULT: <code>10</code> </p> <code>max_dist</code> <pre><code>    Edges longer than this will be ignored. This can lead to a\n    fragmented (i.e. multi-root) skeleton!\n</code></pre> <p> TYPE: <code> float</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>skeleton</code> <p> TYPE: <code>navis.TreeNeuron</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; # Get a mesh neuron\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; # Get the points\n&gt;&gt;&gt; pts = n.nodes[['x', 'y', 'z']].values\n&gt;&gt;&gt; # Convert points back into skeleton\n&gt;&gt;&gt; sk = navis.conversion.points2skeleton(pts)\n</code></pre> Source code in <code>navis/conversion/converters.py</code> <pre><code>@utils.map_neuronlist(desc='Skeletonizing', allow_parallel=True)\ndef points2skeleton(x: Union['core.Dotprops', np.ndarray],\n                    k: int = 10,\n                    max_dist: Optional[float] = None):\n    \"\"\"Turn points into skeleton.\n\n    This function works by:\n     1. Compute the `k` nearest neighbors for each point\n     2. Generate a graph from the nearest-neighbor edges\n     3. Extract a minimum-spanning tree (MST) from the graph\n     4. Process the MST into a skeleton\n\n    Parameters\n    ----------\n    x :         (N, 3) array | Dotprops\n                Points to skeletonize.\n    k :         int\n                Number of nearest neighbors to consider. Too low values of `k`\n                can lead to disconnected skeletons.\n    max_dist :  float, optional\n                Edges longer than this will be ignored. This can lead to a\n                fragmented (i.e. multi-root) skeleton!\n\n    Returns\n    -------\n    skeleton :  navis.TreeNeuron\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; # Get a mesh neuron\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; # Get the points\n    &gt;&gt;&gt; pts = n.nodes[['x', 'y', 'z']].values\n    &gt;&gt;&gt; # Convert points back into skeleton\n    &gt;&gt;&gt; sk = navis.conversion.points2skeleton(pts)\n\n    \"\"\"\n    utils.eval_param(x, name='x', allowed_types=(core.Dotprops, np.ndarray))\n\n    if isinstance(x, core.Dotprops):\n        pts = x.points\n    else:\n        if (x.ndim != 2) and (x.shape[1] != 3):\n            raise ValueError(f'Points must be shape (N, 3), got {x.shape}')\n        pts = x\n\n    # Get the list of nearest neighbours\n    tree = core.dotprop.KDTree(pts)\n\n    defaults = {}\n    if max_dist is not None:\n        # We have to avoid passing `None` because scipy's KDTree does not like\n        # that (pykdtree does not care)\n        defaults['distance_upper_bound'] = max_dist\n    dists, NN = tree.query(pts, k=k + 1, **defaults)\n\n    # Drop self-hits\n    dists, NN = dists[:, 1:], NN[:, 1:]\n\n    # Turn into edges\n    edges = []\n    ix1 = np.arange(len(dists))\n    for i in range(k):\n        ix2 = NN[:, i]\n        le = dists[:, i]\n        # If a max dist was set we have to remove NN that have dist np.inf\n        if max_dist is None:\n            edges += list(zip(ix1, ix2, le))\n        else:\n            not_inf = le != np.inf\n            edges += list(zip(ix1[not_inf], ix2[not_inf], le[not_inf]))\n\n    # Generate graph\n    G = nx.Graph()\n    G.add_nodes_from(ix1)\n    G.add_weighted_edges_from(edges)\n\n    # Extract minimum spanning tree\n    G_mst = nx.minimum_spanning_tree(G)\n\n    # Add the coordinates as node properties\n    nx.set_node_attributes(G_mst, dict(zip(G.nodes, pts[:, 0])), name='x')\n    nx.set_node_attributes(G_mst, dict(zip(G.nodes, pts[:, 1])), name='y')\n    nx.set_node_attributes(G_mst, dict(zip(G.nodes, pts[:, 2])), name='z')\n\n    return graph.nx2neuron(G_mst)\n</code></pre>"},{"location":"reference/navis/conversion/#navis.conversion.tree2meshneuron","title":"<code>navis.conversion.tree2meshneuron</code>","text":"<p>Convert TreeNeuron to MeshNeuron.</p> <p>Uses the <code>radius</code> to convert skeleton to 3D tube mesh. Missing radii are treated as zeros.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Neuron to convert.\n</code></pre> <p> TYPE: <code>            TreeNeuron | NeuronList</code> </p> <code>tube_points</code> <pre><code>        Number of points making up the circle of the cross-section\n        of the tube.\n</code></pre> <p> TYPE: <code>  int</code> DEFAULT: <code>8</code> </p> <code>radius_scale_factor</code> <pre><code>        Factor to scale radii by.\n</code></pre> <p> TYPE: <code>float</code> DEFAULT: <code>1</code> </p> <code>use_normals</code> <pre><code>        If True will rotate tube along its curvature.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>True</code> </p> <code>warn_missing_radii</code> <pre><code>        Whether to warn if radii are missing or &lt;= 0.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>TreeNeuron</code> <p>Data tables (e.g. <code>connectors</code>) are not carried over from the input neuron.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; # Get a skeleton\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; # Convert to mesh neuron\n&gt;&gt;&gt; m = navis.conversion.tree2meshneuron(n)\n</code></pre> Source code in <code>navis/conversion/converters.py</code> <pre><code>@utils.map_neuronlist(desc='Converting', allow_parallel=True)\ndef tree2meshneuron(x: 'core.TreeNeuron',\n                    tube_points: int = 8,\n                    radius_scale_factor: float = 1,\n                    use_normals: bool = True,\n                    warn_missing_radii: bool = True\n                    ) -&gt; 'core.MeshNeuron':\n    \"\"\"Convert TreeNeuron to MeshNeuron.\n\n    Uses the `radius` to convert skeleton to 3D tube mesh. Missing radii are\n    treated as zeros.\n\n    Parameters\n    ----------\n    x :             TreeNeuron | NeuronList\n                    Neuron to convert.\n    tube_points :   int\n                    Number of points making up the circle of the cross-section\n                    of the tube.\n    radius_scale_factor : float\n                    Factor to scale radii by.\n    use_normals :   bool\n                    If True will rotate tube along its curvature.\n    warn_missing_radii : bool\n                    Whether to warn if radii are missing or &lt;= 0.\n\n    Returns\n    -------\n    TreeNeuron\n                    Data tables (e.g. `connectors`) are not carried over from\n                    the input neuron.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; # Get a skeleton\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; # Convert to mesh neuron\n    &gt;&gt;&gt; m = navis.conversion.tree2meshneuron(n)\n\n    \"\"\"\n    # Delay to avoid circular imports\n    from ..plotting.plot_utils import make_tube\n\n    if not isinstance(x, core.TreeNeuron):\n        raise TypeError(f'Expected TreeNeuron, got \"{type(x)}\"')\n\n    # Map segments of node IDs to segments of node indices\n    id2ix = dict(zip(x.nodes.node_id, np.arange(len(x.nodes))))\n    segments = [np.array([id2ix[n] for n in seg]) for seg in x.segments]\n\n    # Note that we are treating missing radii as \"0\"\n    radii_map = x.nodes.radius.fillna(0).values\n    if warn_missing_radii and (radii_map &lt;= 0).any():\n        logger.warning('At least some radii are missing or &lt;= 0. Mesh may look funny.')\n\n    # Map radii onto segments\n    radii = [radii_map[seg] * radius_scale_factor for seg in segments]\n    co_map = x.nodes[['x', 'y', 'z']].values\n    seg_points = [co_map[seg] for seg in segments]\n\n    vertices, faces = make_tube(seg_points,\n                                radii=radii,\n                                tube_points=tube_points,\n                                use_normals=use_normals)\n\n    # Note: the `process=False` is necessary to not break correspondence\n    # by e.g. merging duplicate vertices\n    m = core.MeshNeuron({'vertices': vertices, 'faces': faces},\n                        units=x.units, name=x.name, id=x.id, process=False)\n\n\n    # For each vertex, track the original node: the first `tube_points` vertices\n    # correspond to the first node of the first segment and so on.\n    m.vertex_map = np.concatenate([np.repeat(seg, tube_points) for seg in segments])\n\n    return m\n</code></pre>"},{"location":"reference/navis/conversion/#navis.conversion.voxels2mesh","title":"<code>navis.conversion.voxels2mesh</code>","text":"<p>Generate mesh from voxels using marching cubes.</p> PARAMETER DESCRIPTION <code>voxels</code> <pre><code>        Object to voxelize. Can be a VoxelNeuron or an (N, 3) array\n        of x, y, z voxel coordinates.\n</code></pre> <p> TYPE: <code>       VoxelNeuron | (N, 3) np.array</code> </p> <code>spacing</code> <pre><code>        (3, ) array with x, y, z voxel size. If `auto` and input is\n        a `VoxelNeuron` we will use the neuron's `.units` property,\n        else spacing will be `(1, 1, 1)`.\n</code></pre> <p> TYPE: <code>      np.array</code> DEFAULT: <code>'auto'</code> </p> <code>step_size</code> <pre><code>        Step size for marching cube algorithm.\n        Higher values = faster but coarser.\n</code></pre> <p> TYPE: <code>    int</code> DEFAULT: <code>1</code> </p> <code>chunk_size</code> <pre><code>        Whether to process voxels in chunks to keep memory footprint\n        low:\n          - \"auto\" will set chunk size automatically based on size\n            of input\n          - use `int` to set chunk size - smaller chunk mean lower\n            memory consumption but longer run time - 200 (i.e.\n            chunks of 200x200x200 voxels) appears to be a good value\n          - set to `0` to force processing in one go\n</code></pre> <p> TYPE: <code>   \"auto\" | int</code> DEFAULT: <code>'auto'</code> </p> <code>For</code> <p> </p> <code>pad_chunks</code> <pre><code>        If True, will pad each chunk. This helps making meshes\n        watertight but may introduce internal faces when merging\n        mesh fragments.\n</code></pre> <p> TYPE: <code>   bool</code> DEFAULT: <code>True</code> </p> <code>merge_fragments</code> <pre><code>        If True, will attempt to merge fragments at the chunk\n        boundaries.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>True</code> </p> <code>progress</code> <pre><code>        Whether to show a progress bar.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>mesh</code> <p>Returns a trimesh or MeshNeuron depending on the input. Data tables (e.g. <code>connectors</code>) are not carried over from input neuron.</p> <p> TYPE: <code>trimesh.Trimesh | MeshNeuron</code> </p> Source code in <code>navis/conversion/meshing.py</code> <pre><code>def voxels2mesh(vox: Union['core.VoxelNeuron', np.ndarray],\n                spacing: Union[Literal['auto'], np.ndarray] = 'auto',\n                step_size: int = 1,\n                chunk_size: Optional[int] = 'auto',\n                pad_chunks: bool = True,\n                merge_fragments: bool = True,\n                progress: bool = True) -&gt; Union[tm.Trimesh, 'core.MeshNeuron']:\n    \"\"\"Generate mesh from voxels using marching cubes.\n\n    Parameters\n    ----------\n    voxels :        VoxelNeuron | (N, 3) np.array\n                    Object to voxelize. Can be a VoxelNeuron or an (N, 3) array\n                    of x, y, z voxel coordinates.\n    spacing :       np.array\n                    (3, ) array with x, y, z voxel size. If `auto` and input is\n                    a `VoxelNeuron` we will use the neuron's `.units` property,\n                    else spacing will be `(1, 1, 1)`.\n    step_size :     int, optional\n                    Step size for marching cube algorithm.\n                    Higher values = faster but coarser.\n    chunk_size :    \"auto\" | int, optional\n                    Whether to process voxels in chunks to keep memory footprint\n                    low:\n                      - \"auto\" will set chunk size automatically based on size\n                        of input\n                      - use `int` to set chunk size - smaller chunk mean lower\n                        memory consumption but longer run time - 200 (i.e.\n                        chunks of 200x200x200 voxels) appears to be a good value\n                      - set to `0` to force processing in one go\n\n    For `chunk_size != 0`:\n\n    pad_chunks :    bool\n                    If True, will pad each chunk. This helps making meshes\n                    watertight but may introduce internal faces when merging\n                    mesh fragments.\n    merge_fragments :  bool\n                    If True, will attempt to merge fragments at the chunk\n                    boundaries.\n    progress :      bool\n                    Whether to show a progress bar.\n\n    Returns\n    -------\n    mesh :          trimesh.Trimesh | MeshNeuron\n                    Returns a trimesh or MeshNeuron depending on the input.\n                    Data tables (e.g. `connectors`) are not carried over from\n                    input neuron.\n\n    \"\"\"\n    if not skimage:\n        raise ModuleNotFoundError(\n            'Meshing requires `skimage`:\\n '\n            'pip3 install scikit-image'\n            )\n\n    utils.eval_param(vox, 'vox', allowed_types=(core.VoxelNeuron, np.ndarray))\n\n    if spacing == 'auto':\n        if not isinstance(vox, core.VoxelNeuron):\n            spacing = [1, 1, 1]\n        else:\n            spacing = vox.units_xyz.magnitude\n\n    if isinstance(vox, core.VoxelNeuron):\n        voxels = vox.voxels\n    else:\n        voxels = vox\n\n    if voxels.ndim != 2 or voxels.shape[1] != 3:\n        raise ValueError(f'Voxels must be shape (N, 3), got {voxels.shape}')\n\n    if chunk_size == 'auto':\n        if len(voxels) &gt; 1e6:\n            chunk_size = 200\n        else:\n            chunk_size = 0\n\n    if not chunk_size:\n        mesh = _mesh_from_voxels_single(voxels=voxels,\n                                        spacing=spacing,\n                                        step_size=step_size)\n    else:\n        mesh = _mesh_from_voxels_chunked(voxels=voxels,\n                                         spacing=spacing,\n                                         step_size=step_size,\n                                         chunk_size=chunk_size,\n                                         pad_chunks=pad_chunks,\n                                         merge_fragments=merge_fragments,\n                                         progress=progress)\n\n    if isinstance(vox, core.VoxelNeuron):\n        mesh.vertices += vox.offset\n        mesh = core.MeshNeuron(mesh, units=f'1 {vox.units.units}', id=vox.id)\n\n    return mesh\n</code></pre>"},{"location":"reference/navis/core/","title":"core","text":""},{"location":"reference/navis/core/#navis.core.NeuronProcessor","title":"<code>navis.core.NeuronProcessor</code>","text":"<p>Apply function across all neurons of a neuronlist.</p> <p>This assumes that the first argument for the function accepts a single neuron.</p> Source code in <code>navis/core/core_utils.py</code> <pre><code>class NeuronProcessor:\n    \"\"\"Apply function across all neurons of a neuronlist.\n\n    This assumes that the first argument for the function accepts a single\n    neuron.\n    \"\"\"\n\n    def __init__(self,\n                 nl: 'core.NeuronList',\n                 function: Callable,\n                 parallel: bool = False,\n                 n_cores: int = os.cpu_count() // 2,\n                 chunksize: int = 1,\n                 progress: bool = True,\n                 warn_inplace: bool = True,\n                 omit_failures: bool = False,\n                 exclude_zip: list = [],\n                 desc: Optional[str] = None):\n        if utils.is_iterable(function):\n            if len(function) != len(nl):\n                raise ValueError('Number of functions must match neurons.')\n            self.funcs = function\n            self.function = function[0]\n        elif callable(function):\n            self.funcs = [function] * len(nl)\n            self.function = function\n        else:\n            raise TypeError('Expected `function` to be callable or list '\n                            f'thereof,  got \"{type(function)}\"')\n\n        self.nl = nl\n        self.desc = desc\n        self.parallel = parallel\n        self.n_cores = n_cores\n        self.chunksize = chunksize\n        self.progress = progress\n        self.warn_inplace = warn_inplace\n        self.exclude_zip = exclude_zip\n        self.omit_failures = omit_failures\n\n        # This makes sure that help and name match the functions being called\n        functools.update_wrapper(self, self.function)\n\n    def __call__(self, *args, **kwargs):\n        # Explicitly providing these parameters overwrites defaults\n        parallel = kwargs.pop('parallel', self.parallel)\n        n_cores = kwargs.pop('n_cores', self.n_cores)\n\n        # We will check, for each argument, if it matches the number of\n        # functions to run. If they it does, we will zip the values\n        # with the neurons\n        parsed_args = []\n        parsed_kwargs = []\n\n        for i, n in enumerate(self.nl):\n            parsed_args.append([])\n            parsed_kwargs.append({})\n            for k, a in enumerate(args):\n                if k in self.exclude_zip:\n                    parsed_args[i].append(a)\n                elif not utils.is_iterable(a) or len(a) != len(self.nl):\n                    parsed_args[i].append(a)\n                else:\n                    parsed_args[i].append(a[i])\n\n            for k, v in kwargs.items():\n                if k in self.exclude_zip:\n                    parsed_kwargs[i][k] = v\n                elif not utils.is_iterable(v) or len(v) != len(self.nl):\n                    parsed_kwargs[i][k] = v\n                else:\n                    parsed_kwargs[i][k] = v[i]\n\n        # Silence loggers (except Errors)\n        level = logger.getEffectiveLevel()\n\n        if level &lt; 30:\n            logger.setLevel('WARNING')\n\n        # Apply function\n        if parallel:\n            if not ProcessingPool:\n                raise ModuleNotFoundError(\n                    'navis relies on pathos for multiprocessing!'\n                    'Please install pathos and try again:\\n'\n                    '  pip3 install pathos -U'\n                    )\n\n            if self.warn_inplace and kwargs.get('inplace', False):\n                logger.warning('`inplace=True` does not work with '\n                               'multiprocessing ')\n\n            with ProcessingPool(n_cores) as pool:\n                combinations = list(zip(self.funcs,\n                                        parsed_args,\n                                        parsed_kwargs))\n                chunksize = kwargs.pop('chunksize', self.chunksize)  # max(int(len(combinations) / 100), 1)\n\n                if not self.omit_failures:\n                    wrapper = _call\n                else:\n                    wrapper = _try_call\n\n                res = list(config.tqdm(pool.imap(wrapper,\n                                                 combinations,\n                                                 chunksize=chunksize),\n                                       total=len(combinations),\n                                       desc=self.desc,\n                                       disable=config.pbar_hide or not self.progress,\n                                       leave=config.pbar_leave))\n        else:\n            res = []\n            for i, n in enumerate(config.tqdm(self.nl, desc=self.desc,\n                                              disable=(config.pbar_hide\n                                                       or not self.progress\n                                                       or len(self.nl) &lt;= 1),\n                                              leave=config.pbar_leave)):\n                try:\n                    res.append(self.funcs[i](*parsed_args[i], **parsed_kwargs[i]))\n                except BaseException as e:\n                    if self.omit_failures:\n                        res.append(FailedRun(func=self.funcs[i],\n                                             args=parsed_args[i],\n                                             kwargs=parsed_kwargs[i],\n                                             exception=e))\n                    else:\n                        raise\n\n        # Reset logger level to previous state\n        logger.setLevel(level)\n\n        failed = np.array([isinstance(r, FailedRun) for r in res])\n        res = [r for r in res if not isinstance(r, FailedRun)]\n        if any(failed):\n            logger.warning(f'{sum(failed)} of {len(self.funcs)} runs failed. '\n                        'Set logging to debug (`navis.set_loggers(\"DEBUG\")`) '\n                        'or repeat with `omit_failures=False` for details.')\n            failed_ids = self.nl.id[np.where(failed)].astype(str)\n            logger.debug(f'The following IDs failed to complete: {\", \".join(failed_ids)}')\n\n        # If result is a list of neurons, combine them back into a single list\n        is_neuron = [isinstance(r, (core.NeuronList, core.BaseNeuron)) for r in res]\n        if all(is_neuron):\n            return self.nl.__class__(utils.unpack_neurons(res))\n        # If results are all None return nothing instead of a list of [None, ..]\n        if np.all([r is None for r in res]):\n            res = None\n        # If not all neurons simply return results and let user deal with it\n        return res\n</code></pre>"},{"location":"reference/navis/core/#navis.core.to_neuron_space","title":"<code>navis.core.to_neuron_space</code>","text":"<p>Convert units to match neuron space.</p> <p>Note that trying to convert units for non-isometric neurons will fail.</p> PARAMETER DESCRIPTION <code>units</code> <pre><code>    The units to convert to neuron units. Simple numbers are just\n    passed through.\n</code></pre> <p> TYPE: <code>    number | str | pint.Quantity | pint.Units</code> </p> <code>neuron</code> <pre><code>    A single neuron.\n</code></pre> <p> TYPE: <code>   Neuron</code> </p> <code>on_error</code> <pre><code>    What to do if an error occurs (e.g. because `neuron` does not\n    have units specified). If \"ignore\" will simply return `units`\n    unchanged.\n</code></pre> <p> TYPE: <code> \"raise\" | \"ignore\"</code> DEFAULT: <code>'raise'</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The units in neuron space. Note that this number may be rounded to avoid ugly floating point precision issues such as 0.124999999999999 instead of 0.125.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; # Example neurons are in 8x8x8nm voxel space\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; navis.core.to_neuron_space('1 nm', n)\n0.125\n&gt;&gt;&gt; # Alternatively use the neuron method\n&gt;&gt;&gt; n.map_units('1 nm')\n0.125\n&gt;&gt;&gt; # Numbers are passed-through\n&gt;&gt;&gt; n.map_units(1)\n1\n&gt;&gt;&gt; # For neuronlists\n&gt;&gt;&gt; nl = navis.example_neurons(3)\n&gt;&gt;&gt; nl.map_units(1)\n[1, 1, 1]\n&gt;&gt;&gt; nl.map_units('1 nanometer')\n[0.125, 0.125, 0.125]\n</code></pre> Source code in <code>navis/core/core_utils.py</code> <pre><code>def to_neuron_space(units: Union[int, float, pint.Quantity, pint.Unit],\n                    neuron: core.BaseNeuron,\n                    on_error: Union[Literal['ignore'],\n                                    Literal['raise']] = 'raise'):\n    \"\"\"Convert units to match neuron space.\n\n    Note that trying to convert units for non-isometric neurons will fail.\n\n    Parameters\n    ----------\n    units :     number | str | pint.Quantity | pint.Units\n                The units to convert to neuron units. Simple numbers are just\n                passed through.\n    neuron :    Neuron\n                A single neuron.\n    on_error :  \"raise\" | \"ignore\"\n                What to do if an error occurs (e.g. because `neuron` does not\n                have units specified). If \"ignore\" will simply return `units`\n                unchanged.\n\n    Returns\n    -------\n    float\n                The units in neuron space. Note that this number may be rounded\n                to avoid ugly floating point precision issues such as\n                0.124999999999999 instead of 0.125.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; # Example neurons are in 8x8x8nm voxel space\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; navis.core.to_neuron_space('1 nm', n)\n    0.125\n    &gt;&gt;&gt; # Alternatively use the neuron method\n    &gt;&gt;&gt; n.map_units('1 nm')\n    0.125\n    &gt;&gt;&gt; # Numbers are passed-through\n    &gt;&gt;&gt; n.map_units(1)\n    1\n    &gt;&gt;&gt; # For neuronlists\n    &gt;&gt;&gt; nl = navis.example_neurons(3)\n    &gt;&gt;&gt; nl.map_units(1)\n    [1, 1, 1]\n    &gt;&gt;&gt; nl.map_units('1 nanometer')\n    [0.125, 0.125, 0.125]\n\n    \"\"\"\n    utils.eval_param(on_error, name='on_error',\n                     allowed_values=('ignore', 'raise'))\n    utils.eval_param(neuron, name='neuron', allowed_types=(core.BaseNeuron, ))\n\n    # If string, convert to units\n    if isinstance(units, str):\n        units = pint.Quantity(units)\n    # If not a pint object (i.e. just a number)\n    elif not isinstance(units, (pint.Quantity, pint.Unit)):\n        return units\n\n    if neuron.units.dimensionless:\n        if on_error == 'raise':\n            raise ValueError(f'Unable to convert \"{str(units)}\": Neuron units '\n                             'unknown or dimensionless.')\n        else:\n            return units\n\n    if not neuron.is_isometric:\n        if on_error == 'raise':\n            raise ValueError(f'Unable to convert \"{str(units)}\": neuron is not '\n                             'isometric ({neuron.units}).')\n        else:\n            return units\n\n    # If input was e.g. `units=\"1\"`\n    if units.dimensionless:\n        return units.magnitude\n\n    # First convert to same unit as neuron units\n    units = units.to(neuron.units)\n\n    # Now convert magnitude\n    mag = units.magnitude / neuron.units.magnitude\n\n    # Rounding may not be exactly kosher but it avoids floating point issues\n    # like 124.9999999999999 instead of 125\n    # I hope that in practice it won't screw things up:\n    # even if asking for\n    return utils.round_smart(mag)\n</code></pre>"},{"location":"reference/navis/graph/","title":"graph","text":""},{"location":"reference/navis/graph/#navis.graph.classify_nodes","title":"<code>navis.graph.classify_nodes</code>","text":"<p>Classify neuron's nodes into end nodes, branches, slabs or root.</p> <p>Adds a <code>'type'</code> column to <code>x.nodes</code> table.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Neuron(s) whose nodes to classify.\n</code></pre> <p> TYPE: <code>        TreeNeuron | NeuronList</code> </p> <code>categorical</code> <pre><code>    If True (default), will use categorical data type which takes\n    up much less memory at a small run-time overhead.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>inplace</code> <pre><code>    If `False`, nodes will be classified on a copy which is then\n    returned leaving the original neuron unchanged.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>TreeNeuron / List</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(2)\n&gt;&gt;&gt; _ = navis.graph.classify_nodes(nl, inplace=True)\n</code></pre> Source code in <code>navis/graph/graph_utils.py</code> <pre><code>@utils.map_neuronlist(desc=\"Classifying\", allow_parallel=True)\n@utils.lock_neuron\ndef classify_nodes(x: \"core.NeuronObject\", categorical=True, inplace: bool = True):\n    \"\"\"Classify neuron's nodes into end nodes, branches, slabs or root.\n\n    Adds a `'type'` column to `x.nodes` table.\n\n    Parameters\n    ----------\n    x :         TreeNeuron | NeuronList\n                Neuron(s) whose nodes to classify.\n    categorical : bool\n                If True (default), will use categorical data type which takes\n                up much less memory at a small run-time overhead.\n    inplace :   bool, optional\n                If `False`, nodes will be classified on a copy which is then\n                returned leaving the original neuron unchanged.\n\n    Returns\n    -------\n    TreeNeuron/List\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(2)\n    &gt;&gt;&gt; _ = navis.graph.classify_nodes(nl, inplace=True)\n\n    \"\"\"\n    if not inplace:\n        x = x.copy()\n\n    if not isinstance(x, core.TreeNeuron):\n        raise TypeError(f'Expected TreeNeuron(s), got \"{type(x)}\"')\n\n    if x.nodes.empty:\n        x.nodes[\"type\"] = None\n        return x\n\n    # Make sure there are nodes to classify\n    # Note: I have tried to optimized the s**t out of this, i.e. every\n    # single line of code here has been tested for speed. Do not\n    # change anything unless you know what you're doing!\n\n    # Turns out that numpy.isin() recently started to complain if the\n    # node_ids are uint64 and the parent_ids are int64 (but strangely\n    # not with 32bit integers). If that's the case we have to convert\n    # the node_ids to int64.\n    node_ids = x.nodes.node_id.values\n    parent_ids = x.nodes.parent_id.values\n\n    if node_ids.dtype == np.uint64:\n        node_ids = node_ids.astype(np.int64)\n\n    cl = np.full(len(x.nodes), \"slab\", dtype=\"&lt;U6\")\n    cl[~np.isin(node_ids, parent_ids)] = \"end\"\n    bp = x.nodes.parent_id.value_counts()\n    bp = bp.index.values[bp.values &gt; 1]\n    cl[np.isin(node_ids, bp)] = \"branch\"\n    cl[parent_ids &lt; 0] = \"root\"\n    if categorical:\n        cl = pd.Categorical(\n            cl, categories=[\"end\", \"branch\", \"root\", \"slab\"], ordered=False\n        )\n    x.nodes[\"type\"] = cl\n\n    return x\n</code></pre>"},{"location":"reference/navis/graph/#navis.graph.connected_subgraph","title":"<code>navis.graph.connected_subgraph</code>","text":"<p>Return set of nodes necessary to connect all nodes in subset <code>ss</code>.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Neuron (or graph thereof) to get subgraph for.\n</code></pre> <p> TYPE: <code>        navis.TreeNeuron | nx.DiGraph</code> </p> <code>ss</code> <pre><code>    Node IDs of node to subset to.\n</code></pre> <p> TYPE: <code>       list | array-like</code> </p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>Node IDs of connected subgraph.</p> <code>root ID</code> <p>ID of the node most proximal to the old root in the connected subgraph.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; ends = n.nodes[n.nodes.type.isin(['end', 'root'])].node_id.values\n&gt;&gt;&gt; sg, root = navis.graph.graph_utils.connected_subgraph(n, ends)\n&gt;&gt;&gt; # Since we asked for a subgraph connecting all terminals + root,\n&gt;&gt;&gt; # we expect to see all nodes in the subgraph\n&gt;&gt;&gt; sg.shape[0] == n.nodes.shape[0]\nTrue\n</code></pre> Source code in <code>navis/graph/graph_utils.py</code> <pre><code>def connected_subgraph(\n    x: Union[\"core.TreeNeuron\", nx.DiGraph], ss: Sequence[Union[str, int]]\n) -&gt; Tuple[np.ndarray, Union[int, str]]:\n    \"\"\"Return set of nodes necessary to connect all nodes in subset `ss`.\n\n    Parameters\n    ----------\n    x :         navis.TreeNeuron | nx.DiGraph\n                Neuron (or graph thereof) to get subgraph for.\n    ss :        list | array-like\n                Node IDs of node to subset to.\n\n    Returns\n    -------\n    np.ndarray\n                Node IDs of connected subgraph.\n    root ID\n                ID of the node most proximal to the old root in the\n                connected subgraph.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; ends = n.nodes[n.nodes.type.isin(['end', 'root'])].node_id.values\n    &gt;&gt;&gt; sg, root = navis.graph.graph_utils.connected_subgraph(n, ends)\n    &gt;&gt;&gt; # Since we asked for a subgraph connecting all terminals + root,\n    &gt;&gt;&gt; # we expect to see all nodes in the subgraph\n    &gt;&gt;&gt; sg.shape[0] == n.nodes.shape[0]\n    True\n\n    \"\"\"\n    if isinstance(x, core.NeuronList):\n        if len(x) == 1:\n            g = x[0].graph\n    elif isinstance(x, core.TreeNeuron):\n        g = x.graph\n    elif isinstance(x, nx.DiGraph):\n        g = x\n    else:\n        raise TypeError(f'Input must be a single TreeNeuron or graph, got \"{type(x)}\".')\n\n    ss = set(ss)\n    missing = ss - set(g.nodes)\n    if np.any(missing):\n        missing = np.array(list(missing)).astype(str)  # do NOT remove list() here!\n        raise ValueError(f'Nodes not found: {\",\".join(missing)}')\n\n    # Find nodes that are leafs WITHIN the subset\n    g_ss = g.subgraph(ss)\n    in_degree = dict(g_ss.in_degree)\n    leafs = ss &amp; {n for n, d in in_degree.items() if not d}\n\n    # Run this for each connected component of the neuron\n    include = set()\n    new_roots = []\n    for cc in nx.connected_components(g.to_undirected()):\n        # Walk from each node to root and keep track of path\n        paths = []\n        for n in leafs &amp; cc:\n            this_path = []\n            while n is not None:\n                this_path.append(n)\n                n = next(g.successors(n), None)\n            paths.append(this_path)\n\n        # If none of these cc in subset there won't be paths\n        if not paths:\n            continue\n\n        # Find the nodes that all paths have in common\n        common = set.intersection(*[set(p) for p in paths])\n\n        # Now find the first (most distal from root) common node\n        longest_path = sorted(paths, key=lambda x: len(x))[-1]\n        first_common = sorted(common, key=lambda x: longest_path.index(x))[0]\n\n        # Now go back to paths and collect all nodes until this first common node\n        for p in paths:\n            it = iter(p)\n            n = next(it, None)\n            while n is not None:\n                if n in include:\n                    break\n                if n == first_common:\n                    include.add(n)\n                    break\n                include.add(n)\n                n = next(it, None)\n\n        # In cases where there are even more distal common ancestors\n        # (first common will typically be a branch point)\n        this_ss = ss &amp; cc\n        if this_ss - include:\n            # Make sure the new root is set correctly\n            nr = sorted(this_ss - include, key=lambda x: longest_path.index(x))[-1]\n            new_roots.append(nr)\n            # Add those nodes to be included\n            include = set.union(include, this_ss)\n        else:\n            new_roots.append(first_common)\n\n    return np.array(list(include)), new_roots\n</code></pre>"},{"location":"reference/navis/graph/#navis.graph.generate_list_of_childs","title":"<code>navis.graph.generate_list_of_childs</code>","text":"<p>Return list of childs.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>If List, must contain a SINGLE neuron.\n</code></pre> <p> TYPE: <code>    TreeNeuron | NeuronList</code> </p> RETURNS DESCRIPTION <code>dict</code> <p><code>{parent_id: [child_id, child_id, ...]}</code></p> Source code in <code>navis/graph/graph_utils.py</code> <pre><code>def generate_list_of_childs(x: \"core.NeuronObject\") -&gt; Dict[int, List[int]]:\n    \"\"\"Return list of childs.\n\n    Parameters\n    ----------\n    x :     TreeNeuron | NeuronList\n            If List, must contain a SINGLE neuron.\n\n    Returns\n    -------\n    dict\n        `{parent_id: [child_id, child_id, ...]}`\n\n    \"\"\"\n    assert isinstance(x, core.TreeNeuron)\n    # Grab graph once to avoid overhead from stale checks\n    g = x.graph\n    return {n: [e[0] for e in g.in_edges(n)] for n in g.nodes}\n</code></pre>"},{"location":"reference/navis/graph/#navis.graph.node_label_sorting","title":"<code>navis.graph.node_label_sorting</code>","text":"<p>Return nodes ordered by node label sorting according to Cuntz et al., PLoS Computational Biology (2010).</p> PARAMETER DESCRIPTION <code>x</code> <p> TYPE: <code>        TreeNeuron</code> </p> <code>weighted</code> <pre><code>    If True will use actual distances instead of just node count.\n    Depending on how evenly spaced your points are, this might not\n    make much difference.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>list</code> <p><code>[root, node_id, node_id, ...]</code></p> Source code in <code>navis/graph/graph_utils.py</code> <pre><code>def node_label_sorting(\n    x: \"core.TreeNeuron\", weighted: bool = False\n) -&gt; List[Union[str, int]]:\n    \"\"\"Return nodes ordered by node label sorting according to Cuntz\n    et al., PLoS Computational Biology (2010).\n\n    Parameters\n    ----------\n    x :         TreeNeuron\n    weighted :  bool\n                If True will use actual distances instead of just node count.\n                Depending on how evenly spaced your points are, this might not\n                make much difference.\n\n    Returns\n    -------\n    list\n        `[root, node_id, node_id, ...]`\n\n    \"\"\"\n    if isinstance(x, core.NeuronList) and len(x) == 1:\n        x = x[0]\n\n    if not isinstance(x, core.TreeNeuron):\n        raise TypeError(f'Expected a singleTreeNeuron, got \"{type(x)}\"')\n\n    if len(x.root) &gt; 1:\n        raise ValueError(\"Unable to process multi-root neurons!\")\n\n    # Get relevant terminal nodes\n    term = x.nodes[x.nodes.type == \"end\"].node_id.values\n\n    # Get directed (!) distance from terminals to all other nodes\n    geo = geodesic_matrix(\n        x, from_=x.nodes[x.nodes.type.isin((\"end\", \"root\", \"branch\"))].node_id.values, directed=True, weight=\"weight\" if weighted else None\n    )\n    # Set distance between unreachable points to None\n    # Need to reinitialise SparseMatrix to replace float('inf') with NaN\n    # dist_mat[geo == float('inf')] = None\n    dist_mat = pd.DataFrame(\n        np.where(\n            geo == float(\"inf\"),  # type: ignore  # no stubs for SparseDataFrame\n            np.nan,\n            geo,\n        ),\n        columns=geo.columns,\n        index=geo.index,\n    )\n\n    # Get starting points (i.e. branches off the root) and sort by longest\n    # path to a terminal (note we're operating on the simplified version\n    # of the skeleton)\n    G = graph.simplify_graph(x.graph)\n    curr_points = sorted(\n        list(G.predecessors(x.root[0])),\n        key=lambda n: dist_mat[n].max() + dist_mat.loc[n, x.root[0]],\n        reverse=True,\n    )\n\n    # Walk from root towards terminals, prioritising longer branches\n    nodes_walked = []\n    while curr_points:\n        nodes_walked.append(curr_points.pop(0))\n        # If the current point is a terminal point, stop here\n        if nodes_walked[-1] in term:\n            pass\n        else:\n            new_points = sorted(\n                list(G.predecessors(nodes_walked[-1])),\n                # Use distance to the farthest terminal + distance to current node as sorting key\n                key=lambda n: dist_mat[n].max() + dist_mat.loc[n, nodes_walked[-1]],\n                reverse=True,\n            )\n            curr_points = new_points + curr_points\n\n    # Translate into segments\n    node_list = [x.root[0:]]\n    # Note that we're inverting here so that the segments are ordered\n    # proximal -&gt; distal (i.e. root to tips)\n    seg_dict = {s[0]: s[::-1] for s in _break_segments(x)}\n\n    for n in nodes_walked:\n        # Note that we're skipping the first (proximal) node to avoid double\n        # counting nodes\n        node_list.append(seg_dict[n][1:])\n\n    return np.concatenate(node_list, dtype=int)\n</code></pre>"},{"location":"reference/navis/graph/#navis.graph.simplify_graph","title":"<code>navis.graph.simplify_graph</code>","text":"<p>Simplify skeleton graph (networkX or igraph).</p> <p>This function will simplify the graph by keeping only roots, leafs and branch points. Preserves branch lengths (i.e. weights)!</p> PARAMETER DESCRIPTION <code>G</code> <pre><code>    The skeleton graph to simplify.\n</code></pre> <p> TYPE: <code>        networkx.DiGraph | igraph.Graph</code> </p> <code>inplace</code> <pre><code>    If True, will modify the graph in place.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>G</code> <p>Simplified graph.</p> <p> TYPE: <code>networkx.DiGraph | networkx.DiGraph</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1, kind='skeleton')\n&gt;&gt;&gt; # Simplify skeleton's NetworkX graph representation\n&gt;&gt;&gt; G_simp_nx = navis.graph.simplify_graph(n.graph)\n&gt;&gt;&gt; # Check that we have the expected number of nodes\n&gt;&gt;&gt; assert len(G_simp_nx.nodes) == (n.n_branches + n.n_root + n.n_leafs)\n&gt;&gt;&gt; # Simplify skeleton's iGraph graph representation\n&gt;&gt;&gt; G_simp_ig = navis.graph.simplify_graph(n.igraph)\n&gt;&gt;&gt; # Check that we have the expected number of nodes\n&gt;&gt;&gt; assert len(G_simp_ig.vs) == (n.n_branches + n.n_root + n.n_leafs)\n</code></pre> Source code in <code>navis/graph/converters.py</code> <pre><code>def simplify_graph(G, inplace=False):\n    \"\"\"Simplify skeleton graph (networkX or igraph).\n\n    This function will simplify the graph by keeping only roots, leafs and\n    branch points. Preserves branch lengths (i.e. weights)!\n\n    Parameters\n    ----------\n    G :         networkx.DiGraph | igraph.Graph\n                The skeleton graph to simplify.\n    inplace :   bool\n                If True, will modify the graph in place.\n\n    Returns\n    -------\n    G :         networkx.DiGraph | networkx.DiGraph\n                Simplified graph.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1, kind='skeleton')\n    &gt;&gt;&gt; # Simplify skeleton's NetworkX graph representation\n    &gt;&gt;&gt; G_simp_nx = navis.graph.simplify_graph(n.graph)\n    &gt;&gt;&gt; # Check that we have the expected number of nodes\n    &gt;&gt;&gt; assert len(G_simp_nx.nodes) == (n.n_branches + n.n_root + n.n_leafs)\n    &gt;&gt;&gt; # Simplify skeleton's iGraph graph representation\n    &gt;&gt;&gt; G_simp_ig = navis.graph.simplify_graph(n.igraph)\n    &gt;&gt;&gt; # Check that we have the expected number of nodes\n    &gt;&gt;&gt; assert len(G_simp_ig.vs) == (n.n_branches + n.n_root + n.n_leafs)\n\n    \"\"\"\n    if not inplace:\n        G = G.copy()\n\n    if isinstance(G, nx.Graph):\n        # Find all leaf and branch points\n        leafs = {n for n in G.nodes if G.in_degree(n) == 0 and G.out_degree(n) != 0}\n        branches = {n for n in G.nodes if G.in_degree(n) &gt; 1 and G.out_degree(n) != 0}\n        roots = {n for n in G.nodes if G.out_degree(n) == 0}\n\n        stop_nodes = roots | leafs | branches\n\n        # Walk from each leaf/branch point to the next leaf, branch or root\n        to_remove = []\n        for start_node in leafs | branches:\n            dist = 0\n            node = start_node\n            while True:\n                parent = next(G.successors(node))\n                dist += G.edges[node, parent][\"weight\"]\n\n                if parent in stop_nodes:\n                    G.add_weighted_edges_from([(start_node, parent, dist)])\n                    break\n\n                to_remove.append(parent)\n                node = parent\n\n        G.remove_nodes_from(to_remove)\n    else:\n        # Find all leaf and branch points\n        leafs = G.vs.select(_indegree=0, _outdegree_ne=0)\n        branches = G.vs.select(_indegree_gt=1, _outdegree_ne=0)\n        roots = G.vs.select(_outdegree=0)\n\n        stop_nodes = np.concatenate((roots.indices, leafs.indices, branches.indices))\n\n        # Walk from each leaf/branch point to the next leaf, branch or root\n        to_remove = []\n        for start_node in np.concatenate((leafs.indices, branches.indices)):\n            dist = 0\n            node = start_node\n            while True:\n                parent = G.successors(node)[0]\n                dist += G.es[G.get_eid(node, parent)][\"weight\"]\n\n                if parent in stop_nodes:\n                    G.add_edge(start_node, parent, weight=dist)\n                    break\n\n                to_remove.append(parent)\n                node = parent\n\n        G.delete_vertices(to_remove)\n\n    return G\n</code></pre>"},{"location":"reference/navis/graph/#navis.graph.skeleton_adjacency_matrix","title":"<code>navis.graph.skeleton_adjacency_matrix</code>","text":"<p>Generate adjacency matrix for a skeleton.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Neuron for which to generate adjacency matrix.\n</code></pre> <p> TYPE: <code>        TreeNeuron</code> </p> <code>sort</code> <pre><code>    If True, will sort the adjacency matrix by topology.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>pd.DataFrame</code> <p>Adjacency matrix where rows are nodes and columns are their parents.</p> See Also <p><code>navis.geodesic_matrix</code>     For distances between all points. <code>navis.distal_to</code>     Check if a node A is distal to node B. <code>navis.dist_between</code>     Get point-to-point geodesic (\"along-the-arbor\") distances.</p> Source code in <code>navis/graph/graph_utils.py</code> <pre><code>def skeleton_adjacency_matrix(\n    x: \"core.NeuronObject\", sort: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"Generate adjacency matrix for a skeleton.\n\n    Parameters\n    ----------\n    x :         TreeNeuron\n                Neuron for which to generate adjacency matrix.\n    sort :      bool, optional\n                If True, will sort the adjacency matrix by topology.\n\n    Returns\n    -------\n    pd.DataFrame\n                Adjacency matrix where rows are nodes and columns are\n                their parents.\n\n    See Also\n    --------\n    [`navis.geodesic_matrix`][]\n        For distances between all points.\n    [`navis.distal_to`][]\n        Check if a node A is distal to node B.\n    [`navis.dist_between`][]\n        Get point-to-point geodesic (\"along-the-arbor\") distances.\n\n    \"\"\"\n    if isinstance(x, core.NeuronList):\n        if len(x) == 1:\n            x = x[0]\n        else:\n            raise ValueError(\"Cannot process more than a single neuron.\")\n    elif not isinstance(x, (core.TreeNeuron,)):\n        raise ValueError(f'Unable to process data of type \"{type(x)}\"')\n\n    # Generate the empty adjacency matrix\n    adj = pd.DataFrame(\n        np.zeros((len(x.nodes), len(x.nodes)), dtype=bool),\n        index=x.nodes.node_id.values,\n        columns=x.nodes.node_id.values,\n    )\n\n    # Fill in the parent-child relationships\n    not_root = x.nodes.parent_id.values &gt;= 0\n    node_ix = np.arange(len(x.nodes))[not_root]\n    parent_ids = x.nodes.parent_id.values[not_root]\n    parent_ix = np.searchsorted(x.nodes.node_id.values, parent_ids)\n    adj.values[node_ix, parent_ix] = True\n\n    if sort:\n        sort = node_label_sorting(x)\n        adj = adj.loc[sort, sort]\n\n    return adj\n</code></pre>"},{"location":"reference/navis/meshes/","title":"meshes","text":""},{"location":"reference/navis/meshes/#navis.meshes.available_backends","title":"<code>navis.meshes.available_backends</code>","text":"<p>Search for available backends.</p> Source code in <code>navis/meshes/operations.py</code> <pre><code>def available_backends(only_first=False):\n    \"\"\"Search for available backends.\"\"\"\n    backends = []\n\n    try:\n        if find_spec('pyfqmr') is not None:\n            backends.append('pyfqmr')\n    except ModuleNotFoundError:\n        pass\n    except BaseException:\n        raise\n\n    if only_first and len(backends):\n        return backends\n\n    try:\n        if find_spec('open3d') is not None:\n            backends.append('open3d')\n    except ModuleNotFoundError:\n        pass\n    except BaseException:\n        raise\n\n    if only_first and len(backends):\n        return backends\n\n    try:\n        if find_spec('pymeshlab') is not None:\n            backends.append('pymeshlab')\n    except ModuleNotFoundError:\n        pass\n    except BaseException:\n        raise\n\n    if tm.interfaces.blender.exists:\n        backends.append('blender')\n\n    return backends\n</code></pre>"},{"location":"reference/navis/meshes/#navis.meshes.simplify_mesh_blender","title":"<code>navis.meshes.simplify_mesh_blender</code>","text":"<p>Simplify mesh using Blender.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Mesh object to simplify.\n</code></pre> <p> TYPE: <code>        MeshNeuron | Volume | Trimesh</code> </p> <code>F</code> <pre><code>    Ratio to which to reduce the mesh. For example, `F=0.5`\n    should reduce number of vertices to half that of the original.\n</code></pre> <p> TYPE: <code>        float [0-1]</code> </p> <code>inplace</code> <pre><code>    If True, will perform simplication on `x`. If False, will\n    simplify and return a copy.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>simp</code> <p>Simplified mesh object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1, kind=\"mesh\")\n&gt;&gt;&gt; n_sm = simplify_mesh_blender(n,\n...         F=0.2,\n...         inplace=False)\n&gt;&gt;&gt; n.n_vertices &gt; n_sm.n_vertices\nTrue\n</code></pre> Source code in <code>navis/meshes/b3d.py</code> <pre><code>def simplify_mesh_blender(x, F, inplace=False):\n    \"\"\"Simplify mesh using Blender.\n\n    Parameters\n    ----------\n    x :         MeshNeuron | Volume | Trimesh\n                Mesh object to simplify.\n    F :         float [0-1]\n                Ratio to which to reduce the mesh. For example, `F=0.5`\n                should reduce number of vertices to half that of the original.\n    inplace :   bool\n                If True, will perform simplication on `x`. If False, will\n                simplify and return a copy.\n\n    Returns\n    -------\n    simp\n                Simplified mesh object.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1, kind=\"mesh\")\n    &gt;&gt;&gt; n_sm = simplify_mesh_blender(n,\n    ...         F=0.2,\n    ...         inplace=False)\n    &gt;&gt;&gt; n.n_vertices &gt; n_sm.n_vertices\n    True\n\n    \"\"\"\n    if not tm.interfaces.blender.exists:\n        raise ModuleNotFoundError('No Blender 3D unavailable (executable not found).')\n    _blender_executable = tm.interfaces.blender._blender_executable\n\n    if F &gt; 1 or F &lt; 0:\n        raise ValueError(f'`F` must be between 0 and 1, got \"{F}\"')\n\n    if isinstance(x, core.MeshNeuron):\n        mesh = x.trimesh\n    elif isinstance(x, core.Volume):\n        mesh = tm.Trimesh(x.vertices, x.faces)\n    elif isinstance(x, tm.Trimesh):\n        mesh = x\n    else:\n        raise TypeError('Expected MeshNeuron, Volume or trimesh.Trimesh, '\n                        f'got \"{type(x)}\"')\n\n    assert isinstance(mesh, tm.Trimesh)\n\n    # Load the template\n    temp_name = 'blender_decimate.py.template'\n    if temp_name in _cache:\n        template = _cache[temp_name]\n    else:\n        with open(os.path.join(_pwd, 'templates', temp_name), 'r') as f:\n            template = f.read()\n        _cache[temp_name] = template\n\n    # Replace placeholder with actual ratio\n    script = template.replace('$RATIO', str(F))\n\n    # Let trimesh's MeshScript take care of exectution and clean-up\n    with tm.interfaces.generic.MeshScript(meshes=[mesh],\n                                          script=script,\n                                          debug=False) as blend:\n        result = blend.run(_blender_executable\n                           + ' --background --python $SCRIPT')\n\n    # Blender apparently returns actively incorrect face normals\n    result.face_normals = None\n\n    if not inplace:\n        x = x.copy()\n\n    x.vertices = result.vertices\n    x.faces = result.faces\n\n    return x\n</code></pre>"},{"location":"reference/navis/meshes/#navis.meshes.simplify_mesh_fqmr","title":"<code>navis.meshes.simplify_mesh_fqmr</code>","text":"<p>Simplify mesh using pyfqmr.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Mesh object to simplify.\n</code></pre> <p> TYPE: <code>        MeshNeuron | Volume | Trimesh</code> </p> <code>F</code> <pre><code>    Target face count (integer).\n</code></pre> <p> TYPE: <code>        int</code> </p> <code>inplace</code> <pre><code>    If True, will perform simplication on `x`. If False, will\n    simplify and return a copy.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <pre><code>    Keyword arguments are passed through to pyfqmr's\n    `pyfqmr.Simplify.simplify_mesh`.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>simp</code> <p>Simplified mesh object.</p> Source code in <code>navis/meshes/fqmr.py</code> <pre><code>def simplify_mesh_fqmr(x, F, inplace=False, **kwargs):\n    \"\"\"Simplify mesh using pyfqmr.\n\n    Parameters\n    ----------\n    x :         MeshNeuron | Volume | Trimesh\n                Mesh object to simplify.\n    F :         int\n                Target face count (integer).\n    inplace :   bool\n                If True, will perform simplication on `x`. If False, will\n                simplify and return a copy.\n    **kwargs\n                Keyword arguments are passed through to pyfqmr's\n                `pyfqmr.Simplify.simplify_mesh`.\n\n    Returns\n    -------\n    simp\n                Simplified mesh object.\n\n    \"\"\"\n    if not utils.is_mesh(x):\n        raise TypeError(f'Expected mesh-like, got \"{type(x)}\"')\n\n    try:\n        import pyfqmr\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError('Please install pyfqmr: pip3 install pyfqmr')\n\n    defaults = dict(aggressiveness=7, preserve_border=True, verbose=False)\n    defaults.update(kwargs)\n\n    mesh_simplifier = pyfqmr.Simplify()\n    mesh_simplifier.setMesh(x.vertices, x.faces)\n    mesh_simplifier.simplify_mesh(target_count=F, **defaults)\n    vertices, faces, normals = mesh_simplifier.getMesh()\n\n    if not inplace:\n        x = x.copy()\n\n    x.vertices = vertices\n    x.faces = faces\n\n    if hasattr(x, 'face_normals'):\n        x.face_normals = normals\n\n    return x\n</code></pre>"},{"location":"reference/navis/meshes/#navis.meshes.simplify_mesh_open3d","title":"<code>navis.meshes.simplify_mesh_open3d</code>","text":"<p>Simplify mesh using open3d.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Mesh object to simplify.\n</code></pre> <p> TYPE: <code>        MeshNeuron | Volume | Trimesh</code> </p> <code>F</code> <pre><code>    For method `quadric` this is the target face count (integer).\n    For method `cluster` this is the size of the voxel within which\n    vertices are pooled (larger t = coarser mesh).\n</code></pre> <p> TYPE: <code>        float | int</code> </p> <code>method</code> <pre><code>    Which method to use for simplification: either Quadric Error\n    Metric Decimation (by Garland and Heckbert) or vertex clustering.\n    Note that the intepretation of `F` depends on the method.\n</code></pre> <p> TYPE: <code>   \"quadric\" | \"cluster\"</code> DEFAULT: <code>'quadric'</code> </p> <code>inplace</code> <pre><code>    If True, will perform simplication on `x`. If False, will\n    simplify and return a copy.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <pre><code>    Keyword arguments are passed through to open3d's\n    `simplify_quadric_decimation` and `simplify_vertex_clustering`,\n    respectively.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>simp</code> <p>Simplified mesh object.</p> Source code in <code>navis/meshes/o3d.py</code> <pre><code>def simplify_mesh_open3d(x, F, method='quadric', inplace=False, **kwargs):\n    \"\"\"Simplify mesh using open3d.\n\n    Parameters\n    ----------\n    x :         MeshNeuron | Volume | Trimesh\n                Mesh object to simplify.\n    F :         float | int\n                For method `quadric` this is the target face count (integer).\n                For method `cluster` this is the size of the voxel within which\n                vertices are pooled (larger t = coarser mesh).\n    method :    \"quadric\" | \"cluster\"\n                Which method to use for simplification: either Quadric Error\n                Metric Decimation (by Garland and Heckbert) or vertex clustering.\n                Note that the intepretation of `F` depends on the method.\n    inplace :   bool\n                If True, will perform simplication on `x`. If False, will\n                simplify and return a copy.\n    **kwargs\n                Keyword arguments are passed through to open3d's\n                `simplify_quadric_decimation` and `simplify_vertex_clustering`,\n                respectively.\n\n    Returns\n    -------\n    simp\n                Simplified mesh object.\n\n    \"\"\"\n    if not utils.is_mesh(x):\n        raise TypeError(f'Expected mesh-like, got \"{type(x)}\"')\n\n    mesh_o3d = make_o3d_mesh(x)\n\n    if method == 'quadric':\n        result = mesh_o3d.simplify_quadric_decimation(int(F), **kwargs)\n    elif method == 'cluster':\n        result = mesh_o3d.simplify_vertex_clustering(F,  **kwargs)\n    else:\n        raise ValueError(f'Unknown simplification scheme \"{method}\"')\n\n    if not inplace:\n        x = x.copy()\n\n    x.vertices = np.asarray(result.vertices)\n    x.faces = np.asarray(result.triangles)\n\n    return x\n</code></pre>"},{"location":"reference/navis/meshes/#navis.meshes.simplify_mesh_pyml","title":"<code>navis.meshes.simplify_mesh_pyml</code>","text":"<p>Simplify mesh using pymeshlab.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Mesh object to simplify.\n</code></pre> <p> TYPE: <code>        MeshNeuron | Volume | Trimesh</code> </p> <code>F</code> <pre><code>    For method \"quadric\" this is the target number of faces as\n    fraction of the original face count. For method \"cluster\" this\n    is the size of the cells used for clustering: larger values =\n    coarser mesh.\n</code></pre> <p> TYPE: <code>        float [0-1]</code> </p> <code>method</code> <pre><code>    Which method to use for simplification: quadratic mesh\n    decimation or vertex clustering.\n</code></pre> <p> TYPE: <code>   \"quadric\" | \"cluster\"</code> DEFAULT: <code>'quadric'</code> </p> <code>inplace</code> <pre><code>    If True, will perform simplication on `x`. If False, will\n    simplify and return a copy.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <pre><code>    Passed to pymeshlab filter functions:\n    `simplification_quadric_edge_collapse_decimation` or\n    `simplification_clustering_decimation` depending on method.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>simp</code> <p>Simplified mesh-like object.</p> Source code in <code>navis/meshes/pyml.py</code> <pre><code>def simplify_mesh_pyml(x, F, method='quadric', inplace=False, **kwargs):\n    \"\"\"Simplify mesh using pymeshlab.\n\n    Parameters\n    ----------\n    x :         MeshNeuron | Volume | Trimesh\n                Mesh object to simplify.\n    F :         float [0-1]\n                For method \"quadric\" this is the target number of faces as\n                fraction of the original face count. For method \"cluster\" this\n                is the size of the cells used for clustering: larger values =\n                coarser mesh.\n    method :    \"quadric\" | \"cluster\"\n                Which method to use for simplification: quadratic mesh\n                decimation or vertex clustering.\n    inplace :   bool\n                If True, will perform simplication on `x`. If False, will\n                simplify and return a copy.\n    **kwargs\n                Passed to pymeshlab filter functions:\n                `simplification_quadric_edge_collapse_decimation` or\n                `simplification_clustering_decimation` depending on method.\n\n    Returns\n    -------\n    simp\n                Simplified mesh-like object.\n\n    \"\"\"\n    try:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            import pymeshlab\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError('Please install pymeshlab: pip3 install pymeshlab')\n    except BaseException:\n        raise\n\n    utils.eval_param(method,\n                     name='method',\n                     allowed_values=('quadric', 'cluster'))\n\n    if not isinstance(x, (core.MeshNeuron, tm.Trimesh, core.Volume)):\n        raise TypeError(f'Expected MeshNeuron, Volume or Trimesh, got \"{type(x)}\"')\n\n    if (F &lt;= 0) or (F &gt;= 1):\n        raise ValueError(f'`t` must be between 0-1, got {F}')\n\n    verts, faces = x.vertices, x.faces\n\n    # Create mesh from vertices and faces\n    m = pymeshlab.Mesh(verts, faces)\n\n    # Create a new MeshSet\n    ms = pymeshlab.MeshSet()\n\n    # Add the mesh to the MeshSet\n    ms.add_mesh(m, \"mymesh\")\n\n    # Apply filter\n    if method == 'quadric':\n        defaults = {'targetperc': F}\n        defaults.update(kwargs)\n        if hasattr(ms, 'meshing_decimation_quadric_edge_collapse'):\n            # Post 2022.2\n            ms.meshing_decimation_quadric_edge_collapse(**defaults)\n        else:\n            # Pre 2022.2\n            ms.simplification_quadric_edge_collapse_decimation(**defaults)\n    else:\n        # Threshold is for some reason in percent, not fraction\n        defaults = {'thresholds': F * 100}\n        defaults.update(kwargs)\n        if hasattr(ms, 'meshing_decimation_clustering'):\n            # Post 2022.2\n            ms.meshing_decimation_clustering(**defaults)\n        else:\n            # Pre 2022.2\n            ms.simplification_clustering_decimation(**defaults)\n\n    # Get update mesh\n    m2 = ms.current_mesh()\n\n    # Get new vertices and faces\n    new_verts = m2.vertex_matrix()\n    new_faces = m2.face_matrix()\n\n    # Make copy of the original mesh and assign new vertices + faces\n    if not inplace:\n        x = x.copy()\n\n    x.vertices = new_verts\n    x.faces = new_faces\n\n    if isinstance(x, core.MeshNeuron):\n        x._clear_temp_attr()\n\n    return x\n</code></pre>"},{"location":"reference/navis/models/","title":"models","text":""},{"location":"reference/navis/models/#navis.models.BayesianTraversalModel","title":"<code>navis.models.BayesianTraversalModel</code>","text":"<p>Model for traversing a network starting with given seed nodes.</p> <p>This model is a Bayes net version of <code>navis.models.network_models.TraversalModel</code> that propagates traversal probabilities through the network and converges to a distribution of time of traversal for each node, rather than stochastically sampling.</p> <p>Unlike <code>TraversalModel</code>, this model should only be run once. Note alse that <code>traversal_func</code> should be a function returing probabilities of traversal, rather than a random boolean of traversal.</p> PARAMETER DESCRIPTION <code>edges</code> <pre><code>            DataFrame representing an edge list. Must minimally have\n            a `source` and `target` column.\n</code></pre> <p> TYPE: <code>            pandas.DataFrame</code> </p> <code>seeds</code> <pre><code>            Seed nodes for traversal. Nodes that aren't found in\n            `edges['source']` will be (silently) removed.\n</code></pre> <p> TYPE: <code>            iterable</code> </p> <code>weights</code> <pre><code>            Name of a column in `edges` used as weights. If not\n            provided, all edges will be given a weight of 1. If using\n            the default activation function the weights need to be\n            between 0 and 1.\n</code></pre> <p> TYPE: <code>          str</code> </p> <code>max_steps</code> <pre><code>            Limits the number of steps for each iteration.\n</code></pre> <p> TYPE: <code>        int</code> </p> <code>traversal_func</code> <pre><code>            Function returning probability whether a given edge will be\n            traversed or not in a given step. Must take numpy array\n            (N, 1) of edge weights and return an array with\n            probabilities of equal size. Defaults to\n            [`navis.models.network_models.linear_activation_p`][]\n            which will linearly scale probability of traversal\n            from 0 to 100% between edges weights 0 to 0.3.\n</code></pre> <p> TYPE: <code>   callable</code> DEFAULT: <code>None</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from navis.models import BayesianTraversalModel\n&gt;&gt;&gt; import networkx as nx\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Generate a random graph\n&gt;&gt;&gt; G = nx.fast_gnp_random_graph(1000, .2, directed=True)\n&gt;&gt;&gt; # Turn into edge list\n&gt;&gt;&gt; edges = nx.to_pandas_edgelist(G)\n&gt;&gt;&gt; # Add random edge weights\n&gt;&gt;&gt; edges['weight'] = np.random.random(edges.shape[0])\n&gt;&gt;&gt; # Initialize model\n&gt;&gt;&gt; model = BayesianTraversalModel(edges, seeds=list(G.nodes)[:10])\n&gt;&gt;&gt; # Run model\n&gt;&gt;&gt; res = model.run()\n&gt;&gt;&gt; # Get a summary\n&gt;&gt;&gt; model.summary.tail()\n      layer_min  layer_max  layer_mean  layer_median\nnode\n995           2          2        2.00             2\n996           2          3        2.33             2\n997           2          2        2.00             2\n998           2          2        2.00             2\n999           2          2        2.00             2\n</code></pre> <p>Above Graph was traversed quickly (3 steps max). Let's adjust the traversal function:</p> <pre><code>&gt;&gt;&gt; from navis.models import linear_activation_p\n&gt;&gt;&gt; # Use a lower probability for activation\n&gt;&gt;&gt; def my_act(x):\n...     return linear_activation_p(x, max_w=10)\n&gt;&gt;&gt; model = BayesianTraversalModel(edges, seeds=list(G.nodes)[:10],\n...                                traversal_func=my_act)\n&gt;&gt;&gt; res = model.run()\n&gt;&gt;&gt; res.tail()\n      layer_min  layer_max  layer_mean  layer_median\nnode\n995           2          4       3.210           3.0\n996           2          4       3.280           3.0\n997           2          4       3.260           3.0\n998           2          4       3.320           3.0\n999           2          4       3.195           3.0\n</code></pre> Source code in <code>navis/models/network_models.py</code> <pre><code>class BayesianTraversalModel(TraversalModel):\n    \"\"\"Model for traversing a network starting with given seed nodes.\n\n    This model is a Bayes net version of\n    [`navis.models.network_models.TraversalModel`][] that propagates\n    traversal probabilities through the network and converges to a\n    distribution of time of traversal for each node, rather than\n    stochastically sampling.\n\n    Unlike `TraversalModel`, this model should only be run once.\n    Note alse that `traversal_func` should be a function returing\n    probabilities of traversal, rather than a random boolean of traversal.\n\n    Parameters\n    ----------\n    edges :             pandas.DataFrame\n                        DataFrame representing an edge list. Must minimally have\n                        a `source` and `target` column.\n    seeds :             iterable\n                        Seed nodes for traversal. Nodes that aren't found in\n                        `edges['source']` will be (silently) removed.\n    weights :           str, optional\n                        Name of a column in `edges` used as weights. If not\n                        provided, all edges will be given a weight of 1. If using\n                        the default activation function the weights need to be\n                        between 0 and 1.\n    max_steps :         int\n                        Limits the number of steps for each iteration.\n    traversal_func :    callable, optional\n                        Function returning probability whether a given edge will be\n                        traversed or not in a given step. Must take numpy array\n                        (N, 1) of edge weights and return an array with\n                        probabilities of equal size. Defaults to\n                        [`navis.models.network_models.linear_activation_p`][]\n                        which will linearly scale probability of traversal\n                        from 0 to 100% between edges weights 0 to 0.3.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from navis.models import BayesianTraversalModel\n    &gt;&gt;&gt; import networkx as nx\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; # Generate a random graph\n    &gt;&gt;&gt; G = nx.fast_gnp_random_graph(1000, .2, directed=True)\n    &gt;&gt;&gt; # Turn into edge list\n    &gt;&gt;&gt; edges = nx.to_pandas_edgelist(G)\n    &gt;&gt;&gt; # Add random edge weights\n    &gt;&gt;&gt; edges['weight'] = np.random.random(edges.shape[0])\n    &gt;&gt;&gt; # Initialize model\n    &gt;&gt;&gt; model = BayesianTraversalModel(edges, seeds=list(G.nodes)[:10])\n    &gt;&gt;&gt; # Run model\n    &gt;&gt;&gt; res = model.run()\n    &gt;&gt;&gt; # Get a summary\n    &gt;&gt;&gt; model.summary.tail()                                    # doctest: +SKIP\n          layer_min  layer_max  layer_mean  layer_median\n    node\n    995           2          2        2.00             2\n    996           2          3        2.33             2\n    997           2          2        2.00             2\n    998           2          2        2.00             2\n    999           2          2        2.00             2\n\n    Above Graph was traversed quickly (3 steps max). Let's adjust the\n    traversal function:\n\n    &gt;&gt;&gt; from navis.models import linear_activation_p\n    &gt;&gt;&gt; # Use a lower probability for activation\n    &gt;&gt;&gt; def my_act(x):\n    ...     return linear_activation_p(x, max_w=10)\n    &gt;&gt;&gt; model = BayesianTraversalModel(edges, seeds=list(G.nodes)[:10],\n    ...                                traversal_func=my_act)\n    &gt;&gt;&gt; res = model.run()\n    &gt;&gt;&gt; res.tail()                                              # doctest: +SKIP\n          layer_min  layer_max  layer_mean  layer_median\n    node\n    995           2          4       3.210           3.0\n    996           2          4       3.280           3.0\n    997           2          4       3.260           3.0\n    998           2          4       3.320           3.0\n    999           2          4       3.195           3.0\n\n    \"\"\"\n\n    def __init__(self,\n                 *args,\n                 traversal_func: Optional[Callable] = None,\n                 **kwargs):\n        \"\"\"Initialize model.\"\"\"\n        super().__init__(*args, **kwargs)\n\n        if isinstance(traversal_func, type(None)):\n            self.traversal_func = linear_activation_p\n        elif callable(traversal_func):\n            self.traversal_func = traversal_func\n        else:\n            raise ValueError('`traversal_func` must be None or a callable')\n\n    def make_summary(self) -&gt; pd.DataFrame:\n        \"\"\"Generate summary.\"\"\"\n        if not self.has_results:\n            logger.error('Must run simulation first.')\n\n        cmfs = np.stack(self.results.cmf)\n        layer_min = (cmfs != 0).argmax(axis=1)\n        valid = np.any(cmfs != 0, axis=1)\n        layer_max = (cmfs == 1.).argmax(axis=1)\n        layer_max[~np.any(cmfs == 1., axis=1)] = cmfs.shape[1]\n        layer_median = (cmfs &gt;= .5).argmax(axis=1).astype(float)\n        pmfs = np.diff(cmfs, axis=1, prepend=0.)\n        layer_pmfs = pmfs * np.arange(pmfs.shape[1])\n        layer_mean = np.sum(layer_pmfs, axis=1)\n\n        summary = pd.DataFrame({\n                'layer_min': layer_min + 1,\n                'layer_max': layer_max + 1,\n                'layer_mean': layer_mean + 1,\n                'layer_median': layer_median + 1,\n            }, index=self.results.node)\n\n        # Discard nodes that are never activated\n        summary = summary[valid]\n\n        self._summary = summary\n        return self._summary\n\n    def run(self, **kwargs) -&gt; pd.DataFrame:\n        \"\"\"Run model (single process).\"\"\"\n\n        # For some reason this is required for progress bars in Jupyter to show\n        print(' ', end='', flush=True)\n        # For faster access, use the raw array\n        edges = self.edges[[self.source, self.target, self.weights]].values\n        id_type = self.edges[self.source].dtype\n        # Transform weights to traversal probabilities\n        edges[:, 2] = self.traversal_func(edges[:, 2])\n\n        # Change node IDs into indices in [0, len(nodes))\n        ids, edges_idx = np.unique(edges[:, :2], return_inverse=True)\n        ids = ids.astype(id_type)\n        edges_idx = edges_idx.reshape((edges.shape[0], 2))\n        edges_idx = np.concatenate(\n            (edges_idx, np.expand_dims(edges[:, 2], axis=1)),\n            axis=1)\n\n        cmfs = np.zeros((len(ids), self.max_steps), dtype=np.float64)\n        seed_idx = np.searchsorted(ids, self.seeds)\n        cmfs[seed_idx, :] = 1.\n        changed = set(edges_idx[np.isin(edges_idx[:, 0], seed_idx), 1].astype(id_type))\n\n        with config.tqdm(\n                total=0,\n                disable=config.pbar_hide,\n                leave=config.pbar_leave,\n                position=kwargs.get('position', 0)) as pbar:\n            while len(changed):\n                next_changed = []\n\n                pbar.total += len(changed)\n                pbar.refresh()\n\n                for idx in changed:\n                    cmf = cmfs[idx, :]\n                    inbound = edges_idx[edges_idx[:, 1] == idx, :]\n                    pre = inbound[:, 0].astype(np.int64)\n\n                    # Traversal probability for each inbound edge at each time.\n                    posteriors = cmfs[pre, :] * np.expand_dims(inbound[:, 2], axis=1)\n                    # At each time, compute the probability that at least one inbound edge\n                    # is traversed.\n                    new_pmf = 1 - np.prod(1 - posteriors, axis=0)\n                    new_cmf = cmf.copy()\n                    # Offset the time-cumulative probability by 1 to account for traversal iteration.\n                    # Use maximum of previous CMF as it is monotonic and to include fixed seed traversal.\n                    new_cmf[1:] = np.maximum(cmf[1:], 1 - np.cumprod(1 - new_pmf[:-1]))\n                    np.clip(new_cmf, 0., 1., out=new_cmf)\n\n                    if np.allclose(cmf, new_cmf):\n                        continue\n\n                    cmfs[idx, :] = new_cmf\n\n                    # Notify downstream nodes that they have changed next iteration.\n                    post_idx = edges_idx[edges_idx[:, 0] == idx, 1].astype(id_type)\n                    next_changed.extend(list(post_idx))\n\n                pbar.update(len(changed))\n                changed = set(next_changed)\n\n        self.iterations = 1\n        self.results = pd.DataFrame({'node': ids, 'cmf': list(cmfs)})\n        return self.results\n\n    def run_parallel(self, *args, **kwargs) -&gt; None:\n        warnings.warn(f\"{self.__class__.__name__} should not be run in parallel. Falling back to run.\")\n        self.run(**kwargs)\n</code></pre>"},{"location":"reference/navis/models/#navis.models.BayesianTraversalModel.__init__","title":"<code>__init__</code>","text":"<p>Initialize model.</p> Source code in <code>navis/models/network_models.py</code> <pre><code>def __init__(self,\n             *args,\n             traversal_func: Optional[Callable] = None,\n             **kwargs):\n    \"\"\"Initialize model.\"\"\"\n    super().__init__(*args, **kwargs)\n\n    if isinstance(traversal_func, type(None)):\n        self.traversal_func = linear_activation_p\n    elif callable(traversal_func):\n        self.traversal_func = traversal_func\n    else:\n        raise ValueError('`traversal_func` must be None or a callable')\n</code></pre>"},{"location":"reference/navis/models/#navis.models.BayesianTraversalModel.make_summary","title":"<code>make_summary</code>","text":"<p>Generate summary.</p> Source code in <code>navis/models/network_models.py</code> <pre><code>def make_summary(self) -&gt; pd.DataFrame:\n    \"\"\"Generate summary.\"\"\"\n    if not self.has_results:\n        logger.error('Must run simulation first.')\n\n    cmfs = np.stack(self.results.cmf)\n    layer_min = (cmfs != 0).argmax(axis=1)\n    valid = np.any(cmfs != 0, axis=1)\n    layer_max = (cmfs == 1.).argmax(axis=1)\n    layer_max[~np.any(cmfs == 1., axis=1)] = cmfs.shape[1]\n    layer_median = (cmfs &gt;= .5).argmax(axis=1).astype(float)\n    pmfs = np.diff(cmfs, axis=1, prepend=0.)\n    layer_pmfs = pmfs * np.arange(pmfs.shape[1])\n    layer_mean = np.sum(layer_pmfs, axis=1)\n\n    summary = pd.DataFrame({\n            'layer_min': layer_min + 1,\n            'layer_max': layer_max + 1,\n            'layer_mean': layer_mean + 1,\n            'layer_median': layer_median + 1,\n        }, index=self.results.node)\n\n    # Discard nodes that are never activated\n    summary = summary[valid]\n\n    self._summary = summary\n    return self._summary\n</code></pre>"},{"location":"reference/navis/models/#navis.models.BayesianTraversalModel.run","title":"<code>run</code>","text":"<p>Run model (single process).</p> Source code in <code>navis/models/network_models.py</code> <pre><code>def run(self, **kwargs) -&gt; pd.DataFrame:\n    \"\"\"Run model (single process).\"\"\"\n\n    # For some reason this is required for progress bars in Jupyter to show\n    print(' ', end='', flush=True)\n    # For faster access, use the raw array\n    edges = self.edges[[self.source, self.target, self.weights]].values\n    id_type = self.edges[self.source].dtype\n    # Transform weights to traversal probabilities\n    edges[:, 2] = self.traversal_func(edges[:, 2])\n\n    # Change node IDs into indices in [0, len(nodes))\n    ids, edges_idx = np.unique(edges[:, :2], return_inverse=True)\n    ids = ids.astype(id_type)\n    edges_idx = edges_idx.reshape((edges.shape[0], 2))\n    edges_idx = np.concatenate(\n        (edges_idx, np.expand_dims(edges[:, 2], axis=1)),\n        axis=1)\n\n    cmfs = np.zeros((len(ids), self.max_steps), dtype=np.float64)\n    seed_idx = np.searchsorted(ids, self.seeds)\n    cmfs[seed_idx, :] = 1.\n    changed = set(edges_idx[np.isin(edges_idx[:, 0], seed_idx), 1].astype(id_type))\n\n    with config.tqdm(\n            total=0,\n            disable=config.pbar_hide,\n            leave=config.pbar_leave,\n            position=kwargs.get('position', 0)) as pbar:\n        while len(changed):\n            next_changed = []\n\n            pbar.total += len(changed)\n            pbar.refresh()\n\n            for idx in changed:\n                cmf = cmfs[idx, :]\n                inbound = edges_idx[edges_idx[:, 1] == idx, :]\n                pre = inbound[:, 0].astype(np.int64)\n\n                # Traversal probability for each inbound edge at each time.\n                posteriors = cmfs[pre, :] * np.expand_dims(inbound[:, 2], axis=1)\n                # At each time, compute the probability that at least one inbound edge\n                # is traversed.\n                new_pmf = 1 - np.prod(1 - posteriors, axis=0)\n                new_cmf = cmf.copy()\n                # Offset the time-cumulative probability by 1 to account for traversal iteration.\n                # Use maximum of previous CMF as it is monotonic and to include fixed seed traversal.\n                new_cmf[1:] = np.maximum(cmf[1:], 1 - np.cumprod(1 - new_pmf[:-1]))\n                np.clip(new_cmf, 0., 1., out=new_cmf)\n\n                if np.allclose(cmf, new_cmf):\n                    continue\n\n                cmfs[idx, :] = new_cmf\n\n                # Notify downstream nodes that they have changed next iteration.\n                post_idx = edges_idx[edges_idx[:, 0] == idx, 1].astype(id_type)\n                next_changed.extend(list(post_idx))\n\n            pbar.update(len(changed))\n            changed = set(next_changed)\n\n    self.iterations = 1\n    self.results = pd.DataFrame({'node': ids, 'cmf': list(cmfs)})\n    return self.results\n</code></pre>"},{"location":"reference/navis/models/#navis.models.TraversalModel","title":"<code>navis.models.TraversalModel</code>","text":"<p>Model for traversing a network starting with given seed nodes.</p> <p>What this does:</p> <ol> <li>Grab all already visited nodes (starting with <code>seeds</code> in step 1)</li> <li>Find all downstream nodes of these</li> <li>Probabilistically traverse based on the weight of the connecting edges</li> <li>Add those newly visited nodes to the pool &amp; repeat from beginning</li> <li>Stop when every (connected) neuron was visited or we reached <code>max_steps</code></li> </ol> PARAMETER DESCRIPTION <code>edges</code> <pre><code>            DataFrame representing an edge list. Must minimally have\n            a `source` and `target` column.\n</code></pre> <p> TYPE: <code>            pandas.DataFrame</code> </p> <code>seeds</code> <pre><code>            Seed nodes for traversal. Nodes that aren't found in\n            `edges['source']` will be (silently) removed.\n</code></pre> <p> TYPE: <code>            iterable</code> </p> <code>weights</code> <pre><code>            Name of a column in `edges` used as weights. If not\n            provided, all edges will be given a weight of 1. If using\n            the default activation function the weights need to be\n            between 0 and 1.\n</code></pre> <p> TYPE: <code>          str</code> DEFAULT: <code>'weight'</code> </p> <code>max_steps</code> <pre><code>            Limits the number of steps for each iteration.\n</code></pre> <p> TYPE: <code>        int</code> DEFAULT: <code>15</code> </p> <code>traversal_func</code> <pre><code>            Function that determines whether a given edge will be\n            traversed or not in a given step. Must take numpy array\n            (N, 1) of edge weights and return an array with\n            True/False of equal size. Defaults to\n            [`navis.models.network_models.random_linear_activation_function`][]\n            which will linearly scale probability of traversal\n            from 0 to 100% between edges weights 0 to 0.3.\n</code></pre> <p> TYPE: <code>   callable</code> DEFAULT: <code>None</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from navis.models import TraversalModel\n&gt;&gt;&gt; import networkx as nx\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Generate a random graph\n&gt;&gt;&gt; G = nx.fast_gnp_random_graph(1000, .2, directed=True)\n&gt;&gt;&gt; # Turn into edge list\n&gt;&gt;&gt; edges = nx.to_pandas_edgelist(G)\n&gt;&gt;&gt; # Add random edge weights\n&gt;&gt;&gt; edges['weight'] = np.random.random(edges.shape[0])\n&gt;&gt;&gt; # Initialize model\n&gt;&gt;&gt; model = TraversalModel(edges, seeds=list(G.nodes)[:10])\n&gt;&gt;&gt; # Run model on 2 cores\n&gt;&gt;&gt; model.run_parallel(n_cores=2, iterations=100)\n&gt;&gt;&gt; # Get a summary\n&gt;&gt;&gt; model.summary.tail()\n      layer_min  layer_max  layer_mean  layer_median\nnode\n995           2          2        2.00             2\n996           2          3        2.33             2\n997           2          2        2.00             2\n998           2          2        2.00             2\n999           2          2        2.00             2\n</code></pre> <p>Above Graph was traversed quickly (3 steps max). Let's adjust the traversal function:</p> <pre><code>&gt;&gt;&gt; from navis.models import random_linear_activation_function\n&gt;&gt;&gt; # Use a lower probability for activation\n&gt;&gt;&gt; def my_act(x):\n...     return random_linear_activation_function(x, max_w=10)\n&gt;&gt;&gt; model = TraversalModel(edges, seeds=list(G.nodes)[:10],\n...                        traversal_func=my_act)\n&gt;&gt;&gt; res = model.run(iterations=100)\n&gt;&gt;&gt; res.tail()\n      layer_min  layer_max  layer_mean  layer_median\nnode\n995           2          4       3.210           3.0\n996           2          4       3.280           3.0\n997           2          4       3.260           3.0\n998           2          4       3.320           3.0\n999           2          4       3.195           3.0\n</code></pre> Source code in <code>navis/models/network_models.py</code> <pre><code>class TraversalModel(BaseNetworkModel):\n    \"\"\"Model for traversing a network starting with given seed nodes.\n\n    What this does:\n\n      1. Grab all already visited nodes (starting with `seeds` in step 1)\n      2. Find all downstream nodes of these\n      3. Probabilistically traverse based on the weight of the connecting edges\n      4. Add those newly visited nodes to the pool &amp; repeat from beginning\n      5. Stop when every (connected) neuron was visited or we reached `max_steps`\n\n    Parameters\n    ----------\n    edges :             pandas.DataFrame\n                        DataFrame representing an edge list. Must minimally have\n                        a `source` and `target` column.\n    seeds :             iterable\n                        Seed nodes for traversal. Nodes that aren't found in\n                        `edges['source']` will be (silently) removed.\n    weights :           str, optional\n                        Name of a column in `edges` used as weights. If not\n                        provided, all edges will be given a weight of 1. If using\n                        the default activation function the weights need to be\n                        between 0 and 1.\n    max_steps :         int\n                        Limits the number of steps for each iteration.\n    traversal_func :    callable, optional\n                        Function that determines whether a given edge will be\n                        traversed or not in a given step. Must take numpy array\n                        (N, 1) of edge weights and return an array with\n                        True/False of equal size. Defaults to\n                        [`navis.models.network_models.random_linear_activation_function`][]\n                        which will linearly scale probability of traversal\n                        from 0 to 100% between edges weights 0 to 0.3.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from navis.models import TraversalModel\n    &gt;&gt;&gt; import networkx as nx\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; # Generate a random graph\n    &gt;&gt;&gt; G = nx.fast_gnp_random_graph(1000, .2, directed=True)\n    &gt;&gt;&gt; # Turn into edge list\n    &gt;&gt;&gt; edges = nx.to_pandas_edgelist(G)\n    &gt;&gt;&gt; # Add random edge weights\n    &gt;&gt;&gt; edges['weight'] = np.random.random(edges.shape[0])\n    &gt;&gt;&gt; # Initialize model\n    &gt;&gt;&gt; model = TraversalModel(edges, seeds=list(G.nodes)[:10])\n    &gt;&gt;&gt; # Run model on 2 cores\n    &gt;&gt;&gt; model.run_parallel(n_cores=2, iterations=100)\n    &gt;&gt;&gt; # Get a summary\n    &gt;&gt;&gt; model.summary.tail()                                    # doctest: +SKIP\n          layer_min  layer_max  layer_mean  layer_median\n    node\n    995           2          2        2.00             2\n    996           2          3        2.33             2\n    997           2          2        2.00             2\n    998           2          2        2.00             2\n    999           2          2        2.00             2\n\n    Above Graph was traversed quickly (3 steps max). Let's adjust the\n    traversal function:\n\n    &gt;&gt;&gt; from navis.models import random_linear_activation_function\n    &gt;&gt;&gt; # Use a lower probability for activation\n    &gt;&gt;&gt; def my_act(x):\n    ...     return random_linear_activation_function(x, max_w=10)\n    &gt;&gt;&gt; model = TraversalModel(edges, seeds=list(G.nodes)[:10],\n    ...                        traversal_func=my_act)\n    &gt;&gt;&gt; res = model.run(iterations=100)\n    &gt;&gt;&gt; res.tail()                                              # doctest: +SKIP\n          layer_min  layer_max  layer_mean  layer_median\n    node\n    995           2          4       3.210           3.0\n    996           2          4       3.280           3.0\n    997           2          4       3.260           3.0\n    998           2          4       3.320           3.0\n    999           2          4       3.195           3.0\n\n    \"\"\"\n\n    def __init__(self,\n                 edges: pd.DataFrame,\n                 seeds: Iterable[Union[str, int]],\n                 source: str = 'source',\n                 target: str = 'target',\n                 weights: Optional[str] = 'weight',\n                 max_steps: int = 15,\n                 traversal_func: Optional[Callable] = None):\n        \"\"\"Initialize model.\"\"\"\n        super().__init__(edges=edges, source=source, target=target)\n\n        if not weights:\n            edges['weight'] = 1\n            weights = 'weight'\n\n        assert weights in edges.columns, f'\"{weights}\" must be column in edge list'\n\n        # Remove seeds that don't exist\n        self.seeds = edges[edges[self.source].isin(seeds)][self.source].unique()\n\n        if len(self.seeds) == 0:\n            raise ValueError('None of the seeds where among edge list sources.')\n\n        self.weights = weights\n        self.max_steps = max_steps\n\n        if isinstance(traversal_func, type(None)):\n            self.traversal_func = random_linear_activation_function\n        elif callable(traversal_func):\n            self.traversal_func = traversal_func\n        else:\n            raise ValueError('`traversal_func` must be None or a callable')\n\n    @property\n    def summary(self) -&gt; pd.DataFrame:\n        \"\"\"Per-node summary.\"\"\"\n        return getattr(self, '_summary', self.make_summary())\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __repr__(self):\n        s = f'{self.__class__}: {self.edges.shape[0]} edges; {self.n_nodes}' \\\n            f' unique nodes; {len(self.seeds)} seeds;' \\\n            f' traversal_func {self.traversal_func}.'\n        if self.has_results:\n            s += f' Model ran with {self.iterations} iterations.'\n        else:\n            s += ' Model has not yet been run.'\n        return s\n\n    def make_summary(self) -&gt; pd.DataFrame:\n        \"\"\"Generate summary.\"\"\"\n        if not self.has_results:\n            logger.error('Must run simulation first.')\n\n        summary = self.results.groupby('node',\n                                       as_index=False).steps.agg(['min',\n                                                                  'max',\n                                                                  'mean',\n                                                                  'median'])\n\n        summary.rename({'min': 'layer_min',\n                        'mean': 'layer_mean',\n                        'max': 'layer_max',\n                        'median': 'layer_median'}, axis=1, inplace=True)\n\n        self._summary = summary\n        return self._summary\n\n    def run(self, iterations: int = 100, return_iterations=False, **kwargs) -&gt; pd.DataFrame:\n        \"\"\"Run model (single process).\n\n        Use `.run_parallel` to use parallel processes.\n\n        \"\"\"\n        # For some reason this is required for progress bars in Jupyter to show\n        print(' ', end='', flush=True)\n        # For faster access, use the raw array\n        # Note: we're splitting the columns in case we have different datatypes\n        # (e.g. int64 for IDs and float for weights)\n        sources = self.edges[self.source].values\n        targets = self.edges[self.target].values\n        weights = self.edges[self.weights].values\n\n        # For some reason the progress bar does not show unless we have a print here\n        all_enc_nodes = None\n        for it in config.trange(1, iterations + 1,\n                                disable=config.pbar_hide,\n                                leave=config.pbar_leave,\n                                position=kwargs.get('position', 0)):\n            # Set seeds as encountered in step 1\n            enc_nodes = self.seeds\n            enc_steps = np.repeat(1, len(self.seeds))\n            if return_iterations:\n                enc_it = np.repeat(it, len(self.seeds))\n\n            # Start with all edges\n            this_weights = weights\n            this_sources = sources\n            this_targets = targets\n            for i in range(2, self.max_steps + 1):\n                # Which edges have their presynaptic node already traversed?\n                pre_trav = np.isin(this_sources, enc_nodes)\n                # Among those, which edges have the postsynaptic node traversed?\n                post_trav = np.isin(this_targets[pre_trav], enc_nodes)\n\n                # Combine conditions to find edges where the presynaptic node\n                # has been traversed but not the postsynaptic node\n                pre_not_post = np.where(pre_trav)[0][~post_trav]\n                out_targets = this_targets[pre_not_post]\n                out_weights = this_weights[pre_not_post]\n\n                # Drop edges that have already been traversed - speeds up things\n                pre_and_post = np.where(pre_trav)[0][post_trav]\n                this_targets = np.delete(this_targets, pre_and_post, axis=0)\n                this_sources = np.delete(this_sources, pre_and_post, axis=0)\n                this_weights = np.delete(this_weights, pre_and_post, axis=0)\n\n                # Stop if we traversed the entire (reachable) graph\n                if out_targets.size == 0:\n                    break\n\n                # Edges traversed in this round\n                trav = self.traversal_func(out_weights)\n                if not trav.sum():\n                    continue\n\n                trav_targets = out_targets[trav]\n\n                # Keep track\n                new_trav = np.unique(trav_targets)\n\n                # Store results\n                enc_nodes = np.concatenate((enc_nodes, new_trav))\n                enc_steps = np.concatenate((enc_steps, np.repeat(i, len(new_trav))))\n                if return_iterations:\n                    enc_it = np.concatenate((enc_it, np.repeat(it, len(new_trav))))\n\n            # Save this round of traversal\n            if all_enc_nodes is None:\n                all_enc_nodes = enc_nodes\n                all_enc_steps = enc_steps\n                if return_iterations:\n                    all_enc_it = enc_it\n            else:\n                all_enc_nodes = np.concatenate((all_enc_nodes, enc_nodes))\n                all_enc_steps = np.concatenate((all_enc_steps, enc_steps))\n                if return_iterations:\n                    all_enc_it = np.concatenate((all_enc_it, enc_it))\n\n        self.iterations = iterations\n\n        # Combine results into DataFrame\n        self.results = pd.DataFrame()\n        self.results['steps'] = all_enc_steps\n        self.results['node'] = all_enc_nodes\n        if return_iterations:\n            self.results['iteration'] = all_enc_it\n\n        return self.results\n</code></pre>"},{"location":"reference/navis/models/#navis.models.TraversalModel.summary","title":"<code>summary: pd.DataFrame</code>  <code>property</code>","text":"<p>Per-node summary.</p>"},{"location":"reference/navis/models/#navis.models.TraversalModel.__init__","title":"<code>__init__</code>","text":"<p>Initialize model.</p> Source code in <code>navis/models/network_models.py</code> <pre><code>def __init__(self,\n             edges: pd.DataFrame,\n             seeds: Iterable[Union[str, int]],\n             source: str = 'source',\n             target: str = 'target',\n             weights: Optional[str] = 'weight',\n             max_steps: int = 15,\n             traversal_func: Optional[Callable] = None):\n    \"\"\"Initialize model.\"\"\"\n    super().__init__(edges=edges, source=source, target=target)\n\n    if not weights:\n        edges['weight'] = 1\n        weights = 'weight'\n\n    assert weights in edges.columns, f'\"{weights}\" must be column in edge list'\n\n    # Remove seeds that don't exist\n    self.seeds = edges[edges[self.source].isin(seeds)][self.source].unique()\n\n    if len(self.seeds) == 0:\n        raise ValueError('None of the seeds where among edge list sources.')\n\n    self.weights = weights\n    self.max_steps = max_steps\n\n    if isinstance(traversal_func, type(None)):\n        self.traversal_func = random_linear_activation_function\n    elif callable(traversal_func):\n        self.traversal_func = traversal_func\n    else:\n        raise ValueError('`traversal_func` must be None or a callable')\n</code></pre>"},{"location":"reference/navis/models/#navis.models.TraversalModel.make_summary","title":"<code>make_summary</code>","text":"<p>Generate summary.</p> Source code in <code>navis/models/network_models.py</code> <pre><code>def make_summary(self) -&gt; pd.DataFrame:\n    \"\"\"Generate summary.\"\"\"\n    if not self.has_results:\n        logger.error('Must run simulation first.')\n\n    summary = self.results.groupby('node',\n                                   as_index=False).steps.agg(['min',\n                                                              'max',\n                                                              'mean',\n                                                              'median'])\n\n    summary.rename({'min': 'layer_min',\n                    'mean': 'layer_mean',\n                    'max': 'layer_max',\n                    'median': 'layer_median'}, axis=1, inplace=True)\n\n    self._summary = summary\n    return self._summary\n</code></pre>"},{"location":"reference/navis/models/#navis.models.TraversalModel.run","title":"<code>run</code>","text":"<p>Run model (single process).</p> <p>Use <code>.run_parallel</code> to use parallel processes.</p> Source code in <code>navis/models/network_models.py</code> <pre><code>def run(self, iterations: int = 100, return_iterations=False, **kwargs) -&gt; pd.DataFrame:\n    \"\"\"Run model (single process).\n\n    Use `.run_parallel` to use parallel processes.\n\n    \"\"\"\n    # For some reason this is required for progress bars in Jupyter to show\n    print(' ', end='', flush=True)\n    # For faster access, use the raw array\n    # Note: we're splitting the columns in case we have different datatypes\n    # (e.g. int64 for IDs and float for weights)\n    sources = self.edges[self.source].values\n    targets = self.edges[self.target].values\n    weights = self.edges[self.weights].values\n\n    # For some reason the progress bar does not show unless we have a print here\n    all_enc_nodes = None\n    for it in config.trange(1, iterations + 1,\n                            disable=config.pbar_hide,\n                            leave=config.pbar_leave,\n                            position=kwargs.get('position', 0)):\n        # Set seeds as encountered in step 1\n        enc_nodes = self.seeds\n        enc_steps = np.repeat(1, len(self.seeds))\n        if return_iterations:\n            enc_it = np.repeat(it, len(self.seeds))\n\n        # Start with all edges\n        this_weights = weights\n        this_sources = sources\n        this_targets = targets\n        for i in range(2, self.max_steps + 1):\n            # Which edges have their presynaptic node already traversed?\n            pre_trav = np.isin(this_sources, enc_nodes)\n            # Among those, which edges have the postsynaptic node traversed?\n            post_trav = np.isin(this_targets[pre_trav], enc_nodes)\n\n            # Combine conditions to find edges where the presynaptic node\n            # has been traversed but not the postsynaptic node\n            pre_not_post = np.where(pre_trav)[0][~post_trav]\n            out_targets = this_targets[pre_not_post]\n            out_weights = this_weights[pre_not_post]\n\n            # Drop edges that have already been traversed - speeds up things\n            pre_and_post = np.where(pre_trav)[0][post_trav]\n            this_targets = np.delete(this_targets, pre_and_post, axis=0)\n            this_sources = np.delete(this_sources, pre_and_post, axis=0)\n            this_weights = np.delete(this_weights, pre_and_post, axis=0)\n\n            # Stop if we traversed the entire (reachable) graph\n            if out_targets.size == 0:\n                break\n\n            # Edges traversed in this round\n            trav = self.traversal_func(out_weights)\n            if not trav.sum():\n                continue\n\n            trav_targets = out_targets[trav]\n\n            # Keep track\n            new_trav = np.unique(trav_targets)\n\n            # Store results\n            enc_nodes = np.concatenate((enc_nodes, new_trav))\n            enc_steps = np.concatenate((enc_steps, np.repeat(i, len(new_trav))))\n            if return_iterations:\n                enc_it = np.concatenate((enc_it, np.repeat(it, len(new_trav))))\n\n        # Save this round of traversal\n        if all_enc_nodes is None:\n            all_enc_nodes = enc_nodes\n            all_enc_steps = enc_steps\n            if return_iterations:\n                all_enc_it = enc_it\n        else:\n            all_enc_nodes = np.concatenate((all_enc_nodes, enc_nodes))\n            all_enc_steps = np.concatenate((all_enc_steps, enc_steps))\n            if return_iterations:\n                all_enc_it = np.concatenate((all_enc_it, enc_it))\n\n    self.iterations = iterations\n\n    # Combine results into DataFrame\n    self.results = pd.DataFrame()\n    self.results['steps'] = all_enc_steps\n    self.results['node'] = all_enc_nodes\n    if return_iterations:\n        self.results['iteration'] = all_enc_it\n\n    return self.results\n</code></pre>"},{"location":"reference/navis/models/#navis.models.linear_activation_p","title":"<code>navis.models.linear_activation_p</code>","text":"<p>Linear activation probability.</p> PARAMETER DESCRIPTION <code>w</code> <pre><code>(N, 1) array containing the edge weights.\n</code></pre> <p> TYPE: <code>    np.ndarray</code> </p> <code>min_w</code> <pre><code>Value of `w` at which probability of activation is 0%.\n</code></pre> <p> TYPE: <code>float</code> DEFAULT: <code>0</code> </p> <code>max_w</code> <pre><code>Value of `w` at which probability of activation is 100%.\n</code></pre> <p> TYPE: <code>float</code> DEFAULT: <code>0.3</code> </p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>Probability of activation for each edge.</p> Source code in <code>navis/models/network_models.py</code> <pre><code>def linear_activation_p(\n    w: np.ndarray,\n    min_w: float = 0,\n    max_w: float = .3,\n) -&gt; np.ndarray:\n    \"\"\"Linear activation probability.\n\n    Parameters\n    ----------\n    w :     np.ndarray\n            (N, 1) array containing the edge weights.\n    min_w : float\n            Value of `w` at which probability of activation is 0%.\n    max_w : float\n            Value of `w` at which probability of activation is 100%.\n\n    Returns\n    -------\n    np.ndarray\n            Probability of activation for each edge.\n\n    \"\"\"\n    return np.clip((w - min_w) / (max_w - min_w), 0., 1.)\n</code></pre>"},{"location":"reference/navis/models/#navis.models.random_linear_activation_function","title":"<code>navis.models.random_linear_activation_function</code>","text":"<p>Random linear activation function.</p> PARAMETER DESCRIPTION <code>w</code> <pre><code>(N, 1) array containing the edge weights.\n</code></pre> <p> TYPE: <code>    np.ndarray</code> </p> <code>min_w</code> <pre><code>Value of `w` at which probability of activation is 0%.\n</code></pre> <p> TYPE: <code>float</code> DEFAULT: <code>0</code> </p> <code>max_w</code> <pre><code>Value of `w` at which probability of activation is 100%.\n</code></pre> <p> TYPE: <code>float</code> DEFAULT: <code>0.3</code> </p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>True or False values for each edge.</p> Source code in <code>navis/models/network_models.py</code> <pre><code>def random_linear_activation_function(w: np.ndarray,\n                                      min_w: float = 0,\n                                      max_w: float = .3) -&gt; np.ndarray:\n    \"\"\"Random linear activation function.\n\n    Parameters\n    ----------\n    w :     np.ndarray\n            (N, 1) array containing the edge weights.\n    min_w : float\n            Value of `w` at which probability of activation is 0%.\n    max_w : float\n            Value of `w` at which probability of activation is 100%.\n\n    Returns\n    -------\n    np.ndarray\n            True or False values for each edge.\n\n    \"\"\"\n    return random_activation_function(w, lambda w: linear_activation_p(w, min_w, max_w))\n</code></pre>"},{"location":"reference/navis/morpho/","title":"morpho","text":""},{"location":"reference/navis/morpho/#navis.morpho.cable_length","title":"<code>navis.morpho.cable_length</code>","text":"<p>Calculate cable length.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Neuron(s) for which to calculate cable length.\n</code></pre> <p> TYPE: <code>            TreeNeuron | MeshNeuron | NeuronList</code> </p> <code>mask</code> <pre><code>        If provided, will only consider nodes where\n        `mask` is True. Callable must accept a DataFrame of nodes\n        and return a boolean array of the same length.\n</code></pre> <p> TYPE: <code>         None | boolean array | callable</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>cable_length</code> <p>Cable length of the neuron(s).</p> <p> TYPE: <code>float | array of float</code> </p> Source code in <code>navis/morpho/mmetrics.py</code> <pre><code>@utils.map_neuronlist(desc=\"Cable length\", allow_parallel=True)\n@utils.meshneuron_skeleton(method=\"pass_through\")\ndef cable_length(x, mask=None) -&gt; Union[int, float]:\n    \"\"\"Calculate cable length.\n\n    Parameters\n    ----------\n    x :             TreeNeuron | MeshNeuron | NeuronList\n                    Neuron(s) for which to calculate cable length.\n    mask :          None | boolean array | callable\n                    If provided, will only consider nodes where\n                    `mask` is True. Callable must accept a DataFrame of nodes\n                    and return a boolean array of the same length.\n\n    Returns\n    -------\n    cable_length :  float | array of float\n                    Cable length of the neuron(s).\n\n    \"\"\"\n    utils.eval_param(x, name=\"x\", allowed_types=(core.TreeNeuron,))\n\n    nodes = x.nodes\n    if mask is not None:\n        if callable(mask):\n            mask = mask(x.nodes)\n\n        if isinstance(mask, np.ndarray):\n            if len(mask) != len(x.nodes):\n                raise ValueError(\n                    f\"Length of mask ({len(mask)}) must match number of nodes \"\n                    f\"({len(x.nodes)}).\"\n                )\n        else:\n            raise ValueError(\n                f\"Mask must be callable or boolean array, got {type(mask)}\"\n            )\n\n        nodes = x.nodes.loc[mask, ['node_id','parent_id', 'x', 'y', 'z']].copy()\n\n        # Set the parent IDs to -1 for nodes that are not in the mask\n        nodes.loc[~nodes.parent_id.isin(nodes.node_id), \"parent_id\"] = -1\n\n    if not len(nodes):\n        return 0\n\n    # See if we can use fastcore\n    if not utils.fastcore:\n        # The by far fastest way to get the cable length is to work on the node table\n        # Using the igraph representation is about the same speed... if it is already calculated!\n        # However, one problem with the graph representation is that with large neuronlists\n        # it adds a lot to the memory footprint.\n        not_root = (nodes.parent_id &gt;= 0).values\n        xyz = nodes[[\"x\", \"y\", \"z\"]].values[not_root]\n        xyz_parent = (\n            x.nodes.set_index(\"node_id\")\n            .loc[nodes.parent_id.values[not_root], [\"x\", \"y\", \"z\"]]\n            .values\n        )\n        cable_length = np.sum(np.linalg.norm(xyz - xyz_parent, axis=1))\n    else:\n        cable_length = utils.fastcore.dag.parent_dist(\n            nodes.node_id.values,\n            nodes.parent_id.values,\n            nodes[[\"x\", \"y\", \"z\"]].values,\n            root_dist=0,\n        ).sum()\n\n    return cable_length\n</code></pre>"},{"location":"reference/navis/nbl/","title":"nbl","text":""},{"location":"reference/navis/nbl/#navis.nbl.compress_scores","title":"<code>navis.nbl.compress_scores</code>","text":"<p>Compress scores.</p> <p>This will not necessarily reduce the in-memory footprint but will lead to much smaller file sizes when saved to disk.</p> PARAMETER DESCRIPTION <code>scores</code> <p> TYPE: <code>       pandas.DataFrame</code> </p> <code>threshold</code> <pre><code>        Scores lower than this will be capped at `threshold`.\n</code></pre> <p> TYPE: <code>    float</code> DEFAULT: <code>None</code> </p> <code>digits</code> <pre><code>        Round scores to the Nth digit.\n</code></pre> <p> TYPE: <code>       int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>scores_comp</code> <p>Copy of the original dataframe with the data cast to 32bit floats and the optional filters (see <code>threshold</code> and <code>digits</code>) applied.</p> <p> TYPE: <code>pandas.DataFrame</code> </p> Source code in <code>navis/nbl/utils.py</code> <pre><code>def compress_scores(scores, threshold=None, digits=None):\n    \"\"\"Compress scores.\n\n    This will not necessarily reduce the in-memory footprint but will lead to\n    much smaller file sizes when saved to disk.\n\n    Parameters\n    ----------\n    scores :        pandas.DataFrame\n    threshold :     float, optional\n                    Scores lower than this will be capped at `threshold`.\n    digits :        int, optional\n                    Round scores to the Nth digit.\n\n    Returns\n    -------\n    scores_comp :   pandas.DataFrame\n                    Copy of the original dataframe with the data cast to 32bit\n                    floats and the optional filters (see `threshold` and\n                    `digits`) applied.\n\n    \"\"\"\n    scores = scores.astype(np.float32)\n    if digits is not None:\n        scores = scores.round(digits)\n    if threshold is not None:\n        scores.clip(lower=threshold, inplace=True)\n    return scores\n</code></pre>"},{"location":"reference/navis/nbl/#navis.nbl.dendrogram","title":"<code>navis.nbl.dendrogram</code>","text":"<p>Plot dendrogram.</p> <p>This is just a convenient thin wrapper around scipy's dendrogram function that lets you feed NBLAST scores directly. Note that this causes some overhead for very large NBLASTs.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Pandas DataFrame is assumed to be NBLAST scores. Array is\n        assumed to be a linkage.\n</code></pre> <p> TYPE: <code>            DataFrame | array</code> </p> <code>method</code> <pre><code>        Method for `linkage`. Ignored if `x` is already a linkage.\n</code></pre> <p> TYPE: <code>       str</code> DEFAULT: <code>'ward'</code> </p> <code>**kwargs</code> <pre><code>        Keyword argument passed to scipy's `dendrogram`.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>dendrogram</code> Source code in <code>navis/nbl/utils.py</code> <pre><code>def dendrogram(x, method='ward', **kwargs):\n    \"\"\"Plot dendrogram.\n\n    This is just a convenient thin wrapper around scipy's dendrogram function\n    that lets you feed NBLAST scores directly. Note that this causes some\n    overhead for very large NBLASTs.\n\n    Parameters\n    ----------\n    x :             DataFrame | array\n                    Pandas DataFrame is assumed to be NBLAST scores. Array is\n                    assumed to be a linkage.\n    method :        str\n                    Method for `linkage`. Ignored if `x` is already a linkage.\n    **kwargs\n                    Keyword argument passed to scipy's `dendrogram`.\n\n    Returns\n    -------\n    dendrogram\n\n    \"\"\"\n    # Some sensible defaults that help with large dendrograms\n    DEFAULTS = dict(no_labels=True,\n                    labels=x.index.values.astype(str) if isinstance(x, pd.DataFrame) else None)\n    DEFAULTS.update(kwargs)\n\n    # Make linkage\n    Z = make_linkage(x, method=method)\n\n    return sch.dendrogram(Z, **DEFAULTS)\n</code></pre>"},{"location":"reference/navis/nbl/#navis.nbl.extract_matches","title":"<code>navis.nbl.extract_matches</code>","text":"<p>Extract top matches from score matrix.</p> <p>See <code>N</code>, <code>threshold</code> or <code>percentage</code> for the criterion.</p> PARAMETER DESCRIPTION <code>scores</code> <pre><code>        Score matrix (e.g. from [`navis.nblast`][]).\n</code></pre> <p> TYPE: <code>       pd.DataFrame</code> </p> <code>N</code> <pre><code>        Number of matches to extract.\n</code></pre> <p> TYPE: <code>            int</code> DEFAULT: <code>None</code> </p> <code>threshold</code> <pre><code>        Extract all matches above a given threshold.\n</code></pre> <p> TYPE: <code>    float</code> DEFAULT: <code>None</code> </p> <code>percentage</code> <pre><code>        Extract all matches within a given range of the top match.\n        E.g. `percentage=0.05` will return all matches within\n        5% of the top match.\n</code></pre> <p> TYPE: <code>   float [0-1]</code> DEFAULT: <code>None</code> </p> <code>single_cols</code> <pre><code>        If True will return single columns with comma-separated\n        strings for match ID and match score, respectively.\n</code></pre> <p> TYPE: <code>  bool</code> </p> <code>axis</code> <pre><code>        For which axis to produce matches.\n</code></pre> <p> TYPE: <code>         0 | 1</code> DEFAULT: <code>0</code> </p> <code>distances</code> <pre><code>        Whether `scores` is distances or similarities (i.e. whether\n        we need to look for the lowest instead of the highest values).\n        \"auto\" (default) will infer based on the diagonal of the\n        `scores` matrix. Use boolean to override.\n</code></pre> <p> TYPE: <code>    \"auto\" | bool</code> DEFAULT: <code>'auto'</code> </p> RETURNS DESCRIPTION <code>pd.DataFrame</code> <p>Note that the format is slightly different depending on the criterion.</p> Source code in <code>navis/nbl/utils.py</code> <pre><code>def extract_matches(scores, N=None, threshold=None, percentage=None,\n                    axis=0, distances='auto'):\n    \"\"\"Extract top matches from score matrix.\n\n    See `N`, `threshold` or `percentage` for the criterion.\n\n    Parameters\n    ----------\n    scores :        pd.DataFrame\n                    Score matrix (e.g. from [`navis.nblast`][]).\n    N :             int\n                    Number of matches to extract.\n    threshold :     float\n                    Extract all matches above a given threshold.\n    percentage :    float [0-1]\n                    Extract all matches within a given range of the top match.\n                    E.g. `percentage=0.05` will return all matches within\n                    5% of the top match.\n    single_cols :   bool\n                    If True will return single columns with comma-separated\n                    strings for match ID and match score, respectively.\n    axis :          0 | 1\n                    For which axis to produce matches.\n    distances :     \"auto\" | bool\n                    Whether `scores` is distances or similarities (i.e. whether\n                    we need to look for the lowest instead of the highest values).\n                    \"auto\" (default) will infer based on the diagonal of the\n                    `scores` matrix. Use boolean to override.\n\n    Returns\n    -------\n    pd.DataFrame\n                    Note that the format is slightly different depending on\n                    the criterion.\n\n    \"\"\"\n    assert axis in (0, 1), '`axis` must be 0 or 1'\n\n    if N is None and threshold is None and percentage is None:\n        raise ValueError('Must provide either `N` or `threshold` or '\n                         '`percentage` as criterion for match extraction.')\n    elif len({N, threshold, percentage}) &gt; 2:\n        # We expect {criterion, None}\n        raise ValueError('Please provide either `N`, `threshold` or '\n                         '`percentage` as criterion for match extraction.')\n\n    if distances == 'auto':\n        distances = True if most(np.diag(scores.values).round(2) == 0) else False\n\n    # Transposing is easier than dealing with the different axes further down\n    if axis == 1:\n        scores = scores.T\n\n    if N is not None:\n        return _extract_matches_n(scores,\n                                  N=N,\n                                  distances=distances)\n    elif threshold is not None:\n        return _extract_matches_threshold(scores,\n                                          threshold=threshold,\n                                          distances=distances)\n    elif percentage is not None:\n        return _extract_matches_perc(scores,\n                                     perc=percentage,\n                                     distances=distances)\n</code></pre>"},{"location":"reference/navis/nbl/#navis.nbl.make_clusters","title":"<code>navis.nbl.make_clusters</code>","text":"<p>Form flat clusters.</p> <p>This is a thin wrapper around <code>scipy.cluster.hierarchy.cut_tree</code> and <code>scipy.cluster.hierarchy.fcluster</code> functions.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Pandas DataFrame is assumed to be NBLAST scores. Array is\n        assumed to be a linkage.\n</code></pre> <p> TYPE: <code>            DataFrame | array</code> </p> <code>t</code> <pre><code>        See `method`.\n</code></pre> <p> TYPE: <code>            scalar</code> </p> <code>criterion</code> <pre><code>        Method to use for creating clusters:\n         - `n_clusters` uses `cut_tree` to create `t` clusters\n         - `height` uses `cut_tree` to cut the dendrogram at\n            height `t`\n         - `inconsistent`, `distance`, `maxclust`, etc are passed\n           through to `fcluster`\n</code></pre> <p> TYPE: <code>    str</code> DEFAULT: <code>'n_clusters'</code> </p> <code>method</code> <pre><code>        Method for `linkage`. Ignored if `x` is already a linkage.\n</code></pre> <p> TYPE: <code>       str</code> DEFAULT: <code>'ward'</code> </p> <code>**kwargs</code> <pre><code>        Additional keyword arguments are passed through to the\n        cluster functions `cut_tree` and `fcluster`.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>clusters</code> <p> TYPE: <code>np.ndarray</code> </p> Source code in <code>navis/nbl/utils.py</code> <pre><code>def make_clusters(x, t, criterion='n_clusters', method='ward', **kwargs):\n    \"\"\"Form flat clusters.\n\n    This is a thin wrapper around `scipy.cluster.hierarchy.cut_tree` and\n    `scipy.cluster.hierarchy.fcluster` functions.\n\n    Parameters\n    ----------\n    x :             DataFrame | array\n                    Pandas DataFrame is assumed to be NBLAST scores. Array is\n                    assumed to be a linkage.\n    t :             scalar\n                    See `method`.\n    criterion :     str\n                    Method to use for creating clusters:\n                     - `n_clusters` uses `cut_tree` to create `t` clusters\n                     - `height` uses `cut_tree` to cut the dendrogram at\n                        height `t`\n                     - `inconsistent`, `distance`, `maxclust`, etc are passed\n                       through to `fcluster`\n    method :        str\n                    Method for `linkage`. Ignored if `x` is already a linkage.\n    **kwargs\n                    Additional keyword arguments are passed through to the\n                    cluster functions `cut_tree` and `fcluster`.\n\n    Returns\n    -------\n    clusters :      np.ndarray\n\n    \"\"\"\n    # Make linkage\n    Z = make_linkage(x, method=method)\n\n    if criterion == 'n_clusters':\n        cl = sch.cut_tree(Z, n_clusters=t, **kwargs).flatten()\n    elif criterion == 'height':\n        cl = sch.cut_tree(Z, height=t, **kwargs).flatten()\n    else:\n        cl = sch.fcluster(Z, t=t, criterion=criterion, **kwargs)\n\n    return cl\n</code></pre>"},{"location":"reference/navis/nbl/#navis.nbl.update_scores","title":"<code>navis.nbl.update_scores</code>","text":"<p>Update score matrix by running only new query-&gt;target pairs.</p> PARAMETER DESCRIPTION <code>queries</code> <p> TYPE: <code>      Dotprops</code> </p> <code>targets</code> <p> TYPE: <code>      Dotprops</code> </p> <code>scores_ex</code> <pre><code>        DataFrame with existing scores.\n</code></pre> <p> TYPE: <code>    pandas.DataFrame</code> </p> <code>nblast_func</code> <pre><code>        The NBLAST to use. For example: `navis.nblast`.\n</code></pre> <p> TYPE: <code>  callable</code> </p> <code>**kwargs</code> <pre><code>        Argument passed to `nblast_func`.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>pandas.DataFrame</code> <p>Updated scores.</p> <p>Examples:</p> <p>Mostly for testing but also illustrates the principle:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; nl = navis.example_neurons(n=5)\n&gt;&gt;&gt; dp = navis.make_dotprops(nl, k=5) / 125\n&gt;&gt;&gt; # Full NBLAST\n&gt;&gt;&gt; scores = navis.nblast(dp, dp, n_cores=1)\n&gt;&gt;&gt; # Subset and fill in\n&gt;&gt;&gt; scores2 = navis.nbl.update_scores(dp, dp,\n...                                   scores_ex=scores.iloc[:3, 2:],\n...                                   nblast_func=navis.nblast,\n...                                   n_cores=1)\n&gt;&gt;&gt; np.all(scores == scores2)\nTrue\n</code></pre> Source code in <code>navis/nbl/utils.py</code> <pre><code>def update_scores(queries, targets, scores_ex, nblast_func, **kwargs):\n    \"\"\"Update score matrix by running only new query-&gt;target pairs.\n\n    Parameters\n    ----------\n    queries :       Dotprops\n    targets :       Dotprops\n    scores_ex :     pandas.DataFrame\n                    DataFrame with existing scores.\n    nblast_func :   callable\n                    The NBLAST to use. For example: `navis.nblast`.\n    **kwargs\n                    Argument passed to `nblast_func`.\n\n    Returns\n    -------\n    pandas.DataFrame\n                    Updated scores.\n\n    Examples\n    --------\n\n    Mostly for testing but also illustrates the principle:\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; nl = navis.example_neurons(n=5)\n    &gt;&gt;&gt; dp = navis.make_dotprops(nl, k=5) / 125\n    &gt;&gt;&gt; # Full NBLAST\n    &gt;&gt;&gt; scores = navis.nblast(dp, dp, n_cores=1)\n    &gt;&gt;&gt; # Subset and fill in\n    &gt;&gt;&gt; scores2 = navis.nbl.update_scores(dp, dp,\n    ...                                   scores_ex=scores.iloc[:3, 2:],\n    ...                                   nblast_func=navis.nblast,\n    ...                                   n_cores=1)\n    &gt;&gt;&gt; np.all(scores == scores2)\n    True\n\n    \"\"\"\n    if not callable(nblast_func):\n        raise TypeError('`nblast_func` must be callable.')\n    # The np.isin query is much faster if we force any strings to &lt;U18 by\n    # converting to arrays\n    is_new_q = ~np.isin(queries.id, np.array(scores_ex.index))\n    is_new_t = ~np.isin(targets.id, np.array(scores_ex.columns))\n\n    logger.info(f'Found {is_new_q.sum()} new queries and '\n                f'{is_new_t.sum()} new targets.')\n\n    # Reindex old scores\n    scores = scores_ex.reindex(index=queries.id, columns=targets.id).copy()\n\n    # NBLAST new queries against all targets\n    if 'precision' not in kwargs:\n        kwargs['precision'] = scores.values.dtype\n\n    if any(is_new_q):\n        logger.info(f'Updating new queries -&gt; targets scores')\n        qt = nblast_func(queries[is_new_q], targets, **kwargs)\n        scores.loc[qt.index, qt.columns] = qt.values\n\n    # NBLAST all old queries against new targets\n    if any(is_new_t):\n        logger.info(f'Updating old queries -&gt; new targets scores')\n        tq = nblast_func(queries[~is_new_q], targets[is_new_t], **kwargs)\n        scores.loc[tq.index, tq.columns] = tq.values\n\n    return scores\n</code></pre>"},{"location":"reference/navis/plotting/","title":"plotting","text":""},{"location":"reference/navis/plotting/#navis.plotting.print_skeleton","title":"<code>navis.plotting.print_skeleton</code>","text":"<p>Print skeleton as hierarchical tree.</p> <p>Note: this really only makes sense for small neurons.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Neuron to print.\n</code></pre> <p> TYPE: <code>            TreeNeuron</code> </p> <code>print_func</code> <pre><code>        Function to use for printing. Default is `print`.\n</code></pre> <p> TYPE: <code>   callable</code> DEFAULT: <code>print</code> </p> RETURNS DESCRIPTION <code>None</code> Source code in <code>navis/plotting/flat.py</code> <pre><code>def print_skeleton(x, print_func=print, add_props=None):\n    \"\"\"Print skeleton as hierarchical tree.\n\n    Note: this really only makes sense for small neurons.\n\n    Parameters\n    ----------\n    x :             TreeNeuron\n                    Neuron to print.\n    print_func :    callable, optional\n                    Function to use for printing. Default is `print`.\n\n    Returns\n    -------\n    None\n    \"\"\"\n\n    def _print_tree(node, last=True, header=\"\"):\n        \"\"\"Recursive print function.\"\"\"\n        elbow = \"\u2514\u2500\u2500\"\n        pipe = \"\u2502  \"\n        tee = \"\u251c\u2500\u2500\"\n        blank = \"   \"\n        line = f\"{header}{elbow if last else tee}{node}\"\n        if add_props:\n            line += f\" ({add_props[node]})\"\n        print_func(line)\n        children = list(x.graph.predecessors(node))\n        for i, c in enumerate(children):\n            _print_tree(\n                c,\n                header=header + (blank if last else pipe),\n                last=i == len(children) - 1,\n            )\n\n    if isinstance(add_props, str):\n        add_props = x.nodes.set_index(\"node_id\")[add_props].to_dict()\n    elif isinstance(add_props, (list, np.ndarray, tuple)):\n        if len(add_props) != len(x.nodes):\n            raise ValueError(\"Length of add_props must be the same as number of\")\n        add_props = dict(zip(x.nodes.node_id, add_props))\n    elif not isinstance(add_props, (dict, type(None))):\n        raise ValueError(\n            \"add_props must be either None, a string, list, tuple or dict.\"\n        )\n\n    for root in x.root:\n        _print_tree(root)\n</code></pre>"},{"location":"reference/navis/transforms/","title":"transforms","text":""},{"location":"reference/navis/transforms/#navis.transforms.AffineTransform","title":"<code>navis.transforms.AffineTransform</code>","text":"<p>Affine transformation of 3D spatial data.</p> PARAMETER DESCRIPTION <code>matrix</code> <pre><code>        Affine matrix.\n</code></pre> <p> TYPE: <code>       (4, 4) np.ndarray</code> </p> <p>Examples:</p> <p>A simple scaling transform</p> <pre><code>&gt;&gt;&gt; from navis import transforms\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; M = np.diag([1e3, 1e3, 1e3, 1])\n&gt;&gt;&gt; tr = transforms.affine.AffineTransform(M)\n&gt;&gt;&gt; points = np.array([[0, 0, 0], [1, 1, 1]])\n&gt;&gt;&gt; tr.xform(points)\narray([[   0.,    0.,    0.],\n       [1000., 1000., 1000.]])\n</code></pre> Source code in <code>navis/transforms/affine.py</code> <pre><code>class AffineTransform(BaseTransform):\n    \"\"\"Affine transformation of 3D spatial data.\n\n    Parameters\n    ----------\n    matrix :        (4, 4) np.ndarray\n                    Affine matrix.\n\n    Examples\n    --------\n    A simple scaling transform\n\n    &gt;&gt;&gt; from navis import transforms\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; M = np.diag([1e3, 1e3, 1e3, 1])\n    &gt;&gt;&gt; tr = transforms.affine.AffineTransform(M)\n    &gt;&gt;&gt; points = np.array([[0, 0, 0], [1, 1, 1]])\n    &gt;&gt;&gt; tr.xform(points)\n    array([[   0.,    0.,    0.],\n           [1000., 1000., 1000.]])\n\n    \"\"\"\n\n    def __init__(self, matrix: np.ndarray, direction: str = 'forward'):\n        \"\"\"Initialize transform.\"\"\"\n        assert direction in ('forward', 'inverse')\n\n        self.matrix = matrix\n\n        if direction == 'inverse':\n            self.matrix = np.linalg.inv(self.matrix)\n\n    def __eq__(self, other: 'AffineTransform') -&gt; bool:\n        \"\"\"Implements equality comparison.\"\"\"\n        if isinstance(other, AffineTransform):\n            if np.all(self.matrix == other.matrix):\n                return True\n        return False\n\n    def __neg__(self):\n        \"\"\"Invert direction.\"\"\"\n        x = self.copy()\n\n        # Invert affine matrix\n        x.matrix = np.linalg.inv(x.matrix)\n\n        return x\n\n    def __add__(self, other: 'AffineTransform') -&gt; 'AffineTransform':\n        \"\"\"Add two affine transforms.\"\"\"\n        if isinstance(other, AffineTransform):\n            x = self.copy()\n            x.matrix = np.dot(self.matrix, other.matrix)\n            return x\n        raise ValueError('Can only add `AffineTransform` objects.')\n\n    def copy(self) -&gt; 'AffineTransform':\n        \"\"\"Return copy of transform.\"\"\"\n        # Attributes not to copy\n        no_copy = []\n        # Generate new empty transform\n        x = self.__class__(None)\n        # Override with this neuron's data\n        x.__dict__.update({k: copy.copy(v) for k, v in self.__dict__.items() if k not in no_copy})\n\n        return x\n\n    def xform(self, points: np.ndarray, invert: bool = False) -&gt; np.ndarray:\n        \"\"\"Apply transform to points.\n\n        Parameters\n        ----------\n        points :        np.ndarray\n                        (N, 3) array of x/y/z locations.\n        invert :        bool\n                        If True, will invert the transform.\n\n        Returns\n        -------\n        pointsxf :      np.ndarray\n                        The transformed points.\n\n        \"\"\"\n        points = np.asarray(points)\n\n        if points.ndim != 2 and points.shape[1] != 3:\n            raise ValueError('`points` must be of shape (N, 3)')\n\n        # Add a fourth column to points\n        points_mat = np.ones((points.shape[0], 4))\n        points_mat[:, :3] = points\n\n        # Apply transform\n        if not invert:\n            mat = self.matrix\n        else:\n            mat = np.linalg.inv(self.matrix)\n\n        return np.dot(mat, points_mat.T).T[:, :3]\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.AffineTransform.__init__","title":"<code>__init__</code>","text":"<p>Initialize transform.</p> Source code in <code>navis/transforms/affine.py</code> <pre><code>def __init__(self, matrix: np.ndarray, direction: str = 'forward'):\n    \"\"\"Initialize transform.\"\"\"\n    assert direction in ('forward', 'inverse')\n\n    self.matrix = matrix\n\n    if direction == 'inverse':\n        self.matrix = np.linalg.inv(self.matrix)\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.AffineTransform.copy","title":"<code>copy</code>","text":"<p>Return copy of transform.</p> Source code in <code>navis/transforms/affine.py</code> <pre><code>def copy(self) -&gt; 'AffineTransform':\n    \"\"\"Return copy of transform.\"\"\"\n    # Attributes not to copy\n    no_copy = []\n    # Generate new empty transform\n    x = self.__class__(None)\n    # Override with this neuron's data\n    x.__dict__.update({k: copy.copy(v) for k, v in self.__dict__.items() if k not in no_copy})\n\n    return x\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.AffineTransform.xform","title":"<code>xform</code>","text":"<p>Apply transform to points.</p> PARAMETER DESCRIPTION <code>points</code> <pre><code>        (N, 3) array of x/y/z locations.\n</code></pre> <p> TYPE: <code>       np.ndarray</code> </p> <code>invert</code> <pre><code>        If True, will invert the transform.\n</code></pre> <p> TYPE: <code>       bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>pointsxf</code> <p>The transformed points.</p> <p> TYPE: <code>np.ndarray</code> </p> Source code in <code>navis/transforms/affine.py</code> <pre><code>def xform(self, points: np.ndarray, invert: bool = False) -&gt; np.ndarray:\n    \"\"\"Apply transform to points.\n\n    Parameters\n    ----------\n    points :        np.ndarray\n                    (N, 3) array of x/y/z locations.\n    invert :        bool\n                    If True, will invert the transform.\n\n    Returns\n    -------\n    pointsxf :      np.ndarray\n                    The transformed points.\n\n    \"\"\"\n    points = np.asarray(points)\n\n    if points.ndim != 2 and points.shape[1] != 3:\n        raise ValueError('`points` must be of shape (N, 3)')\n\n    # Add a fourth column to points\n    points_mat = np.ones((points.shape[0], 4))\n    points_mat[:, :3] = points\n\n    # Apply transform\n    if not invert:\n        mat = self.matrix\n    else:\n        mat = np.linalg.inv(self.matrix)\n\n    return np.dot(mat, points_mat.T).T[:, :3]\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.AliasTransform","title":"<code>navis.transforms.AliasTransform</code>","text":"<p>Helper transform that simply passes points through.</p> <p>Useful for defining aliases.</p> Source code in <code>navis/transforms/base.py</code> <pre><code>class AliasTransform(BaseTransform):\n    \"\"\"Helper transform that simply passes points through.\n\n    Useful for defining aliases.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize.\"\"\"\n        pass\n\n    def __neg__(self) -&gt; 'AliasTransform':\n        \"\"\"Invert transform.\"\"\"\n        return self.copy()\n\n    def __eq__(self, other):\n        \"\"\"Check if the same.\"\"\"\n        if isinstance(other, AliasTransform):\n            True\n        return False\n\n    def copy(self):\n        \"\"\"Return copy.\"\"\"\n        x = AliasTransform()\n        x.__dict__.update(self.__dict__)\n        return x\n\n    def xform(self, points: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Pass through.\n\n        Note that the returned points are NOT a copy but the originals.\n        \"\"\"\n        return points\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.AliasTransform.__init__","title":"<code>__init__</code>","text":"<p>Initialize.</p> Source code in <code>navis/transforms/base.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.AliasTransform.copy","title":"<code>copy</code>","text":"<p>Return copy.</p> Source code in <code>navis/transforms/base.py</code> <pre><code>def copy(self):\n    \"\"\"Return copy.\"\"\"\n    x = AliasTransform()\n    x.__dict__.update(self.__dict__)\n    return x\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.AliasTransform.xform","title":"<code>xform</code>","text":"<p>Pass through.</p> <p>Note that the returned points are NOT a copy but the originals.</p> Source code in <code>navis/transforms/base.py</code> <pre><code>def xform(self, points: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Pass through.\n\n    Note that the returned points are NOT a copy but the originals.\n    \"\"\"\n    return points\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.CMTKtransform","title":"<code>navis.transforms.CMTKtransform</code>","text":"<p>CMTK transforms of 3D spatial data.</p> <p>Requires CMTK to be installed.</p> PARAMETER DESCRIPTION <code>regs</code> <pre><code>        Path(s) to CMTK transformations(s).\n</code></pre> <p> TYPE: <code>         str | list of str</code> </p> <code>directions</code> <pre><code>        Direction of transformation. Must provide one direction per\n        `reg`.\n</code></pre> <p> TYPE: <code>   \"forward\" | \"inverse\" | list thereof</code> DEFAULT: <code>'forward'</code> </p> <code>threads</code> <pre><code>        Number of threads to use.\n</code></pre> <p> TYPE: <code>      int</code> DEFAULT: <code>None</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from navis import transforms\n&gt;&gt;&gt; tr = transforms.cmtk.CMTKtransform('/path/to/CMTK_directory.list')\n&gt;&gt;&gt; tr.xform(points)\n</code></pre> Source code in <code>navis/transforms/cmtk.py</code> <pre><code>class CMTKtransform(BaseTransform):\n    \"\"\"CMTK transforms of 3D spatial data.\n\n    Requires [CMTK](https://www.nitrc.org/projects/cmtk/) to be installed.\n\n    Parameters\n    ----------\n    regs :          str | list of str\n                    Path(s) to CMTK transformations(s).\n    directions :    \"forward\" | \"inverse\" | list thereof\n                    Direction of transformation. Must provide one direction per\n                    `reg`.\n    threads :       int, optional\n                    Number of threads to use.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from navis import transforms\n    &gt;&gt;&gt; tr = transforms.cmtk.CMTKtransform('/path/to/CMTK_directory.list')\n    &gt;&gt;&gt; tr.xform(points) # doctest: +SKIP\n\n    \"\"\"\n\n    def __init__(self, regs: list, directions: str = \"forward\", threads: int = None):\n        self.directions = list(utils.make_iterable(directions))\n        for d in self.directions:\n            assert d in (\"forward\", \"inverse\"), (\n                '`direction` must be \"foward\"' f'or \"inverse\", not \"{d}\"'\n            )\n\n        self.regs = list(utils.make_iterable(regs))\n        self.command = \"streamxform\"\n        self.threads = threads\n\n        if len(directions) == 1 and len(regs) &gt;= 1:\n            directions = directions * len(regs)\n\n        if len(self.regs) != len(self.directions):\n            raise ValueError(\"Must provide one direction per regs\")\n\n    def __add__(self, other: \"CMTKtransform\") -&gt; \"CMTKtransform\":\n        \"\"\"Implement addition operator to concatenate transforms.\"\"\"\n        if not isinstance(other, CMTKtransform):\n            raise TypeError(f\"Cannot add {type(other)} to CMTKtransform\")\n\n        if self.command != other.command:\n            raise ValueError(\"Unable to merge CMTKtransforms using different commands.\")\n\n        x = self.copy()\n        x.regs += other.regs\n        x.directions += other.directions\n\n        return x\n\n    def __eq__(self, other: \"CMTKtransform\") -&gt; bool:\n        \"\"\"Implement equality comparison.\"\"\"\n        if isinstance(other, CMTKtransform):\n            if len(self) == len(other):\n                if all([self.regs[i] == other.regs[i] for i in range(len(self))]):\n                    if all(\n                        [\n                            self.directions[i] == other.directions[i]\n                            for i in range(len(self))\n                        ]\n                    ):\n                        return True\n        return False\n\n    def __len__(self) -&gt; int:\n        return len(self.regs)\n\n    def __neg__(self) -&gt; \"CMTKtransform\":\n        \"\"\"Invert direction.\"\"\"\n        x = self.copy()\n\n        # Swap directions\n        x.directions = [\n            {\"forward\": \"inverse\", \"inverse\": \"forward\"}[d] for d in x.directions\n        ]\n\n        # Reverse order\n        x.regs = x.regs[::-1]\n        x.directions = x.directions[::-1]\n\n        return x\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __repr__(self):\n        return f\"CMTKtransform with {len(self)} transform(s)\"\n\n    @staticmethod\n    def from_file(filepath: str, **kwargs) -&gt; \"CMTKtransform\":\n        \"\"\"Generate CMTKtransform from file.\n\n        Parameters\n        ----------\n        filepath :  str\n                    Path to CMTK transform.\n        **kwargs\n                    Keyword arguments passed to CMTKtransform.__init__\n\n        Returns\n        -------\n        CMTKtransform\n\n        \"\"\"\n        defaults = {\"directions\": \"forward\"}\n        defaults.update(kwargs)\n        return CMTKtransform(str(filepath), **defaults)\n\n    def make_args(self, affine_only: bool = False) -&gt; list:\n        \"\"\"Generate arguments passed to subprocess.\"\"\"\n        # Generate the arguments\n        # The actual command (i.e. streamxform)\n        args = [str(_cmtkbin / self.command)]\n\n        if affine_only:\n            args.append(\"--affine-only\")\n\n        if self.threads:\n            args.append(f\"--threads {int(self.threads)}\")\n\n        # Add the regargs\n        args += self.regargs\n\n        return args\n\n    @property\n    def regargs(self) -&gt; list:\n        \"\"\"Generate regargs.\"\"\"\n        regargs = []\n        for i, (reg, dir) in enumerate(zip(self.regs, self.directions)):\n            if dir == \"inverse\":\n                # For the first transform we need to prefix \"--inverse\" with\n                # a solitary \"--\"\n                if i == 0:\n                    regargs.append(\"--\")\n                regargs.append(\"--inverse\")\n            # Note no double quotes!\n            regargs.append(f\"{reg}\")\n        return regargs\n\n    def append(self, transform: \"CMTKtransform\", direction: str = None):\n        \"\"\"Add another transform.\n\n        Parameters\n        ----------\n        transform :     str | CMTKtransform\n                        Either another CMTKtransform or filepath to registration.\n        direction :     \"forward\" | \"inverse\"\n                        Only relevant if transform is filepath.\n\n        \"\"\"\n        if isinstance(transform, CMTKtransform):\n            if self.command != transform.command:\n                raise ValueError(\n                    \"Unable to merge CMTKtransforms using \" \"different commands.\"\n                )\n\n            self.regs += transform.regs\n            self.directions += transform.directions\n        elif isinstance(transform, str):\n            if not direction:\n                raise ValueError(\"Must provide direction along with new transform\")\n            self.regs.append(transform)\n            self.directions.append(direction)\n        else:\n            raise NotImplementedError(\n                f\"Unable to append {type(transform)} to {type(self)}\"\n            )\n\n    def check_if_possible(self, on_error: str = \"raise\"):\n        \"\"\"Check if this transform is possible.\"\"\"\n        if not _cmtkbin:\n            msg = (\n                \"Folder with CMTK binaries not found. Make sure the \"\n                \"directory is in your PATH environment variable.\"\n            )\n            if on_error == \"raise\":\n                raise BaseException(msg)\n            return msg\n        for r in self.regs:\n            if not os.path.isdir(r) and not os.path.isfile(r):\n                msg = f\"Registration {r} not found.\"\n                if on_error == \"raise\":\n                    raise BaseException(msg)\n                return msg\n\n    def copy(self) -&gt; \"CMTKtransform\":\n        \"\"\"Return copy.\"\"\"\n        # Attributes not to copy\n        no_copy = []\n        # Generate new empty transform\n        x = self.__class__(None)\n        # Override with this neuron's data\n        x.__dict__.update(\n            {k: copy.copy(v) for k, v in self.__dict__.items() if k not in no_copy}\n        )\n\n        return x\n\n    def parse_cmtk_output(self, output: str, fail_value=np.nan) -&gt; np.ndarray:\n        r\"\"\"Parse CMTK output.\n\n        Briefly, CMTK output will be a byte literal like this:\n\n            b'311 63 23 \\n275 54 25 \\n'\n\n        In case of failed transforms we will get something like this where the\n        original coordinates are returned with a \"FAILED\" flag\n\n            b'343 72 23 \\n-10 -10 -10 FAILED \\n'\n\n        Parameter\n        ---------\n        output :        tuple of (b'', None)\n                        Stdout of CMTK call.\n        fail_value\n                        Value to use for points that failed to transform. By\n                        default we use `np.nan`.\n\n        Returns\n        -------\n        pointsxf :      (N, 3) numpy array\n                        The parse transformed points.\n\n        \"\"\"\n        # The original stout is tuple where we care only about the second one\n        if isinstance(output, tuple):\n            output = output[0]\n\n        pointsx = []\n        # Split string into rows - lazily using a generator\n        for row in (x.group(1) for x in re.finditer(r\"(.*?) \\n\", output.decode())):\n            # Split into values\n            values = row.split(\" \")\n\n            # If this point failed\n            if len(values) != 3:\n                values = [fail_value] * 3\n            else:\n                values = [float(v) for v in values]\n\n            pointsx.append(values)\n\n        return np.asarray(pointsx)\n\n    def xform(\n        self,\n        points: np.ndarray,\n        affine_only: bool = False,\n        affine_fallback: bool = False,\n    ) -&gt; np.ndarray:\n        \"\"\"Xform data.\n\n        Parameters\n        ----------\n        points :            (N, 3) numpy array | pandas.DataFrame\n                            Points to xform. DataFrame must have x/y/z columns.\n        affine_only :       bool\n                            Whether to apply only the non-rigid affine\n                            transform. This is useful if points are outside\n                            the deformation field and would therefore not\n                            transform properly.\n        affine_fallback :   bool\n                            If True and some points did not transform during the\n                            non-rigid part of the transformation, we will apply\n                            only the affine transformation to those points.\n\n        Returns\n        -------\n        pointsxf :      (N, 3) numpy array\n                        Transformed points. Points that failed to transform will\n                        be `np.nan`.\n\n        \"\"\"\n        self.check_if_possible(on_error=\"raise\")\n\n        if isinstance(points, pd.DataFrame):\n            # Make sure x/y/z columns are present\n            if np.any([c not in points for c in [\"x\", \"y\", \"z\"]]):\n                raise ValueError(\"points DataFrame must have x/y/z columns.\")\n        elif (\n            isinstance(points, np.ndarray) and points.ndim == 2 and points.shape[1] == 3\n        ):\n            points = pd.DataFrame(points, columns=[\"x\", \"y\", \"z\"])\n        else:\n            raise TypeError(\n                \"`points` must be numpy array of shape (N, 3) or \"\n                \"pandas DataFrame with x/y/z columns\"\n            )\n\n        # Generate the result\n        args = self.make_args(affine_only=affine_only)\n        proc = subprocess.Popen(args, stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n\n        # Pipe in the points\n        points_str = points[[\"x\", \"y\", \"z\"]].to_string(index=False, header=False)\n\n        # Do not use proc.stdin.write to avoid output buffer becoming full\n        # before we finish piping in stdin.\n        # proc.stdin.write(points_str.encode())\n        # output = proc.communicate()\n\n        # Read out results\n        # This is equivalent to e.g.:\n        # $ streamxform -args &lt;&lt;&lt; \"10, 10, 10\"\n        output = proc.communicate(input=points_str.encode())\n\n        # If no output, something went wrong\n        if not output[0]:\n            raise utils.CMTKError(\"CMTK produced no output. Check points?\")\n\n        # Xformed points\n        xf = self.parse_cmtk_output(output, fail_value=np.nan)\n\n        # Check if any points not xformed\n        if affine_fallback and not affine_only:\n            not_xf = np.any(np.isnan(xf), axis=1)\n            if np.any(not_xf):\n                xf[not_xf] = self.xform(points.loc[not_xf], affine_only=True)\n\n        return xf\n\n    def to_grid_transform(self, template, absolute: bool = True, verbose: bool = False):\n        \"\"\"Convert to GridTransform via dense deformation field.\n\n        Parameters\n        ----------\n        template :  str | TemplateBrain | (Nx, Ny, Nz, dx, dy, dz) tuple\n                    This defines the bounds and voxel size of the deformation\n                    field. Typically, this would correspond to the source space\n                    (i.e. the moving image). We can work with:\n                      - str: a filepath to a NRRD file\n                      - TemplateBrain: a navis TemplateBrain object\n                      - a tuple/list/array with (Nx, Ny, Nz, dx, dy, dz)\n                        where N is the number of voxels in each dimension\n                        and d is the voxel size.\n        absolute :  bool\n                    Whether to return absolute coordinates or offsets.\n        verbose :   bool\n                    Whether to print CMTK output.\n\n        Returns\n        -------\n        transform:  GridTransform\n                    A few notes on using the resulting GridTransform:\n                      1. The transform will expect input coordinates in voxel space.\n                         See the returned `voxel_size` array!\n                      2. The transformed coordinates, on the other hand, will already\n                         be in physical space (e.g. microns).\n        voxel_size :   (3,) numpy array\n                    The voxel size of the input space. This is important because\n                    the GridTransform will expect input coordinates in voxel\n                    space.\n        \"\"\"\n        from .grid import GridTransform\n\n        dfield, header = self.to_dfield(\n            template, out=None, absolute=absolute, verbose=verbose\n        )\n\n        # Generate the transform\n        transform = GridTransform(\n            np.transpose(dfield, (1, 2, 3, 0)),\n            type=\"coordinates\" if absolute else \"offsets\",\n        )\n        # Get voxel size from header - note that the first row is for the 4th dimension\n        # (i.e. the vector dimension of the offsets/coordinates) which we don't care about\n        space_dir = np.diagonal(header[\"space directions\"][1:])\n\n        return transform, space_dir\n\n    def to_dfield(\n        self, template, out=None, absolute: bool = True, verbose: bool = False\n    ) -&gt; np.ndarray:\n        \"\"\"Convert transform to dense deformation field.\n\n        Parameters\n        ----------\n        template :  str | TemplateBrain | (Nx, Ny, Nz, dx, dy, dz) tuple\n                    This defines the bounds and voxel size of the deformation\n                    field. Typically, this would correspond to the source space\n                    (i.e. the moving image). We can work with:\n                      - str: a filepath to a NRRD file\n                      - TemplateBrain: a navis TemplateBrain object\n                      - a tuple/list/array with (Nx, Ny, Nz, dx, dy, dz)\n                        where N is the number of voxels in each dimension\n                        and d is the voxel size.\n        out :       str, optional\n                    NRRD filepath to save the deformation field. If None (default),\n                    the deformation field will be returned as numpy array.\n        absolute :  bool\n                    Whether to return absolute coordinates or offsets.\n        verbose :   bool\n                    Whether to print CMTK output.\n\n        Returns\n        -------\n        dfield :    np.ndarray\n                    The dense deformation field as (3, Nx, Ny, Nz) numpy array\n                    with either absolute coordinates or offsets where the\n                    first dimension contains the x/y/z coordinates. Note that\n                    the coordinates stored in the field are in physical space\n                    (e.g. microns), not voxel space.\n        header :    dict\n                    The NRRD header associated with the deformation field.\n\n\n        See Also\n        --------\n        CMTKtransform.to_grid_transform\n                    Method to directly convert to GridTransform. Please see\n                    that method's notes for details on how to handle the\n                    deformation field.\n\n        \"\"\"\n        from .templates import TemplateBrain  # avoid circular import\n\n        # Translate template info into shape and voxel size\n        if isinstance(template, TemplateBrain):\n            if not hasattr(template, \"dims\") or not hasattr(template, \"voxdims\"):\n                raise ValueError(\n                    \"TemplateBrain must have `dims` and `voxdims` attributes\"\n                )\n            if len(template.dims) != 3 or len(template.voxdims) != 3:\n                raise ValueError(\n                    \"TemplateBrain `dims` and `voxdims` must be of length 3\"\n                )\n            template = list(template.dims) + list(template.voxdims)\n\n        to_remove = []  # track temporary files to clean up later\n\n        try:\n            if isinstance(template, str):\n                # Assume NRRD filepath\n                if not os.path.isfile(template):\n                    raise ValueError(f\"Template file not found: {template}\")\n                ref_image = template\n            elif isinstance(template, (tuple, list, np.ndarray)):\n                if len(template) != 6:\n                    raise ValueError(\n                        \"`template` tuple/list/array must be of length 6: \"\n                        \"(Nx, Ny, Nz, dx, dy, dz)\"\n                    )\n\n                # Create a temporary NRRD file to use as reference\n                # (we're using zeros because that's highly compressible)\n                im = np.zeros(np.array(template[:3]).astype(int), dtype=np.uint8)\n                header = {\n                    \"space directions\": np.diag(template[3:6]),\n                    \"kinds\": [\"domain\", \"domain\", \"domain\"],\n                    \"space units\": [\"microns\", \"microns\", \"microns\"],\n                    \"space origin\": [0.0, 0.0, 0.0],\n                    \"labels\": [\"x\", \"y\", \"z\"],\n                    \"space dimension\": 3,\n                }\n                with tempfile.NamedTemporaryFile(suffix=\".nrrd\", delete=False) as tf:\n                    nrrd.write(tf.name, im, header=header)\n                    to_remove.append(tf.name)\n                    ref_image = tf.name\n            else:\n                raise TypeError(\n                    f\"`template` must be str, TemplateBrain or tuple, not {type(template)}\"\n                )\n\n            if out is None:\n                outfile = tempfile.NamedTemporaryFile(suffix=\".nrrd\", delete=False).name\n                to_remove.append(outfile)\n            elif isinstance(out, (str, pathlib.Path)):\n                outfile = pathlib.Path(out).resolve()\n            else:\n                raise ValueError(f\"Invalid output type: {type(out)}\")\n\n            # Compile the command\n            args = [str(_cmtkbin / \"xform2dfield\")]\n\n            if absolute:\n                args += [\"--output-absolute\"]\n\n            args += [f\"{outfile}\"]\n            args += [f\"{ref_image}\"]\n            args += self.regargs\n\n            # run the binary\n            # avoid resourcewarnings with null\n            with open(os.devnull, \"w\") as devnull:\n                startupinfo = None\n                if platform.system() == \"Windows\":\n                    startupinfo = subprocess.STARTUPINFO()\n                    startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n                if verbose:\n                    # in debug mode print the output\n                    stdout = None\n                else:\n                    stdout = devnull\n\n                if verbose:\n                    config.logger.info(\"executing: {}\".format(\" \".join(args)))\n                check_call(\n                    args,\n                    stdout=stdout,\n                    stderr=subprocess.STDOUT,\n                    startupinfo=startupinfo,\n                )\n\n            if out is None:\n                # Return transformed image\n                return nrrd.read(outfile)\n            elif verbose:\n                config.logger.info(f\"Transformed image saved to {outfile}\")\n\n        except BaseException:\n            raise\n        finally:\n            # Clean up temporary files\n            for f in to_remove:\n                os.remove(f)\n\n    def xform_image(\n        self,\n        im,\n        target,\n        out=None,\n        interpolation=\"linear\",\n        verbose=False,\n    ):\n        \"\"\"Transform an image using CMTK's reformatx.\n\n        Parameters\n        ----------\n        im :        3D numpy array | filepath\n                    The floating image to transform.\n        target :    str | TemplateBrain | (Nx, Ny, Nz, dx, dy, dz) | (Nx, Ny, Nz, dx, dy, dz, Ox, Oy, Oz)\n                    Defines the target image: dimensions in voxels (N), the voxel size (d) and optionally\n                    an origin (0) for the target image. Can be provided as a string (name of a template),\n                    a TemplateBrain object, a tuple/list/array with the target specs.\n        out :       str, optional\n                    The filepath to save the transformed image. If None (default), will return the\n                    transformed image as np.ndarray.\n        interpolation : \"linear\" | \"nn\" | \"cubic\" | \"pv\" | \"sinc-cosine\" | \"sinc-hamming\"\n                    The interpolation method to use.\n        verbose :   bool\n                    Whether to print CMTK output.\n\n        Returns\n        -------\n        np.ndarray | None\n                    If out is None, returns the transformed image as np.ndarray. Otherwise, None.\n\n        \"\"\"\n        assert interpolation in (\n            \"linear\",\n            \"nn\",\n            \"cubic\",\n            \"pv\",\n            \"sinc-cosine\",\n            \"sinc-hamming\",\n        )\n\n        # `reformatx` expects this format:\n        # ./reformatx --floating {INPUT_FILE} -o {OUTPUT_FILE} {REFERENCE_SPECS} {TRANSFORMS}\n        # where:\n        # - {INPUT_FILE} is the image to transform\n        # - {OUTPUT_FILE} is where the output will be saved\n        # - {REFERENCE_SPECS} defines the target space; this needs to be either a NRRD\n        #   file from which CMTK can extract the target grid or the actual specs:\n        #   \"--target-grid Nx,Ny,Nz:dX,dY,dZ:[Ox,Oy,Oz]\" where N is the number of\n        #   voxels in each dimension and d is the voxel size. The optional O is the\n        #   origin of the image. If not provided, it is assumed to be (0, 0, 0).\n        # - {TRANSFORMS} are the CMTK transform(s) to apply; prefix with \"--inverse\" to invert\n        # Below command works to convert JFRC2 to FCWB:\n        # /opt/local/lib/cmtk/bin/reformatx --verbose --floating JFRC2.nrrd -o JFRC2_xf.nrrd FCWB.nrrd ~/flybrain-data/BridgingRegistrations/FCWB_JFRC2.list\n        # This took 2min 28s - should check if that is actually faster than the look-up approach we use in `images.py`\n        # Note that inversion of transforms always comes with an overhead! When using the inverse transform instead, the above command took 2h 40min!\n\n        if verbose and any((d == \"inverse \" for d in self.directions)):\n            config.logger.warning(\n                \"Using inverse CMTK transforms with reformatx can be very slow! If possible, consider using only forward transforms.\"\n            )\n\n        target_specs = parse_target_specs(target)\n\n        to_remove = []\n        if isinstance(im, (str, pathlib.Path)):\n            floating = pathlib.Path(im)\n            if not im.is_file():\n                raise ValueError(f\"Image file not found: {im}\")\n        elif isinstance(im, np.ndarray):\n            assert im.ndim == 3\n            # Save to temporary file\n            with tempfile.NamedTemporaryFile(suffix=\".nrrd\", delete=False) as tf:\n                nrrd.write(tf.name, im)\n                floating = tf.name\n                to_remove.append(tf.name)\n        else:\n            raise ValueError(f\"Invalid image type: {type(im)}\")\n\n        if out is None:\n            outfile = tempfile.NamedTemporaryFile(suffix=\".nrrd\", delete=False).name\n            to_remove.append(outfile)\n        elif isinstance(out, (str, pathlib.Path)):\n            outfile = pathlib.Path(out).resolve()\n        else:\n            raise ValueError(f\"Invalid output type: {type(out)}\")\n\n        # Compile the command\n        args = [str(_cmtkbin / \"reformatx\")]\n        args += [f\"-o {outfile}\"]\n        args += [f\"--floating {floating}\"]\n        args += [f\"--{interpolation}\"]\n        args += [target_specs]\n\n        # Add the regargs\n        args += self.regargs\n\n        try:\n            # run the binary\n            # avoid resourcewarnings with null\n            with open(os.devnull, \"w\") as devnull:\n                startupinfo = None\n                if platform.system() == \"Windows\":\n                    startupinfo = subprocess.STARTUPINFO()\n                    startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n                if verbose:\n                    # in debug mode print the output\n                    stdout = None\n                else:\n                    stdout = devnull\n\n                if verbose:\n                    config.logger.info(\"executing: {}\".format(\" \".join(args)))\n                check_call(\n                    args,\n                    stdout=stdout,\n                    stderr=subprocess.STDOUT,\n                    startupinfo=startupinfo,\n                )\n\n            if out is None:\n                # Return transformed image\n                return nrrd.read(outfile)\n            elif verbose:\n                config.logger.info(f\"Transformed image saved to {outfile}\")\n        except BaseException:\n            raise\n        finally:\n            # Clean up temporary files\n            for f in to_remove:\n                os.remove(f)\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.CMTKtransform.regargs","title":"<code>regargs: list</code>  <code>property</code>","text":"<p>Generate regargs.</p>"},{"location":"reference/navis/transforms/#navis.transforms.CMTKtransform.append","title":"<code>append</code>","text":"<p>Add another transform.</p> PARAMETER DESCRIPTION <code>transform</code> <pre><code>        Either another CMTKtransform or filepath to registration.\n</code></pre> <p> TYPE: <code>    str | CMTKtransform</code> </p> <code>direction</code> <pre><code>        Only relevant if transform is filepath.\n</code></pre> <p> TYPE: <code>    \"forward\" | \"inverse\"</code> DEFAULT: <code>None</code> </p> Source code in <code>navis/transforms/cmtk.py</code> <pre><code>def append(self, transform: \"CMTKtransform\", direction: str = None):\n    \"\"\"Add another transform.\n\n    Parameters\n    ----------\n    transform :     str | CMTKtransform\n                    Either another CMTKtransform or filepath to registration.\n    direction :     \"forward\" | \"inverse\"\n                    Only relevant if transform is filepath.\n\n    \"\"\"\n    if isinstance(transform, CMTKtransform):\n        if self.command != transform.command:\n            raise ValueError(\n                \"Unable to merge CMTKtransforms using \" \"different commands.\"\n            )\n\n        self.regs += transform.regs\n        self.directions += transform.directions\n    elif isinstance(transform, str):\n        if not direction:\n            raise ValueError(\"Must provide direction along with new transform\")\n        self.regs.append(transform)\n        self.directions.append(direction)\n    else:\n        raise NotImplementedError(\n            f\"Unable to append {type(transform)} to {type(self)}\"\n        )\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.CMTKtransform.check_if_possible","title":"<code>check_if_possible</code>","text":"<p>Check if this transform is possible.</p> Source code in <code>navis/transforms/cmtk.py</code> <pre><code>def check_if_possible(self, on_error: str = \"raise\"):\n    \"\"\"Check if this transform is possible.\"\"\"\n    if not _cmtkbin:\n        msg = (\n            \"Folder with CMTK binaries not found. Make sure the \"\n            \"directory is in your PATH environment variable.\"\n        )\n        if on_error == \"raise\":\n            raise BaseException(msg)\n        return msg\n    for r in self.regs:\n        if not os.path.isdir(r) and not os.path.isfile(r):\n            msg = f\"Registration {r} not found.\"\n            if on_error == \"raise\":\n                raise BaseException(msg)\n            return msg\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.CMTKtransform.copy","title":"<code>copy</code>","text":"<p>Return copy.</p> Source code in <code>navis/transforms/cmtk.py</code> <pre><code>def copy(self) -&gt; \"CMTKtransform\":\n    \"\"\"Return copy.\"\"\"\n    # Attributes not to copy\n    no_copy = []\n    # Generate new empty transform\n    x = self.__class__(None)\n    # Override with this neuron's data\n    x.__dict__.update(\n        {k: copy.copy(v) for k, v in self.__dict__.items() if k not in no_copy}\n    )\n\n    return x\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.CMTKtransform.from_file","title":"<code>from_file</code>  <code>staticmethod</code>","text":"<p>Generate CMTKtransform from file.</p> PARAMETER DESCRIPTION <code>filepath</code> <pre><code>    Path to CMTK transform.\n</code></pre> <p> TYPE: <code> str</code> </p> <code>**kwargs</code> <pre><code>    Keyword arguments passed to CMTKtransform.__init__\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>CMTKtransform</code> Source code in <code>navis/transforms/cmtk.py</code> <pre><code>@staticmethod\ndef from_file(filepath: str, **kwargs) -&gt; \"CMTKtransform\":\n    \"\"\"Generate CMTKtransform from file.\n\n    Parameters\n    ----------\n    filepath :  str\n                Path to CMTK transform.\n    **kwargs\n                Keyword arguments passed to CMTKtransform.__init__\n\n    Returns\n    -------\n    CMTKtransform\n\n    \"\"\"\n    defaults = {\"directions\": \"forward\"}\n    defaults.update(kwargs)\n    return CMTKtransform(str(filepath), **defaults)\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.CMTKtransform.make_args","title":"<code>make_args</code>","text":"<p>Generate arguments passed to subprocess.</p> Source code in <code>navis/transforms/cmtk.py</code> <pre><code>def make_args(self, affine_only: bool = False) -&gt; list:\n    \"\"\"Generate arguments passed to subprocess.\"\"\"\n    # Generate the arguments\n    # The actual command (i.e. streamxform)\n    args = [str(_cmtkbin / self.command)]\n\n    if affine_only:\n        args.append(\"--affine-only\")\n\n    if self.threads:\n        args.append(f\"--threads {int(self.threads)}\")\n\n    # Add the regargs\n    args += self.regargs\n\n    return args\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.CMTKtransform.parse_cmtk_output","title":"<code>parse_cmtk_output</code>","text":"<p>Parse CMTK output.</p> <p>Briefly, CMTK output will be a byte literal like this:</p> <pre><code>b'311 63 23 \\n275 54 25 \\n'\n</code></pre> <p>In case of failed transforms we will get something like this where the original coordinates are returned with a \"FAILED\" flag</p> <pre><code>b'343 72 23 \\n-10 -10 -10 FAILED \\n'\n</code></pre> Parameter <p>output :        tuple of (b'', None)                 Stdout of CMTK call. fail_value                 Value to use for points that failed to transform. By                 default we use <code>np.nan</code>.</p> RETURNS DESCRIPTION <code>pointsxf</code> <p>The parse transformed points.</p> <p> TYPE: <code>(N, 3) numpy array</code> </p> Source code in <code>navis/transforms/cmtk.py</code> <pre><code>def parse_cmtk_output(self, output: str, fail_value=np.nan) -&gt; np.ndarray:\n    r\"\"\"Parse CMTK output.\n\n    Briefly, CMTK output will be a byte literal like this:\n\n        b'311 63 23 \\n275 54 25 \\n'\n\n    In case of failed transforms we will get something like this where the\n    original coordinates are returned with a \"FAILED\" flag\n\n        b'343 72 23 \\n-10 -10 -10 FAILED \\n'\n\n    Parameter\n    ---------\n    output :        tuple of (b'', None)\n                    Stdout of CMTK call.\n    fail_value\n                    Value to use for points that failed to transform. By\n                    default we use `np.nan`.\n\n    Returns\n    -------\n    pointsxf :      (N, 3) numpy array\n                    The parse transformed points.\n\n    \"\"\"\n    # The original stout is tuple where we care only about the second one\n    if isinstance(output, tuple):\n        output = output[0]\n\n    pointsx = []\n    # Split string into rows - lazily using a generator\n    for row in (x.group(1) for x in re.finditer(r\"(.*?) \\n\", output.decode())):\n        # Split into values\n        values = row.split(\" \")\n\n        # If this point failed\n        if len(values) != 3:\n            values = [fail_value] * 3\n        else:\n            values = [float(v) for v in values]\n\n        pointsx.append(values)\n\n    return np.asarray(pointsx)\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.CMTKtransform.to_dfield","title":"<code>to_dfield</code>","text":"<p>Convert transform to dense deformation field.</p> PARAMETER DESCRIPTION <code>template</code> <pre><code>    This defines the bounds and voxel size of the deformation\n    field. Typically, this would correspond to the source space\n    (i.e. the moving image). We can work with:\n      - str: a filepath to a NRRD file\n      - TemplateBrain: a navis TemplateBrain object\n      - a tuple/list/array with (Nx, Ny, Nz, dx, dy, dz)\n        where N is the number of voxels in each dimension\n        and d is the voxel size.\n</code></pre> <p> TYPE: <code> str | TemplateBrain | (Nx, Ny, Nz, dx, dy, dz) tuple</code> </p> <code>out</code> <pre><code>    NRRD filepath to save the deformation field. If None (default),\n    the deformation field will be returned as numpy array.\n</code></pre> <p> TYPE: <code>      str</code> DEFAULT: <code>None</code> </p> <code>absolute</code> <pre><code>    Whether to return absolute coordinates or offsets.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>True</code> </p> <code>verbose</code> <pre><code>    Whether to print CMTK output.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>dfield</code> <p>The dense deformation field as (3, Nx, Ny, Nz) numpy array with either absolute coordinates or offsets where the first dimension contains the x/y/z coordinates. Note that the coordinates stored in the field are in physical space (e.g. microns), not voxel space.</p> <p> TYPE: <code>np.ndarray</code> </p> <code>header</code> <p>The NRRD header associated with the deformation field.</p> <p> TYPE: <code>dict</code> </p> See Also <p>CMTKtransform.to_grid_transform             Method to directly convert to GridTransform. Please see             that method's notes for details on how to handle the             deformation field.</p> Source code in <code>navis/transforms/cmtk.py</code> <pre><code>def to_dfield(\n    self, template, out=None, absolute: bool = True, verbose: bool = False\n) -&gt; np.ndarray:\n    \"\"\"Convert transform to dense deformation field.\n\n    Parameters\n    ----------\n    template :  str | TemplateBrain | (Nx, Ny, Nz, dx, dy, dz) tuple\n                This defines the bounds and voxel size of the deformation\n                field. Typically, this would correspond to the source space\n                (i.e. the moving image). We can work with:\n                  - str: a filepath to a NRRD file\n                  - TemplateBrain: a navis TemplateBrain object\n                  - a tuple/list/array with (Nx, Ny, Nz, dx, dy, dz)\n                    where N is the number of voxels in each dimension\n                    and d is the voxel size.\n    out :       str, optional\n                NRRD filepath to save the deformation field. If None (default),\n                the deformation field will be returned as numpy array.\n    absolute :  bool\n                Whether to return absolute coordinates or offsets.\n    verbose :   bool\n                Whether to print CMTK output.\n\n    Returns\n    -------\n    dfield :    np.ndarray\n                The dense deformation field as (3, Nx, Ny, Nz) numpy array\n                with either absolute coordinates or offsets where the\n                first dimension contains the x/y/z coordinates. Note that\n                the coordinates stored in the field are in physical space\n                (e.g. microns), not voxel space.\n    header :    dict\n                The NRRD header associated with the deformation field.\n\n\n    See Also\n    --------\n    CMTKtransform.to_grid_transform\n                Method to directly convert to GridTransform. Please see\n                that method's notes for details on how to handle the\n                deformation field.\n\n    \"\"\"\n    from .templates import TemplateBrain  # avoid circular import\n\n    # Translate template info into shape and voxel size\n    if isinstance(template, TemplateBrain):\n        if not hasattr(template, \"dims\") or not hasattr(template, \"voxdims\"):\n            raise ValueError(\n                \"TemplateBrain must have `dims` and `voxdims` attributes\"\n            )\n        if len(template.dims) != 3 or len(template.voxdims) != 3:\n            raise ValueError(\n                \"TemplateBrain `dims` and `voxdims` must be of length 3\"\n            )\n        template = list(template.dims) + list(template.voxdims)\n\n    to_remove = []  # track temporary files to clean up later\n\n    try:\n        if isinstance(template, str):\n            # Assume NRRD filepath\n            if not os.path.isfile(template):\n                raise ValueError(f\"Template file not found: {template}\")\n            ref_image = template\n        elif isinstance(template, (tuple, list, np.ndarray)):\n            if len(template) != 6:\n                raise ValueError(\n                    \"`template` tuple/list/array must be of length 6: \"\n                    \"(Nx, Ny, Nz, dx, dy, dz)\"\n                )\n\n            # Create a temporary NRRD file to use as reference\n            # (we're using zeros because that's highly compressible)\n            im = np.zeros(np.array(template[:3]).astype(int), dtype=np.uint8)\n            header = {\n                \"space directions\": np.diag(template[3:6]),\n                \"kinds\": [\"domain\", \"domain\", \"domain\"],\n                \"space units\": [\"microns\", \"microns\", \"microns\"],\n                \"space origin\": [0.0, 0.0, 0.0],\n                \"labels\": [\"x\", \"y\", \"z\"],\n                \"space dimension\": 3,\n            }\n            with tempfile.NamedTemporaryFile(suffix=\".nrrd\", delete=False) as tf:\n                nrrd.write(tf.name, im, header=header)\n                to_remove.append(tf.name)\n                ref_image = tf.name\n        else:\n            raise TypeError(\n                f\"`template` must be str, TemplateBrain or tuple, not {type(template)}\"\n            )\n\n        if out is None:\n            outfile = tempfile.NamedTemporaryFile(suffix=\".nrrd\", delete=False).name\n            to_remove.append(outfile)\n        elif isinstance(out, (str, pathlib.Path)):\n            outfile = pathlib.Path(out).resolve()\n        else:\n            raise ValueError(f\"Invalid output type: {type(out)}\")\n\n        # Compile the command\n        args = [str(_cmtkbin / \"xform2dfield\")]\n\n        if absolute:\n            args += [\"--output-absolute\"]\n\n        args += [f\"{outfile}\"]\n        args += [f\"{ref_image}\"]\n        args += self.regargs\n\n        # run the binary\n        # avoid resourcewarnings with null\n        with open(os.devnull, \"w\") as devnull:\n            startupinfo = None\n            if platform.system() == \"Windows\":\n                startupinfo = subprocess.STARTUPINFO()\n                startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n            if verbose:\n                # in debug mode print the output\n                stdout = None\n            else:\n                stdout = devnull\n\n            if verbose:\n                config.logger.info(\"executing: {}\".format(\" \".join(args)))\n            check_call(\n                args,\n                stdout=stdout,\n                stderr=subprocess.STDOUT,\n                startupinfo=startupinfo,\n            )\n\n        if out is None:\n            # Return transformed image\n            return nrrd.read(outfile)\n        elif verbose:\n            config.logger.info(f\"Transformed image saved to {outfile}\")\n\n    except BaseException:\n        raise\n    finally:\n        # Clean up temporary files\n        for f in to_remove:\n            os.remove(f)\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.CMTKtransform.to_grid_transform","title":"<code>to_grid_transform</code>","text":"<p>Convert to GridTransform via dense deformation field.</p> PARAMETER DESCRIPTION <code>template</code> <pre><code>    This defines the bounds and voxel size of the deformation\n    field. Typically, this would correspond to the source space\n    (i.e. the moving image). We can work with:\n      - str: a filepath to a NRRD file\n      - TemplateBrain: a navis TemplateBrain object\n      - a tuple/list/array with (Nx, Ny, Nz, dx, dy, dz)\n        where N is the number of voxels in each dimension\n        and d is the voxel size.\n</code></pre> <p> TYPE: <code> str | TemplateBrain | (Nx, Ny, Nz, dx, dy, dz) tuple</code> </p> <code>absolute</code> <pre><code>    Whether to return absolute coordinates or offsets.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>True</code> </p> <code>verbose</code> <pre><code>    Whether to print CMTK output.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>transform</code> <p>A few notes on using the resulting GridTransform:   1. The transform will expect input coordinates in voxel space.      See the returned <code>voxel_size</code> array!   2. The transformed coordinates, on the other hand, will already      be in physical space (e.g. microns).</p> <p> TYPE: <code>GridTransform</code> </p> <code>voxel_size</code> <p>The voxel size of the input space. This is important because the GridTransform will expect input coordinates in voxel space.</p> <p> TYPE: <code>(3,) numpy array</code> </p> Source code in <code>navis/transforms/cmtk.py</code> <pre><code>def to_grid_transform(self, template, absolute: bool = True, verbose: bool = False):\n    \"\"\"Convert to GridTransform via dense deformation field.\n\n    Parameters\n    ----------\n    template :  str | TemplateBrain | (Nx, Ny, Nz, dx, dy, dz) tuple\n                This defines the bounds and voxel size of the deformation\n                field. Typically, this would correspond to the source space\n                (i.e. the moving image). We can work with:\n                  - str: a filepath to a NRRD file\n                  - TemplateBrain: a navis TemplateBrain object\n                  - a tuple/list/array with (Nx, Ny, Nz, dx, dy, dz)\n                    where N is the number of voxels in each dimension\n                    and d is the voxel size.\n    absolute :  bool\n                Whether to return absolute coordinates or offsets.\n    verbose :   bool\n                Whether to print CMTK output.\n\n    Returns\n    -------\n    transform:  GridTransform\n                A few notes on using the resulting GridTransform:\n                  1. The transform will expect input coordinates in voxel space.\n                     See the returned `voxel_size` array!\n                  2. The transformed coordinates, on the other hand, will already\n                     be in physical space (e.g. microns).\n    voxel_size :   (3,) numpy array\n                The voxel size of the input space. This is important because\n                the GridTransform will expect input coordinates in voxel\n                space.\n    \"\"\"\n    from .grid import GridTransform\n\n    dfield, header = self.to_dfield(\n        template, out=None, absolute=absolute, verbose=verbose\n    )\n\n    # Generate the transform\n    transform = GridTransform(\n        np.transpose(dfield, (1, 2, 3, 0)),\n        type=\"coordinates\" if absolute else \"offsets\",\n    )\n    # Get voxel size from header - note that the first row is for the 4th dimension\n    # (i.e. the vector dimension of the offsets/coordinates) which we don't care about\n    space_dir = np.diagonal(header[\"space directions\"][1:])\n\n    return transform, space_dir\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.CMTKtransform.xform","title":"<code>xform</code>","text":"<p>Xform data.</p> PARAMETER DESCRIPTION <code>points</code> <pre><code>            Points to xform. DataFrame must have x/y/z columns.\n</code></pre> <p> TYPE: <code>           (N, 3) numpy array | pandas.DataFrame</code> </p> <code>affine_only</code> <pre><code>            Whether to apply only the non-rigid affine\n            transform. This is useful if points are outside\n            the deformation field and would therefore not\n            transform properly.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> <code>affine_fallback</code> <pre><code>            If True and some points did not transform during the\n            non-rigid part of the transformation, we will apply\n            only the affine transformation to those points.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>pointsxf</code> <p>Transformed points. Points that failed to transform will be <code>np.nan</code>.</p> <p> TYPE: <code>(N, 3) numpy array</code> </p> Source code in <code>navis/transforms/cmtk.py</code> <pre><code>def xform(\n    self,\n    points: np.ndarray,\n    affine_only: bool = False,\n    affine_fallback: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Xform data.\n\n    Parameters\n    ----------\n    points :            (N, 3) numpy array | pandas.DataFrame\n                        Points to xform. DataFrame must have x/y/z columns.\n    affine_only :       bool\n                        Whether to apply only the non-rigid affine\n                        transform. This is useful if points are outside\n                        the deformation field and would therefore not\n                        transform properly.\n    affine_fallback :   bool\n                        If True and some points did not transform during the\n                        non-rigid part of the transformation, we will apply\n                        only the affine transformation to those points.\n\n    Returns\n    -------\n    pointsxf :      (N, 3) numpy array\n                    Transformed points. Points that failed to transform will\n                    be `np.nan`.\n\n    \"\"\"\n    self.check_if_possible(on_error=\"raise\")\n\n    if isinstance(points, pd.DataFrame):\n        # Make sure x/y/z columns are present\n        if np.any([c not in points for c in [\"x\", \"y\", \"z\"]]):\n            raise ValueError(\"points DataFrame must have x/y/z columns.\")\n    elif (\n        isinstance(points, np.ndarray) and points.ndim == 2 and points.shape[1] == 3\n    ):\n        points = pd.DataFrame(points, columns=[\"x\", \"y\", \"z\"])\n    else:\n        raise TypeError(\n            \"`points` must be numpy array of shape (N, 3) or \"\n            \"pandas DataFrame with x/y/z columns\"\n        )\n\n    # Generate the result\n    args = self.make_args(affine_only=affine_only)\n    proc = subprocess.Popen(args, stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n\n    # Pipe in the points\n    points_str = points[[\"x\", \"y\", \"z\"]].to_string(index=False, header=False)\n\n    # Do not use proc.stdin.write to avoid output buffer becoming full\n    # before we finish piping in stdin.\n    # proc.stdin.write(points_str.encode())\n    # output = proc.communicate()\n\n    # Read out results\n    # This is equivalent to e.g.:\n    # $ streamxform -args &lt;&lt;&lt; \"10, 10, 10\"\n    output = proc.communicate(input=points_str.encode())\n\n    # If no output, something went wrong\n    if not output[0]:\n        raise utils.CMTKError(\"CMTK produced no output. Check points?\")\n\n    # Xformed points\n    xf = self.parse_cmtk_output(output, fail_value=np.nan)\n\n    # Check if any points not xformed\n    if affine_fallback and not affine_only:\n        not_xf = np.any(np.isnan(xf), axis=1)\n        if np.any(not_xf):\n            xf[not_xf] = self.xform(points.loc[not_xf], affine_only=True)\n\n    return xf\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.CMTKtransform.xform_image","title":"<code>xform_image</code>","text":"<p>Transform an image using CMTK's reformatx.</p> PARAMETER DESCRIPTION <code>im</code> <pre><code>    The floating image to transform.\n</code></pre> <p> TYPE: <code>       3D numpy array | filepath</code> </p> <code>target</code> <pre><code>    Defines the target image: dimensions in voxels (N), the voxel size (d) and optionally\n    an origin (0) for the target image. Can be provided as a string (name of a template),\n    a TemplateBrain object, a tuple/list/array with the target specs.\n</code></pre> <p> TYPE: <code>   str | TemplateBrain | (Nx, Ny, Nz, dx, dy, dz) | (Nx, Ny, Nz, dx, dy, dz, Ox, Oy, Oz)</code> </p> <code>out</code> <pre><code>    The filepath to save the transformed image. If None (default), will return the\n    transformed image as np.ndarray.\n</code></pre> <p> TYPE: <code>      str</code> DEFAULT: <code>None</code> </p> <code>interpolation</code> <pre><code>    The interpolation method to use.\n</code></pre> <p> TYPE: <code>linear | nn | cubic | pv | sinc - cosine | sinc - hamming</code> DEFAULT: <code>'linear'</code> </p> <code>verbose</code> <pre><code>    Whether to print CMTK output.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>np.ndarray | None</code> <p>If out is None, returns the transformed image as np.ndarray. Otherwise, None.</p> Source code in <code>navis/transforms/cmtk.py</code> <pre><code>def xform_image(\n    self,\n    im,\n    target,\n    out=None,\n    interpolation=\"linear\",\n    verbose=False,\n):\n    \"\"\"Transform an image using CMTK's reformatx.\n\n    Parameters\n    ----------\n    im :        3D numpy array | filepath\n                The floating image to transform.\n    target :    str | TemplateBrain | (Nx, Ny, Nz, dx, dy, dz) | (Nx, Ny, Nz, dx, dy, dz, Ox, Oy, Oz)\n                Defines the target image: dimensions in voxels (N), the voxel size (d) and optionally\n                an origin (0) for the target image. Can be provided as a string (name of a template),\n                a TemplateBrain object, a tuple/list/array with the target specs.\n    out :       str, optional\n                The filepath to save the transformed image. If None (default), will return the\n                transformed image as np.ndarray.\n    interpolation : \"linear\" | \"nn\" | \"cubic\" | \"pv\" | \"sinc-cosine\" | \"sinc-hamming\"\n                The interpolation method to use.\n    verbose :   bool\n                Whether to print CMTK output.\n\n    Returns\n    -------\n    np.ndarray | None\n                If out is None, returns the transformed image as np.ndarray. Otherwise, None.\n\n    \"\"\"\n    assert interpolation in (\n        \"linear\",\n        \"nn\",\n        \"cubic\",\n        \"pv\",\n        \"sinc-cosine\",\n        \"sinc-hamming\",\n    )\n\n    # `reformatx` expects this format:\n    # ./reformatx --floating {INPUT_FILE} -o {OUTPUT_FILE} {REFERENCE_SPECS} {TRANSFORMS}\n    # where:\n    # - {INPUT_FILE} is the image to transform\n    # - {OUTPUT_FILE} is where the output will be saved\n    # - {REFERENCE_SPECS} defines the target space; this needs to be either a NRRD\n    #   file from which CMTK can extract the target grid or the actual specs:\n    #   \"--target-grid Nx,Ny,Nz:dX,dY,dZ:[Ox,Oy,Oz]\" where N is the number of\n    #   voxels in each dimension and d is the voxel size. The optional O is the\n    #   origin of the image. If not provided, it is assumed to be (0, 0, 0).\n    # - {TRANSFORMS} are the CMTK transform(s) to apply; prefix with \"--inverse\" to invert\n    # Below command works to convert JFRC2 to FCWB:\n    # /opt/local/lib/cmtk/bin/reformatx --verbose --floating JFRC2.nrrd -o JFRC2_xf.nrrd FCWB.nrrd ~/flybrain-data/BridgingRegistrations/FCWB_JFRC2.list\n    # This took 2min 28s - should check if that is actually faster than the look-up approach we use in `images.py`\n    # Note that inversion of transforms always comes with an overhead! When using the inverse transform instead, the above command took 2h 40min!\n\n    if verbose and any((d == \"inverse \" for d in self.directions)):\n        config.logger.warning(\n            \"Using inverse CMTK transforms with reformatx can be very slow! If possible, consider using only forward transforms.\"\n        )\n\n    target_specs = parse_target_specs(target)\n\n    to_remove = []\n    if isinstance(im, (str, pathlib.Path)):\n        floating = pathlib.Path(im)\n        if not im.is_file():\n            raise ValueError(f\"Image file not found: {im}\")\n    elif isinstance(im, np.ndarray):\n        assert im.ndim == 3\n        # Save to temporary file\n        with tempfile.NamedTemporaryFile(suffix=\".nrrd\", delete=False) as tf:\n            nrrd.write(tf.name, im)\n            floating = tf.name\n            to_remove.append(tf.name)\n    else:\n        raise ValueError(f\"Invalid image type: {type(im)}\")\n\n    if out is None:\n        outfile = tempfile.NamedTemporaryFile(suffix=\".nrrd\", delete=False).name\n        to_remove.append(outfile)\n    elif isinstance(out, (str, pathlib.Path)):\n        outfile = pathlib.Path(out).resolve()\n    else:\n        raise ValueError(f\"Invalid output type: {type(out)}\")\n\n    # Compile the command\n    args = [str(_cmtkbin / \"reformatx\")]\n    args += [f\"-o {outfile}\"]\n    args += [f\"--floating {floating}\"]\n    args += [f\"--{interpolation}\"]\n    args += [target_specs]\n\n    # Add the regargs\n    args += self.regargs\n\n    try:\n        # run the binary\n        # avoid resourcewarnings with null\n        with open(os.devnull, \"w\") as devnull:\n            startupinfo = None\n            if platform.system() == \"Windows\":\n                startupinfo = subprocess.STARTUPINFO()\n                startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n            if verbose:\n                # in debug mode print the output\n                stdout = None\n            else:\n                stdout = devnull\n\n            if verbose:\n                config.logger.info(\"executing: {}\".format(\" \".join(args)))\n            check_call(\n                args,\n                stdout=stdout,\n                stderr=subprocess.STDOUT,\n                startupinfo=startupinfo,\n            )\n\n        if out is None:\n            # Return transformed image\n            return nrrd.read(outfile)\n        elif verbose:\n            config.logger.info(f\"Transformed image saved to {outfile}\")\n    except BaseException:\n        raise\n    finally:\n        # Clean up temporary files\n        for f in to_remove:\n            os.remove(f)\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.ElastixTransform","title":"<code>navis.transforms.ElastixTransform</code>","text":"<p>Elastix transforms of 3D spatial data.</p> <p>Requires Elastix. Based on code by Jasper Phelps (https://github.com/jasper-tms/pytransformix).</p> <p>Note that elastix transforms can not be inverted!</p> PARAMETER DESCRIPTION <code>file</code> <pre><code>            Filepath to elastix transformation file.\n</code></pre> <p> TYPE: <code>             str</code> </p> <code>copy_files</code> <pre><code>            Any files that need to be copied into the temporary\n            directory where we perform the transform. These are\n            typically files supplemental to the main transform\n            file (e.g. defining an additional affine transform).\n</code></pre> <p> TYPE: <code>       filepath | list</code> DEFAULT: <code>[]</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from navis import transforms\n&gt;&gt;&gt; tr = transforms.ElastixTransform('/path/to/transform/transform')\n&gt;&gt;&gt; tr.xform(points)\n</code></pre> Source code in <code>navis/transforms/elastix.py</code> <pre><code>class ElastixTransform(BaseTransform):\n    \"\"\"Elastix transforms of 3D spatial data.\n\n    Requires [Elastix](https://github.com/SuperElastix/elastix/). Based on\n    code by Jasper Phelps (&lt;https://github.com/jasper-tms/pytransformix&gt;).\n\n    Note that elastix transforms can not be inverted!\n\n    Parameters\n    ----------\n    file :              str\n                        Filepath to elastix transformation file.\n    copy_files :        filepath | list, optional\n                        Any files that need to be copied into the temporary\n                        directory where we perform the transform. These are\n                        typically files supplemental to the main transform\n                        file (e.g. defining an additional affine transform).\n\n    Examples\n    --------\n    &gt;&gt;&gt; from navis import transforms\n    &gt;&gt;&gt; tr = transforms.ElastixTransform('/path/to/transform/transform')\n    &gt;&gt;&gt; tr.xform(points) # doctest: +SKIP\n\n    \"\"\"\n\n    def __init__(self, file: str, copy_files=[]):\n        self.file = pathlib.Path(file)\n        self.copy_files = copy_files\n\n    def __eq__(self, other: \"ElastixTransform\") -&gt; bool:\n        \"\"\"Implement equality comparison.\"\"\"\n        if isinstance(other, ElastixTransform):\n            if self.file == other.file:\n                return True\n        return False\n\n    def check_if_possible(self, on_error: str = \"raise\"):\n        \"\"\"Check if this transform is possible.\"\"\"\n        if not _elastixbin:\n            msg = (\n                \"Folder with elastix binaries not found. Make sure the \"\n                \"directory is in your PATH environment variable.\"\n            )\n            if on_error == \"raise\":\n                raise BaseException(msg)\n            return msg\n        if not self.file.is_file():\n            msg = f\"Transformation file {self.file} not found.\"\n            if on_error == \"raise\":\n                raise BaseException(msg)\n                return msg\n\n    def copy(self) -&gt; \"ElastixTransform\":\n        \"\"\"Return copy.\"\"\"\n        # Attributes not to copy\n        no_copy = []\n        # Generate new empty transform\n        x = self.__class__(self.file)\n        # Override with this neuron's data\n        x.__dict__.update(\n            {k: copy.copy(v) for k, v in self.__dict__.items() if k not in no_copy}\n        )\n\n        return x\n\n    def write_input_file(self, points, filepath):\n        \"\"\"Write a numpy array in format required by transformix.\"\"\"\n        with open(filepath, \"w\") as f:\n            f.write(\"point\\n{}\\n\".format(len(points)))\n            for x, y, z in points:\n                f.write(f\"{x:f} {y:f} {z:f}\\n\")\n\n    def read_output_file(self, filepath) -&gt; np.ndarray:\n        \"\"\"Load output file.\n\n        Parameter\n        ---------\n        filepath :      str\n                        Filepath to output file.\n\n        Returns\n        -------\n        pointsxf :      (N, 3) numpy array\n                        The parse transformed points.\n\n        \"\"\"\n        points = []\n        with open(filepath, \"r\") as f:\n            for line in f.readlines():\n                output = line.split(\"OutputPoint = [ \")[1].split(\" ]\")[0]\n                points.append([float(i) for i in output.split(\" \")])\n        return np.array(points)\n\n    def xform(self, points: np.ndarray, return_logs=False) -&gt; np.ndarray:\n        \"\"\"Xform data.\n\n        Parameters\n        ----------\n        points :        (N, 3) numpy array | pandas.DataFrame\n                        Points to xform. DataFrame must have x/y/z columns.\n        return_logs :   bool\n                        If True, will return logs instead of transformed points.\n                        Really only useful for debugging.\n\n        Returns\n        -------\n        pointsxf :      (N, 3) numpy array\n                        Transformed points.\n\n        \"\"\"\n        self.check_if_possible(on_error=\"raise\")\n\n        if isinstance(points, pd.DataFrame):\n            # Make sure x/y/z columns are present\n            if np.any([c not in points for c in [\"x\", \"y\", \"z\"]]):\n                raise ValueError(\"points DataFrame must have x/y/z columns.\")\n            points = points[[\"x\", \"y\", \"z\"]].values\n        elif not (\n            isinstance(points, np.ndarray) and points.ndim == 2 and points.shape[1] == 3\n        ):\n            raise TypeError(\n                \"`points` must be numpy array of shape (N, 3) or \"\n                \"pandas DataFrame with x/y/z columns\"\n            )\n\n        # Everything happens in a temporary directory\n        with tempfile.TemporaryDirectory() as tempdir:\n            p = pathlib.Path(tempdir)\n\n            # If required, copy additional files into the temporary directory\n            if self.copy_files:\n                for f in make_iterable(self.copy_files):\n                    _ = pathlib.Path(shutil.copy(f, p))\n\n            # Write points to file\n            in_file = p / \"inputpoints.txt\"\n            self.write_input_file(points, in_file)\n\n            out_file = p / \"outputpoints.txt\"\n\n            # Prepare the command\n            command = [\n                _elastixbin / \"transformix\",\n                \"-out\",\n                str(p),\n                \"-tp\",\n                str(self.file),\n                \"-def\",\n                str(in_file),\n            ]\n\n            # Keep track of current working directory\n            cwd = os.getcwd()\n            try:\n                # Change working directory to the temporary directory\n                # This is apparently required because elastix stupidly expects\n                # any secondary transform files to be in the current directory\n                # (as opposed to where the main transform is)\n                os.chdir(p)\n                # Run the actual transform\n                proc = subprocess.run(command, stdout=subprocess.PIPE)\n            except BaseException:\n                raise\n            finally:\n                # This makes sure we land on our feet even in case of an error\n                os.chdir(cwd)\n\n            if return_logs:\n                logfile = p / \"transformix.log\"\n                if not logfile.is_file():\n                    raise FileNotFoundError(\"No log file found.\")\n                with open(logfile) as f:\n                    logs = f.read()\n                return logs\n\n            if not out_file.is_file():\n                raise FileNotFoundError(\n                    \"Elastix transform did not produce any \"\n                    f\"output:\\n {proc.stdout.decode()}\"\n                )\n\n            points_xf = self.read_output_file(out_file)\n\n        return points_xf\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.ElastixTransform.check_if_possible","title":"<code>check_if_possible</code>","text":"<p>Check if this transform is possible.</p> Source code in <code>navis/transforms/elastix.py</code> <pre><code>def check_if_possible(self, on_error: str = \"raise\"):\n    \"\"\"Check if this transform is possible.\"\"\"\n    if not _elastixbin:\n        msg = (\n            \"Folder with elastix binaries not found. Make sure the \"\n            \"directory is in your PATH environment variable.\"\n        )\n        if on_error == \"raise\":\n            raise BaseException(msg)\n        return msg\n    if not self.file.is_file():\n        msg = f\"Transformation file {self.file} not found.\"\n        if on_error == \"raise\":\n            raise BaseException(msg)\n            return msg\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.ElastixTransform.copy","title":"<code>copy</code>","text":"<p>Return copy.</p> Source code in <code>navis/transforms/elastix.py</code> <pre><code>def copy(self) -&gt; \"ElastixTransform\":\n    \"\"\"Return copy.\"\"\"\n    # Attributes not to copy\n    no_copy = []\n    # Generate new empty transform\n    x = self.__class__(self.file)\n    # Override with this neuron's data\n    x.__dict__.update(\n        {k: copy.copy(v) for k, v in self.__dict__.items() if k not in no_copy}\n    )\n\n    return x\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.ElastixTransform.read_output_file","title":"<code>read_output_file</code>","text":"<p>Load output file.</p> Parameter <p>filepath :      str                 Filepath to output file.</p> RETURNS DESCRIPTION <code>pointsxf</code> <p>The parse transformed points.</p> <p> TYPE: <code>(N, 3) numpy array</code> </p> Source code in <code>navis/transforms/elastix.py</code> <pre><code>def read_output_file(self, filepath) -&gt; np.ndarray:\n    \"\"\"Load output file.\n\n    Parameter\n    ---------\n    filepath :      str\n                    Filepath to output file.\n\n    Returns\n    -------\n    pointsxf :      (N, 3) numpy array\n                    The parse transformed points.\n\n    \"\"\"\n    points = []\n    with open(filepath, \"r\") as f:\n        for line in f.readlines():\n            output = line.split(\"OutputPoint = [ \")[1].split(\" ]\")[0]\n            points.append([float(i) for i in output.split(\" \")])\n    return np.array(points)\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.ElastixTransform.write_input_file","title":"<code>write_input_file</code>","text":"<p>Write a numpy array in format required by transformix.</p> Source code in <code>navis/transforms/elastix.py</code> <pre><code>def write_input_file(self, points, filepath):\n    \"\"\"Write a numpy array in format required by transformix.\"\"\"\n    with open(filepath, \"w\") as f:\n        f.write(\"point\\n{}\\n\".format(len(points)))\n        for x, y, z in points:\n            f.write(f\"{x:f} {y:f} {z:f}\\n\")\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.ElastixTransform.xform","title":"<code>xform</code>","text":"<p>Xform data.</p> PARAMETER DESCRIPTION <code>points</code> <pre><code>        Points to xform. DataFrame must have x/y/z columns.\n</code></pre> <p> TYPE: <code>       (N, 3) numpy array | pandas.DataFrame</code> </p> <code>return_logs</code> <pre><code>        If True, will return logs instead of transformed points.\n        Really only useful for debugging.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>pointsxf</code> <p>Transformed points.</p> <p> TYPE: <code>(N, 3) numpy array</code> </p> Source code in <code>navis/transforms/elastix.py</code> <pre><code>def xform(self, points: np.ndarray, return_logs=False) -&gt; np.ndarray:\n    \"\"\"Xform data.\n\n    Parameters\n    ----------\n    points :        (N, 3) numpy array | pandas.DataFrame\n                    Points to xform. DataFrame must have x/y/z columns.\n    return_logs :   bool\n                    If True, will return logs instead of transformed points.\n                    Really only useful for debugging.\n\n    Returns\n    -------\n    pointsxf :      (N, 3) numpy array\n                    Transformed points.\n\n    \"\"\"\n    self.check_if_possible(on_error=\"raise\")\n\n    if isinstance(points, pd.DataFrame):\n        # Make sure x/y/z columns are present\n        if np.any([c not in points for c in [\"x\", \"y\", \"z\"]]):\n            raise ValueError(\"points DataFrame must have x/y/z columns.\")\n        points = points[[\"x\", \"y\", \"z\"]].values\n    elif not (\n        isinstance(points, np.ndarray) and points.ndim == 2 and points.shape[1] == 3\n    ):\n        raise TypeError(\n            \"`points` must be numpy array of shape (N, 3) or \"\n            \"pandas DataFrame with x/y/z columns\"\n        )\n\n    # Everything happens in a temporary directory\n    with tempfile.TemporaryDirectory() as tempdir:\n        p = pathlib.Path(tempdir)\n\n        # If required, copy additional files into the temporary directory\n        if self.copy_files:\n            for f in make_iterable(self.copy_files):\n                _ = pathlib.Path(shutil.copy(f, p))\n\n        # Write points to file\n        in_file = p / \"inputpoints.txt\"\n        self.write_input_file(points, in_file)\n\n        out_file = p / \"outputpoints.txt\"\n\n        # Prepare the command\n        command = [\n            _elastixbin / \"transformix\",\n            \"-out\",\n            str(p),\n            \"-tp\",\n            str(self.file),\n            \"-def\",\n            str(in_file),\n        ]\n\n        # Keep track of current working directory\n        cwd = os.getcwd()\n        try:\n            # Change working directory to the temporary directory\n            # This is apparently required because elastix stupidly expects\n            # any secondary transform files to be in the current directory\n            # (as opposed to where the main transform is)\n            os.chdir(p)\n            # Run the actual transform\n            proc = subprocess.run(command, stdout=subprocess.PIPE)\n        except BaseException:\n            raise\n        finally:\n            # This makes sure we land on our feet even in case of an error\n            os.chdir(cwd)\n\n        if return_logs:\n            logfile = p / \"transformix.log\"\n            if not logfile.is_file():\n                raise FileNotFoundError(\"No log file found.\")\n            with open(logfile) as f:\n                logs = f.read()\n            return logs\n\n        if not out_file.is_file():\n            raise FileNotFoundError(\n                \"Elastix transform did not produce any \"\n                f\"output:\\n {proc.stdout.decode()}\"\n            )\n\n        points_xf = self.read_output_file(out_file)\n\n    return points_xf\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.FunctionTransform","title":"<code>navis.transforms.FunctionTransform</code>","text":"<p>Apply custom function as transform.</p> PARAMETER DESCRIPTION <code>func</code> <pre><code>    Function that accepts and returns an (N, 3) array.\n</code></pre> <p> TYPE: <code>     callable</code> </p> Source code in <code>navis/transforms/base.py</code> <pre><code>class FunctionTransform(BaseTransform):\n    \"\"\"Apply custom function as transform.\n\n    Parameters\n    ----------\n    func :      callable\n                Function that accepts and returns an (N, 3) array.\n\n    \"\"\"\n\n    def __init__(self, func):\n        \"\"\"Initialize.\"\"\"\n        if not callable(func):\n            raise TypeError('`func` must be callable')\n        self.func = func\n\n    def __eq__(self, other):\n        \"\"\"Check if the same.\"\"\"\n        if not isinstance(other, FunctionTransform):\n            return False\n        if self.func != other.func:\n            return False\n        return True\n\n    def copy(self):\n        \"\"\"Return copy.\"\"\"\n        x = self.__class__(self.func)\n        x.__dict__.update(self.__dict__)\n        return x\n\n    def xform(self, points: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Xform data.\n\n        Parameters\n        ----------\n        points :        (N, 3) numpy array | pandas.DataFrame\n                        Points to xform. DataFrame must have x/y/z columns.\n\n        Returns\n        -------\n        pointsxf :      (N, 3) numpy array\n                        Transformed points.\n\n        \"\"\"\n        if isinstance(points, pd.DataFrame):\n            # Make sure x/y/z columns are present\n            if np.any([c not in points for c in ['x', 'y', 'z']]):\n                raise ValueError('points DataFrame must have x/y/z columns.')\n            points = points[['x', 'y', 'z']].values\n        elif not (isinstance(points, np.ndarray) and points.ndim == 2 and points.shape[1] == 3):\n            raise TypeError('`points` must be numpy array of shape (N, 3) or '\n                            'pandas DataFrame with x/y/z columns')\n        return self.func(points.copy())\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.FunctionTransform.__init__","title":"<code>__init__</code>","text":"<p>Initialize.</p> Source code in <code>navis/transforms/base.py</code> <pre><code>def __init__(self, func):\n    \"\"\"Initialize.\"\"\"\n    if not callable(func):\n        raise TypeError('`func` must be callable')\n    self.func = func\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.FunctionTransform.copy","title":"<code>copy</code>","text":"<p>Return copy.</p> Source code in <code>navis/transforms/base.py</code> <pre><code>def copy(self):\n    \"\"\"Return copy.\"\"\"\n    x = self.__class__(self.func)\n    x.__dict__.update(self.__dict__)\n    return x\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.FunctionTransform.xform","title":"<code>xform</code>","text":"<p>Xform data.</p> PARAMETER DESCRIPTION <code>points</code> <pre><code>        Points to xform. DataFrame must have x/y/z columns.\n</code></pre> <p> TYPE: <code>       (N, 3) numpy array | pandas.DataFrame</code> </p> RETURNS DESCRIPTION <code>pointsxf</code> <p>Transformed points.</p> <p> TYPE: <code>(N, 3) numpy array</code> </p> Source code in <code>navis/transforms/base.py</code> <pre><code>def xform(self, points: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Xform data.\n\n    Parameters\n    ----------\n    points :        (N, 3) numpy array | pandas.DataFrame\n                    Points to xform. DataFrame must have x/y/z columns.\n\n    Returns\n    -------\n    pointsxf :      (N, 3) numpy array\n                    Transformed points.\n\n    \"\"\"\n    if isinstance(points, pd.DataFrame):\n        # Make sure x/y/z columns are present\n        if np.any([c not in points for c in ['x', 'y', 'z']]):\n            raise ValueError('points DataFrame must have x/y/z columns.')\n        points = points[['x', 'y', 'z']].values\n    elif not (isinstance(points, np.ndarray) and points.ndim == 2 and points.shape[1] == 3):\n        raise TypeError('`points` must be numpy array of shape (N, 3) or '\n                        'pandas DataFrame with x/y/z columns')\n    return self.func(points.copy())\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.GridTransform","title":"<code>navis.transforms.GridTransform</code>","text":"<p>Deformation or coordinate field transform of 3D spatial data.</p> <p>This is effectively a simpler version of the H5transform class and only supports deformation fields stored as numpy arrays in memory.</p> PARAMETER DESCRIPTION <code>field</code> <pre><code>        Deformation/coordinate field. The last dimension must\n        contain the x/y/z coordinates.\n</code></pre> <p> TYPE: <code>        (Nx, Ny, Nz, 3) numpy array</code> </p> <code>type</code> <pre><code>        Whether the field contains absolute coordinates or offsets/displacements.\n        Offsets are added to the input coordinates, while\n        coordinates returned as is.\n</code></pre> <p> TYPE: <code>         \"offsets\" | \"coordinates\"</code> DEFAULT: <code>'offsets'</code> </p> <code>spacing</code> <pre><code>        Spacing of the deformation field in x/y/z. If not provided,\n        spacing of 1 is assumed.\n</code></pre> <p> TYPE: <code>      tuple | list | numpy array</code> DEFAULT: <code>None</code> </p> <code>offset</code> <pre><code>        Offset of the deformation field in x/y/z. If not provided,\n        offset of 0 is assumed. N.B. offsets are applied _after_ spacing,\n        i.e. need to be provided in voxel space of the deformation field.\n</code></pre> <p> TYPE: <code>       tuple | list | numpy array</code> DEFAULT: <code>None</code> </p> Source code in <code>navis/transforms/grid.py</code> <pre><code>class GridTransform(BaseTransform):\n    \"\"\"Deformation or coordinate field transform of 3D spatial data.\n\n    This is effectively a simpler version of the H5transform class and\n    only supports deformation fields stored as numpy arrays in memory.\n\n    Parameters\n    ----------\n    field :         (Nx, Ny, Nz, 3) numpy array\n                    Deformation/coordinate field. The last dimension must\n                    contain the x/y/z coordinates.\n    type :          \"offsets\" | \"coordinates\"\n                    Whether the field contains absolute coordinates or offsets/displacements.\n                    Offsets are added to the input coordinates, while\n                    coordinates returned as is.\n    spacing :       tuple | list | numpy array, optional\n                    Spacing of the deformation field in x/y/z. If not provided,\n                    spacing of 1 is assumed.\n    offset :        tuple | list | numpy array, optional\n                    Offset of the deformation field in x/y/z. If not provided,\n                    offset of 0 is assumed. N.B. offsets are applied _after_ spacing,\n                    i.e. need to be provided in voxel space of the deformation field.\n\n    \"\"\"\n\n    def __init__(\n        self, field: np.ndarray, type: str = \"offsets\", spacing=None, offset=None\n    ):\n        \"\"\"Init class.\"\"\"\n        assert (\n            field.ndim == 4 and field.shape[-1] == 3\n        ), \"Field must be a 4D numpy array with the last dimension of size 3.\"\n        assert type in (\n            \"coordinates\",\n            \"offsets\",\n        ), \"type must be 'coordinates' or 'offsets'.\"\n        self.field = field\n        self.type = type\n        self.dtype = field.dtype\n        self.spacing = spacing\n        self.offset = offset\n\n    def __eq__(self, other) -&gt; bool:\n        \"\"\"Compare with other Transform.\"\"\"\n        if isinstance(other, GridTransform):\n            if np.array_equal(self.field, other.field):\n                return True\n        return False\n\n    def __neg__(self) -&gt; \"GridTransform\":\n        \"\"\"Invert direction.\"\"\"\n        # Note to future self: we could implement this by computing the inverse\n        # deformation field using fixed-point iteration, but that's\n        # non-trivial and not needed right now.\n        raise NotImplementedError(\"Inversion of GridTransform is not implemented.\")\n\n    @property\n    def spacing(self):\n        return self._spacing\n\n    @spacing.setter\n    def spacing(self, value):\n        if value is None:\n            self._spacing = None\n        else:\n            value = np.asarray(value)\n            assert (\n                value.ndim == 1 and value.size == 3\n            ), \"spacing must be a tuple/list/array of size 3.\"\n            self._spacing = np.array(value, dtype=self.dtype)\n\n    @property\n    def offset(self):\n        return self._offset\n\n    @offset.setter\n    def offset(self, value):\n        if value is None:\n            self._offset = None\n        else:\n            value = np.asarray(value)\n            assert (\n                value.ndim == 1 and value.size == 3\n            ), \"offset must be a tuple/list/array of size 3.\"\n            self._offset = np.array(value, dtype=self.dtype)\n\n    @property\n    def affine(self) -&gt; AffineTransform:\n        \"\"\"Return affine part of the transform.\"\"\"\n        # We're delaying the calculation of the affine part until it's needed\n        if not hasattr(self, \"_affine\"):\n            self.calculate_affine()\n        return self._affine\n\n    @property\n    def shape(self) -&gt; tuple:\n        \"\"\"Return shape of the deformation field.\"\"\"\n        return self.field.shape\n\n    def calculate_affine(self) -&gt; None:\n        \"\"\"Calculate affine part of the transform.\"\"\"\n        # The strategy here is this:\n        # 1. Take the 8 corners of the deformation field\n        # 2. Transform them using the deformation field\n        # 3. Treat them as landmarks and compute the affine transform using morphops\n\n        mx = np.array(self.shape) - 1  # max indices in each dimension\n        points = np.array(\n            [\n                [0, 0, 0],\n                [0, 0, mx[2]],\n                [0, mx[1], 0],\n                [0, mx[1], mx[2]],\n                [mx[0], 0, 0],\n                [mx[0], 0, mx[2]],\n                [mx[0], mx[1], 0],\n                [mx[0], mx[1], mx[2]],\n            ]\n        )\n\n        if self.offset is not None:\n            points = points - self.offset[np.newaxis, :]\n\n        if self.spacing is not None:\n            points = points * self.spacing[np.newaxis, :]\n\n        points_xf = self.xform(points, affine_fallback=False)\n\n        m = TPStransform(points, points_xf).matrix_rigid\n\n        # Calculate the affine part as the mean displacement across the field\n        self._affine = AffineTransform(m)\n\n    def copy(self, copy_data: bool = False) -&gt; \"GridTransform\":\n        \"\"\"Return copy.\"\"\"\n        if copy_data:\n            return GridTransform(self.field.copy(), type=self.type)\n        else:\n            return GridTransform(self.field, type=self.type)\n\n    @classmethod\n    def from_file(cls, filepath: str) -&gt; \"GridTransform\":\n        \"\"\"Create GridTransform a file.\n\n        Parameters\n        ----------\n        file :          str\n                        Path to file. Currently supported formats:\n                          - NRRD files with deformation fields\n                          - Numpy .npy files with deformation fields\n                          - Nifti files (.nii, .nii.gz) with deformation fields\n\n        Returns\n        -------\n        GridTransform instance.\n\n        \"\"\"\n        filepath = Path(filepath)\n        if not filepath.exists():\n            raise FileNotFoundError(f\"File {filepath} does not exist.\")\n\n        if filepath.suffix in [\".npy\"]:\n            field = np.load(filepath)\n        elif filepath.suffix in [\".nii\", \".nii.gz\"]:\n            try:\n                import nibabel as nib\n            except ModuleNotFoundError:\n                raise ImportError(\n                    \"`nibabel` package is required to read Nifti files (.nii, .nii.gz)\"\n                )\n\n            img = nib.load(str(filepath))\n            field = img.get_fdata()\n        elif filepath.suffix in [\".nrrd\"]:\n            import nrrd\n\n            field, _ = nrrd.read(str(filepath))\n        else:\n            raise ValueError(\n                f\"Unsupported file format: {filepath.suffix}. \"\n                \"Supported formats are: .npy, .nii, .nii.gz, .nrrd\"\n            )\n\n        return cls(field)\n\n    @classmethod\n    def from_warpfield(cls, warpfield):\n        \"\"\"Create GridTransform from a Warpfield deformation field.\n\n        Parameters\n        ----------\n        warpfield :     warpfield.WarpMap | str\n                        Warpfield WarpMap instance or path to a WarpMap h5 file.\n\n        \"\"\"\n        if isinstance(warpfield, str):\n            import h5py\n\n            with h5py.File(warpfield, \"r\") as h5:\n                wm = h5[\"warp_map\"]\n                field = wm[\"warp_field\"][:]\n                block_size = wm[\"block_size\"][:]\n                block_stride = wm[\"block_stride\"][:]\n                # mov_shape = wm[\"moving_shape\"][:]\n                # ref_shape = wm[\"ref_shape\"][:]\n        else:\n            field = warpfield.warp_field\n            block_size = warpfield.block_size\n            block_stride = warpfield.block_stride\n            # mov_shape = warpfield.mov_shape\n            # ref_shape = warpfield.ref_shape\n\n        # Reshape the field from (3, X, Y, Z) to (X, Y, Z, 3)\n        field = np.moveaxis(field, 0, -1)\n\n        spacing = block_stride\n        offset = -block_size / block_stride / 2\n        # Note to self regarding the offset: this code is taken straight from warpfield.\n\n        return cls(field, type=\"offsets\", spacing=spacing, offset=offset)\n\n    def xform(\n        self,\n        points: np.ndarray,\n        affine_fallback: bool = False,\n    ) -&gt; np.ndarray:\n        \"\"\"Xform data.\n\n        Parameters\n        ----------\n        points :            (N, 3) numpy array | pandas.DataFrame\n                            Points to xform. DataFrame must have x/y/z columns.\n        affine_fallback :   bool\n                            If True, points that are outside the deformation field\n                            will be transformed using an affine transform defined by\n                            the affine part of the deformation field.\n                            If False, points outside the field will be set to np.nan.\n\n        Returns\n        -------\n        pointsxf :          (N, 3) numpy.ndarray\n                            Transformed points. Will contain `np.nan` for points\n                            that did not transform.\n\n        \"\"\"\n        if isinstance(points, pd.DataFrame):\n            # Make sure x/y/z columns are present\n            if np.any([c not in points for c in [\"x\", \"y\", \"z\"]]):\n                raise ValueError(\"points DataFrame must have x/y/z columns.\")\n            points = points[[\"x\", \"y\", \"z\"]].values\n\n        if (\n            not isinstance(points, np.ndarray)\n            or points.ndim != 2\n            or points.shape[1] != 3\n        ):\n            raise TypeError(\n                \"`points` must be numpy array of shape (N, 3) or \"\n                \"pandas DataFrame with x/y/z columns\"\n            )\n\n        points_vxl = points\n\n        if self.spacing is not None:\n            points_vxl = points_vxl / self.spacing[np.newaxis, :]\n\n        if self.offset is not None:\n            points_vxl = points_vxl + self.offset[np.newaxis, :]\n\n        # For interpolation, we need to split the offsets into their x, y\n        # and z component\n        xgrid = self.field[:, :, :, 0]\n        ygrid = self.field[:, :, :, 1]\n        zgrid = self.field[:, :, :, 2]\n\n        # Prepare points for interpolation\n        xx = np.arange(0, self.shape[0])\n        yy = np.arange(0, self.shape[1])\n        zz = np.arange(0, self.shape[2])\n\n        # The RegularGridInterpolator is the fastest but the results are\n        # are ever so slightly (4th decimal) different from the Java implementation\n        xinterp = RegularGridInterpolator(\n            (xx, yy, zz), xgrid, bounds_error=False, fill_value=0\n        )\n        yinterp = RegularGridInterpolator(\n            (xx, yy, zz), ygrid, bounds_error=False, fill_value=0\n        )\n        zinterp = RegularGridInterpolator(\n            (xx, yy, zz), zgrid, bounds_error=False, fill_value=0\n        )\n\n        # Before we interpolate check how many points are outside the deformation field\n        is_out = (points_vxl.min(axis=1) &lt; 0) | np.any(points_vxl &gt;= self.shape[:-1], axis=1)\n\n        # Prepare output array\n        if self.type == \"coordinates\":\n            points_xf = np.zeros(points.shape, dtype=self.dtype)\n        else:  # offsets\n            points_xf = points.astype(self.dtype, copy=True)\n\n        if is_out.any():\n            if not affine_fallback:\n                points_xf[is_out, :] = np.nan\n            else:\n                # Apply affine part to out-of-bounds points\n                points_xf[is_out, :] = self.affine.xform(\n                    points[is_out, :],\n                )\n\n        # Interpolate coordinates, re-combine to an x/y/z array and\n        # add to the input points (if coordinates, the points are zeroed out above)\n        if not is_out.all():\n            points_xf[~is_out, :] += np.vstack(\n                (\n                    xinterp(points_vxl[~is_out, :], method=\"linear\"),\n                    yinterp(points_vxl[~is_out, :], method=\"linear\"),\n                    zinterp(points_vxl[~is_out, :], method=\"linear\"),\n                )\n            ).T.astype(self.dtype)\n\n        return points_xf\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.GridTransform.affine","title":"<code>affine: AffineTransform</code>  <code>property</code>","text":"<p>Return affine part of the transform.</p>"},{"location":"reference/navis/transforms/#navis.transforms.GridTransform.shape","title":"<code>shape: tuple</code>  <code>property</code>","text":"<p>Return shape of the deformation field.</p>"},{"location":"reference/navis/transforms/#navis.transforms.GridTransform.__init__","title":"<code>__init__</code>","text":"<p>Init class.</p> Source code in <code>navis/transforms/grid.py</code> <pre><code>def __init__(\n    self, field: np.ndarray, type: str = \"offsets\", spacing=None, offset=None\n):\n    \"\"\"Init class.\"\"\"\n    assert (\n        field.ndim == 4 and field.shape[-1] == 3\n    ), \"Field must be a 4D numpy array with the last dimension of size 3.\"\n    assert type in (\n        \"coordinates\",\n        \"offsets\",\n    ), \"type must be 'coordinates' or 'offsets'.\"\n    self.field = field\n    self.type = type\n    self.dtype = field.dtype\n    self.spacing = spacing\n    self.offset = offset\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.GridTransform.calculate_affine","title":"<code>calculate_affine</code>","text":"<p>Calculate affine part of the transform.</p> Source code in <code>navis/transforms/grid.py</code> <pre><code>def calculate_affine(self) -&gt; None:\n    \"\"\"Calculate affine part of the transform.\"\"\"\n    # The strategy here is this:\n    # 1. Take the 8 corners of the deformation field\n    # 2. Transform them using the deformation field\n    # 3. Treat them as landmarks and compute the affine transform using morphops\n\n    mx = np.array(self.shape) - 1  # max indices in each dimension\n    points = np.array(\n        [\n            [0, 0, 0],\n            [0, 0, mx[2]],\n            [0, mx[1], 0],\n            [0, mx[1], mx[2]],\n            [mx[0], 0, 0],\n            [mx[0], 0, mx[2]],\n            [mx[0], mx[1], 0],\n            [mx[0], mx[1], mx[2]],\n        ]\n    )\n\n    if self.offset is not None:\n        points = points - self.offset[np.newaxis, :]\n\n    if self.spacing is not None:\n        points = points * self.spacing[np.newaxis, :]\n\n    points_xf = self.xform(points, affine_fallback=False)\n\n    m = TPStransform(points, points_xf).matrix_rigid\n\n    # Calculate the affine part as the mean displacement across the field\n    self._affine = AffineTransform(m)\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.GridTransform.copy","title":"<code>copy</code>","text":"<p>Return copy.</p> Source code in <code>navis/transforms/grid.py</code> <pre><code>def copy(self, copy_data: bool = False) -&gt; \"GridTransform\":\n    \"\"\"Return copy.\"\"\"\n    if copy_data:\n        return GridTransform(self.field.copy(), type=self.type)\n    else:\n        return GridTransform(self.field, type=self.type)\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.GridTransform.from_file","title":"<code>from_file</code>  <code>classmethod</code>","text":"<p>Create GridTransform a file.</p> PARAMETER DESCRIPTION <code>file</code> <pre><code>        Path to file. Currently supported formats:\n          - NRRD files with deformation fields\n          - Numpy .npy files with deformation fields\n          - Nifti files (.nii, .nii.gz) with deformation fields\n</code></pre> <p> TYPE: <code>         str</code> </p> RETURNS DESCRIPTION <code>GridTransform instance.</code> Source code in <code>navis/transforms/grid.py</code> <pre><code>@classmethod\ndef from_file(cls, filepath: str) -&gt; \"GridTransform\":\n    \"\"\"Create GridTransform a file.\n\n    Parameters\n    ----------\n    file :          str\n                    Path to file. Currently supported formats:\n                      - NRRD files with deformation fields\n                      - Numpy .npy files with deformation fields\n                      - Nifti files (.nii, .nii.gz) with deformation fields\n\n    Returns\n    -------\n    GridTransform instance.\n\n    \"\"\"\n    filepath = Path(filepath)\n    if not filepath.exists():\n        raise FileNotFoundError(f\"File {filepath} does not exist.\")\n\n    if filepath.suffix in [\".npy\"]:\n        field = np.load(filepath)\n    elif filepath.suffix in [\".nii\", \".nii.gz\"]:\n        try:\n            import nibabel as nib\n        except ModuleNotFoundError:\n            raise ImportError(\n                \"`nibabel` package is required to read Nifti files (.nii, .nii.gz)\"\n            )\n\n        img = nib.load(str(filepath))\n        field = img.get_fdata()\n    elif filepath.suffix in [\".nrrd\"]:\n        import nrrd\n\n        field, _ = nrrd.read(str(filepath))\n    else:\n        raise ValueError(\n            f\"Unsupported file format: {filepath.suffix}. \"\n            \"Supported formats are: .npy, .nii, .nii.gz, .nrrd\"\n        )\n\n    return cls(field)\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.GridTransform.from_warpfield","title":"<code>from_warpfield</code>  <code>classmethod</code>","text":"<p>Create GridTransform from a Warpfield deformation field.</p> PARAMETER DESCRIPTION <code>warpfield</code> <pre><code>        Warpfield WarpMap instance or path to a WarpMap h5 file.\n</code></pre> <p> TYPE: <code>    warpfield.WarpMap | str</code> </p> Source code in <code>navis/transforms/grid.py</code> <pre><code>@classmethod\ndef from_warpfield(cls, warpfield):\n    \"\"\"Create GridTransform from a Warpfield deformation field.\n\n    Parameters\n    ----------\n    warpfield :     warpfield.WarpMap | str\n                    Warpfield WarpMap instance or path to a WarpMap h5 file.\n\n    \"\"\"\n    if isinstance(warpfield, str):\n        import h5py\n\n        with h5py.File(warpfield, \"r\") as h5:\n            wm = h5[\"warp_map\"]\n            field = wm[\"warp_field\"][:]\n            block_size = wm[\"block_size\"][:]\n            block_stride = wm[\"block_stride\"][:]\n            # mov_shape = wm[\"moving_shape\"][:]\n            # ref_shape = wm[\"ref_shape\"][:]\n    else:\n        field = warpfield.warp_field\n        block_size = warpfield.block_size\n        block_stride = warpfield.block_stride\n        # mov_shape = warpfield.mov_shape\n        # ref_shape = warpfield.ref_shape\n\n    # Reshape the field from (3, X, Y, Z) to (X, Y, Z, 3)\n    field = np.moveaxis(field, 0, -1)\n\n    spacing = block_stride\n    offset = -block_size / block_stride / 2\n    # Note to self regarding the offset: this code is taken straight from warpfield.\n\n    return cls(field, type=\"offsets\", spacing=spacing, offset=offset)\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.GridTransform.xform","title":"<code>xform</code>","text":"<p>Xform data.</p> PARAMETER DESCRIPTION <code>points</code> <pre><code>            Points to xform. DataFrame must have x/y/z columns.\n</code></pre> <p> TYPE: <code>           (N, 3) numpy array | pandas.DataFrame</code> </p> <code>affine_fallback</code> <pre><code>            If True, points that are outside the deformation field\n            will be transformed using an affine transform defined by\n            the affine part of the deformation field.\n            If False, points outside the field will be set to np.nan.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>pointsxf</code> <p>Transformed points. Will contain <code>np.nan</code> for points that did not transform.</p> <p> TYPE: <code>(N, 3) numpy.ndarray</code> </p> Source code in <code>navis/transforms/grid.py</code> <pre><code>def xform(\n    self,\n    points: np.ndarray,\n    affine_fallback: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Xform data.\n\n    Parameters\n    ----------\n    points :            (N, 3) numpy array | pandas.DataFrame\n                        Points to xform. DataFrame must have x/y/z columns.\n    affine_fallback :   bool\n                        If True, points that are outside the deformation field\n                        will be transformed using an affine transform defined by\n                        the affine part of the deformation field.\n                        If False, points outside the field will be set to np.nan.\n\n    Returns\n    -------\n    pointsxf :          (N, 3) numpy.ndarray\n                        Transformed points. Will contain `np.nan` for points\n                        that did not transform.\n\n    \"\"\"\n    if isinstance(points, pd.DataFrame):\n        # Make sure x/y/z columns are present\n        if np.any([c not in points for c in [\"x\", \"y\", \"z\"]]):\n            raise ValueError(\"points DataFrame must have x/y/z columns.\")\n        points = points[[\"x\", \"y\", \"z\"]].values\n\n    if (\n        not isinstance(points, np.ndarray)\n        or points.ndim != 2\n        or points.shape[1] != 3\n    ):\n        raise TypeError(\n            \"`points` must be numpy array of shape (N, 3) or \"\n            \"pandas DataFrame with x/y/z columns\"\n        )\n\n    points_vxl = points\n\n    if self.spacing is not None:\n        points_vxl = points_vxl / self.spacing[np.newaxis, :]\n\n    if self.offset is not None:\n        points_vxl = points_vxl + self.offset[np.newaxis, :]\n\n    # For interpolation, we need to split the offsets into their x, y\n    # and z component\n    xgrid = self.field[:, :, :, 0]\n    ygrid = self.field[:, :, :, 1]\n    zgrid = self.field[:, :, :, 2]\n\n    # Prepare points for interpolation\n    xx = np.arange(0, self.shape[0])\n    yy = np.arange(0, self.shape[1])\n    zz = np.arange(0, self.shape[2])\n\n    # The RegularGridInterpolator is the fastest but the results are\n    # are ever so slightly (4th decimal) different from the Java implementation\n    xinterp = RegularGridInterpolator(\n        (xx, yy, zz), xgrid, bounds_error=False, fill_value=0\n    )\n    yinterp = RegularGridInterpolator(\n        (xx, yy, zz), ygrid, bounds_error=False, fill_value=0\n    )\n    zinterp = RegularGridInterpolator(\n        (xx, yy, zz), zgrid, bounds_error=False, fill_value=0\n    )\n\n    # Before we interpolate check how many points are outside the deformation field\n    is_out = (points_vxl.min(axis=1) &lt; 0) | np.any(points_vxl &gt;= self.shape[:-1], axis=1)\n\n    # Prepare output array\n    if self.type == \"coordinates\":\n        points_xf = np.zeros(points.shape, dtype=self.dtype)\n    else:  # offsets\n        points_xf = points.astype(self.dtype, copy=True)\n\n    if is_out.any():\n        if not affine_fallback:\n            points_xf[is_out, :] = np.nan\n        else:\n            # Apply affine part to out-of-bounds points\n            points_xf[is_out, :] = self.affine.xform(\n                points[is_out, :],\n            )\n\n    # Interpolate coordinates, re-combine to an x/y/z array and\n    # add to the input points (if coordinates, the points are zeroed out above)\n    if not is_out.all():\n        points_xf[~is_out, :] += np.vstack(\n            (\n                xinterp(points_vxl[~is_out, :], method=\"linear\"),\n                yinterp(points_vxl[~is_out, :], method=\"linear\"),\n                zinterp(points_vxl[~is_out, :], method=\"linear\"),\n            )\n        ).T.astype(self.dtype)\n\n    return points_xf\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.H5transform","title":"<code>navis.transforms.H5transform</code>","text":"<p>Hdf5 transform of 3D spatial data.</p> <p>See here for specifications of the format.</p> PARAMETER DESCRIPTION <code>f</code> <pre><code>        Path to Hdf5 transformation.\n</code></pre> <p> TYPE: <code>            str</code> </p> <code>direction</code> <pre><code>        Direction of transformation.\n</code></pre> <p> TYPE: <code>    \"forward\" | \"inverse\"</code> DEFAULT: <code>'forward'</code> </p> <code>level</code> <pre><code>        For Hdf5 files with deformation fields at multiple\n        resolutions: what level of detail to use. Negative values\n        go backwards from the highest available resolution\n        (-1 = highest, -2 = second highest, etc). Ignored if only a\n        single deformation field present.\n</code></pre> <p> TYPE: <code>        int</code> DEFAULT: <code>-1</code> </p> <code>cache</code> <pre><code>        If True, we will cache the deformation field for subsequent\n        future transforms. This will speed up future calculations\n        in the future but comes at a memory cost.\n</code></pre> <p> TYPE: <code>        bool</code> DEFAULT: <code>False</code> </p> <code>full_ingest</code> <pre><code>        If True, will read and cache the full deformation field at\n        initialization. This additional upfront cost can pay off if\n        you are about to make many transforms across the volume.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> Source code in <code>navis/transforms/h5reg.py</code> <pre><code>class H5transform(BaseTransform):\n    \"\"\"Hdf5 transform of 3D spatial data.\n\n    See [here](https://github.com/saalfeldlab/template-building/wiki/Hdf5-Deformation-fields)\n    for specifications of the format.\n\n    Parameters\n    ----------\n    f :             str\n                    Path to Hdf5 transformation.\n    direction :     \"forward\" | \"inverse\"\n                    Direction of transformation.\n    level :         int, optional\n                    For Hdf5 files with deformation fields at multiple\n                    resolutions: what level of detail to use. Negative values\n                    go backwards from the highest available resolution\n                    (-1 = highest, -2 = second highest, etc). Ignored if only a\n                    single deformation field present.\n    cache :         bool\n                    If True, we will cache the deformation field for subsequent\n                    future transforms. This will speed up future calculations\n                    in the future but comes at a memory cost.\n    full_ingest :   bool\n                    If True, will read and cache the full deformation field at\n                    initialization. This additional upfront cost can pay off if\n                    you are about to make many transforms across the volume.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        f: str,\n        direction: str = \"forward\",\n        level: Optional[int] = -1,\n        cache: bool = False,\n        full_ingest: bool = False,\n    ):\n        \"\"\"Init class.\"\"\"\n        assert direction in (\"forward\", \"inverse\"), (\n            '`direction` must be \"forward\"' f'or \"inverse\", not \"{direction}\"'\n        )\n\n        self.file = f\n        self.direction = direction\n        self.field = {\"forward\": \"dfield\", \"inverse\": \"invdfield\"}[direction]\n\n        # Trying to avoid the file repeatedly so we are making these initial\n        # adjustments all in one go even though it would be more Pythonic to\n        # delegate to property getter/setter methods\n        with h5py.File(self.file, \"r\") as h5:\n            # Get the available levels\n            available_levels = []\n            for k in h5.keys():\n                try:\n                    available_levels.append(int(k))\n                except ValueError:\n                    continue\n            available_levels = sorted(available_levels)\n\n            # Check if there are indeed deformation fields at various resolutions\n            if available_levels:\n                if isinstance(level, type(None)):\n                    level = available_levels[0]\n                elif level &lt; 0:\n                    ix = level * -1 - 1\n                    level = available_levels[ix]\n\n                # Set level\n                self._level = str(level)\n\n                # Shape of deformation field\n                self._shape = h5[self.level][self.field].shape\n\n                # Data type of deformation field\n                self.dtype = h5[self.level][self.field].dtype\n            elif self.field in h5.keys():\n                # Set level\n                self._level = None\n\n                # Shape of deformation field\n                self._shape = h5[self.field].shape\n\n                # Data type of deformation field\n                self.dtype = h5[self.field].dtype\n            else:\n                raise ValueError(\n                    \"Unable to parse deformation fields from \" f\" {self.file}.\"\n                )\n\n        # Prepare cache if applicable\n        if full_ingest:\n            # Ingest the whole deformation field\n            self.full_ingest()\n        elif cache:\n            self.use_cache = True\n\n    def __eq__(self, other) -&gt; bool:\n        \"\"\"Compare with other Transform.\"\"\"\n        if isinstance(other, H5transform):\n            if self.file == other.file:\n                if self.direction == other.direction:\n                    if self.level == other.level:\n                        return True\n        return False\n\n    def __neg__(self) -&gt; \"H5transform\":\n        \"\"\"Invert direction.\"\"\"\n        # Swap direction\n        new_direction = {\"forward\": \"inverse\", \"inverse\": \"forward\"}[self.direction]\n        # We will re-iniatialize\n        x = H5transform(\n            self.file,\n            direction=new_direction,\n            level=int(self.level) if self.level else None,\n            cache=self.use_cache,\n            full_ingest=False,\n        )\n\n        return x\n\n    @property\n    def level(self):\n        return self._level\n\n    @level.setter\n    def level(self, value):\n        raise ValueError(\"`level` cannot be changed after initialization.\")\n\n    @property\n    def shape(self):\n        \"\"\"Shape of the deformation field.\n\n        Note that the deformation field is likely to be (z, y, x, 3) where\n        the last dimension contains the x/y/z offsets.\n        \"\"\"\n        return self._shape\n\n    @shape.setter\n    def shape(self, value):\n        raise ValueError(\"`shape` cannot be changed after initialization.\")\n\n    @property\n    def spacing(self):\n        \"\"\"Voxel spacing of the deformation field (z, y, x).\"\"\"\n        if not hasattr(self, \"_spacing\"):\n            with h5py.File(self.file, \"r\") as h5:\n                if self.level:\n                    self._spacing = h5[self.level][self.field].attrs[\"spacing\"]\n                else:\n                    self._spacing = h5[self.field].attrs[\"spacing\"]\n        return self._spacing\n\n    @property\n    def quantization_multiplier(self):\n        \"\"\"Quantization multiplier of the deformation field.\"\"\"\n        if not hasattr(self, \"_quantization_multiplier\"):\n            with h5py.File(self.file, \"r\") as h5:\n                if self.level:\n                    self._quantization_multiplier = h5[self.level][\n                        self.field\n                    ].attrs.get(\"quantization_multiplier\", 1)\n                else:\n                    self._quantization_multiplier = h5[self.field].attrs.get(\n                        \"quantization_multiplier\", 1\n                    )\n        return self._quantization_multiplier\n\n    @property\n    def use_cache(self):\n        \"\"\"Whether to cache the deformation field.\"\"\"\n        if not hasattr(self, \"_use_cache\"):\n            self._use_cache = False\n        return self._use_cache\n\n    @use_cache.setter\n    def use_cache(self, value):\n        \"\"\"Set whether to cache the deformation field.\"\"\"\n        assert isinstance(value, bool)\n\n        # If was False and now set to True, build the cache\n        if not getattr(self, \"_use_cache\", False) and value:\n            # This is the actual cache\n            self.cache = np.zeros(self.shape, dtype=self.dtype)\n            # This is a mask that tells us which values have already been cached\n            self.cached = np.zeros(self.shape[:-1], dtype=bool)\n            self._use_cache = True\n        # If was True and now is set to False, deconstruct cache\n        elif getattr(self, \"_use_cache\", False) and not value:\n            del self.cache\n            del self.cached\n            self._use_cache = False\n\n            if hasattr(self, \"_fully_ingested\"):\n                del self._fully_ingested\n\n        # Note: should explore whether we can use sparse N-dimensional\n        # arrays for caching to save memory\n        # See https://github.com/pydata/sparse/\n\n    def copy(self):\n        \"\"\"Return copy.\"\"\"\n        return H5transform(\n            self.file,\n            direction=self.direction,\n            level=int(self.level) if self.level else None,\n            cache=self.use_cache,\n            full_ingest=False,\n        )\n\n    def full_ingest(self):\n        \"\"\"Fully ingest the deformation field.\"\"\"\n        # Skip if already ingested\n        if getattr(self, \"_fully_ingested\", False):\n            return\n\n        with h5py.File(self.file, \"r\") as h5:\n            # Read in the entire field\n            if self.level:\n                self.cache = h5[self.level][self.field][:, :, :]\n            else:\n                self.cache = h5[self.field][:, :, :]\n            # Keep a flag of this\n            self._fully_ingested = True\n            # We set `cached` to True instead of using a mask\n            self.cached = True\n            # Keep track of the caching\n            self._use_cache = True\n\n    def precache(self, bbox: Union[list, np.ndarray], padding=True):\n        \"\"\"Cache deformation field for given bounding box.\n\n        Parameters\n        ----------\n        bbox :      list | array\n                    Must be `[[x1, x2], [y1, y2], [z1, z2]]`.\n        padding :   bool\n                    If True, will add the (required!) padding to the bounding\n                    box.\n\n        \"\"\"\n        bbox = np.asarray(bbox)\n\n        if bbox.ndim != 2 or bbox.shape != (3, 2):\n            raise ValueError(f\"Expected (3, 2) bounding box, got {bbox.shape}\")\n\n        # Set use_cache=True -&gt; this also prepares the cache array(s)\n        self.use_cache = True\n\n        with h5py.File(self.file, \"r\") as h5:\n            if self.level:\n                spacing = h5[self.level][self.field].attrs[\"spacing\"]\n            else:\n                spacing = h5[self.field].attrs[\"spacing\"]\n\n            # Note that we invert because spacing is given in (z, y, x)\n            bbox_vxl = (bbox.T / spacing[::-1]).T\n            # Digitize into voxels\n            bbox_vxl = bbox_vxl.round().astype(int)\n\n            if padding:\n                bbox_vxl[:, 0] -= 2\n                bbox_vxl[:, 1] += 2\n\n            # Make sure we are within bounds\n            bbox_vxl = np.clip(bbox_vxl.T, 0, self.shape[:-1][::-1]).T\n\n            # Extract values\n            x1, x2, y1, y2, z1, z2 = bbox_vxl.flatten()\n\n            # Cache values in this bounding box\n            if self.level:\n                read_from = h5[self.level][self.field]\n            else:\n                read_from = h5[self.field]\n\n            self.cache[z1:z2, y1:y2, x1:x2] = read_from[z1:z2, y1:y2, x1:x2]\n            self.cached[z1:z2, y1:y2, x1:x2] = True\n\n    @staticmethod\n    def from_file(filepath: str, **kwargs) -&gt; \"H5transform\":\n        \"\"\"Generate H5transform from file.\n\n        Parameters\n        ----------\n        filepath :  str\n                    Path to H5 transform.\n        **kwargs\n                    Keyword arguments passed to H5transform.__init__\n\n        Returns\n        -------\n        H5transform\n\n        \"\"\"\n        return H5transform(str(filepath), **kwargs)\n\n    def xform(\n        self,\n        points: np.ndarray,\n        affine_fallback: bool = True,\n        force_deform: bool = True,\n    ) -&gt; np.ndarray:\n        \"\"\"Xform data.\n\n        Parameters\n        ----------\n        points :            (N, 3) numpy array | pandas.DataFrame\n                            Points to xform. DataFrame must have x/y/z columns.\n        affine_fallback :   bool\n                            If False, points outside the deformation field will\n                            be returned as `np.nan`. If True, these points will\n                            only receive the affine part of the transformation.\n        force_deform :      bools\n                            If True, points outside the deformation field be\n                            deformed using the closest point inside the\n                            deformation field. Ignored if `affine_fallback` is\n                            `False`.\n\n        Returns\n        -------\n        pointsxf :      (N, 3) numpy array\n                        Transformed points. Points outside the deformation field\n                        will have only the affine part of the transform applied.\n\n        \"\"\"\n        if isinstance(points, pd.DataFrame):\n            # Make sure x/y/z columns are present\n            if np.any([c not in points for c in [\"x\", \"y\", \"z\"]]):\n                raise ValueError(\"points DataFrame must have x/y/z columns.\")\n            points = points[[\"x\", \"y\", \"z\"]].values\n\n        if (\n            not isinstance(points, np.ndarray)\n            or points.ndim != 2\n            or points.shape[1] != 3\n        ):\n            raise TypeError(\n                \"`points` must be numpy array of shape (N, 3) or \"\n                \"pandas DataFrame with x/y/z columns\"\n            )\n\n        # Read the file\n        with h5py.File(self.file, \"r\") as h5:\n            if self.level:\n                field = h5[self.level][self.field]\n            else:\n                field = h5[self.field]\n\n            # We need the field to be (z, y, x, offsets) with `offsets` being\n            # three values - for example: (293, 470, 1010, 3)\n            # If that's not the case, something is fishy!\n            if field.shape[-1] != 3:\n                logger.warning(\n                    \"Expected the deformation field to be of shape \"\n                    f\"(z, y, x, 3), got {field.shape}.\"\n                )\n\n            if \"affine\" in field.attrs:\n                # The affine part of the transform is a 4 x 4 matrix where the upper\n                # 3 x 4 part (row x columns) is an attribute of the h5 dataset\n                M = np.ones((4, 4))\n                M[:3, :4] = field.attrs[\"affine\"].reshape(3, 4)\n                affine = AffineTransform(M)\n            else:\n                affine = False\n\n            # Get quantization multiplier for later use\n            quantization_multiplier = field.attrs.get(\"quantization_multiplier\", 1)\n\n            # For inverse direction, the affine part is applied first\n            if self.direction == \"inverse\" and affine:\n                xf = affine.xform(points)\n            else:\n                xf = points\n\n            # Translate points into voxel space\n            spacing = field.attrs[\"spacing\"]\n            # Note that we invert because spacing is given in (z, y, x)\n            xf_voxel = xf / spacing[::-1]\n            # Digitize points into voxels\n            xf_indices = xf_voxel.round().astype(int)\n            # Determine the bounding box of the deformation vectors we need\n            # Note that we are grabbing a bit more than required - this is\n            # necessary for interpolation later down the line\n            mn = xf_indices.min(axis=0) - 2\n            mx = xf_indices.max(axis=0) + 2\n\n            # Make sure we are within bounds\n            # Note that we clip `mn` at 0 and `mx` at 2 at the lower end?\n            # This is to make sure we have enough of the deformation field\n            # to interpolate later on `offsets`\n            mn = np.clip(mn, 2, np.array(self.shape[:-1][::-1])) - 2\n            mx = np.clip(mx, 0, np.array(self.shape[:-1][::-1]) - 2) + 2\n\n            # Check if we can use cached values\n            if self.use_cache and (\n                hasattr(self, \"_fully_ingested\")\n                or np.all(self.cached[mn[2] : mx[2], mn[1] : mx[1], mn[0] : mx[0]])\n            ):\n                offsets = self.cache[mn[2] : mx[2], mn[1] : mx[1], mn[0] : mx[0]]\n            else:\n                # Load the deformation values for this bounding box\n                # This is faster than grabbing individual voxels and\n                offsets = field[mn[2] : mx[2], mn[1] : mx[1], mn[0] : mx[0]]\n\n                if self.use_cache:\n                    # Write these offsets to cache\n                    self.cache[mn[2] : mx[2], mn[1] : mx[1], mn[0] : mx[0]] = offsets\n                    self.cached[mn[2] : mx[2], mn[1] : mx[1], mn[0] : mx[0]] = True\n\n        # For interpolation, we need to split the offsets into their x, y\n        # and z component\n        xgrid = offsets[:, :, :, 0]\n        ygrid = offsets[:, :, :, 1]\n        zgrid = offsets[:, :, :, 2]\n\n        xx = np.arange(mn[0], mx[0])\n        yy = np.arange(mn[1], mx[1])\n        zz = np.arange(mn[2], mx[2])\n\n        # The RegularGridInterpolator is the fastest one but the results are\n        # are ever so slightly (4th decimal) different from the Java implementation\n        xinterp = RegularGridInterpolator(\n            (zz, yy, xx), xgrid, bounds_error=False, fill_value=0\n        )\n        yinterp = RegularGridInterpolator(\n            (zz, yy, xx), ygrid, bounds_error=False, fill_value=0\n        )\n        zinterp = RegularGridInterpolator(\n            (zz, yy, xx), zgrid, bounds_error=False, fill_value=0\n        )\n\n        # Before we interpolate check how many points are outside the\n        # deformation field -&gt; these will only receive the affine part of the\n        # transform\n        is_out = (xf_voxel.min(axis=1) &lt; 0) | np.any(\n            xf_voxel &gt;= self.shape[:-1][::-1], axis=1\n        )\n\n        # If more than 20% (arbitrary number) of voxels are out, there is\n        # something suspicious going on\n        frac_out = is_out.sum() / xf_voxel.shape[0]\n        if frac_out &gt; 0.2 and not getattr(self, \"_silenced_large_out_warning\", False):\n            logger.warning(\n                f\"A suspiciously large fraction ({frac_out:.1%}) \"\n                f\"of {xf_voxel.shape[0]} points appear to be outside \"\n                \"the H5 deformation field. Please make doubly sure \"\n                \"that the input coordinates are in the correct \"\n                \"space/units\"\n            )\n        # If all points are outside the volume, the interpolation complains\n        if frac_out &lt; 1 or (force_deform and affine_fallback):\n            if force_deform:\n                # For the purpose of finding offsets, we will snap points\n                # outside the deformation field to the closest inside voxel\n                q_voxel = np.clip(\n                    xf_voxel, a_min=0, a_max=np.array(self.shape[:-1][::-1]) - 1\n                )\n            else:\n                q_voxel = xf_voxel\n\n            # Interpolate coordinates and re-combine to an x/y/z array\n            offset_vxl = np.vstack(\n                (\n                    xinterp(q_voxel[:, ::-1], method=\"linear\"),\n                    yinterp(q_voxel[:, ::-1], method=\"linear\"),\n                    zinterp(q_voxel[:, ::-1], method=\"linear\"),\n                )\n            ).T\n\n            # Turn offsets into real-world coordinates\n            offset_real = offset_vxl * quantization_multiplier\n\n            # Apply offsets\n            # Please note that we must not use += here\n            # That's to avoid running into data type errors where numpy\n            # will refuse to add e.g. float64 to int64.\n            # By using \"+\" instead of \"+=\" we are creating a new array that\n            # is potentially upcast from e.g. int64 to float64\n            xf = xf + offset_real\n\n        # For forward direction, the affine part is applied last\n        if self.direction == \"forward\" and affine:\n            xf = affine.xform(xf)\n\n        # If no affine_fallback, set outside points to np.nan\n        if not affine_fallback:\n            xf[is_out, :] = np.nan\n\n        return xf\n\n    def xform_image(\n        self,\n        image: np.ndarray,\n        image_res: Union[tuple, list],\n        out_res: Optional[Union[tuple, list]] = None,\n        out_shape: Optional[Union[tuple, list]] = None,\n        order: int = 1,\n        mode: str = \"constant\",\n        cval: float = 0,\n        cache: bool = True,\n        progress: bool = True,\n    ) -&gt; np.ndarray:\n        \"\"\"Transform a 3D image using backward mapping.\n\n        If available, this method will use `numba` to accelerate the\n        transformation. This is highly recommended as it can speed up\n        the transformation by several orders of magnitude:\n\n          pip install numba\n\n        Note though that the numba-accelerated path only supports linear\n        interpolation and constant-mode boundary handling (default).\n\n        Parameters\n        ----------\n        image :         (N, M, K) numpy array\n                        Image to be transformed.\n        image_res :     (3, ) tuple | list\n                        Voxel resolution of the input image.\n        out_res :       (3, ) tuple | list, optional\n                        Voxel resolution of the output image. If None, assumed\n                        to be the same as `image_res`.\n        out_shape :     (3, ) tuple | list, optional\n                        Shape of the output image in voxels. If None, uses the\n                        shape of the deformation field. This works as long as\n                        the deformation field fully samples the target space.\n        order :         int\n                        The order of the spline interpolation, default is 1\n                        (linear). The order has to be in the range 0-5.\n        mode :          str\n                        How to handle values outside the image bounds.\n                        Default is 'constant' (pad with cval).\n        cval :          float\n                        Value used for points outside the boundaries when\n                        mode='constant'.\n        chunk_size :    int\n                        Size of chunks to process along each dimension.\n                        Larger chunks are faster but use more memory.\n                        Only relevant for Python (not numba) path.\n        cache :         bool\n                        If True, we will cache the deformation field for\n                        subsequent future transforms. This is generally\n                        recommended unless memory is very tight or the\n                        deformation field is huge.\n        progress :      bool\n                        Whether to show a progress bar during processing.\n                        Not available (and probably not necessary) if\n                        numba-accelerated path is used.\n\n        Returns\n        -------\n        transformed :   (N, M, K) numpy array\n                        Transformed image in target space. The shape is\n                        determined by `out_shape` and `out_res`.\n\n        Notes\n        -----\n        This method uses backward mapping: for each output chunk, we compute\n        where the voxels came from in the source image using the transformation,\n        then interpolate the source image at those locations. So if your\n        transform goes from A -&gt; B, this method actually uses the inverse transform\n        under the hood to figure out where in A the voxels in B come from.\n\n        \"\"\"\n        if not isinstance(image, np.ndarray) or image.ndim != 3:\n            raise TypeError(\"`image` must be a 3D numpy array\")\n\n        # Because we are using backward mapping, we need to invert the transform,\n        # i.e. we're taking points in the target space, mapping them back to the\n        # source space and sampling the source image at those locations)\n        if hasattr(self, \"_inverted_copy\"):\n            # We're tracking the copy so that we can use the cache for subsequent\n            # transformations instead of having to re-read the file and re-ingest\n            # the deformation field\n            reg_inv = self._inverted_copy\n        else:\n            reg_inv = -self\n            # If cache, track the inverted copy (and the deformation field)\n            if cache:\n                reg_inv.use_cache = True\n                self._inverted_copy = reg_inv\n\n        if out_res is None:\n            out_res = image_res\n\n        if out_shape is None:\n            # Bounds in physical space\n            bounds = np.array(list(reg_inv.shape)[:-1][::-1]) * reg_inv.spacing\n            # Back to voxel space in output resolution\n            out_shape = np.ceil(bounds / out_res).astype(int)\n\n        # Generate the empty output array\n        out = np.zeros(out_shape, dtype=image.dtype)\n\n        try:\n            from .h5reg_numba import h5reg_warp_image_linear_constant\n\n            nb_available = True\n        except ModuleNotFoundError:\n            nb_available = False\n\n        # Numba path: linear interpolation + constant mode only but much faster\n        if order == 1 and mode == \"constant\":\n            if not nb_available:\n                global NUMBA_WARNING\n                if not NUMBA_WARNING:\n                    logger.warning(\n                        \"For faster transforming of images, please install numba:\\n\"\n                        \"  pip install numba\"\n                    )\n                    NUMBA_WARNING = True\n            else:\n                reg_inv.full_ingest()\n                field = reg_inv.cache\n\n                with h5py.File(reg_inv.file, \"r\") as h5:\n                    if reg_inv.level:\n                        ds = h5[reg_inv.level][reg_inv.field]\n                    else:\n                        ds = h5[reg_inv.field]\n\n                    spacing = np.asarray(ds.attrs[\"spacing\"], dtype=np.float64)\n                    qmult = float(ds.attrs.get(\"quantization_multiplier\", 1))\n\n                    if \"affine\" in ds.attrs:\n                        affine = np.ones((4, 4), dtype=np.float64)\n                        affine[:3, :4] = ds.attrs[\"affine\"].reshape(3, 4)\n                        apply_affine = 1 if reg_inv.direction == \"inverse\" else 2\n                    else:\n                        affine = np.eye(4, dtype=np.float64)\n                        apply_affine = 0\n\n                return h5reg_warp_image_linear_constant(\n                    image,\n                    field,\n                    spacing,\n                    qmult,\n                    affine,\n                    apply_affine,\n                    out,\n                    out_res,\n                    image_res,\n                    float(cval),\n                )\n\n        # Pre-compute prefilter if needed for higher order interpolation\n        if order &gt; 1:\n            from scipy.ndimage import spline_filter\n\n            image = spline_filter(image, order=order)\n\n        for z in config.trange(\n            out_shape[2],\n            disable=not progress or config.pbar_hide,\n            desc=\"Warping image\",\n        ):\n            # Generate a grid of points in target voxel space for this z-slice\n            chunk_coords = np.mgrid[\n                0 : out_shape[0] : 1, 0 : out_shape[1] : 1, z : z + 1\n            ]\n            points = np.stack(\n                [\n                    chunk_coords[0].flatten(),\n                    chunk_coords[1].flatten(),\n                    chunk_coords[2].flatten(),\n                ],\n                axis=-1,\n            )\n\n            # Convert to physical space\n            points = points.astype(np.float32) * out_res\n\n            # Transform points to source space using the inverse transform\n            try:\n                reg_inv._silenced_large_out_warning = True\n                points_xf = reg_inv.xform(points)\n            except Exception as e:\n                reg_inv._silenced_large_out_warning = False\n                raise e\n\n            # Convert back to voxel space in source image\n            points_xf_vox = points_xf / image_res\n\n            # Sample the source image at these transformed coordinates\n            sampled_values = map_coordinates(\n                image,\n                [points_xf_vox[:, 0], points_xf_vox[:, 1], points_xf_vox[:, 2]],\n                order=1,\n                mode=\"constant\",\n                cval=0,\n            )\n\n            # Reshape back to 2D and assign to this slice of the output image\n            out[:, :, z] = sampled_values.reshape(out_shape[0], out_shape[1])\n\n        return out\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.H5transform.quantization_multiplier","title":"<code>quantization_multiplier</code>  <code>property</code>","text":"<p>Quantization multiplier of the deformation field.</p>"},{"location":"reference/navis/transforms/#navis.transforms.H5transform.shape","title":"<code>shape</code>  <code>property</code> <code>writable</code>","text":"<p>Shape of the deformation field.</p> <p>Note that the deformation field is likely to be (z, y, x, 3) where the last dimension contains the x/y/z offsets.</p>"},{"location":"reference/navis/transforms/#navis.transforms.H5transform.spacing","title":"<code>spacing</code>  <code>property</code>","text":"<p>Voxel spacing of the deformation field (z, y, x).</p>"},{"location":"reference/navis/transforms/#navis.transforms.H5transform.use_cache","title":"<code>use_cache</code>  <code>property</code> <code>writable</code>","text":"<p>Whether to cache the deformation field.</p>"},{"location":"reference/navis/transforms/#navis.transforms.H5transform.__init__","title":"<code>__init__</code>","text":"<p>Init class.</p> Source code in <code>navis/transforms/h5reg.py</code> <pre><code>def __init__(\n    self,\n    f: str,\n    direction: str = \"forward\",\n    level: Optional[int] = -1,\n    cache: bool = False,\n    full_ingest: bool = False,\n):\n    \"\"\"Init class.\"\"\"\n    assert direction in (\"forward\", \"inverse\"), (\n        '`direction` must be \"forward\"' f'or \"inverse\", not \"{direction}\"'\n    )\n\n    self.file = f\n    self.direction = direction\n    self.field = {\"forward\": \"dfield\", \"inverse\": \"invdfield\"}[direction]\n\n    # Trying to avoid the file repeatedly so we are making these initial\n    # adjustments all in one go even though it would be more Pythonic to\n    # delegate to property getter/setter methods\n    with h5py.File(self.file, \"r\") as h5:\n        # Get the available levels\n        available_levels = []\n        for k in h5.keys():\n            try:\n                available_levels.append(int(k))\n            except ValueError:\n                continue\n        available_levels = sorted(available_levels)\n\n        # Check if there are indeed deformation fields at various resolutions\n        if available_levels:\n            if isinstance(level, type(None)):\n                level = available_levels[0]\n            elif level &lt; 0:\n                ix = level * -1 - 1\n                level = available_levels[ix]\n\n            # Set level\n            self._level = str(level)\n\n            # Shape of deformation field\n            self._shape = h5[self.level][self.field].shape\n\n            # Data type of deformation field\n            self.dtype = h5[self.level][self.field].dtype\n        elif self.field in h5.keys():\n            # Set level\n            self._level = None\n\n            # Shape of deformation field\n            self._shape = h5[self.field].shape\n\n            # Data type of deformation field\n            self.dtype = h5[self.field].dtype\n        else:\n            raise ValueError(\n                \"Unable to parse deformation fields from \" f\" {self.file}.\"\n            )\n\n    # Prepare cache if applicable\n    if full_ingest:\n        # Ingest the whole deformation field\n        self.full_ingest()\n    elif cache:\n        self.use_cache = True\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.H5transform.copy","title":"<code>copy</code>","text":"<p>Return copy.</p> Source code in <code>navis/transforms/h5reg.py</code> <pre><code>def copy(self):\n    \"\"\"Return copy.\"\"\"\n    return H5transform(\n        self.file,\n        direction=self.direction,\n        level=int(self.level) if self.level else None,\n        cache=self.use_cache,\n        full_ingest=False,\n    )\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.H5transform.from_file","title":"<code>from_file</code>  <code>staticmethod</code>","text":"<p>Generate H5transform from file.</p> PARAMETER DESCRIPTION <code>filepath</code> <pre><code>    Path to H5 transform.\n</code></pre> <p> TYPE: <code> str</code> </p> <code>**kwargs</code> <pre><code>    Keyword arguments passed to H5transform.__init__\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>H5transform</code> Source code in <code>navis/transforms/h5reg.py</code> <pre><code>@staticmethod\ndef from_file(filepath: str, **kwargs) -&gt; \"H5transform\":\n    \"\"\"Generate H5transform from file.\n\n    Parameters\n    ----------\n    filepath :  str\n                Path to H5 transform.\n    **kwargs\n                Keyword arguments passed to H5transform.__init__\n\n    Returns\n    -------\n    H5transform\n\n    \"\"\"\n    return H5transform(str(filepath), **kwargs)\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.H5transform.full_ingest","title":"<code>full_ingest</code>","text":"<p>Fully ingest the deformation field.</p> Source code in <code>navis/transforms/h5reg.py</code> <pre><code>def full_ingest(self):\n    \"\"\"Fully ingest the deformation field.\"\"\"\n    # Skip if already ingested\n    if getattr(self, \"_fully_ingested\", False):\n        return\n\n    with h5py.File(self.file, \"r\") as h5:\n        # Read in the entire field\n        if self.level:\n            self.cache = h5[self.level][self.field][:, :, :]\n        else:\n            self.cache = h5[self.field][:, :, :]\n        # Keep a flag of this\n        self._fully_ingested = True\n        # We set `cached` to True instead of using a mask\n        self.cached = True\n        # Keep track of the caching\n        self._use_cache = True\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.H5transform.precache","title":"<code>precache</code>","text":"<p>Cache deformation field for given bounding box.</p> PARAMETER DESCRIPTION <code>bbox</code> <pre><code>    Must be `[[x1, x2], [y1, y2], [z1, z2]]`.\n</code></pre> <p> TYPE: <code>     list | array</code> </p> <code>padding</code> <pre><code>    If True, will add the (required!) padding to the bounding\n    box.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>True</code> </p> Source code in <code>navis/transforms/h5reg.py</code> <pre><code>def precache(self, bbox: Union[list, np.ndarray], padding=True):\n    \"\"\"Cache deformation field for given bounding box.\n\n    Parameters\n    ----------\n    bbox :      list | array\n                Must be `[[x1, x2], [y1, y2], [z1, z2]]`.\n    padding :   bool\n                If True, will add the (required!) padding to the bounding\n                box.\n\n    \"\"\"\n    bbox = np.asarray(bbox)\n\n    if bbox.ndim != 2 or bbox.shape != (3, 2):\n        raise ValueError(f\"Expected (3, 2) bounding box, got {bbox.shape}\")\n\n    # Set use_cache=True -&gt; this also prepares the cache array(s)\n    self.use_cache = True\n\n    with h5py.File(self.file, \"r\") as h5:\n        if self.level:\n            spacing = h5[self.level][self.field].attrs[\"spacing\"]\n        else:\n            spacing = h5[self.field].attrs[\"spacing\"]\n\n        # Note that we invert because spacing is given in (z, y, x)\n        bbox_vxl = (bbox.T / spacing[::-1]).T\n        # Digitize into voxels\n        bbox_vxl = bbox_vxl.round().astype(int)\n\n        if padding:\n            bbox_vxl[:, 0] -= 2\n            bbox_vxl[:, 1] += 2\n\n        # Make sure we are within bounds\n        bbox_vxl = np.clip(bbox_vxl.T, 0, self.shape[:-1][::-1]).T\n\n        # Extract values\n        x1, x2, y1, y2, z1, z2 = bbox_vxl.flatten()\n\n        # Cache values in this bounding box\n        if self.level:\n            read_from = h5[self.level][self.field]\n        else:\n            read_from = h5[self.field]\n\n        self.cache[z1:z2, y1:y2, x1:x2] = read_from[z1:z2, y1:y2, x1:x2]\n        self.cached[z1:z2, y1:y2, x1:x2] = True\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.H5transform.xform","title":"<code>xform</code>","text":"<p>Xform data.</p> PARAMETER DESCRIPTION <code>points</code> <pre><code>            Points to xform. DataFrame must have x/y/z columns.\n</code></pre> <p> TYPE: <code>           (N, 3) numpy array | pandas.DataFrame</code> </p> <code>affine_fallback</code> <pre><code>            If False, points outside the deformation field will\n            be returned as `np.nan`. If True, these points will\n            only receive the affine part of the transformation.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>True</code> </p> <code>force_deform</code> <pre><code>            If True, points outside the deformation field be\n            deformed using the closest point inside the\n            deformation field. Ignored if `affine_fallback` is\n            `False`.\n</code></pre> <p> TYPE: <code>     bools</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>pointsxf</code> <p>Transformed points. Points outside the deformation field will have only the affine part of the transform applied.</p> <p> TYPE: <code>(N, 3) numpy array</code> </p> Source code in <code>navis/transforms/h5reg.py</code> <pre><code>def xform(\n    self,\n    points: np.ndarray,\n    affine_fallback: bool = True,\n    force_deform: bool = True,\n) -&gt; np.ndarray:\n    \"\"\"Xform data.\n\n    Parameters\n    ----------\n    points :            (N, 3) numpy array | pandas.DataFrame\n                        Points to xform. DataFrame must have x/y/z columns.\n    affine_fallback :   bool\n                        If False, points outside the deformation field will\n                        be returned as `np.nan`. If True, these points will\n                        only receive the affine part of the transformation.\n    force_deform :      bools\n                        If True, points outside the deformation field be\n                        deformed using the closest point inside the\n                        deformation field. Ignored if `affine_fallback` is\n                        `False`.\n\n    Returns\n    -------\n    pointsxf :      (N, 3) numpy array\n                    Transformed points. Points outside the deformation field\n                    will have only the affine part of the transform applied.\n\n    \"\"\"\n    if isinstance(points, pd.DataFrame):\n        # Make sure x/y/z columns are present\n        if np.any([c not in points for c in [\"x\", \"y\", \"z\"]]):\n            raise ValueError(\"points DataFrame must have x/y/z columns.\")\n        points = points[[\"x\", \"y\", \"z\"]].values\n\n    if (\n        not isinstance(points, np.ndarray)\n        or points.ndim != 2\n        or points.shape[1] != 3\n    ):\n        raise TypeError(\n            \"`points` must be numpy array of shape (N, 3) or \"\n            \"pandas DataFrame with x/y/z columns\"\n        )\n\n    # Read the file\n    with h5py.File(self.file, \"r\") as h5:\n        if self.level:\n            field = h5[self.level][self.field]\n        else:\n            field = h5[self.field]\n\n        # We need the field to be (z, y, x, offsets) with `offsets` being\n        # three values - for example: (293, 470, 1010, 3)\n        # If that's not the case, something is fishy!\n        if field.shape[-1] != 3:\n            logger.warning(\n                \"Expected the deformation field to be of shape \"\n                f\"(z, y, x, 3), got {field.shape}.\"\n            )\n\n        if \"affine\" in field.attrs:\n            # The affine part of the transform is a 4 x 4 matrix where the upper\n            # 3 x 4 part (row x columns) is an attribute of the h5 dataset\n            M = np.ones((4, 4))\n            M[:3, :4] = field.attrs[\"affine\"].reshape(3, 4)\n            affine = AffineTransform(M)\n        else:\n            affine = False\n\n        # Get quantization multiplier for later use\n        quantization_multiplier = field.attrs.get(\"quantization_multiplier\", 1)\n\n        # For inverse direction, the affine part is applied first\n        if self.direction == \"inverse\" and affine:\n            xf = affine.xform(points)\n        else:\n            xf = points\n\n        # Translate points into voxel space\n        spacing = field.attrs[\"spacing\"]\n        # Note that we invert because spacing is given in (z, y, x)\n        xf_voxel = xf / spacing[::-1]\n        # Digitize points into voxels\n        xf_indices = xf_voxel.round().astype(int)\n        # Determine the bounding box of the deformation vectors we need\n        # Note that we are grabbing a bit more than required - this is\n        # necessary for interpolation later down the line\n        mn = xf_indices.min(axis=0) - 2\n        mx = xf_indices.max(axis=0) + 2\n\n        # Make sure we are within bounds\n        # Note that we clip `mn` at 0 and `mx` at 2 at the lower end?\n        # This is to make sure we have enough of the deformation field\n        # to interpolate later on `offsets`\n        mn = np.clip(mn, 2, np.array(self.shape[:-1][::-1])) - 2\n        mx = np.clip(mx, 0, np.array(self.shape[:-1][::-1]) - 2) + 2\n\n        # Check if we can use cached values\n        if self.use_cache and (\n            hasattr(self, \"_fully_ingested\")\n            or np.all(self.cached[mn[2] : mx[2], mn[1] : mx[1], mn[0] : mx[0]])\n        ):\n            offsets = self.cache[mn[2] : mx[2], mn[1] : mx[1], mn[0] : mx[0]]\n        else:\n            # Load the deformation values for this bounding box\n            # This is faster than grabbing individual voxels and\n            offsets = field[mn[2] : mx[2], mn[1] : mx[1], mn[0] : mx[0]]\n\n            if self.use_cache:\n                # Write these offsets to cache\n                self.cache[mn[2] : mx[2], mn[1] : mx[1], mn[0] : mx[0]] = offsets\n                self.cached[mn[2] : mx[2], mn[1] : mx[1], mn[0] : mx[0]] = True\n\n    # For interpolation, we need to split the offsets into their x, y\n    # and z component\n    xgrid = offsets[:, :, :, 0]\n    ygrid = offsets[:, :, :, 1]\n    zgrid = offsets[:, :, :, 2]\n\n    xx = np.arange(mn[0], mx[0])\n    yy = np.arange(mn[1], mx[1])\n    zz = np.arange(mn[2], mx[2])\n\n    # The RegularGridInterpolator is the fastest one but the results are\n    # are ever so slightly (4th decimal) different from the Java implementation\n    xinterp = RegularGridInterpolator(\n        (zz, yy, xx), xgrid, bounds_error=False, fill_value=0\n    )\n    yinterp = RegularGridInterpolator(\n        (zz, yy, xx), ygrid, bounds_error=False, fill_value=0\n    )\n    zinterp = RegularGridInterpolator(\n        (zz, yy, xx), zgrid, bounds_error=False, fill_value=0\n    )\n\n    # Before we interpolate check how many points are outside the\n    # deformation field -&gt; these will only receive the affine part of the\n    # transform\n    is_out = (xf_voxel.min(axis=1) &lt; 0) | np.any(\n        xf_voxel &gt;= self.shape[:-1][::-1], axis=1\n    )\n\n    # If more than 20% (arbitrary number) of voxels are out, there is\n    # something suspicious going on\n    frac_out = is_out.sum() / xf_voxel.shape[0]\n    if frac_out &gt; 0.2 and not getattr(self, \"_silenced_large_out_warning\", False):\n        logger.warning(\n            f\"A suspiciously large fraction ({frac_out:.1%}) \"\n            f\"of {xf_voxel.shape[0]} points appear to be outside \"\n            \"the H5 deformation field. Please make doubly sure \"\n            \"that the input coordinates are in the correct \"\n            \"space/units\"\n        )\n    # If all points are outside the volume, the interpolation complains\n    if frac_out &lt; 1 or (force_deform and affine_fallback):\n        if force_deform:\n            # For the purpose of finding offsets, we will snap points\n            # outside the deformation field to the closest inside voxel\n            q_voxel = np.clip(\n                xf_voxel, a_min=0, a_max=np.array(self.shape[:-1][::-1]) - 1\n            )\n        else:\n            q_voxel = xf_voxel\n\n        # Interpolate coordinates and re-combine to an x/y/z array\n        offset_vxl = np.vstack(\n            (\n                xinterp(q_voxel[:, ::-1], method=\"linear\"),\n                yinterp(q_voxel[:, ::-1], method=\"linear\"),\n                zinterp(q_voxel[:, ::-1], method=\"linear\"),\n            )\n        ).T\n\n        # Turn offsets into real-world coordinates\n        offset_real = offset_vxl * quantization_multiplier\n\n        # Apply offsets\n        # Please note that we must not use += here\n        # That's to avoid running into data type errors where numpy\n        # will refuse to add e.g. float64 to int64.\n        # By using \"+\" instead of \"+=\" we are creating a new array that\n        # is potentially upcast from e.g. int64 to float64\n        xf = xf + offset_real\n\n    # For forward direction, the affine part is applied last\n    if self.direction == \"forward\" and affine:\n        xf = affine.xform(xf)\n\n    # If no affine_fallback, set outside points to np.nan\n    if not affine_fallback:\n        xf[is_out, :] = np.nan\n\n    return xf\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.H5transform.xform_image","title":"<code>xform_image</code>","text":"<p>Transform a 3D image using backward mapping.</p> <p>If available, this method will use <code>numba</code> to accelerate the transformation. This is highly recommended as it can speed up the transformation by several orders of magnitude:</p> <p>pip install numba</p> <p>Note though that the numba-accelerated path only supports linear interpolation and constant-mode boundary handling (default).</p> PARAMETER DESCRIPTION <code>image</code> <pre><code>        Image to be transformed.\n</code></pre> <p> TYPE: <code>        (N, M, K) numpy array</code> </p> <code>image_res</code> <pre><code>        Voxel resolution of the input image.\n</code></pre> <p> TYPE: <code>    (3, ) tuple | list</code> </p> <code>out_res</code> <pre><code>        Voxel resolution of the output image. If None, assumed\n        to be the same as `image_res`.\n</code></pre> <p> TYPE: <code>      (3, ) tuple | list</code> DEFAULT: <code>None</code> </p> <code>out_shape</code> <pre><code>        Shape of the output image in voxels. If None, uses the\n        shape of the deformation field. This works as long as\n        the deformation field fully samples the target space.\n</code></pre> <p> TYPE: <code>    (3, ) tuple | list</code> DEFAULT: <code>None</code> </p> <code>order</code> <pre><code>        The order of the spline interpolation, default is 1\n        (linear). The order has to be in the range 0-5.\n</code></pre> <p> TYPE: <code>        int</code> DEFAULT: <code>1</code> </p> <code>mode</code> <pre><code>        How to handle values outside the image bounds.\n        Default is 'constant' (pad with cval).\n</code></pre> <p> TYPE: <code>         str</code> DEFAULT: <code>'constant'</code> </p> <code>cval</code> <pre><code>        Value used for points outside the boundaries when\n        mode='constant'.\n</code></pre> <p> TYPE: <code>         float</code> DEFAULT: <code>0</code> </p> <code>chunk_size</code> <pre><code>        Size of chunks to process along each dimension.\n        Larger chunks are faster but use more memory.\n        Only relevant for Python (not numba) path.\n</code></pre> <p> TYPE: <code>   int</code> </p> <code>cache</code> <pre><code>        If True, we will cache the deformation field for\n        subsequent future transforms. This is generally\n        recommended unless memory is very tight or the\n        deformation field is huge.\n</code></pre> <p> TYPE: <code>        bool</code> DEFAULT: <code>True</code> </p> <code>progress</code> <pre><code>        Whether to show a progress bar during processing.\n        Not available (and probably not necessary) if\n        numba-accelerated path is used.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>transformed</code> <p>Transformed image in target space. The shape is determined by <code>out_shape</code> and <code>out_res</code>.</p> <p> TYPE: <code>(N, M, K) numpy array</code> </p> Notes <p>This method uses backward mapping: for each output chunk, we compute where the voxels came from in the source image using the transformation, then interpolate the source image at those locations. So if your transform goes from A -&gt; B, this method actually uses the inverse transform under the hood to figure out where in A the voxels in B come from.</p> Source code in <code>navis/transforms/h5reg.py</code> <pre><code>def xform_image(\n    self,\n    image: np.ndarray,\n    image_res: Union[tuple, list],\n    out_res: Optional[Union[tuple, list]] = None,\n    out_shape: Optional[Union[tuple, list]] = None,\n    order: int = 1,\n    mode: str = \"constant\",\n    cval: float = 0,\n    cache: bool = True,\n    progress: bool = True,\n) -&gt; np.ndarray:\n    \"\"\"Transform a 3D image using backward mapping.\n\n    If available, this method will use `numba` to accelerate the\n    transformation. This is highly recommended as it can speed up\n    the transformation by several orders of magnitude:\n\n      pip install numba\n\n    Note though that the numba-accelerated path only supports linear\n    interpolation and constant-mode boundary handling (default).\n\n    Parameters\n    ----------\n    image :         (N, M, K) numpy array\n                    Image to be transformed.\n    image_res :     (3, ) tuple | list\n                    Voxel resolution of the input image.\n    out_res :       (3, ) tuple | list, optional\n                    Voxel resolution of the output image. If None, assumed\n                    to be the same as `image_res`.\n    out_shape :     (3, ) tuple | list, optional\n                    Shape of the output image in voxels. If None, uses the\n                    shape of the deformation field. This works as long as\n                    the deformation field fully samples the target space.\n    order :         int\n                    The order of the spline interpolation, default is 1\n                    (linear). The order has to be in the range 0-5.\n    mode :          str\n                    How to handle values outside the image bounds.\n                    Default is 'constant' (pad with cval).\n    cval :          float\n                    Value used for points outside the boundaries when\n                    mode='constant'.\n    chunk_size :    int\n                    Size of chunks to process along each dimension.\n                    Larger chunks are faster but use more memory.\n                    Only relevant for Python (not numba) path.\n    cache :         bool\n                    If True, we will cache the deformation field for\n                    subsequent future transforms. This is generally\n                    recommended unless memory is very tight or the\n                    deformation field is huge.\n    progress :      bool\n                    Whether to show a progress bar during processing.\n                    Not available (and probably not necessary) if\n                    numba-accelerated path is used.\n\n    Returns\n    -------\n    transformed :   (N, M, K) numpy array\n                    Transformed image in target space. The shape is\n                    determined by `out_shape` and `out_res`.\n\n    Notes\n    -----\n    This method uses backward mapping: for each output chunk, we compute\n    where the voxels came from in the source image using the transformation,\n    then interpolate the source image at those locations. So if your\n    transform goes from A -&gt; B, this method actually uses the inverse transform\n    under the hood to figure out where in A the voxels in B come from.\n\n    \"\"\"\n    if not isinstance(image, np.ndarray) or image.ndim != 3:\n        raise TypeError(\"`image` must be a 3D numpy array\")\n\n    # Because we are using backward mapping, we need to invert the transform,\n    # i.e. we're taking points in the target space, mapping them back to the\n    # source space and sampling the source image at those locations)\n    if hasattr(self, \"_inverted_copy\"):\n        # We're tracking the copy so that we can use the cache for subsequent\n        # transformations instead of having to re-read the file and re-ingest\n        # the deformation field\n        reg_inv = self._inverted_copy\n    else:\n        reg_inv = -self\n        # If cache, track the inverted copy (and the deformation field)\n        if cache:\n            reg_inv.use_cache = True\n            self._inverted_copy = reg_inv\n\n    if out_res is None:\n        out_res = image_res\n\n    if out_shape is None:\n        # Bounds in physical space\n        bounds = np.array(list(reg_inv.shape)[:-1][::-1]) * reg_inv.spacing\n        # Back to voxel space in output resolution\n        out_shape = np.ceil(bounds / out_res).astype(int)\n\n    # Generate the empty output array\n    out = np.zeros(out_shape, dtype=image.dtype)\n\n    try:\n        from .h5reg_numba import h5reg_warp_image_linear_constant\n\n        nb_available = True\n    except ModuleNotFoundError:\n        nb_available = False\n\n    # Numba path: linear interpolation + constant mode only but much faster\n    if order == 1 and mode == \"constant\":\n        if not nb_available:\n            global NUMBA_WARNING\n            if not NUMBA_WARNING:\n                logger.warning(\n                    \"For faster transforming of images, please install numba:\\n\"\n                    \"  pip install numba\"\n                )\n                NUMBA_WARNING = True\n        else:\n            reg_inv.full_ingest()\n            field = reg_inv.cache\n\n            with h5py.File(reg_inv.file, \"r\") as h5:\n                if reg_inv.level:\n                    ds = h5[reg_inv.level][reg_inv.field]\n                else:\n                    ds = h5[reg_inv.field]\n\n                spacing = np.asarray(ds.attrs[\"spacing\"], dtype=np.float64)\n                qmult = float(ds.attrs.get(\"quantization_multiplier\", 1))\n\n                if \"affine\" in ds.attrs:\n                    affine = np.ones((4, 4), dtype=np.float64)\n                    affine[:3, :4] = ds.attrs[\"affine\"].reshape(3, 4)\n                    apply_affine = 1 if reg_inv.direction == \"inverse\" else 2\n                else:\n                    affine = np.eye(4, dtype=np.float64)\n                    apply_affine = 0\n\n            return h5reg_warp_image_linear_constant(\n                image,\n                field,\n                spacing,\n                qmult,\n                affine,\n                apply_affine,\n                out,\n                out_res,\n                image_res,\n                float(cval),\n            )\n\n    # Pre-compute prefilter if needed for higher order interpolation\n    if order &gt; 1:\n        from scipy.ndimage import spline_filter\n\n        image = spline_filter(image, order=order)\n\n    for z in config.trange(\n        out_shape[2],\n        disable=not progress or config.pbar_hide,\n        desc=\"Warping image\",\n    ):\n        # Generate a grid of points in target voxel space for this z-slice\n        chunk_coords = np.mgrid[\n            0 : out_shape[0] : 1, 0 : out_shape[1] : 1, z : z + 1\n        ]\n        points = np.stack(\n            [\n                chunk_coords[0].flatten(),\n                chunk_coords[1].flatten(),\n                chunk_coords[2].flatten(),\n            ],\n            axis=-1,\n        )\n\n        # Convert to physical space\n        points = points.astype(np.float32) * out_res\n\n        # Transform points to source space using the inverse transform\n        try:\n            reg_inv._silenced_large_out_warning = True\n            points_xf = reg_inv.xform(points)\n        except Exception as e:\n            reg_inv._silenced_large_out_warning = False\n            raise e\n\n        # Convert back to voxel space in source image\n        points_xf_vox = points_xf / image_res\n\n        # Sample the source image at these transformed coordinates\n        sampled_values = map_coordinates(\n            image,\n            [points_xf_vox[:, 0], points_xf_vox[:, 1], points_xf_vox[:, 2]],\n            order=1,\n            mode=\"constant\",\n            cval=0,\n        )\n\n        # Reshape back to 2D and assign to this slice of the output image\n        out[:, :, z] = sampled_values.reshape(out_shape[0], out_shape[1])\n\n    return out\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.TPStransform","title":"<code>navis.transforms.TPStransform</code>","text":"<p>Thin Plate Spline transforms of 3D spatial data.</p> Notes <p>At least in my hands, <code>TPStransforms</code> are significantly faster than <code>MovingLeastSquaresTransforms</code>. The results are similar but not identical, so make sure to use the one that works best for your use case.</p> PARAMETER DESCRIPTION <code>landmarks_source</code> <pre><code>            Source landmarks as x/y/z coordinates.\n</code></pre> <p> TYPE: <code> (M, 3) numpy array</code> </p> <code>landmarks_target</code> <pre><code>            Target landmarks as x/y/z coordinates.\n</code></pre> <p> TYPE: <code> (M, 3) numpy array</code> </p> <code>batch_size</code> <pre><code>            Batch size for transforming points. The\n            thin-plate spline generating a (N, M) distance\n            matrix, where N is the number of points and M\n            is the number of source landmarks. Because\n            this can get prohibitively expensive, we're\n            batching the transformation by default.\n            Please note that the the overhead from batching\n            seems negligible.\n</code></pre> <p> TYPE: <code>       int</code> DEFAULT: <code>100000</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from navis import transforms\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Generate some mock landmarks\n&gt;&gt;&gt; src = np.array([[0, 0, 0], [10, 10, 10], [100, 100, 100], [80, 10, 30]])\n&gt;&gt;&gt; trg = np.array([[1, 15, 5], [9, 18, 21], [80, 99, 120], [5, 10, 80]])\n&gt;&gt;&gt; tr = transforms.thinplate.TPStransform(src, trg)\n&gt;&gt;&gt; points = np.array([[0, 0, 0], [50, 50, 50]])\n&gt;&gt;&gt; tr.xform(points)\narray([[ 1.        , 15.        ,  5.        ],\n       [40.55555556, 54.        , 65.        ]])\n</code></pre> Source code in <code>navis/transforms/thinplate.py</code> <pre><code>class TPStransform(BaseTransform):\n    \"\"\"Thin Plate Spline transforms of 3D spatial data.\n\n    Notes\n    -----\n    At least in my hands, `TPStransforms` are significantly faster than\n    `MovingLeastSquaresTransforms`. The results are similar but not identical,\n    so make sure to use the one that works best for your use case.\n\n    Parameters\n    ----------\n    landmarks_source :  (M, 3) numpy array\n                        Source landmarks as x/y/z coordinates.\n    landmarks_target :  (M, 3) numpy array\n                        Target landmarks as x/y/z coordinates.\n    batch_size :        int, optional\n                        Batch size for transforming points. The\n                        thin-plate spline generating a (N, M) distance\n                        matrix, where N is the number of points and M\n                        is the number of source landmarks. Because\n                        this can get prohibitively expensive, we're\n                        batching the transformation by default.\n                        Please note that the the overhead from batching\n                        seems negligible.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from navis import transforms\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; # Generate some mock landmarks\n    &gt;&gt;&gt; src = np.array([[0, 0, 0], [10, 10, 10], [100, 100, 100], [80, 10, 30]])\n    &gt;&gt;&gt; trg = np.array([[1, 15, 5], [9, 18, 21], [80, 99, 120], [5, 10, 80]])\n    &gt;&gt;&gt; tr = transforms.thinplate.TPStransform(src, trg)\n    &gt;&gt;&gt; points = np.array([[0, 0, 0], [50, 50, 50]])\n    &gt;&gt;&gt; tr.xform(points)\n    array([[ 1.        , 15.        ,  5.        ],\n           [40.55555556, 54.        , 65.        ]])\n\n    \"\"\"\n\n    def __init__(\n        self,\n        landmarks_source: np.ndarray,\n        landmarks_target: np.ndarray,\n        batch_size: int = 100_000,\n    ):\n        \"\"\"Initialize class.\"\"\"\n        self.batch_size = batch_size\n        self.source = np.asarray(landmarks_source)\n        self.target = np.asarray(landmarks_target)\n\n        # Some checks\n        if self.source.shape[1] != 3:\n            raise ValueError(f\"Expected (N, 3) array, got {self.source.shape}\")\n        if self.target.shape[1] != 3:\n            raise ValueError(f\"Expected (N, 3) array, got {self.target.shape}\")\n\n        if self.source.shape[0] != self.target.shape[0]:\n            raise ValueError(\n                \"Number of source landmarks must match number of target landmarks.\"\n            )\n\n        self._W, self._A = None, None\n\n    def __eq__(self, other) -&gt; bool:\n        \"\"\"Implement equality comparison.\"\"\"\n        if isinstance(other, TPStransform):\n            if self.source.shape[0] == other.source.shape[0]:\n                if np.all(self.source == other.source):\n                    if np.all(self.target == other.target):\n                        return True\n        return False\n\n    def __neg__(self) -&gt; \"TPStransform\":\n        \"\"\"Invert direction.\"\"\"\n        # Switch source and target\n        return TPStransform(self.target, self.source)\n\n    def _calc_tps_coefs(self):\n        # Calculate thinplate coefficients\n        self._W, self._A = mops.tps_coefs(self.source, self.target)\n\n    @property\n    def W(self):\n        if isinstance(self._W, type(None)):\n            # Calculate coefficients\n            self._calc_tps_coefs()\n        return self._W\n\n    @property\n    def A(self):\n        if isinstance(self._A, type(None)):\n            # Calculate coefficients\n            self._calc_tps_coefs()\n        return self._A\n\n    @property\n    def matrix_rigid(self):\n        \"\"\"Return the rigid transformation matrix.\"\"\"\n        # The first row in self.A is the translation vector\n        # The next 3x3 block is the rotation matrix\n        # Let's combine these into a typical 4x4 transformation matrix\n        # where the last row is [0, 0, 0, 1]\n        m = np.zeros((4, 4))\n        m[0:3, 0:3] = self.A[1:4, :].T\n        m[0:3, 3] = self.A[0, :]\n        m[3] = [0, 0, 0, 1]\n        return m\n\n    def copy(self):\n        \"\"\"Make copy.\"\"\"\n        x = TPStransform(self.source, self.target)\n\n        x.__dict__.update(self.__dict__)\n\n        return x\n\n    def xform(self, points: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Transform points.\n\n        Parameters\n        ----------\n        points :    (N, 3) array\n                    Points to transform.\n\n        Returns\n        -------\n        pointsxf :  (N, 3) array\n                    Transformed points.\n\n        \"\"\"\n        if isinstance(points, pd.DataFrame):\n            if any(c not in points for c in [\"x\", \"y\", \"z\"]):\n                raise ValueError(\"DataFrame must have x/y/z columns.\")\n            points = points[[\"x\", \"y\", \"z\"]].values\n\n        batch_size = self.batch_size if self.batch_size else points.shape[0]\n        points_xf = []\n        for i in range(0, points.shape[0], batch_size):\n            # Get the current batch of points\n            batch = points[i : i + batch_size]\n\n            # N.B. U is of shape (N, M) where N is the number of points and M is the\n            # number of source landmarks. This can get fairly expensive\n            # (which is precisely why we batch the transformation)!\n            U = mops.K_matrix(batch, self.source)\n            P = mops.P_matrix(batch)\n            # The warped pts are the affine part + the non-uniform part\n            points_xf.append(np.matmul(P, self.A) + np.matmul(U, self.W))\n\n        # Concatenate all batches\n        return np.concatenate(points_xf, axis=0)\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.TPStransform.matrix_rigid","title":"<code>matrix_rigid</code>  <code>property</code>","text":"<p>Return the rigid transformation matrix.</p>"},{"location":"reference/navis/transforms/#navis.transforms.TPStransform.__init__","title":"<code>__init__</code>","text":"<p>Initialize class.</p> Source code in <code>navis/transforms/thinplate.py</code> <pre><code>def __init__(\n    self,\n    landmarks_source: np.ndarray,\n    landmarks_target: np.ndarray,\n    batch_size: int = 100_000,\n):\n    \"\"\"Initialize class.\"\"\"\n    self.batch_size = batch_size\n    self.source = np.asarray(landmarks_source)\n    self.target = np.asarray(landmarks_target)\n\n    # Some checks\n    if self.source.shape[1] != 3:\n        raise ValueError(f\"Expected (N, 3) array, got {self.source.shape}\")\n    if self.target.shape[1] != 3:\n        raise ValueError(f\"Expected (N, 3) array, got {self.target.shape}\")\n\n    if self.source.shape[0] != self.target.shape[0]:\n        raise ValueError(\n            \"Number of source landmarks must match number of target landmarks.\"\n        )\n\n    self._W, self._A = None, None\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.TPStransform.copy","title":"<code>copy</code>","text":"<p>Make copy.</p> Source code in <code>navis/transforms/thinplate.py</code> <pre><code>def copy(self):\n    \"\"\"Make copy.\"\"\"\n    x = TPStransform(self.source, self.target)\n\n    x.__dict__.update(self.__dict__)\n\n    return x\n</code></pre>"},{"location":"reference/navis/transforms/#navis.transforms.TPStransform.xform","title":"<code>xform</code>","text":"<p>Transform points.</p> PARAMETER DESCRIPTION <code>points</code> <pre><code>    Points to transform.\n</code></pre> <p> TYPE: <code>   (N, 3) array</code> </p> RETURNS DESCRIPTION <code>pointsxf</code> <p>Transformed points.</p> <p> TYPE: <code>(N, 3) array</code> </p> Source code in <code>navis/transforms/thinplate.py</code> <pre><code>def xform(self, points: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Transform points.\n\n    Parameters\n    ----------\n    points :    (N, 3) array\n                Points to transform.\n\n    Returns\n    -------\n    pointsxf :  (N, 3) array\n                Transformed points.\n\n    \"\"\"\n    if isinstance(points, pd.DataFrame):\n        if any(c not in points for c in [\"x\", \"y\", \"z\"]):\n            raise ValueError(\"DataFrame must have x/y/z columns.\")\n        points = points[[\"x\", \"y\", \"z\"]].values\n\n    batch_size = self.batch_size if self.batch_size else points.shape[0]\n    points_xf = []\n    for i in range(0, points.shape[0], batch_size):\n        # Get the current batch of points\n        batch = points[i : i + batch_size]\n\n        # N.B. U is of shape (N, M) where N is the number of points and M is the\n        # number of source landmarks. This can get fairly expensive\n        # (which is precisely why we batch the transformation)!\n        U = mops.K_matrix(batch, self.source)\n        P = mops.P_matrix(batch)\n        # The warped pts are the affine part + the non-uniform part\n        points_xf.append(np.matmul(P, self.A) + np.matmul(U, self.W))\n\n    # Concatenate all batches\n    return np.concatenate(points_xf, axis=0)\n</code></pre>"},{"location":"reference/navis/utils/","title":"utils","text":""},{"location":"reference/navis/utils/#navis.utils.CMTKError","title":"<code>navis.utils.CMTKError</code>","text":"<p>Raise on CMTK transform error.</p> Source code in <code>navis/utils/exceptions.py</code> <pre><code>class CMTKError(Exception):\n    \"\"\"Raise on CMTK transform error.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.ConstructionError","title":"<code>navis.utils.ConstructionError</code>","text":"<p>Raised when class can't be constructed from input.</p> Source code in <code>navis/utils/exceptions.py</code> <pre><code>class ConstructionError(Exception):\n    \"\"\"Raised when class can't be constructed from input.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.VolumeError","title":"<code>navis.utils.VolumeError</code>","text":"<p>Raise when volume is invalid (e.g. not watertight).</p> Source code in <code>navis/utils/exceptions.py</code> <pre><code>class VolumeError(Exception):\n    \"\"\"Raise when volume is invalid (e.g. not watertight).\"\"\"\n    pass\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.check_vispy","title":"<code>navis.utils.check_vispy</code>","text":"<p>Check that vispy works.</p> RETURNS DESCRIPTION <code>vispy.Viewer</code> <p>A viewer which can be closed after use.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; viewer = navis.utils.check_vispy()\n&gt;&gt;&gt; # When the viewer and neurons show up...\n&gt;&gt;&gt; navis.close3d()\n</code></pre> Source code in <code>navis/utils/misc.py</code> <pre><code>def check_vispy():\n    \"\"\"Check that vispy works.\n\n    Returns\n    -------\n    vispy.Viewer\n        A viewer which can be closed after use.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; viewer = navis.utils.check_vispy()\n    &gt;&gt;&gt; # When the viewer and neurons show up...\n    &gt;&gt;&gt; navis.close3d()\n    \"\"\"\n    from ..data import example_neurons\n    nl = example_neurons()\n    return nl.plot3d(backend='vispy')\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.eval_conditions","title":"<code>navis.utils.eval_conditions</code>","text":"<p>Split list of strings into positive (no \"~\") and negative (\"~\").</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; eval_conditions('~negative condition')\n([], ['negative condition'])\n&gt;&gt;&gt; eval_conditions(['positive cond1', '~negative cond1', 'positive cond2'])\n(['positive cond1', 'positive cond2'], ['negative cond1'])\n</code></pre> Source code in <code>navis/utils/eval.py</code> <pre><code>def eval_conditions(x) -&gt; Tuple[List[bool], List[bool]]:\n    \"\"\"Split list of strings into positive (no \"~\") and negative (\"~\").\n\n    Examples\n    --------\n    &gt;&gt;&gt; eval_conditions('~negative condition')\n    ([], ['negative condition'])\n    &gt;&gt;&gt; eval_conditions(['positive cond1', '~negative cond1', 'positive cond2'])\n    (['positive cond1', 'positive cond2'], ['negative cond1'])\n\n    \"\"\"\n    x = make_iterable(x, force_type=str)\n\n    return [i for i in x if not i.startswith('~')], [i[1:] for i in x if i.startswith('~')]\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.eval_id","title":"<code>navis.utils.eval_id</code>","text":"<p>Evaluate neuron ID(s).</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>           For Neuron/List or pandas.DataFrames/Series will\n           look for `id` attribute/column.\n</code></pre> <p> TYPE: <code>               str | uuid.UUID | Tree/MeshNeuron | NeuronList | DataFrame</code> </p> <code>warn_duplicates</code> <pre><code>           If True, will warn if duplicate IDs are found.\n           Only applies to NeuronLists.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>list</code> <p>List containing IDs.</p> Source code in <code>navis/utils/eval.py</code> <pre><code>def eval_id(x: Union[uuid.UUID, str, 'core.NeuronObject', pd.DataFrame],\n              warn_duplicates: bool = True) -&gt; List[uuid.UUID]:\n    \"\"\"Evaluate neuron ID(s).\n\n    Parameters\n    ----------\n    x :                str | uuid.UUID | Tree/MeshNeuron | NeuronList | DataFrame\n                       For Neuron/List or pandas.DataFrames/Series will\n                       look for `id` attribute/column.\n    warn_duplicates :  bool, optional\n                       If True, will warn if duplicate IDs are found.\n                       Only applies to NeuronLists.\n\n    Returns\n    -------\n    list\n                    List containing IDs.\n\n    \"\"\"\n    if isinstance(x, (uuid.UUID, str, int, np.integer)):\n        return [x]\n    elif isinstance(x, (list, np.ndarray, set)):\n        uu: List[uuid.UUID] = []\n        for e in x:\n            temp = eval_id(e, warn_duplicates=warn_duplicates)\n            if isinstance(temp, (list, np.ndarray)):\n                uu += temp\n            else:\n                uu.append(temp)  # type: ignore\n        return sorted(set(uu), key=uu.index)\n    elif isinstance(x, core.BaseNeuron):\n        return [x.id]\n    elif isinstance(x, core.NeuronList):\n        if len(x.id) != len(set(x.id)) and warn_duplicates:\n            logger.warning('Duplicate IDs found in NeuronList.'\n                           'The function you are using might not respect '\n                           'fragments of the same neuron. For explanation see '\n                           'http://navis.readthedocs.io/en/latest/source/conn'\n                           'ectivity_analysis.html.')\n        return list(x.id)\n    elif isinstance(x, pd.DataFrame):\n        if 'id' not in x.columns:\n            raise ValueError('Expect \"id\" column in pandas DataFrames')\n        return x.id.tolist()\n    elif isinstance(x, pd.Series):\n        if x.name == 'id':\n            return x.tolist()\n        elif 'id' in x:\n            return [x.id]\n        else:\n            raise ValueError(f'Unable to extract ID from pandas series {x}')\n    elif isinstance(x, type(None)):\n        return None\n    else:\n        msg = f'Unable to extract ID(s) from data of type \"{type(x)}\"'\n        logger.error(msg)\n        raise TypeError(msg)\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.eval_neurons","title":"<code>navis.utils.eval_neurons</code>","text":"<p>Extract neurons.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>           Data to be checked for neurons.\n</code></pre> <p> TYPE: <code>Any</code> </p> <code>warn_duplicates</code> <pre><code>           If True, will warn if duplicate neurons are found.\n           Only applies to NeuronLists.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>True</code> </p> <code>raise_other</code> <pre><code>           If True, will raise error if non- neurons are found.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>list</code> <p>List containing neurons.</p> <code>None</code> <p>If no neurons found.</p> Source code in <code>navis/utils/eval.py</code> <pre><code>def eval_neurons(x: Any,\n                 warn_duplicates: bool = True,\n                 raise_other: bool = True) -&gt; Optional[List['core.TreeNeuron']]:\n    \"\"\"Extract neurons.\n\n    Parameters\n    ----------\n    x\n                       Data to be checked for neurons.\n    warn_duplicates :  bool, optional\n                       If True, will warn if duplicate neurons are found.\n                       Only applies to NeuronLists.\n    raise_other :      bool, optional\n                       If True, will raise error if non- neurons are found.\n\n    Returns\n    -------\n    list\n                    List containing neurons.\n    None\n                    If no neurons found.\n\n    \"\"\"\n    if isinstance(x, core.BaseNeuron):\n        return [x]\n    elif isinstance(x, (list, np.ndarray, set)):\n        neurons: List['core.BaseNeuron'] = []\n        for e in x:\n            temp = eval_neurons(e, warn_duplicates=warn_duplicates,\n                                raise_other=raise_other)\n            if isinstance(temp, (list, np.ndarray)):\n                neurons += temp\n            elif temp:\n                neurons.append(temp)\n        return sorted(set(neurons), key=neurons.index)\n    elif isinstance(x, core.NeuronList):\n        if len(x.id) != len(set(x.id)) and warn_duplicates:\n            logger.warning('Duplicate IDs found in NeuronList.'\n                           'The function you are using might not respect '\n                           'fragments of the same neuron. For explanation see '\n                           'http://navis.readthedocs.io/en/latest/source/conn'\n                           'ectivity_analysis.html.')\n        return x.neurons\n    elif isinstance(x, type(None)):\n        return None\n    elif raise_other:\n        msg = f'Unable to extract neurons from data of type \"{type(x)}\"'\n        logger.error(msg)\n        raise TypeError(msg)\n    return None\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.eval_node_ids","title":"<code>navis.utils.eval_node_ids</code>","text":"<p>Extract node IDs from data.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Your options are either::\n        1. int or list of ints will be assumed to be node IDs\n        2. str or list of str will be checked if convertible to int\n        3. For TreeNeuron/List or pandas.DataFrames will try\n           to extract node IDs\n</code></pre> <p> TYPE: <code>            int | str | TreeNeuron | NeuronList | DataFrame</code> </p> RETURNS DESCRIPTION <code>list</code> <p>List containing node IDs (integer)</p> Source code in <code>navis/utils/eval.py</code> <pre><code>def eval_node_ids(x: Union[int, str,\n                           Sequence[Union[str, int]],\n                           'core.NeuronObject',\n                           pd.DataFrame]\n                  ) -&gt; List[int]:\n    \"\"\"Extract node IDs from data.\n\n    Parameters\n    ----------\n    x :             int | str | TreeNeuron | NeuronList | DataFrame\n                    Your options are either::\n                    1. int or list of ints will be assumed to be node IDs\n                    2. str or list of str will be checked if convertible to int\n                    3. For TreeNeuron/List or pandas.DataFrames will try\n                       to extract node IDs\n\n    Returns\n    -------\n    list\n                    List containing node IDs (integer)\n\n    \"\"\"\n    if isinstance(x, (int, np.integer)):\n        return [x]\n    elif isinstance(x, str):\n        try:\n            return [int(x)]\n        except BaseException:\n            raise TypeError(f'Unable to extract node ID from string \"{x}\"')\n    elif isinstance(x, (set, list, np.ndarray)):\n        # Check non-integer entries\n        ids: List[int] = []\n        for e in x:\n            temp = eval_node_ids(e)\n            if isinstance(temp, (list, np.ndarray)):\n                ids += temp\n            else:\n                ids.append(temp)  # type: ignore\n        # Preserving the order after making a set is super costly\n        # return sorted(set(ids), key=ids.index)\n        return list(set(ids))\n    elif isinstance(x, core.TreeNeuron):\n        return x.nodes.node_id.astype(int).tolist()\n    elif isinstance(x, core.NeuronList):\n        to_return: List[int] = []\n        for n in x:\n            to_return += n.nodes.node_id.astype(int).tolist()\n        return to_return\n    elif isinstance(x, (pd.DataFrame, pd.Series)):\n        to_return = []\n        if 'node_id' in x:\n            to_return += x.node_id.astype(int).tolist()\n        return to_return\n    else:\n        raise TypeError(f'Unable to extract node IDs from type {type(x)}')\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.eval_param","title":"<code>navis.utils.eval_param</code>","text":"<p>Check if parameter has expected type and/or value.</p> PARAMETER DESCRIPTION <code>value</code> <pre><code>            Value to be checked.\n</code></pre> <p> TYPE: <code>            any</code> </p> <code>name</code> <pre><code>            Name of the parameter. Used for warnings/exceptions.\n</code></pre> <p> TYPE: <code>             str</code> </p> <code>allowed_values</code> <pre><code>            Iterable containing the allowed values.\n</code></pre> <p> TYPE: <code>   tuple</code> DEFAULT: <code>None</code> </p> <code>allowed_types</code> <pre><code>            Iterable containing the allowed types.\n</code></pre> <p> TYPE: <code>Optional[tuple]</code> DEFAULT: <code>None</code> </p> <code>on_error</code> <pre><code>            What to do if `value` is not in `allowed_values`.\n</code></pre> <p> TYPE: <code>         \"raise\" | \"warn\"</code> DEFAULT: <code>'raise'</code> </p> RETURNS DESCRIPTION <code>None</code> Source code in <code>navis/utils/eval.py</code> <pre><code>def eval_param(value: Any,\n               name: str,\n               allowed_values: Optional[tuple] = None,\n               allowed_types: Optional[tuple] = None,\n               on_error: str = 'raise'):\n    \"\"\"Check if parameter has expected type and/or value.\n\n    Parameters\n    ----------\n    value :             any\n                        Value to be checked.\n    name :              str\n                        Name of the parameter. Used for warnings/exceptions.\n    allowed_values :    tuple\n                        Iterable containing the allowed values.\n    allowed_types  :    tuple\n                        Iterable containing the allowed types.\n    on_error :          \"raise\" | \"warn\"\n                        What to do if `value` is not in `allowed_values`.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    assert on_error in ('raise', 'warn')\n    assert isinstance(allowed_values, (tuple, type(None)))\n    assert isinstance(allowed_types, (tuple, type(None)))\n\n    if allowed_types:\n        if not isinstance(value, allowed_types):\n            msg = (f'Unexpected type for \"{name}\": {type(value)}. '\n                   f'Allowed type(s): {\", \".join([str(t) for t in allowed_types])}')\n            if on_error == 'raise':\n                raise ValueError(msg)\n            elif on_error == 'warn':\n                logger.warning(msg)\n\n    if allowed_values:\n        if value not in allowed_values:\n            msg = (f'Unexpected value for \"{name}\": {value}. '\n                   f'Allowed value(s): {\", \".join([str(t) for t in allowed_values])}')\n            if on_error == 'raise':\n                raise ValueError(msg)\n            elif on_error == 'warn':\n                logger.warning(msg)\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.is_blender","title":"<code>navis.utils.is_blender</code>","text":"<p>Test if navis is run inside Blender.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from navis.utils import is_blender\n&gt;&gt;&gt; # If run outside Blender\n&gt;&gt;&gt; is_blender()\nFalse\n</code></pre> Source code in <code>navis/utils/misc.py</code> <pre><code>def is_blender() -&gt; bool:\n    \"\"\"Test if navis is run inside Blender.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from navis.utils import is_blender\n    &gt;&gt;&gt; # If run outside Blender\n    &gt;&gt;&gt; is_blender()\n    False\n\n    \"\"\"\n    return 'blender' in sys.executable.lower()\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.is_iterable","title":"<code>navis.utils.is_iterable</code>","text":"<p>Test if input is iterable (but not str).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from navis.utils import is_iterable\n&gt;&gt;&gt; is_iterable(['a'])\nTrue\n&gt;&gt;&gt; is_iterable('a')\nFalse\n&gt;&gt;&gt; is_iterable({'a': 1})\nTrue\n</code></pre> Source code in <code>navis/utils/iterables.py</code> <pre><code>def is_iterable(x: Any) -&gt; bool:\n    \"\"\"Test if input is iterable (but not str).\n\n    Examples\n    --------\n    &gt;&gt;&gt; from navis.utils import is_iterable\n    &gt;&gt;&gt; is_iterable(['a'])\n    True\n    &gt;&gt;&gt; is_iterable('a')\n    False\n    &gt;&gt;&gt; is_iterable({'a': 1})\n    True\n\n    \"\"\"\n    if isinstance(x, pint.Quantity):\n        x = x.magnitude\n\n    if isinstance(x, Iterable) and not isinstance(x, (six.string_types, pd.DataFrame)):\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.is_jupyter","title":"<code>navis.utils.is_jupyter</code>","text":"<p>Test if navis is run in a Jupyter notebook.</p> <p>Also returns True if inside Google colaboratory!</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from navis.utils import is_jupyter\n&gt;&gt;&gt; # If run outside a Jupyter environment\n&gt;&gt;&gt; is_jupyter()\nFalse\n</code></pre> Source code in <code>navis/utils/misc.py</code> <pre><code>def is_jupyter() -&gt; bool:\n    \"\"\"Test if navis is run in a Jupyter notebook.\n\n    Also returns True if inside Google colaboratory!\n\n    Examples\n    --------\n    &gt;&gt;&gt; from navis.utils import is_jupyter\n    &gt;&gt;&gt; # If run outside a Jupyter environment\n    &gt;&gt;&gt; is_jupyter()\n    False\n\n    \"\"\"\n    return _type_of_script() in ('jupyter', 'colab')\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.is_mesh","title":"<code>navis.utils.is_mesh</code>","text":"<p>Check if object is mesh (i.e. contains vertices and faces).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; is_mesh(navis.example_neurons(1))\nFalse\n&gt;&gt;&gt; is_mesh(navis.example_volume('LH'))\nTrue\n</code></pre> Source code in <code>navis/utils/eval.py</code> <pre><code>def is_mesh(x) -&gt; Tuple[List[bool], List[bool]]:\n    \"\"\"Check if object is mesh (i.e. contains vertices and faces).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; is_mesh(navis.example_neurons(1))\n    False\n    &gt;&gt;&gt; is_mesh(navis.example_volume('LH'))\n    True\n\n    \"\"\"\n    if hasattr(x, 'vertices') and hasattr(x, 'faces'):\n        return True\n\n    return False\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.is_numeric","title":"<code>navis.utils.is_numeric</code>","text":"<p>Determine whether the argument has a numeric datatype.</p> <p>Booleans, unsigned integers, signed integers, floats and complex numbers are the kinds of numeric datatype.</p> <p>Arrays with \"dtype=object\" will return True if data can be cast to floats.</p> PARAMETER DESCRIPTION <code>array</code> <pre><code>        The array to check.\n</code></pre> <p> TYPE: <code>        array-like</code> </p> <code>bool_numeric</code> <pre><code>        If True (default), we count booleans as numeric data types.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>True</code> </p> <code>try_convert</code> <pre><code>        If True, will try to convert array to floats.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>is_numeric</code> <p>True if the array has a numeric datatype, False if not.</p> <p> TYPE: <code>`bool`</code> </p> Source code in <code>navis/utils/eval.py</code> <pre><code>def is_numeric(array: np.ndarray,\n               bool_numeric: bool = True,\n               try_convert: bool = False) -&gt; bool:\n    \"\"\"Determine whether the argument has a numeric datatype.\n\n    Booleans, unsigned integers, signed integers, floats and complex\n    numbers are the kinds of numeric datatype.\n\n    Arrays with \"dtype=object\" will return True if data can be cast to floats.\n\n    Parameters\n    ----------\n    array :         array-like\n                    The array to check.\n    bool_numeric :  bool\n                    If True (default), we count booleans as numeric data types.\n    try_convert :   bool\n                    If True, will try to convert array to floats.\n\n    Returns\n    -------\n    is_numeric :    `bool`\n                    True if the array has a numeric datatype, False if not.\n\n    \"\"\"\n    array = np.asarray(array)\n\n    # If array\n    if array.dtype.kind == 'O' and try_convert:\n        try:\n            array = array.astype(float)\n        except ValueError:\n            pass\n\n    if not bool_numeric:\n        _NUMERIC_KINDS_NO_BOOL = _NUMERIC_KINDS.copy()\n        _NUMERIC_KINDS_NO_BOOL.remove('b')\n        return array.dtype.kind in _NUMERIC_KINDS_NO_BOOL\n\n    return array.dtype.kind in _NUMERIC_KINDS\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.is_url","title":"<code>navis.utils.is_url</code>","text":"<p>Return True if str is URL.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from navis.utils import is_url\n&gt;&gt;&gt; is_url('www.google.com')\nFalse\n&gt;&gt;&gt; is_url('http://www.google.com')\nTrue\n&gt;&gt;&gt; is_url(\"ftp://download.ft-server.org:8000\")\nTrue\n</code></pre> Source code in <code>navis/utils/misc.py</code> <pre><code>def is_url(x: str) -&gt; bool:\n    \"\"\"Return True if str is URL.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from navis.utils import is_url\n    &gt;&gt;&gt; is_url('www.google.com')\n    False\n    &gt;&gt;&gt; is_url('http://www.google.com')\n    True\n    &gt;&gt;&gt; is_url(\"ftp://download.ft-server.org:8000\")\n    True\n\n    \"\"\"\n    parsed = urllib.parse.urlparse(x)\n\n    if parsed.netloc and parsed.scheme:\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.lock_neuron","title":"<code>navis.utils.lock_neuron</code>","text":"<p>Lock neuron while function is executed.</p> <p>This makes sure that temporary attributes aren't re-calculated as changes are being made.</p> Source code in <code>navis/utils/decorators.py</code> <pre><code>def lock_neuron(function):\n    \"\"\"Lock neuron while function is executed.\n\n    This makes sure that temporary attributes aren't re-calculated as changes\n    are being made.\n\n    \"\"\"\n\n    @wraps(function)\n    def wrapper(*args, **kwargs):\n        # Lazy import to avoid issues with circular imports and pickling\n        from .. import core\n\n        # Lock if first argument is a neuron\n        if isinstance(args[0], core.BaseNeuron):\n            args[0]._lock = getattr(args[0], \"_lock\", 0) + 1\n        try:\n            # Execute function\n            res = function(*args, **kwargs)\n        except BaseException:\n            raise\n        finally:\n            # Unlock neuron\n            if isinstance(args[0], core.BaseNeuron):\n                args[0]._lock -= 1\n        # Return result\n        return res\n\n    return wrapper\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.make_iterable","title":"<code>navis.utils.make_iterable</code>","text":"<p>Force input into a numpy array.</p> <p>For dicts, keys will be turned into array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from navis.utils import make_iterable\n&gt;&gt;&gt; make_iterable(1)\narray([1])\n&gt;&gt;&gt; make_iterable([1])\narray([1])\n&gt;&gt;&gt; make_iterable({'a': 1})\narray(['a'], dtype='&lt;U1')\n</code></pre> Source code in <code>navis/utils/iterables.py</code> <pre><code>def make_iterable(x,\n                  force_type: Optional[type] = None\n                  ) -&gt; np.ndarray:\n    \"\"\"Force input into a numpy array.\n\n    For dicts, keys will be turned into array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from navis.utils import make_iterable\n    &gt;&gt;&gt; make_iterable(1)\n    array([1])\n    &gt;&gt;&gt; make_iterable([1])\n    array([1])\n    &gt;&gt;&gt; make_iterable({'a': 1})\n    array(['a'], dtype='&lt;U1')\n\n    \"\"\"\n    # Quantities are a special case\n    if isinstance(x, pint.Quantity) and not isinstance(x.magnitude, np.ndarray):\n        return config.ureg.Quantity(np.array([x.magnitude]), x.units)\n\n    if not isinstance(x, Iterable) or isinstance(x, six.string_types):\n        x = [x]\n\n    if isinstance(x, (dict, set)):\n        x = list(x)\n\n    return np.asarray(x, dtype=force_type)\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.make_non_iterable","title":"<code>navis.utils.make_non_iterable</code>","text":"<p>Turn input into non-iterable, if it isn't already.</p> <p>Will raise error if <code>len(x) &gt; 1</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from navis.utils import make_non_iterable\n&gt;&gt;&gt; make_non_iterable([1])\n1\n&gt;&gt;&gt; make_non_iterable(1)\n1\n&gt;&gt;&gt; make_non_iterable([1, 2])\nTraceback (most recent call last):\nValueError: Iterable must not contain more than one entry.\n</code></pre> Source code in <code>navis/utils/iterables.py</code> <pre><code>def make_non_iterable(x):\n    \"\"\"Turn input into non-iterable, if it isn't already.\n\n    Will raise error if `len(x) &gt; 1`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from navis.utils import make_non_iterable\n    &gt;&gt;&gt; make_non_iterable([1])\n    1\n    &gt;&gt;&gt; make_non_iterable(1)\n    1\n    &gt;&gt;&gt; make_non_iterable([1, 2])\n    Traceback (most recent call last):\n    ValueError: Iterable must not contain more than one entry.\n\n    \"\"\"\n    if not is_iterable(x):\n        return x\n    elif len(x) == 1:\n        return x[0]\n    else:\n        raise ValueError('Iterable must not contain more than one entry.')\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.make_url","title":"<code>navis.utils.make_url</code>","text":"<p>Generate URL.</p> PARAMETER DESCRIPTION <code>*args</code> <pre><code>    Will be turned into the URL. For example:\n\n        &gt;&gt;&gt; make_url('http://neuromorpho.org', 'neuron', 'fields')\n        'http://neuromorpho.org/neuron/fields'\n</code></pre> <p> TYPE: <code>str</code> DEFAULT: <code>()</code> </p> <code>**GET</code> <pre><code>    Keyword arguments are assumed to be GET request queries\n    and will be encoded in the url. For example:\n\n        &gt;&gt;&gt; make_url('http://neuromorpho.org', 'neuron', 'fields',\n        ...          page=1)\n        'http://neuromorpho.org/neuron/fields?page=1'\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>url</code> <p> TYPE: <code>str</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from navis.utils import is_url, make_url\n&gt;&gt;&gt; url = make_url('http://www.google.com', 'test', query='test')\n&gt;&gt;&gt; url\n'http://www.google.com/test?query=test'\n&gt;&gt;&gt; is_url(url)\nTrue\n</code></pre> Source code in <code>navis/utils/misc.py</code> <pre><code>def make_url(baseurl, *args: str, **GET) -&gt; str:\n    \"\"\"Generate URL.\n\n    Parameters\n    ----------\n    *args\n                Will be turned into the URL. For example:\n\n                    &gt;&gt;&gt; make_url('http://neuromorpho.org', 'neuron', 'fields')\n                    'http://neuromorpho.org/neuron/fields'\n\n    **GET\n                Keyword arguments are assumed to be GET request queries\n                and will be encoded in the url. For example:\n\n                    &gt;&gt;&gt; make_url('http://neuromorpho.org', 'neuron', 'fields',\n                    ...          page=1)\n                    'http://neuromorpho.org/neuron/fields?page=1'\n\n    Returns\n    -------\n    url :       str\n\n\n    Examples\n    --------\n    &gt;&gt;&gt; from navis.utils import is_url, make_url\n    &gt;&gt;&gt; url = make_url('http://www.google.com', 'test', query='test')\n    &gt;&gt;&gt; url\n    'http://www.google.com/test?query=test'\n    &gt;&gt;&gt; is_url(url)\n    True\n\n    \"\"\"\n    url = baseurl\n    # Generate the URL\n    for arg in args:\n        arg_str = str(arg)\n        joiner = '' if url.endswith('/') else '/'\n        relative = arg_str[1:] if arg_str.startswith('/') else arg_str\n        url = requests.compat.urljoin(url + joiner, relative)\n    if GET:\n        url += f'?{urllib.parse.urlencode(GET)}'\n    return url\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.make_volume","title":"<code>navis.utils.make_volume</code>","text":"<p>Try making a navis.Volume from input object.</p> Source code in <code>navis/utils/misc.py</code> <pre><code>def make_volume(x: Any) -&gt; 'core.Volume':\n    \"\"\"Try making a navis.Volume from input object.\"\"\"\n    if isinstance(x, core.Volume):\n        return x\n    if is_mesh(x):\n        inits = dict(vertices=x.vertices, faces=x.faces)\n        for p in ['name', 'id', 'color']:\n            if hasattr(x, p):\n                inits[p] = getattr(x, p, None)\n        return core.Volume(**inits)\n\n    raise TypeError(f'Unable to coerce input of type \"{type(x)}\" to navis.Volume')\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.map_neuronlist","title":"<code>navis.utils.map_neuronlist</code>","text":"<p>Decorate function to run on all neurons in the NeuronList.</p> <p>This also updates the docstring.</p> PARAMETER DESCRIPTION <code>desc</code> <pre><code>         Descriptor to show in the progress bar if run over multiple\n         neurons.\n</code></pre> <p> TYPE: <code>          str</code> DEFAULT: <code>''</code> </p> <code>can_zip</code> <p> TYPE: <code>List[Union[str, int]]</code> DEFAULT: <code>[]</code> </p> <code>must_zip</code> <pre><code>         Names of keyword arguments that need to be zipped together\n         with the neurons in the neuronlist. For example:\n\n           some_function(NeuronList([n1, n2, n3]), [p1, p2, p3])\n\n         Should be executed as:\n\n           some_function(n1, p1)\n           some_function(n2, p2)\n           some_function(n3, p3)\n\n         `can_zip` will be zipped only if the length matches the\n         length of the neuronlist. If a `can_zip` argument has only\n         one value it will be re-used for all neurons.\n\n         `must_zip` arguments have to have one value for each of the\n         neurons.\n\n         Single `None` values are always just passed through.\n\n         Note that for this to consistently work the parameters in\n         question have to be keyword-only (*).\n</code></pre> <p> TYPE: <code>      list</code> DEFAULT: <code>[]</code> </p> <code>allow_parallel</code> <pre><code>         If True and the function is called with `parallel=True`,\n         will use multiple cores to process the neuronlist. Number\n         of cores a can be set using `n_cores` keyword argument.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>navis/utils/decorators.py</code> <pre><code>def map_neuronlist(\n    desc: str = \"\",\n    can_zip: List[Union[str, int]] = [],\n    must_zip: List[Union[str, int]] = [],\n    allow_parallel: bool = False,\n):\n    \"\"\"Decorate function to run on all neurons in the NeuronList.\n\n    This also updates the docstring.\n\n    Parameters\n    ----------\n    desc :           str\n                     Descriptor to show in the progress bar if run over multiple\n                     neurons.\n    can_zip/\n    must_zip :       list\n                     Names of keyword arguments that need to be zipped together\n                     with the neurons in the neuronlist. For example:\n\n                       some_function(NeuronList([n1, n2, n3]), [p1, p2, p3])\n\n                     Should be executed as:\n\n                       some_function(n1, p1)\n                       some_function(n2, p2)\n                       some_function(n3, p3)\n\n                     `can_zip` will be zipped only if the length matches the\n                     length of the neuronlist. If a `can_zip` argument has only\n                     one value it will be re-used for all neurons.\n\n                     `must_zip` arguments have to have one value for each of the\n                     neurons.\n\n                     Single `None` values are always just passed through.\n\n                     Note that for this to consistently work the parameters in\n                     question have to be keyword-only (*).\n    allow_parallel : bool\n                     If True and the function is called with `parallel=True`,\n                     will use multiple cores to process the neuronlist. Number\n                     of cores a can be set using `n_cores` keyword argument.\n\n    \"\"\"\n\n    # TODO:\n    # - make can_zip/must_zip work with positional-only argumens to, i.e. let\n    #   it work with integers instead of strings\n    def decorator(function):\n        @wraps(function)\n        def wrapper(*args, **kwargs):\n            from .. import core\n\n            # Get the function's signature\n            sig = inspect.signature(function)\n\n            try:\n                fnname = function.__name__\n            except BaseException:\n                fnname = str(function)\n\n            parallel = kwargs.pop(\"parallel\", False)\n            if parallel and not allow_parallel:\n                raise ValueError(\n                    f\"Function {fnname} does not support parallel processing.\"\n                )\n\n            # First, we need to extract the neuronlist\n            if args:\n                # If there are positional arguments, the first one is\n                # the input neuron(s)\n                nl = args[0]\n                nl_key = \"__args\"\n            else:\n                # If not, we need to look for the name of the first argument\n                # in the signature\n                nl_key = list(sig.parameters.keys())[0]\n                nl = kwargs.get(nl_key, None)\n\n            # Complain if we did not get what we expected\n            if isinstance(nl, type(None)):\n                raise ValueError(\n                    \"Unable to identify the neurons for call\"\n                    f\"{fnname}:\\n {args}\\n {kwargs}\"\n                )\n\n            # If we have a neuronlist\n            if isinstance(nl, core.NeuronList):\n                # Pop the neurons from kwargs or args so we don't pass the\n                # neurons twice\n                if nl_key == \"__args\":\n                    args = args[1:]\n                else:\n                    _ = kwargs.pop(nl_key)\n\n                # Check \"can zip\" arguments\n                for p in can_zip:\n                    # Skip if not present or is None\n                    if p not in kwargs or isinstance(kwargs[p], type(None)):\n                        continue\n\n                    if is_iterable(kwargs[p]):\n                        # If iterable but length does not match: complain\n                        le = len(kwargs[p])\n                        if le != len(nl):\n                            raise ValueError(\n                                f\"Got {le} values of `{p}` for {len(nl)} neurons.\"\n                            )\n\n                # Parse \"must zip\" arguments\n                for p in must_zip:\n                    # Skip if not present or is None\n                    if p not in kwargs or isinstance(kwargs[p], type(None)):\n                        continue\n\n                    values = make_iterable(kwargs[p])\n                    if len(values) != len(nl):\n                        raise ValueError(\n                            f\"Got {len(values)} values of `{p}` for {len(nl)} neurons.\"\n                        )\n\n                # If we use parallel processing it makes sense to modify neurons\n                # \"inplace\" since they will be copied into the child processes\n                # anyway and that way we can avoid making an additional copy\n                if \"inplace\" in kwargs:\n                    # First check keyword arguments\n                    inplace = kwargs[\"inplace\"]\n                elif \"inplace\" in sig.parameters:\n                    # Next check signatures default\n                    inplace = sig.parameters[\"inplace\"].default\n                else:\n                    # All things failing assume it's not inplace\n                    inplace = False\n\n                if parallel and \"inplace\" in sig.parameters:\n                    kwargs[\"inplace\"] = True\n\n                # Prepare processor\n                n_cores = kwargs.pop(\"n_cores\", os.cpu_count() // 2)\n                chunksize = kwargs.pop(\"chunksize\", 1)\n                excl = list(kwargs.keys()) + list(range(1, len(args) + 1))\n                proc = core.NeuronProcessor(\n                    nl,\n                    function,\n                    parallel=parallel,\n                    desc=desc,\n                    warn_inplace=False,\n                    progress=kwargs.pop(\"progress\", True),\n                    omit_failures=kwargs.pop(\"omit_failures\", False),\n                    chunksize=chunksize,\n                    exclude_zip=excl,\n                    n_cores=n_cores,\n                )\n                # Apply function\n                res = proc(nl, *args, **kwargs)\n\n                # When using parallel processing, the neurons will not actually\n                # have been modified inplace - in that case we will simply\n                # replace the neurons in `nl`\n                if inplace:\n                    nl.neurons = res.neurons\n                else:\n                    nl = res\n\n                return nl\n            else:\n                # If single neuron just pass through\n                return function(*args, **kwargs)\n\n        # Update the docstring\n        wrapper = map_neuronlist_update_docstring(wrapper, allow_parallel)\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.map_neuronlist_df","title":"<code>navis.utils.map_neuronlist_df</code>","text":"<p>Decorate function to run on all neurons in the NeuronList.</p> <p>This version of the decorator is meant for functions that return a DataFrame. This decorator will add a <code>neuron</code> column with the respective neuron's ID and will then concatenate the dataframes.</p> PARAMETER DESCRIPTION <code>desc</code> <pre><code>         Descriptor to show in the progress bar if run over multiple\n         neurons.\n</code></pre> <p> TYPE: <code>          str</code> DEFAULT: <code>''</code> </p> <code>id_col</code> <pre><code>         Name of the ID column to be added to the results dataframe.\n</code></pre> <p> TYPE: <code>        str</code> DEFAULT: <code>'neuron'</code> </p> <code>reset_index</code> <pre><code>         Whether to reset the index of the dataframe after\n         concatenating.\n</code></pre> <p> TYPE: <code>   bool</code> DEFAULT: <code>True</code> </p> <code>allow_parallel</code> <pre><code>         If True and the function is called with `parallel=True`,\n         will use multiple cores to process the neuronlist. Number\n         of cores a can be set using `n_cores` keyword argument.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>navis/utils/decorators.py</code> <pre><code>def map_neuronlist_df(\n    desc: str = \"\",\n    id_col: str = \"neuron\",\n    reset_index: bool = True,\n    allow_parallel: bool = False,\n):\n    \"\"\"Decorate function to run on all neurons in the NeuronList.\n\n    This version of the decorator is meant for functions that return a\n    DataFrame. This decorator will add a `neuron` column with the respective\n    neuron's ID and will then concatenate the dataframes.\n\n    Parameters\n    ----------\n    desc :           str\n                     Descriptor to show in the progress bar if run over multiple\n                     neurons.\n    id_col :         str\n                     Name of the ID column to be added to the results dataframe.\n    reset_index :    bool\n                     Whether to reset the index of the dataframe after\n                     concatenating.\n    allow_parallel : bool\n                     If True and the function is called with `parallel=True`,\n                     will use multiple cores to process the neuronlist. Number\n                     of cores a can be set using `n_cores` keyword argument.\n\n    \"\"\"\n\n    # TODO:\n    # - make can_zip/must_zip work with positional-only argumens to, i.e. let\n    #   it work with integers instead of strings\n    def decorator(function):\n        @wraps(function)\n        def wrapper(*args, **kwargs):\n            # Lazy import to avoid issues with circular imports and pickling\n            from .. import core\n\n            # Get the function's signature\n            sig = inspect.signature(function)\n\n            try:\n                fnname = function.__name__\n            except BaseException:\n                fnname = str(function)\n\n            parallel = kwargs.pop(\"parallel\", False)\n            if parallel and not allow_parallel:\n                raise ValueError(\n                    f\"Function {fnname} does not allow parallel processing.\"\n                )\n\n            # First, we need to extract the neuronlist\n            if args:\n                # If there are positional arguments, the first one is\n                # the input neuron(s)\n                nl = args[0]\n                nl_key = \"__args\"\n            else:\n                # If not, we need to look for the name of the first argument\n                # in the signature\n                nl_key = list(sig.parameters.keys())[0]\n                nl = kwargs.get(nl_key, None)\n\n            # Complain if we did not get what we expected\n            if isinstance(nl, type(None)):\n                raise ValueError(\n                    \"Unable to identify the neurons for call\"\n                    f\"{fnname}:\\n {args}\\n {kwargs}\"\n                )\n\n            # If we have a neuronlist\n            if isinstance(nl, core.NeuronList):\n                # Pop the neurons from kwargs or args so we don't pass the\n                # neurons twice\n                if nl_key == \"__args\":\n                    args = args[1:]\n                else:\n                    _ = kwargs.pop(nl_key)\n\n                # Prepare processor\n                n_cores = kwargs.pop(\"n_cores\", os.cpu_count() // 2)\n                chunksize = kwargs.pop(\"chunksize\", 1)\n                excl = list(kwargs.keys()) + list(range(1, len(args) + 1))\n                proc = core.NeuronProcessor(\n                    nl,\n                    function,\n                    parallel=parallel,\n                    desc=desc,\n                    warn_inplace=False,\n                    progress=kwargs.pop(\"progress\", True),\n                    omit_failures=kwargs.pop(\"omit_failures\", False),\n                    chunksize=chunksize,\n                    exclude_zip=excl,\n                    n_cores=n_cores,\n                )\n                # Apply function\n                res = proc(nl, *args, **kwargs)\n\n                for n, df in zip(nl, res):\n                    df.insert(0, column=id_col, value=n.id)\n\n                df = pd.concat(res, axis=0)\n\n                if reset_index:\n                    df = df.reset_index(drop=True)\n\n            else:\n                # If single neuron just pass through\n                df = function(*args, **kwargs)\n                # df.insert(0, column=id_col, value=nl.id)\n\n            return df\n\n        # Update the docstring\n        wrapper = map_neuronlist_update_docstring(wrapper, allow_parallel)\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.meshneuron_skeleton","title":"<code>navis.utils.meshneuron_skeleton</code>","text":"<p>Decorate function such that MeshNeurons are automatically skeletonized, the function is run on the skeleton and changes are propagated back to the meshe.</p> PARAMETER DESCRIPTION <code>method</code> <pre><code>    What to do with the results:\n      - 'subset': subset MeshNeuron to what's left of the skeleton\n      - 'split': split MeshNeuron following the skeleton's splits\n      - 'node_to_vertex': map the returned node ID to the vertex IDs\n      - 'node_properties' map node properties to vertices (requires\n        `node_props` parameter)\n      - 'pass_through' simply passes through the return value\n</code></pre> <p> TYPE: <code>   str</code> </p> <code>include_connectors</code> <pre><code>    If True, will try to make sure that if the MeshNeuron has\n    connectors, they will be carried over to the skeleton.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>copy_properties</code> <pre><code>    Any additional properties that need to be copied from the\n    skeleton to the mesh.\n</code></pre> <p> TYPE: <code>list</code> DEFAULT: <code>[]</code> </p> <code>disallowed_kwargs</code> <pre><code>    Keyword arguments (name + value) that are not permitted when\n    input is MeshNeuron.\n</code></pre> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> <code>node_props</code> <pre><code>    For method 'node_properties'. String must be column names in\n    node table of skeleton.\n</code></pre> <p> TYPE: <code>list</code> DEFAULT: <code>[]</code> </p> <code>reroot_soma</code> <pre><code>    If True and neuron has a soma (.soma_pos), will reroot to\n    that soma.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>False</code> </p> <code>heal</code> <pre><code>    Whether or not to heal the skeleton if the mesh is fragmented.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>False</code> </p> Source code in <code>navis/utils/decorators.py</code> <pre><code>def meshneuron_skeleton(\n    method: Union[\n        Literal[\"subset\"],\n        Literal[\"split\"],\n        Literal[\"node_properties\"],\n        Literal[\"node_to_vertex\"],\n        Literal[\"pass_through\"],\n    ],\n    include_connectors: bool = False,\n    copy_properties: list = [],\n    disallowed_kwargs: dict = {},\n    node_props: list = [],\n    reroot_soma: bool = False,\n    heal: bool = False,\n):\n    \"\"\"Decorate function such that MeshNeurons are automatically skeletonized,\n    the function is run on the skeleton and changes are propagated\n    back to the meshe.\n\n    Parameters\n    ----------\n    method :    str\n                What to do with the results:\n                  - 'subset': subset MeshNeuron to what's left of the skeleton\n                  - 'split': split MeshNeuron following the skeleton's splits\n                  - 'node_to_vertex': map the returned node ID to the vertex IDs\n                  - 'node_properties' map node properties to vertices (requires\n                    `node_props` parameter)\n                  - 'pass_through' simply passes through the return value\n    include_connectors : bool\n                If True, will try to make sure that if the MeshNeuron has\n                connectors, they will be carried over to the skeleton.\n    copy_properties : list\n                Any additional properties that need to be copied from the\n                skeleton to the mesh.\n    disallowed_kwargs : dict\n                Keyword arguments (name + value) that are not permitted when\n                input is MeshNeuron.\n    node_props : list\n                For method 'node_properties'. String must be column names in\n                node table of skeleton.\n    reroot_soma :  bool\n                If True and neuron has a soma (.soma_pos), will reroot to\n                that soma.\n    heal :      bool\n                Whether or not to heal the skeleton if the mesh is fragmented.\n\n    \"\"\"\n    assert isinstance(copy_properties, list)\n    assert isinstance(disallowed_kwargs, dict)\n    assert isinstance(node_props, list)\n\n    allowed_methods = (\n        \"subset\",\n        \"node_to_vertex\",\n        \"split\",\n        \"node_properties\",\n        \"pass_through\",\n    )\n    if method not in allowed_methods:\n        raise ValueError(f'Unknown method \"{method}\"')\n\n    if method == \"node_properties\" and not node_props:\n        raise ValueError('Must provide `node_props` for method \"node_properties\"')\n\n    def decorator(function):\n        @wraps(function)\n        def wrapper(*args, **kwargs):\n            # Get the function's signature\n            sig = inspect.signature(function)\n\n            try:\n                fnname = function.__name__\n            except BaseException:\n                fnname = str(function)\n\n            # First, we need to extract the neuron from args and kwargs\n            if args:\n                # If there are positional arguments, the first one is assumed to\n                # be the input neuron\n                x = args[0]\n                args = args[1:]\n                x_key = \"__args\"\n            else:\n                # If not, we need to look for the name of the first argument\n                # in the signature\n                x_key = list(sig.parameters.keys())[0]\n                x = kwargs.pop(x_key, None)\n\n            # Complain if we did not get what we expected\n            if isinstance(x, type(None)):\n                raise ValueError(\n                    \"Unable to identify the neurons for call\"\n                    f\"{fnname}:\\n {args}\\n {kwargs}\"\n                )\n\n            # If input not a MeshNeuron, just pass through\n            # Note delayed import to avoid circular imports and IMPORTANTLY\n            # funky interactions with pickle/dill\n            from .. import core\n\n            if not isinstance(x, core.MeshNeuron):\n                return function(x, *args, **kwargs)\n\n            # Check for disallowed kwargs\n            for k, v in disallowed_kwargs.items():\n                if k in kwargs and kwargs[k] == v:\n                    raise ValueError(\n                        f\"{k}={v} is not allowed when input is MeshNeuron(s).\"\n                    )\n\n            # See if this is meant to be done inplace\n            if \"inplace\" in kwargs:\n                # First check keyword arguments\n                inplace = kwargs[\"inplace\"]\n            elif \"inplace\" in sig.parameters:\n                # Next check signatures default\n                inplace = sig.parameters[\"inplace\"].default\n            else:\n                # All things failing assume it's not inplace\n                inplace = False\n\n            # Now skeletonize\n            sk = x.skeleton\n\n            # Delayed import to avoid circular imports\n            # Note that this HAS to be in the inner function otherwise\n            # we get a weird error when pickling for parallel processing\n            from .. import morpho\n\n            if heal:\n                sk = morpho.heal_skeleton(sk, method=\"LEAFS\")\n\n            if reroot_soma and sk.has_soma:\n                sk = sk.reroot(sk.soma)\n\n            if include_connectors and x.has_connectors and not sk.has_connectors:\n                sk._connectors = x.connectors.copy()\n                sk._connectors[\"node_id\"] = sk.snap(\n                    sk.connectors[[\"x\", \"y\", \"z\"]].values\n                )[0]\n\n            # Apply function\n            res = function(sk, *args, **kwargs)\n\n            if method == \"subset\":\n                # See which vertices we need to keep\n                keep = np.isin(sk.vertex_map, res.nodes.node_id.values)\n\n                x = morpho.subset_neuron(x, keep, inplace=inplace)\n\n                for p in copy_properties:\n                    setattr(x, p, getattr(sk, p, None))\n            elif method == \"split\":\n                meshes = []\n                for n in res:\n                    # See which vertices we need to keep\n                    keep = np.isin(sk.vertex_map, n.nodes.node_id.values)\n\n                    meshes.append(morpho.subset_neuron(x, keep, inplace=False))\n\n                    for p in copy_properties:\n                        setattr(meshes[-1], p, getattr(n, p, None))\n                x = core.NeuronList(meshes)\n            elif method == \"node_to_vertex\":\n                x = np.where(sk.vertex_map == res)[0]\n            elif method == \"node_properties\":\n                for p in node_props:\n                    node_map = sk.nodes.set_index(\"node_id\")[p].to_dict()\n                    vertex_props = np.array([node_map[n] for n in sk.vertex_map])\n                    setattr(x, p, vertex_props)\n            elif method == \"pass_through\":\n                return res\n\n            return x\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.multi_split","title":"<code>navis.utils.multi_split</code>","text":"<p>Split string at any of the given separators.</p> RETURNS DESCRIPTION <code>list</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from navis.utils import multi_split\n&gt;&gt;&gt; s = '123;456,789:'\n&gt;&gt;&gt; multi_split(s, ';')\n['123', '456,789']\n&gt;&gt;&gt; multi_split(s, [';', ','])\n['123', '456', '789']\n</code></pre> Source code in <code>navis/utils/iterables.py</code> <pre><code>def multi_split(s, sep, strip=False):\n    \"\"\"Split string at any of the given separators.\n\n    Returns\n    -------\n    list\n\n    Examples\n    --------\n    &gt;&gt;&gt; from navis.utils import multi_split\n    &gt;&gt;&gt; s = '123;456,789:'\n    &gt;&gt;&gt; multi_split(s, ';')\n    ['123', '456,789']\n    &gt;&gt;&gt; multi_split(s, [';', ','])\n    ['123', '456', '789']\n\n    \"\"\"\n    assert isinstance(s, str)\n\n    if isinstance(sep, str):\n        sep = [sep]\n\n    splits = []\n    prev_sp = 0\n    for i in range(len(s)):\n        if s[i] in sep or i == (len(s) - 1):\n            splits.append(s[prev_sp:i])\n            prev_sp = i + 1\n\n    if strip:\n        splits = [sp.strip() for sp in splits]\n\n    return splits\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.parse_objects","title":"<code>navis.utils.parse_objects</code>","text":"<p>Categorize objects e.g. for plotting.</p> RETURNS DESCRIPTION <code>Neurons</code> <p> TYPE: <code>navis.NeuronList</code> </p> <code>Volume</code> <p> TYPE: <code>list of navis.Volume (trimesh.Trimesh will be converted)</code> </p> <code>Points</code> <p> TYPE: <code>list of arrays</code> </p> <code>Visuals</code> <p> TYPE: <code>list of vispy and pygfx visuals</code> </p> <p>Examples:</p> <p>This is mostly for doc tests:</p> <pre><code>&gt;&gt;&gt; from navis.utils import parse_objects\n&gt;&gt;&gt; from navis.data import example_neurons, example_volume\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; nl = example_neurons(3)\n&gt;&gt;&gt; v = example_volume('LH')\n&gt;&gt;&gt; p = nl[0].nodes[['x', 'y', 'z']].values\n&gt;&gt;&gt; n, vols, points, vis = parse_objects([nl, v, p])\n&gt;&gt;&gt; type(n), len(n)\n(&lt;class 'navis.core.neuronlist.NeuronList'&gt;, 3)\n&gt;&gt;&gt; type(vols), len(vols)\n(&lt;class 'list'&gt;, 1)\n&gt;&gt;&gt; type(vols[0])\n&lt;class 'navis.core.volumes.Volume'&gt;\n&gt;&gt;&gt; type(points), len(points)\n(&lt;class 'list'&gt;, 1)\n&gt;&gt;&gt; type(points[0])\n&lt;class 'numpy.ndarray'&gt;\n&gt;&gt;&gt; type(vis), len(vis)\n(&lt;class 'list'&gt;, 0)\n</code></pre> Source code in <code>navis/utils/misc.py</code> <pre><code>def parse_objects(x) -&gt; Tuple['core.NeuronList',\n                              List['core.Volume'],\n                              List[np.ndarray],\n                              List]:\n    \"\"\"Categorize objects e.g. for plotting.\n\n    Returns\n    -------\n    Neurons :       navis.NeuronList\n    Volume :        list of navis.Volume (trimesh.Trimesh will be converted)\n    Points :        list of arrays\n    Visuals :       list of vispy and pygfx visuals\n\n    Examples\n    --------\n    This is mostly for doc tests:\n\n    &gt;&gt;&gt; from navis.utils import parse_objects\n    &gt;&gt;&gt; from navis.data import example_neurons, example_volume\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; nl = example_neurons(3)\n    &gt;&gt;&gt; v = example_volume('LH')\n    &gt;&gt;&gt; p = nl[0].nodes[['x', 'y', 'z']].values\n    &gt;&gt;&gt; n, vols, points, vis = parse_objects([nl, v, p])\n    &gt;&gt;&gt; type(n), len(n)\n    (&lt;class 'navis.core.neuronlist.NeuronList'&gt;, 3)\n    &gt;&gt;&gt; type(vols), len(vols)\n    (&lt;class 'list'&gt;, 1)\n    &gt;&gt;&gt; type(vols[0])\n    &lt;class 'navis.core.volumes.Volume'&gt;\n    &gt;&gt;&gt; type(points), len(points)\n    (&lt;class 'list'&gt;, 1)\n    &gt;&gt;&gt; type(points[0])\n    &lt;class 'numpy.ndarray'&gt;\n    &gt;&gt;&gt; type(vis), len(vis)\n    (&lt;class 'list'&gt;, 0)\n\n    \"\"\"\n    # Make sure this is a list.\n    if not isinstance(x, list):\n        x = [x]\n\n    # If any list in x, flatten first\n    if any([isinstance(i, list) for i in x]):\n        # We need to be careful to preserve order because of colors\n        y = []\n        for i in x:\n            y += i if isinstance(i, list) else [i]\n        x = y\n\n    # Collect neuron objects, make a single NeuronList and split into types\n    neurons = core.NeuronList([ob for ob in x if isinstance(ob,\n                                                            (core.BaseNeuron,\n                                                             core.NeuronList))],\n                              make_copy=False)\n\n    # Collect visuals\n    visuals = [ob for ob in x if 'vispy' in str(type(ob)) or 'pygfx.objects' in str(type(ob))]\n\n    # Collect and parse volumes\n    volumes = [ob for ob in x if not isinstance(ob, (core.BaseNeuron,\n                                                     core.NeuronList))\n               and is_mesh(ob)]\n    # Add templatebrains\n    volumes += [ob.mesh for ob in x if isinstance(ob, TemplateBrain)]\n    # Converts any non-navis meshes into Volumes\n    volumes = [core.Volume(v) if not isinstance(v, core.Volume) else v for v in volumes]\n\n    # Collect dataframes with X/Y/Z coordinates\n    dataframes = [ob for ob in x if isinstance(ob, pd.DataFrame)]\n    if [d for d in dataframes if False in np.isin(['x', 'y', 'z'], d.columns)]:\n        logger.warning('DataFrames must have x, y and z columns.')\n    # Filter to and extract x/y/z coordinates\n    dataframes = [d for d in dataframes if False not in [c in d.columns for c in ['x', 'y', 'z']]]\n    dataframes = [d[['x', 'y', 'z']].values for d in dataframes]\n\n    # Collect arrays\n    arrays = [ob.copy() for ob in x if isinstance(ob, np.ndarray)]\n    # Remove arrays with wrong dimensions\n    if [ob for ob in arrays if ob.shape[1] != 3 and ob.shape[0] != 2]:\n        logger.warning('Arrays need to be of shape (N, 3) for scatter or (2, N)'\n                       ' for line plots.')\n    arrays = [ob for ob in arrays if any(np.isin(ob.shape, [2, 3]))]\n\n    points = dataframes + arrays\n\n    return neurons, volumes, points, visuals\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.round_smart","title":"<code>navis.utils.round_smart</code>","text":"<p>Round number intelligently to produce Human-readable numbers.</p> <p>This functions rounds to the Nth decimal, where N is <code>precision</code> minus the number of digits before the decimal. The general idea is that the bigger the number, the less we care about decimals - and vice versa.</p> PARAMETER DESCRIPTION <code>num</code> <pre><code>    A number.\n</code></pre> <p> TYPE: <code>      float | int</code> </p> <code>prec</code> <pre><code>    The precision we are aiming for.\n</code></pre> <p> TYPE: <code>     int</code> DEFAULT: <code>8</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; navis.utils.round_smart(0.00999)\n0.00999\n&gt;&gt;&gt; navis.utils.round_smart(10000000.00999)\n10000000.0\n</code></pre> Source code in <code>navis/utils/misc.py</code> <pre><code>def round_smart(num: Union[int, float], prec: int = 8) -&gt; float:\n    \"\"\"Round number intelligently to produce Human-readable numbers.\n\n    This functions rounds to the Nth decimal, where N is `precision` minus the\n    number of digits before the decimal. The general idea is that the bigger\n    the number, the less we care about decimals - and vice versa.\n\n    Parameters\n    ----------\n    num :       float | int\n                A number.\n    prec :      int\n                The precision we are aiming for.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; navis.utils.round_smart(0.00999)\n    0.00999\n    &gt;&gt;&gt; navis.utils.round_smart(10000000.00999)\n    10000000.0\n\n    \"\"\"\n    # Number of digits before decimal\n    lg = math.log10(num)\n    if lg &lt; 0:\n        N = 0\n    else:\n        N = int(lg)\n\n    return round(num, max(prec - N, 0))\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.sizeof_fmt","title":"<code>navis.utils.sizeof_fmt</code>","text":"<p>Bytes to Human readable.</p> Source code in <code>navis/utils/misc.py</code> <pre><code>def sizeof_fmt(num, suffix='B'):\n    \"\"\"Bytes to Human readable.\"\"\"\n    for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n        if abs(num) &lt; 1024.0:\n            return \"%3.1f%s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.unpack_neurons","title":"<code>navis.utils.unpack_neurons</code>","text":"<p>Unpack neurons and returns a list of individual neurons.</p> <p>Examples:</p> <p>This is mostly for doc tests:</p> <pre><code>&gt;&gt;&gt; from navis.utils import unpack_neurons\n&gt;&gt;&gt; from navis.data import example_neurons\n&gt;&gt;&gt; nl = example_neurons(3)\n&gt;&gt;&gt; type(nl)\n&lt;class 'navis.core.neuronlist.NeuronList'&gt;\n&gt;&gt;&gt; # Unpack list of neuronlists\n&gt;&gt;&gt; unpacked = unpack_neurons([nl, nl])\n&gt;&gt;&gt; type(unpacked)\n&lt;class 'list'&gt;\n&gt;&gt;&gt; type(unpacked[0])\n&lt;class 'navis.core.skeleton.TreeNeuron'&gt;\n&gt;&gt;&gt; len(unpacked)\n6\n</code></pre> Source code in <code>navis/utils/misc.py</code> <pre><code>def unpack_neurons(x: Union[Iterable, 'core.NeuronList', 'core.NeuronObject'],\n                   raise_on_error: bool = True\n                   ) -&gt; List['core.NeuronObject']:\n    \"\"\"Unpack neurons and returns a list of individual neurons.\n\n    Examples\n    --------\n    This is mostly for doc tests:\n\n    &gt;&gt;&gt; from navis.utils import unpack_neurons\n    &gt;&gt;&gt; from navis.data import example_neurons\n    &gt;&gt;&gt; nl = example_neurons(3)\n    &gt;&gt;&gt; type(nl)\n    &lt;class 'navis.core.neuronlist.NeuronList'&gt;\n    &gt;&gt;&gt; # Unpack list of neuronlists\n    &gt;&gt;&gt; unpacked = unpack_neurons([nl, nl])\n    &gt;&gt;&gt; type(unpacked)\n    &lt;class 'list'&gt;\n    &gt;&gt;&gt; type(unpacked[0])\n    &lt;class 'navis.core.skeleton.TreeNeuron'&gt;\n    &gt;&gt;&gt; len(unpacked)\n    6\n\n    \"\"\"\n    neurons: list = []\n\n    if isinstance(x, (list, np.ndarray, tuple)):\n        for l in x:\n            neurons += unpack_neurons(l)\n    elif isinstance(x, core.BaseNeuron):\n        neurons.append(x)\n    elif isinstance(x, core.NeuronList):\n        neurons += x.neurons\n    elif raise_on_error:\n        raise TypeError(f'Unknown neuron format: \"{type(x)}\"')\n\n    return neurons\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.validate_options","title":"<code>navis.utils.validate_options</code>","text":"<p>Check if neuron contains all required data.</p> <p>E.g. for <code>plot3d(plot_connectors=True)</code>.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            Neuron to check for data.\n</code></pre> <p> TYPE: <code>                TreeNeuron</code> </p> <code>options</code> <pre><code>            Options to check, e.g. \"plot\".\n</code></pre> <p> TYPE: <code>          str | list of str</code> </p> <code>kwargs</code> <pre><code>            Keyword arguments to check for options.\n</code></pre> <p> TYPE: <code>           dict</code> </p> <code>raise_on_error</code> <pre><code>            If True, will raise error if data not found.\n</code></pre> <p> TYPE: <code>   bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>None</code> Source code in <code>navis/utils/validate.py</code> <pre><code>def validate_options(x: 'core.TreeNeuron',\n                     options: Union[List[str], str],\n                     kwargs: dict,\n                     raise_on_error: bool = True) -&gt; None:\n    \"\"\"Check if neuron contains all required data.\n\n    E.g. for `plot3d(plot_connectors=True)`.\n\n    Parameters\n    ----------\n    x :                 TreeNeuron\n                        Neuron to check for data.\n    options :           str | list of str\n                        Options to check, e.g. \"plot\".\n    kwargs :            dict\n                        Keyword arguments to check for options.\n    raise_on_error :    bool, optional\n                        If True, will raise error if data not found.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    options = make_iterable(options)\n\n    for o in options:\n        for k in kwargs:\n            if isinstance(k, str) and k.startswith(o):\n                d = k[k.index('_'):]\n                if not hasattr(x, d):\n                    msg = f'Option \"{k}\" but {type(x)} has no \"{d}\"'\n                    if raise_on_error:\n                        raise ValueError(msg)\n                    else:\n                        logger.warning(msg)\n</code></pre>"},{"location":"reference/navis/utils/#navis.utils.validate_table","title":"<code>navis.utils.validate_table</code>","text":"<p>Validate DataFrame.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    DataFrame to validate.\n</code></pre> <p> TYPE: <code>        pd.DataFrame</code> </p> <code>required</code> <pre><code>    Columns to check for. If column is given as tuple (e.g.\n    `('type', 'relation', 'label')` one of these columns\n    has to exist)\n</code></pre> <p> TYPE: <code> iterable</code> </p> <code>rename</code> <pre><code>    If True and a required column is given as tuple, will rename\n    that column to the first entry in tuple.\n</code></pre> <p> TYPE: <code>   bool</code> DEFAULT: <code>False</code> </p> <code>restrict</code> <pre><code>    If True, will return only `required` columns.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>False</code> </p> <code>optional</code> <pre><code>    Optional columns. If column not present will be generated.\n    Dictionary must map column name to default value. Keys can also\n    be tuples - like `required`.\n</code></pre> <p> TYPE: <code> dict</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>pandas.DataFrame</code> <p>If <code>restrict=True</code> will return DataFrame subset to only the required columns. Columns defined in <code>optional</code> will be added if they don't already exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from navis.utils import validate_table\n&gt;&gt;&gt; from navis.data import example_neurons\n&gt;&gt;&gt; n = example_neurons(1)\n&gt;&gt;&gt; tbl = validate_table(n.nodes, ['x', 'y', 'z', 'node_id'])\n&gt;&gt;&gt; tbl = validate_table(n.nodes, ['does_not_exist'])\nValueError: Table missing required column: \"does_not_exist\"\n</code></pre> RAISES DESCRIPTION <code>ValueError</code> <p>If any of the required columns are not in the table.</p> Source code in <code>navis/utils/validate.py</code> <pre><code>def validate_table(x: pd.DataFrame,\n                   required: List[Union[str, Tuple[str]]],\n                   rename: bool = False,\n                   restrict: bool = False,\n                   optional: dict = {}) -&gt; pd.DataFrame:\n    \"\"\"Validate DataFrame.\n\n    Parameters\n    ----------\n    x :         pd.DataFrame\n                DataFrame to validate.\n    required :  iterable\n                Columns to check for. If column is given as tuple (e.g.\n                `('type', 'relation', 'label')` one of these columns\n                has to exist)\n    rename :    bool, optional\n                If True and a required column is given as tuple, will rename\n                that column to the first entry in tuple.\n    restrict :  bool, optional\n                If True, will return only `required` columns.\n    optional :  dict, optional\n                Optional columns. If column not present will be generated.\n                Dictionary must map column name to default value. Keys can also\n                be tuples - like `required`.\n\n    Returns\n    -------\n    pandas.DataFrame\n                If `restrict=True` will return DataFrame subset to only the\n                required columns. Columns defined in `optional` will be\n                added if they don't already exist.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from navis.utils import validate_table\n    &gt;&gt;&gt; from navis.data import example_neurons\n    &gt;&gt;&gt; n = example_neurons(1)\n    &gt;&gt;&gt; tbl = validate_table(n.nodes, ['x', 'y', 'z', 'node_id'])\n    &gt;&gt;&gt; tbl = validate_table(n.nodes, ['does_not_exist'])       # doctest: +SKIP\n    ValueError: Table missing required column: \"does_not_exist\"\n\n    Raises\n    ------\n    ValueError\n            If any of the required columns are not in the table.\n\n    \"\"\"\n    if not isinstance(x, pd.DataFrame):\n        raise TypeError(f'Need DataFrame, got \"{type(x)}\"')\n\n    for r in required:\n        if isinstance(r, (tuple, list)):\n            if not any(set(r) &amp; set(x.columns)):\n                raise ValueError('Table must contain either of these columns'\n                                 f' {\", \".join(r)}')\n        else:\n            if r not in x.columns:\n                raise ValueError(f'Table missing required column: \"{r}\"')\n\n    # Rename columns if necessary\n    if rename:\n        # Generate mapping. Order makes sure that required columns take\n        # precedence in case of a name clash\n        new_name = {c: t[0] for t in optional if isinstance(t, (tuple, list)) for c in t[1:]}\n        new_name.update({c: t[0] for t in required if isinstance(t, (tuple, list)) for c in t[1:]})\n\n        # Apply mapping\n        x.columns = [new_name.get(c, c) for c in x.columns]\n\n    if restrict:\n        flat_req = [r for r in required if not isinstance(r, (list, tuple))]\n        flat_req += [r for l in required if isinstance(l, (list, tuple)) for r in l]\n\n        x = x[[r for r in flat_req if r in x.columns]]\n\n    for c, v in optional.items():\n        # Convert to tuples\n        if not isinstance(c, (tuple, list)):\n            c = (c, )\n\n        if not any(set(c) &amp; set(x.columns)):\n            x[c[0]] = v\n\n    return x\n</code></pre>"},{"location":"reference/navis/connectivity/adjacency/","title":"adjacency","text":""},{"location":"reference/navis/connectivity/adjacency/#navis.connectivity.adjacency.Edge","title":"<code>navis.connectivity.adjacency.Edge</code>","text":"Source code in <code>navis/connectivity/adjacency.py</code> <pre><code>class Edge(NamedTuple):\n    connector_id: int\n    source_name: str\n    target_name: str\n    source_node: Optional[int]\n    target_node: Optional[int]\n</code></pre>"},{"location":"reference/navis/connectivity/similarity/","title":"similarity","text":""},{"location":"reference/navis/connectivity/similarity/#navis.connectivity.similarity.combinations_generator","title":"<code>navis.connectivity.similarity.combinations_generator</code>","text":"<p>Lazy generation of connectivity vector combinations.</p> Source code in <code>navis/connectivity/similarity.py</code> <pre><code>def combinations_generator(func, adjacency, *args, **kwargs):\n    \"\"\"Lazy generation of connectivity vector combinations.\"\"\"\n    comb = product(adjacency.values, adjacency.values)\n    for i in range(adjacency.shape[0]**2):\n        this = next(comb)\n        #non_zero = (this[0] &gt; 0) | (this[1] &gt; 0)\n        #yield (func, this[0][non_zero], this[1][non_zero], args, kwargs)\n        yield (func, this[0], this[1], args, kwargs)\n</code></pre>"},{"location":"reference/navis/conversion/meshing/","title":"meshing","text":""},{"location":"reference/navis/conversion/meshing/#navis.conversion.meshing.get_surface_voxels","title":"<code>navis.conversion.meshing.get_surface_voxels</code>","text":"<p>Return surface voxels.</p> Source code in <code>navis/conversion/meshing.py</code> <pre><code>def get_surface_voxels(voxels):\n    \"\"\"Return surface voxels.\"\"\"\n    # Use bounding boxes to keep matrix small\n    bb_min = voxels.min(axis=0)\n    #bb_max = voxels.max(axis=0)\n    #dim = bb_max - bb_min\n\n    # Voxel offset\n    voxel_off = voxels - bb_min\n\n    # Generate empty array\n    mat = _voxels_to_matrix(voxel_off)\n\n    # Erode\n    mat_erode = binary_erosion(mat)\n\n    # Substract\n    mat_surface = np.bitwise_and(mat, np.invert(mat_erode))\n\n    # Turn back into voxels\n    voxels_surface = _matrix_to_voxels(mat_surface) + bb_min\n\n    return voxels_surface\n</code></pre>"},{"location":"reference/navis/conversion/meshing/#navis.conversion.meshing.pack_array","title":"<code>navis.conversion.meshing.pack_array</code>","text":"<p>Pack 2-d array along second axis.</p> Source code in <code>navis/conversion/meshing.py</code> <pre><code>def pack_array(arr, base=8):\n    \"\"\"Pack 2-d array along second axis.\"\"\"\n    N = 2 ** base\n    packed = np.zeros(arr.shape, dtype='uint64')\n    packed[:, 0] = arr[:, 0] * N ** 2\n    packed[:, 1] = arr[:, 1] * N\n    packed[:, 2] = arr[:, 2]\n    return packed.sum(axis=1)\n</code></pre>"},{"location":"reference/navis/conversion/meshing/#navis.conversion.meshing.parse_obj","title":"<code>navis.conversion.meshing.parse_obj</code>","text":"<p>Parse .obj string and return vertices and faces.</p> Source code in <code>navis/conversion/meshing.py</code> <pre><code>def parse_obj(obj):\n    \"\"\"Parse .obj string and return vertices and faces.\"\"\"\n    lines = obj.split('\\n')\n    verts = []\n    faces = []\n    for l in lines:\n        if l.startswith('v '):\n            verts.append([float(v) for v in l[2:].split(' ')])\n        elif l.startswith('f '):\n            f = [v.split('//')[0] for v in l[2:].split(' ')]\n            faces.append([int(v) for v in f])\n\n    # `.obj` faces start with vertex indices of 1 -&gt; set to 0\n    return np.array(verts), np.array(faces) - 1\n</code></pre>"},{"location":"reference/navis/conversion/meshing/#navis.conversion.meshing.remove_surface_voxels","title":"<code>navis.conversion.meshing.remove_surface_voxels</code>","text":"<p>Removes surface voxels.</p> Source code in <code>navis/conversion/meshing.py</code> <pre><code>def remove_surface_voxels(voxels, **kwargs):\n    \"\"\"Removes surface voxels.\"\"\"\n    # Use bounding boxes to keep matrix small\n    bb_min = voxels.min(axis=0)\n    #bb_max = voxels.max(axis=0)\n    #dim = bb_max - bb_min\n\n    # Voxel offset\n    voxel_off = voxels - bb_min\n\n    # Generate empty array\n    mat = _voxels_to_matrix(voxel_off)\n\n    # Erode\n    mat_erode = binary_erosion(mat, **kwargs)\n\n    # Turn back into voxels\n    voxels_erode = _matrix_to_voxels(mat_erode) + bb_min\n\n    return voxels_erode\n</code></pre>"},{"location":"reference/navis/conversion/meshing/#navis.conversion.meshing.unpack_array","title":"<code>navis.conversion.meshing.unpack_array</code>","text":"<p>Unpack 2-d array.</p> Source code in <code>navis/conversion/meshing.py</code> <pre><code>def unpack_array(arr, base=8):\n    \"\"\"Unpack 2-d array.\"\"\"\n    N = 2 ** base - 1\n    unpacked = np.zeros((arr.shape[0], 3), dtype='uint64')\n    unpacked[:, 0] = (arr &gt;&gt; (base * 2)) &amp; N\n    unpacked[:, 1] = (arr &gt;&gt; base) &amp; N\n    unpacked[:, 2] = arr &amp; N\n    return unpacked\n</code></pre>"},{"location":"reference/navis/core/base/","title":"base","text":""},{"location":"reference/navis/core/base/#navis.core.base.UnitObject","title":"<code>navis.core.base.UnitObject</code>","text":"<p>Base class for things that have units.</p> Source code in <code>navis/core/base.py</code> <pre><code>class UnitObject:\n    \"\"\"Base class for things that have units.\"\"\"\n\n    @property\n    def units(self) -&gt; Union[numbers.Number, np.ndarray]:\n        \"\"\"Units for coordinate space.\"\"\"\n        # Note that we are regenerating the pint.Quantity from the string\n        # That is to avoid problems with pickling e.g. when using multiprocessing\n        unit_str = getattr(self, \"_unit_str\", None)\n\n        if utils.is_iterable(unit_str):\n            values = [config.ureg(u) for u in unit_str]\n            conv = [v.to(values[0]).magnitude for v in values]\n            return config.ureg.Quantity(np.array(conv), values[0].units)\n        else:\n            return config.ureg(unit_str)\n\n    @property\n    def units_xyz(self) -&gt; np.ndarray:\n        \"\"\"Units for coordinate space. Always returns x/y/z array.\"\"\"\n        units = self.units\n\n        if not utils.is_iterable(units):\n            units = config.ureg.Quantity([units.magnitude] * 3, units.units)\n\n        return units\n\n    @units.setter\n    def units(self, units: Union[pint.Unit, pint.Quantity, str, None]):\n        # Note that we are storing the string, not the actual pint.Quantity\n        # That is to avoid problems with pickling e.g. when using multiprocessing\n\n        # Do NOT remove the is_iterable condition - otherwise we might\n        # accidentally strip the units from a pint Quantity vector\n        if not utils.is_iterable(units):\n            units = utils.make_iterable(units)\n\n        if len(units) not in [1, 3]:\n            raise ValueError(\n                \"Must provide either a single unit or one for \"\n                \"for x, y and z dimension.\"\n            )\n\n        # Make sure we actually have valid unit(s)\n        unit_str = []\n        for v in units:\n            if isinstance(v, str):\n                # This makes sure we have meters (i.e. nm, um, etc) because\n                # \"microns\", for example, produces odd behaviour like\n                # \"millimicrons\" on division\n                v = v.replace(\"microns\", \"um\").replace(\"micron\", \"um\")\n                unit_str.append(str(v))\n            elif isinstance(v, (pint.Unit, pint.Quantity)):\n                unit_str.append(str(v))\n            elif isinstance(v, type(None)):\n                unit_str.append(None)\n            elif isinstance(v, numbers.Number):\n                unit_str.append(str(config.ureg(f\"{v} dimensionless\")))\n            else:\n                raise TypeError(f'Expect str or pint Unit/Quantity, got \"{type(v)}\"')\n\n        # Some clean-up\n        if len(set(unit_str)) == 1:\n            unit_str = unit_str[0]\n        else:\n            # Check if all base units (e.g. \"microns\") are the same\n            unique_units = set([str(config.ureg(u).units) for u in unit_str])\n            if len(unique_units) != 1:\n                raise ValueError(\n                    'Non-isometric units must share the same base,'\n                    f' got: {\", \".join(unique_units)}'\n                )\n            unit_str = tuple(unit_str)\n\n        self._unit_str = unit_str\n\n    @property\n    def is_isometric(self):\n        \"\"\"Test if neuron is isometric.\"\"\"\n        u = self.units\n        if utils.is_iterable(u) and len(set(u)) &gt; 1:\n            return False\n        return True\n</code></pre>"},{"location":"reference/navis/core/base/#navis.core.base.UnitObject.is_isometric","title":"<code>is_isometric</code>  <code>property</code>","text":"<p>Test if neuron is isometric.</p>"},{"location":"reference/navis/core/base/#navis.core.base.UnitObject.units","title":"<code>units: Union[numbers.Number, np.ndarray]</code>  <code>property</code> <code>writable</code>","text":"<p>Units for coordinate space.</p>"},{"location":"reference/navis/core/base/#navis.core.base.UnitObject.units_xyz","title":"<code>units_xyz: np.ndarray</code>  <code>property</code>","text":"<p>Units for coordinate space. Always returns x/y/z array.</p>"},{"location":"reference/navis/core/core_utils/","title":"core_utils","text":""},{"location":"reference/navis/core/core_utils/#navis.core.core_utils.FailedRun","title":"<code>navis.core.core_utils.FailedRun</code>","text":"<p>Class representing a failed run.</p> Source code in <code>navis/core/core_utils.py</code> <pre><code>class FailedRun:\n    \"\"\"Class representing a failed run.\"\"\"\n    def __init__(self, func, args, kwargs, exception='NA'):\n        self.args = args\n        self.func = func\n        self.kwargs = kwargs\n        self.exception = exception\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return (f'Failed run(function={self.func}, args={self.args}, '\n                f'kwargs={self.kwargs}, exception={self.exception})')\n</code></pre>"},{"location":"reference/navis/core/core_utils/#navis.core.core_utils.add_units","title":"<code>navis.core.core_utils.add_units</code>","text":"<p>Add neuron units (if present) to output of function.</p> Source code in <code>navis/core/core_utils.py</code> <pre><code>def add_units(compact=True, power=1):\n    \"\"\"Add neuron units (if present) to output of function.\"\"\"\n    def outer(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            self = args[0]\n            res = func(*args, **kwargs)\n\n            if config.add_units and self.has_units and not self.units.dimensionless:\n                res = res * np.power(self.units, power)\n                if compact:\n                    res = res.to_compact()\n\n            return res\n\n        return wrapper\n\n    return outer\n</code></pre>"},{"location":"reference/navis/core/core_utils/#navis.core.core_utils.temp_property","title":"<code>navis.core.core_utils.temp_property</code>","text":"<p>Check if neuron is stale. Clear cached temporary attributes if it is.</p> Source code in <code>navis/core/core_utils.py</code> <pre><code>def temp_property(func):\n    \"\"\"Check if neuron is stale. Clear cached temporary attributes if it is.\"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        self = args[0]\n        # Do nothing if neurons is locked\n        if not self.is_locked:\n            if self.is_stale:\n                self._clear_temp_attr()\n        return func(*args, **kwargs)\n    return wrapper\n</code></pre>"},{"location":"reference/navis/core/skeleton/","title":"skeleton","text":""},{"location":"reference/navis/core/skeleton/#navis.core.skeleton.requires_nodes","title":"<code>navis.core.skeleton.requires_nodes</code>","text":"<p>Return <code>None</code> if neuron has no nodes.</p> Source code in <code>navis/core/skeleton.py</code> <pre><code>def requires_nodes(func):\n    \"\"\"Return `None` if neuron has no nodes.\"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        self = args[0]\n        # Return 0\n        if isinstance(self.nodes, str) and self.nodes == 'NA':\n            return 'NA'\n        if not isinstance(self.nodes, pd.DataFrame):\n            return None\n        return func(*args, **kwargs)\n    return wrapper\n</code></pre>"},{"location":"reference/navis/graph/clinic/","title":"clinic","text":""},{"location":"reference/navis/graph/clinic/#navis.graph.clinic.merge_duplicate_nodes","title":"<code>navis.graph.clinic.merge_duplicate_nodes</code>","text":"<p>Merge nodes the occupy the exact same position in space.</p> <p>Note that this might produce connections where there previously weren't any!</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Neuron(s) to fix.\n</code></pre> <p> TYPE: <code>        TreeNeuron | NeuronList</code> </p> <code>round</code> <pre><code>    If provided will round node locations to given decimals. This\n    can be useful if the positions are floats and not `exactly` the\n    the same.\n</code></pre> <p> TYPE: <code>    int</code> DEFAULT: <code>False</code> </p> <code>inplace</code> <pre><code>    If True, perform operation on neuron inplace.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>TreeNeuron</code> <p>Fixed neuron. Only if <code>inplace=False</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; n.nodes.loc[1, ['x', 'y' ,'z']] = n.nodes.loc[0, ['x', 'y' ,'z']]\n&gt;&gt;&gt; fx = navis.graph.clinic.merge_duplicate_nodes(n)\n&gt;&gt;&gt; n.n_nodes, fx.n_nodes\n(4465, 4464)\n</code></pre> Source code in <code>navis/graph/clinic.py</code> <pre><code>def merge_duplicate_nodes(x, round=False, inplace=False):\n    \"\"\"Merge nodes the occupy the exact same position in space.\n\n    Note that this might produce connections where there previously weren't\n    any!\n\n    Parameters\n    ----------\n    x :         TreeNeuron | NeuronList\n                Neuron(s) to fix.\n    round :     int, optional\n                If provided will round node locations to given decimals. This\n                can be useful if the positions are floats and not `exactly` the\n                the same.\n    inplace :   bool\n                If True, perform operation on neuron inplace.\n\n    Returns\n    -------\n    TreeNeuron\n                Fixed neuron. Only if `inplace=False`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; n.nodes.loc[1, ['x', 'y' ,'z']] = n.nodes.loc[0, ['x', 'y' ,'z']]\n    &gt;&gt;&gt; fx = navis.graph.clinic.merge_duplicate_nodes(n)\n    &gt;&gt;&gt; n.n_nodes, fx.n_nodes\n    (4465, 4464)\n\n    \"\"\"\n    if isinstance(x, core.NeuronList):\n        if not inplace:\n            x = x.copy()\n\n        for n in x:\n            _ = merge_duplicate_nodes(n, round=round, inplace=True)\n\n        if not inplace:\n            return x\n        return\n\n    if not isinstance(x, core.TreeNeuron):\n        raise TypeError(f'Expected TreeNeuron, got \"{type(x)}\"')\n\n    if not inplace:\n        x = x.copy()\n\n    # Figure out which nodes are duplicated\n    if round:\n        dupl = x.nodes[['x', 'y', 'z']].round(round).duplicated(keep=False)\n    else:\n        dupl = x.nodes[['x', 'y', 'z']].duplicated(keep=False)\n\n    if dupl.sum():\n        # Operate on the edge list\n        edges = x.nodes[['node_id', 'parent_id']].values.copy()\n\n        # Go over each non-unique location\n        ids = x.nodes.loc[dupl].groupby(['x', 'y', 'z']).node_id.apply(list)\n        for i in ids:\n            # Keep the first node and collapse all others into it\n            edges[np.isin(edges[:, 0], i[1:]), 0] = i[0]\n            edges[np.isin(edges[:, 1], i[1:]), 1] = i[0]\n\n        # Drop self-loops\n        edges = edges[edges[:, 0] != edges[:, 1]]\n\n        # Make sure we don't have a-&gt;b and b&lt;-a edges\n        edges = np.unique(np.sort(edges, axis=1), axis=0)\n\n        G = nx.Graph()\n\n        # Get nodes but drop \"\"-1\"\n        nodes = edges.flatten()\n        nodes = nodes[nodes &gt;= 0]\n\n        # Add nodes\n        G.add_nodes_from(nodes)\n\n        # Drop edges that point away from root (e.g. (1, -1))\n        # Don't do this before because we would loose isolated nodes otherwise\n        edges = edges[edges.min(axis=1) &gt;= 0]\n\n        # Add edges\n        G.add_edges_from([(e[0], e[1]) for e in edges])\n\n        # First remove cycles\n        while True:\n            try:\n                # Find cycle\n                cycle = nx.find_cycle(G)\n            except nx.exception.NetworkXNoCycle:\n                break\n            except BaseException:\n                raise\n\n            # Sort by degree\n            cycle = sorted(cycle, key=lambda x: G.degree[x[0]])\n\n            # Remove the edge with the lowest degree\n            G.remove_edge(cycle[0][0], cycle[0][1])\n\n        # Now make sure this is a DAG, i.e. that all edges point in the same direction\n        new_edges = []\n        for c in nx.connected_components(G.to_undirected()):\n            sg = nx.subgraph(G, c)\n\n            # Try picking a node that was root in the original neuron\n            is_present = np.isin(x.root, sg.nodes)\n            if any(is_present):\n                r = x.root[is_present][0]\n            else:\n                r = list(sg.nodes)[0]\n\n            # Generate parent-&gt;child dictionary by graph traversal\n            this_lop = nx.predecessor(sg, r)\n\n            # Note that we assign -1 as root's parent\n            new_edges += [(k, v[0]) for k, v in this_lop.items() if v]\n\n        # We need a directed Graph for this as otherwise the child -&gt; parent\n        # order in the edges might get lost\n        G2 = nx.DiGraph()\n        G2.add_nodes_from(G.nodes)\n        G2.add_edges_from(new_edges)\n\n        # Generate list of parents\n        new_edges = np.array(G2.edges)\n        new_parents = dict(zip(new_edges[:, 0], new_edges[:, 1]))\n\n        # Drop nodes that aren't present anymore\n        x._nodes = x._nodes.loc[x._nodes.node_id.isin(new_edges.flatten())].copy()\n\n        # Rewire kept nodes\n        x.nodes['parent_id'] = x.nodes.node_id.map(lambda x: new_parents.get(x, -1))\n\n        # Reset temporary attributes\n        x._clear_temp_attr()\n\n    if not inplace:\n        return x\n</code></pre>"},{"location":"reference/navis/graph/graph_utils/","title":"graph_utils","text":""},{"location":"reference/navis/graph/graph_utils/#navis.graph.graph_utils.match_mesh_skeleton","title":"<code>navis.graph.graph_utils.match_mesh_skeleton</code>","text":"<p>Match vertices of MeshNeuron to nodes of TreeNeuron.</p> PARAMETER DESCRIPTION <code>mesh</code> <pre><code>    MeshNeuron to match.\n</code></pre> <p> TYPE: <code>     MeshNeuron</code> </p> <code>skeleton</code> <pre><code>    Skeleton to match.\n</code></pre> <p> TYPE: <code> TreeNeuron</code> </p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>Array of skeleton node IDs for each vertex in the mesh.</p> Source code in <code>navis/graph/graph_utils.py</code> <pre><code>def match_mesh_skeleton(mesh, skeleton):\n    \"\"\"Match vertices of MeshNeuron to nodes of TreeNeuron.\n\n    Parameters\n    ----------\n    mesh :      MeshNeuron\n                MeshNeuron to match.\n    skeleton :  TreeNeuron\n                Skeleton to match.\n\n    Returns\n    -------\n    np.ndarray\n                Array of skeleton node IDs for each vertex in the mesh.\n\n    \"\"\"\n    if not isinstance(mesh, core.MeshNeuron):\n        raise TypeError(f\"Expected MeshNeuron, got {type(mesh)}\")\n\n    if not isinstance(skeleton, core.TreeNeuron):\n        raise TypeError(f\"Expected TreeNeuron, got {type(skeleton)}\")\n\n    # Generate a KDTree for the skeleton\n    tree = graph.neuron2KDTree(skeleton)\n\n    # Find closest node for each vertex\n    dist, ix = tree.query(mesh.vertices, k=1)\n\n    return skeleton.nodes.node_id.values[ix]\n</code></pre>"},{"location":"reference/navis/interfaces/blender/","title":"blender","text":""},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.Handler","title":"<code>navis.interfaces.blender.Handler</code>","text":"<p>Class that interfaces with scene in Blender.</p> PARAMETER DESCRIPTION <code>scaling</code> <pre><code>       scaling factor between navis and Blender coordinates.\n</code></pre> <p> TYPE: <code>  float</code> DEFAULT: <code>1 / 10000</code> </p> Notes <pre><code>(1) The handler adds neurons and keeps track of them in the scene.\n(2) If you request a list of objects via its attributes (e.g. `Handler.neurons`)\n    or via [`navis.interfaces.blender.Handler.select`][], a [`navis.interfaces.blender.ObjectList`][]\n    is returned. This class lets you change basic parameters of your selected\n    neurons.\n</code></pre> ATTRIBUTE DESCRIPTION <code>neurons</code> <p> TYPE: <code>returns list containing all neurons</code> </p> <code>connectors</code> <p> TYPE: <code>returns list containing all connectors</code> </p> <code>soma</code> <p> TYPE: <code>returns list containing all somata</code> </p> <code>selected</code> <p> TYPE: <code>returns list containing selected objects</code> </p> <code>presynapses</code> <p> TYPE: <code>returns list containing all presynapses</code> </p> <code>postsynapses</code> <p> TYPE: <code>returns list containing all postsynapses</code> </p> <code>gapjunctions</code> <p> TYPE: <code>returns list containing all gap junctions</code> </p> <code>abutting</code> <p> TYPE: <code>returns list containing all abutting connectors</code> </p> <code>all</code> <p> TYPE: <code>returns list containing all objects</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # This example assumes you have alread imported and set up navis\n&gt;&gt;&gt; # b3d module has to be imported explicitly\n&gt;&gt;&gt; from navis import b3d\n&gt;&gt;&gt; # Get some neurons (you have already set up a remote instance?)\n&gt;&gt;&gt; nl = navis.example_neurons()\n&gt;&gt;&gt; # Initialize handler\n&gt;&gt;&gt; h = b3d.Handler()\n&gt;&gt;&gt; # Add neurons\n&gt;&gt;&gt; h.add(nl)\n&gt;&gt;&gt; # Assign colors to all neurons\n&gt;&gt;&gt; h.colorize()\n&gt;&gt;&gt; # Select all somas and change color to black\n&gt;&gt;&gt; h.soma.color(0, 0, 0)\n&gt;&gt;&gt; # Clear scene\n&gt;&gt;&gt; h.clear()\n&gt;&gt;&gt; # Add only soma\n&gt;&gt;&gt; h.add(nl, neurites=False, connectors=False)\n</code></pre> Source code in <code>navis/interfaces/blender.py</code> <pre><code>class Handler:\n    \"\"\"Class that interfaces with scene in Blender.\n\n    Parameters\n    ----------\n    scaling :   float, optional\n                   scaling factor between navis and Blender coordinates.\n\n    Notes\n    -----\n\n        (1) The handler adds neurons and keeps track of them in the scene.\n        (2) If you request a list of objects via its attributes (e.g. `Handler.neurons`)\n            or via [`navis.interfaces.blender.Handler.select`][], a [`navis.interfaces.blender.ObjectList`][]\n            is returned. This class lets you change basic parameters of your selected\n            neurons.\n\n    Attributes\n    ----------\n    neurons :       returns list containing all neurons\n    connectors :    returns list containing all connectors\n    soma :          returns list containing all somata\n    selected :      returns list containing selected objects\n    presynapses :   returns list containing all presynapses\n    postsynapses :  returns list containing all postsynapses\n    gapjunctions :  returns list containing all gap junctions\n    abutting :      returns list containing all abutting connectors\n    all :           returns list containing all objects\n\n    Examples\n    --------\n    &gt;&gt;&gt; # This example assumes you have alread imported and set up navis\n    &gt;&gt;&gt; # b3d module has to be imported explicitly\n    &gt;&gt;&gt; from navis import b3d\n    &gt;&gt;&gt; # Get some neurons (you have already set up a remote instance?)\n    &gt;&gt;&gt; nl = navis.example_neurons()\n    &gt;&gt;&gt; # Initialize handler\n    &gt;&gt;&gt; h = b3d.Handler()\n    &gt;&gt;&gt; # Add neurons\n    &gt;&gt;&gt; h.add(nl)\n    &gt;&gt;&gt; # Assign colors to all neurons\n    &gt;&gt;&gt; h.colorize()\n    &gt;&gt;&gt; # Select all somas and change color to black\n    &gt;&gt;&gt; h.soma.color(0, 0, 0)\n    &gt;&gt;&gt; # Clear scene\n    &gt;&gt;&gt; h.clear()\n    &gt;&gt;&gt; # Add only soma\n    &gt;&gt;&gt; h.add(nl, neurites=False, connectors=False)\n\n    \"\"\"\n    cn_dict = {\n        0: dict(name='presynapses',\n                color=(1, 0, 0)),\n        1: dict(name='postsynapses',\n                color=(0, 0, 1)),\n        2: dict(name='gapjunction',\n                color=(0, 1, 0)),\n        3: dict(name='abutting',\n                color=(1, 0, 1))\n\n    }  # : defines default colours/names for different connector types\n\n    # Some synonyms\n    cn_dict['pre'] = cn_dict[0]\n    cn_dict['post'] = cn_dict[1]\n    cn_dict['gap'] = cn_dict['gapjunction'] = cn_dict[2]\n    cn_dict['abutting'] = cn_dict[3]\n\n    # Some other parameters\n    cn_dict['display'] = 'lines'  # \"lines\" or \"spheres\", overriden if MeshNeuron\n    cn_dict['size'] = 0.01  # sets size of spheres only\n\n    defaults = dict(bevel_depth=0.007,\n                    bevel_resolution=5,\n                    resolution_u=10)\n\n    def __init__(self,\n                 scaling=1 / 10000,\n                 axes_order=[0, 1, 2],\n                 ax_translate=[1, 1, 1]):\n        self.scaling = scaling\n        self.cn_dict = Handler.cn_dict\n        self.axes_order = axes_order\n        self.ax_translate = ax_translate\n\n    def _selection_helper(self, type):\n        return [ob.name for ob in bpy.data.objects if 'type' in ob and ob['type'] == type]\n\n    def _cn_selection_helper(self, cn_type):\n        return [ob.name for ob in bpy.data.objects if 'type' in ob and ob['type'] == 'CONNECTORS' and ob['cn_type'] == cn_type]\n\n    def __getattr__(self, key):\n        if key == 'neurons' or key == 'neuron' or key == 'neurites':\n            return ObjectList(self._selection_helper('NEURON'))\n        elif key == 'connectors' or key == 'connector':\n            return ObjectList(self._selection_helper('CONNECTORS'))\n        elif key == 'soma' or key == 'somas':\n            return ObjectList(self._selection_helper('SOMA'))\n        elif key == 'selected':\n            return ObjectList([ob.name for ob in bpy.context.selected_objects if 'navis_object' in ob])\n        elif key == 'visible':\n            objects = [o for o in self.neurons if not o.hide]\n            return ObjectList(objects)\n        elif key == 'presynapses':\n            return ObjectList(self._cn_selection_helper(0))\n        elif key == 'postsynapses':\n            return ObjectList(self._cn_selection_helper(1))\n        elif key == 'gapjunctions':\n            return ObjectList(self._cn_selection_helper(2))\n        elif key == 'abutting':\n            return ObjectList(self._cn_selection_helper(3))\n        elif key == 'all':\n            return self.neurons + self.connectors + self.soma\n        else:\n            raise AttributeError('Unknown attribute ' + key)\n\n    def add(self, x, neurites=True, soma=True, connectors=True, redraw=False,\n            use_radii=False, skip_existing=False, downsample=False,\n            collection=None, **kwargs):\n        \"\"\"Add neuron(s) to scene.\n\n        Parameters\n        ----------\n        x :             TreeNeuron | MeshNeuron | NeuronList | core.Volume\n                        Objects to import into Blender.\n        neurites :      bool, optional\n                        Plot neurites. TreeNeurons only.\n        soma :          bool, optional\n                        Plot somas. TreeNeurons only.\n        connectors :    bool, optional\n                        Plot connectors. Uses a defaults dictionary to set\n                        color/type. See Examples on how to change.\n        redraw :        bool, optional\n                        If True, will redraw window after each neuron. This\n                        will slow down loading!\n        use_radii :     bool, optional\n                        If True, will use node radii. For TreeNeurons only.\n        skip_existing : bool, optional\n                        If True, will skip neurons that are already loaded.\n        downsample :    False | int, optional\n                        If integer &lt; 1, will downsample neurites upon import.\n                        Preserves branch point/roots. TreeNeurons only.\n        collection :    str, optional\n                        Only for Blender &gt;2.8: add object(s) to given collection.\n                        If collection does not exist, will be created.\n\n        Examples\n        --------\n        Add one of the example neurons:\n\n        &gt;&gt;&gt; h = navis.interfaces.blender.Handler()\n        &gt;&gt;&gt; n = navis.example_neurons(1)\n        &gt;&gt;&gt; h.add(n, connectors=True)\n\n        Change connector settings:\n\n        &gt;&gt;&gt; h.cn_dict['display'] = 'sphere'\n        &gt;&gt;&gt; h.cn_dict[0]['color'] = (1, 1, 0)\n\n        \"\"\"\n        start = time.time()\n\n        if skip_existing:\n            exists = [ob.get('id', None) for ob in bpy.data.objects]\n\n        if isinstance(x, (core.BaseNeuron, core.NeuronList)):\n            if redraw:\n                print('Set \"redraw=False\" to vastly speed up import!')\n            if isinstance(x, core.BaseNeuron):\n                x = [x]\n            wm = bpy.context.window_manager\n            wm.progress_begin(0, len(x))\n            for i, n in enumerate(x):\n                # Skip existing if applicable\n                if skip_existing and n.id in exists:\n                    continue\n                self._create_neuron(n,\n                                    neurites=neurites,\n                                    soma=soma,\n                                    connectors=connectors,\n                                    collection=collection,\n                                    downsample=downsample,\n                                    use_radii=use_radii)\n                if redraw:\n                    bpy.ops.wm.redraw_timer(type='DRAW_WIN_SWAP', iterations=1)\n                wm.progress_update(i)\n            wm.progress_end()\n        elif isinstance(x, tm.Trimesh):\n            self._create_volume(x, collection=collection)\n        elif isinstance(x, np.ndarray):\n            self._create_scatter(x, collection=collection, **kwargs)\n        elif isinstance(x, core.Dotprops):\n            self._create_dotprops(x, collection=collection, **kwargs)\n        else:\n            raise AttributeError(f'Unable add data type of type {type(x)}')\n\n        print(f'Import done in {time.time()-start:.2f}s')\n\n        return\n\n    def clear(self):\n        \"\"\"Clear all neurons \"\"\"\n        self.all.delete()\n\n    def _create_scatter2(self, x, collection=None, **kwargs):\n        \"\"\"Create scatter by reusing mesh data.\n\n        This generate an individual objects for each data point. This is slower!\n\n        \"\"\"\n        if x.ndim != 2 or x.shape[1] != 3:\n            raise ValueError('Array must be of shape N,3')\n\n        # Get &amp; scale coordinates and invert y\n        coords = x.astype(float)[:, self.axes_order]\n        coords *= float(self.scaling)\n        coords *= self.ax_translate\n\n        verts, faces = calc_sphere(kwargs.get('size', 0.02),\n                                   kwargs.get('sp_res', 7),\n                                   kwargs.get('sp_res', 7))\n\n        mesh = bpy.data.meshes.new(kwargs.get('name', 'scatter'))\n        mesh.from_pydata(verts, [], faces)\n        mesh.polygons.foreach_set('use_smooth', [True] * len(mesh.polygons))\n\n        objects = []\n        for i, co in enumerate(coords):\n            obj = bpy.data.objects.new(kwargs.get('name', 'scatter') + str(i),\n                                       mesh)\n            obj.location = co\n            obj.show_name = False\n            objects.append(obj)\n\n        # Link to scene and add to group\n        group_name = kwargs.get('name', 'scatter')\n        if group_name != 'scatter' and group_name in bpy.data.groups:\n            group = bpy.data.groups[group_name]\n        else:\n            group = bpy.data.groups.new(group_name)\n\n        if not collection:\n            col = bpy.context.scene.collection\n        elif collection in bpy.data.collections:\n            col = bpy.data.collections[collection]\n        else:\n            col = bpy.data.collections.new(collection)\n            bpy.context.scene.collection.children.link(col)\n\n        for obj in objects:\n            col.objects.link(obj)\n            group.objects.link(obj)\n\n        return\n\n    def _create_scatter(self, x, collection=None, **kwargs):\n        \"\"\"Create scatter.\"\"\"\n        if x.ndim != 2 or x.shape[1] != 3:\n            raise ValueError('Array must be of shape N,3')\n\n        # Get &amp; scale coordinates and invert y\n        coords = x.astype(float)[:, self.axes_order]\n        coords *= float(self.scaling)\n        coords *= self.ax_translate\n\n        # Generate a base sphere\n        base_sphere = tm.creation.uv_sphere(radius=kwargs.get('size', 0.02),\n                                            count=[kwargs.get('sp_res', 7),\n                                                   kwargs.get('sp_res', 7)])\n        base_verts, base_faces = base_sphere.vertices, base_sphere.faces\n\n        # Repeat sphere vertices\n        sp_verts = np.tile(base_verts.T, coords.shape[0]).T\n        # Add coords offsets to each sphere\n        offsets = np.repeat(coords, base_verts.shape[0], axis=0)\n        sp_verts += offsets\n\n        # Repeat sphere faces and offset vertex indices\n        sp_faces = np.tile(base_faces.T, coords.shape[0]).T\n        face_offsets = np.repeat(np.arange(coords.shape[0]),\n                                 base_faces.shape[0], axis=0)\n        face_offsets *= base_verts.shape[0]\n        sp_faces += face_offsets.reshape((face_offsets.size, 1))\n\n        # Generate mesh\n        mesh = bpy.data.meshes.new(kwargs.get('name', 'scatter'))\n        mesh.from_pydata(sp_verts, [], sp_faces.tolist())\n        mesh.polygons.foreach_set('use_smooth', [True] * len(mesh.polygons))\n        obj = bpy.data.objects.new(kwargs.get('name', 'scatter'), mesh)\n\n        if not collection:\n            col = bpy.context.scene.collection\n        elif collection in bpy.data.collections:\n            col = bpy.data.collections[collection]\n        else:\n            col = bpy.data.collections.new(collection)\n            bpy.context.scene.collection.children.link(col)\n\n        col.objects.link(obj)\n\n        obj.location = (0, 0, 0)\n        obj.show_name = False\n\n        return obj\n\n    def _create_neuron(self, x, neurites=True, soma=True, connectors=True,\n                       use_radii=False, downsample=False, collection=None):\n        \"\"\"Create neuron object.\"\"\"\n        mat_name = (f'M#{x.id}')[:59]\n\n        mat = bpy.data.materials.get(mat_name,\n                                     bpy.data.materials.new(mat_name))\n\n        if isinstance(x, core.TreeNeuron):\n            if neurites:\n                self._create_skeleton(x, mat,\n                                      use_radii=use_radii,\n                                      downsample=downsample,\n                                      collection=collection)\n            if soma and not isinstance(x.soma, type(None)):\n                self._create_soma(x, mat, collection=collection)\n        elif isinstance(x, core.MeshNeuron):\n            self._create_mesh(x, mat, collection=collection)\n        else:\n            raise TypeError(f'Expected Mesh/TreeNeuron, got \"{type(x)}\"')\n\n        if connectors and x.has_connectors:\n            self._create_connectors(x, collection=collection)\n\n        return\n\n    def _create_mesh(self, x, mat, collection=None):\n        \"\"\"Create mesh from MeshNeuron.\"\"\"\n        name = getattr(x, 'name', '')\n\n        # Make copy of vertices as we are potentially modifying them\n        verts = x.vertices.copy()\n\n        # Convert to Blender space\n        verts = verts * self.scaling\n        verts = verts[:, self.axes_order]\n        verts *= self.ax_translate\n\n        me = bpy.data.meshes.new(f'{name} mesh')\n        ob = bpy.data.objects.new(f\"#{x.id} - {name}\", me)\n        ob.location = (0, 0, 0)\n        ob.show_name = True\n        ob['type'] = 'NEURON'\n        ob['navis_object'] = True\n        ob['id'] = str(x.id)\n\n        blender_verts = verts.tolist()\n        me.from_pydata(list(blender_verts), [], list(x.faces))\n        me.update()\n\n        me.polygons.foreach_set('use_smooth', [True] * len(me.polygons))\n\n        ob.active_material = mat\n\n        if not collection:\n            col = bpy.context.scene.collection\n        elif collection in bpy.data.collections:\n            col = bpy.data.collections[collection]\n        else:\n            col = bpy.data.collections.new(collection)\n            bpy.context.scene.collection.children.link(col)\n\n        col.objects.link(ob)\n\n    def _create_skeleton(self, x, mat, use_radii=False, downsample=False,\n                         collection=None):\n        \"\"\"Create neuron branches.\"\"\"\n        name = getattr(x, 'name', '')\n\n        cu = bpy.data.curves.new(f\"{name} mesh\", 'CURVE')\n        ob = bpy.data.objects.new(f\"#{x.id} - {name}\", cu)\n        ob.location = (0, 0, 0)\n        ob.show_name = True\n        ob['type'] = 'NEURON'\n        ob['navis_object'] = True\n        ob['id'] = str(x.id)\n        cu.dimensions = '3D'\n        cu.fill_mode = 'FULL'\n        cu.bevel_resolution = self.defaults.get('bevel_resolution', 5)\n        cu.resolution_u = self.defaults.get('resolution_u', 10)\n\n        if use_radii:\n            cu.bevel_depth = 1\n        else:\n            cu.bevel_depth = self.defaults.get('bevel_depth', 0.007)\n\n        # DO NOT touch this: lookup via dict is &gt;10X faster!\n        tn_coords = {r.node_id: (r.x * self.scaling,\n                                 r.y * self.scaling,\n                                 r.z * self.scaling) for r in x.nodes.itertuples()}\n        if use_radii:\n            tn_radii = {r.node_id: r.radius * self.scaling for r in x.nodes.itertuples()}\n\n        for s in x.segments:\n            if isinstance(downsample, int) and downsample &gt; 1:\n                mask = np.zeros(len(s), dtype=bool)\n                mask[downsample::downsample] = True\n\n                keep = np.isin(s, x.nodes[x.nodes['type'] != 'slab'].node_id.values)\n\n                s = np.array(s)[mask | keep]\n\n            sp = cu.splines.new('POLY')\n\n            coords = np.array([tn_coords[tn] for tn in s])\n            coords = coords[:, self.axes_order]\n            coords *= self.ax_translate\n\n            # Add points\n            sp.points.add(len(coords) - 1)\n\n            # Add this weird fourth coordinate\n            coords = np.c_[coords, [0] * coords.shape[0]]\n\n            # Set point coordinates\n            sp.points.foreach_set('co', coords.ravel())\n            sp.points.foreach_set('weight', s)\n\n            if use_radii:\n                r = [tn_radii[tn] for tn in s]\n                sp.points.foreach_set('radius', r)\n\n        ob.active_material = mat\n\n        if not collection:\n            col = bpy.context.scene.collection\n        elif collection in bpy.data.collections:\n            col = bpy.data.collections[collection]\n        else:\n            col = bpy.data.collections.new(collection)\n            bpy.context.scene.collection.children.link(col)\n\n        col.objects.link(ob)\n\n        return\n\n    def _create_dotprops(self, x, scale_vect=1, collection=None):\n        \"\"\"Create neuron branches.\"\"\"\n        # Generate uuid\n        object_id = str(uuid.uuid4())\n\n        mat_name = (f'M#{object_id}')[:59]\n        mat = bpy.data.materials.get(mat_name,\n                                     bpy.data.materials.new(mat_name))\n\n        cu = bpy.data.curves.new(f\"{getattr(x, 'dotprop', '')} mesh\", 'CURVE')\n        ob = bpy.data.objects.new(f\"#{object_id} - {getattr(x, 'neuron_name', '')}\",\n                                  cu)\n        ob.location = (0, 0, 0)\n        ob.show_name = True\n        ob['type'] = 'DOTPROP'\n        ob['navis_object'] = True\n        ob['id'] = object_id\n        cu.dimensions = '3D'\n        cu.fill_mode = 'FULL'\n        cu.bevel_resolution = 5\n        cu.bevel_depth = 0.007\n\n        # Prepare lines - this is based on nat:::plot3d.dotprops\n        halfvect = (np.vstack(x.vector) / 2 * scale_vect)\n        starts = (np.vstack(x.point) - halfvect)\n        ends = (np.vstack(x.point) + halfvect)\n\n        halfvect *= self.scaling\n        starts *= self.scaling\n        ends *= self.scaling\n\n        halfvect = halfvect[:, self.axes_order] * self.ax_translate\n        starts = starts[:, self.axes_order] * self.ax_translate\n        ends = ends[:, self.axes_order] * self.ax_translate\n\n        segments = list(zip(starts, ends))\n\n        for s in segments:\n            sp = cu.splines.new('POLY')\n\n            # Add points\n            sp.points.add(1)\n\n            # Add this weird fourth coordinate\n            coords = np.c_[s, [0, 0]]\n\n            # Set point coordinates\n            sp.points.foreach_set('co', coords.ravel())\n\n        ob.active_material = mat\n\n        if not collection:\n            col = bpy.context.scene.collection\n        elif collection in bpy.data.collections:\n            col = bpy.data.collections[collection]\n        else:\n            col = bpy.data.collections.new(collection)\n            bpy.context.scene.collection.children.link(col)\n\n        col.objects.link(ob)\n\n        return\n\n    def _create_soma(self, x, mat, collection=None):\n        \"\"\"Create soma.\"\"\"\n        if not collection:\n            col = bpy.context.scene.collection\n        elif collection in bpy.data.collections:\n            col = bpy.data.collections[collection]\n        else:\n            col = bpy.data.collections.new(collection)\n            bpy.context.scene.collection.children.link(col)\n\n        for nid in utils.make_iterable(x.soma):\n            s = x.nodes[x.nodes.node_id == nid]\n\n            # In theory, we should expect there to only be one row per soma, but just in case\n            if s.shape[0] &gt; 1:\n                logger.warning(\n                    f\"Expected only one node for soma, but got {s.shape[0]} - using first one only.\"\n                )\n            elif s.empty:\n                logger.warning(f\"No node found for soma with id {nid} - skipping.\")\n                continue\n\n            s = s.iloc[0]\n\n            loc = s[[\"x\", \"y\", \"z\"]].values.astype(\n                float\n            )  # do not remove the `astype` here, otherwise the dtype will be object!\n            loc = loc[self.axes_order] * self.scaling * self.ax_translate\n\n            rad = s.radius * self.scaling\n\n            mesh = bpy.data.meshes.new(f\"Soma of #{x.id} - mesh\")\n            soma_ob = bpy.data.objects.new(f\"Soma of #{x.id}\", mesh)\n\n            soma_ob.location = loc\n\n            # Construct the bmesh cube and assign it to the blender mesh.\n            bm = bmesh.new()\n            # Newer versions of Blender use `radius` instead of `diameter`\n            try:\n                bmesh.ops.create_uvsphere(bm, u_segments=16, v_segments=8, radius=rad)\n            except BaseException:\n                bmesh.ops.create_uvsphere(\n                    bm, u_segments=16, v_segments=8, diameter=rad * 2\n                )\n            bm.to_mesh(mesh)\n            bm.free()\n\n            mesh.polygons.foreach_set(\"use_smooth\", [True] * len(mesh.polygons))\n\n            soma_ob.name = f\"Soma of #{x.id}\"\n            soma_ob[\"type\"] = \"SOMA\"\n            soma_ob[\"navis_object\"] = True\n            soma_ob[\"id\"] = str(x.id)\n\n            soma_ob.active_material = mat\n\n            # Add the object into the scene.\n            col.objects.link(soma_ob)\n\n        return\n\n    def _create_connectors(self, x, collection=None):\n        \"\"\"Create connectors.\"\"\"\n        if not x.has_connectors:\n            return\n\n        if not collection:\n            col = bpy.context.scene.collection\n        elif collection in bpy.data.collections:\n            col = bpy.data.collections[collection]\n        else:\n            col = bpy.data.collections.new(collection)\n            bpy.context.scene.collection.children.link(col)\n\n        for t in x.connectors['type'].unique():\n            con = x.connectors[x.connectors.type == t]\n\n            # See if we have pre-defined names/colors for this\n            settings = self.cn_dict.get(t, {'name': t, 'color': (0, 0, 0)})\n\n            if con.empty:\n                continue\n\n            # Get &amp; scale coordinates and invert y\n            cn_coords = con[['x', 'y', 'z']].values.astype(float)\n\n            ob_name = f'{settings[\"name\"]} of {x.id}'\n\n            # Only plot as lines if this is a TreeNeuron\n            if self.cn_dict.get('display', 'lines') == 'lines' and isinstance(x, core.TreeNeuron):\n                cn_coords = cn_coords[:, self.axes_order]\n                cn_coords *= float(self.scaling)\n                cn_coords *= self.ax_translate\n\n                tn_coords = x.nodes.set_index('node_id').loc[con.node_id.values,\n                                                             ['x', 'y', 'z']].values.astype(float)\n                tn_coords = tn_coords[:, self.axes_order]\n                tn_coords *= float(self.scaling)\n                tn_coords *= self.ax_translate\n\n                # Add 4th coordinate for blender\n                cn_coords = np.c_[cn_coords, [0] * con.shape[0]]\n                tn_coords = np.c_[tn_coords, [0] * con.shape[0]]\n\n                # Combine cn and tn coords in pairs\n                # This will have to be transposed to get pairs of cn and tn\n                # (see below)\n                coords = np.dstack([cn_coords, tn_coords])\n                cu = bpy.data.curves.new(ob_name + ' mesh', 'CURVE')\n                ob = bpy.data.objects.new(ob_name, cu)\n                cu.dimensions = '3D'\n                cu.fill_mode = 'FULL'\n                cu.bevel_resolution = 0\n                cu.bevel_depth = 0.007\n                cu.resolution_u = 0\n\n                for cn in coords:\n                    sp = cu.splines.new('POLY')\n\n                    # Add a second point\n                    sp.points.add(1)\n\n                    # Move points\n                    sp.points.foreach_set('co', cn.T.ravel())\n                col.objects.link(ob)\n            else:\n                ob = self._create_scatter(cn_coords,\n                                          collection=collection,\n                                          size=self.cn_dict.get('size', 0.01))\n                ob.name = ob_name\n\n            ob['type'] = 'CONNECTORS'\n            ob['navis_object'] = True\n            ob['cn_type'] = t\n            ob['id'] = str(x.id)\n            ob.location = (0, 0, 0)\n            ob.show_name = False\n\n            mat_name = f'{settings[\"name\"]} of #{str(x.id)}'\n            mat = bpy.data.materials.get(mat_name,\n                                         bpy.data.materials.new(mat_name))\n            mat.diffuse_color = eval_color(settings['color'],\n                                           color_range=1,\n                                           force_alpha=True)\n            ob.active_material = mat\n\n        return\n\n    def _create_volume(self, volume, collection=None):\n        \"\"\"Create mesh from volume.\n\n        Parameters\n        ----------\n        volume :    core.Volume | dict\n                    Must contain 'faces', 'vertices'\n\n        \"\"\"\n        mesh_name = str(getattr(volume, 'name', 'mesh'))\n\n        verts = volume.vertices.copy()\n\n        # Convert to Blender space\n        verts = verts * self.scaling\n        verts = verts[:, self.axes_order]\n        verts *= self.ax_translate\n\n        blender_verts = verts.tolist()\n\n        me = bpy.data.meshes.new(mesh_name + '_mesh')\n        ob = bpy.data.objects.new(mesh_name, me)\n\n        scn = bpy.context.scene\n        scn.collection.objects.link(ob)\n\n        me.from_pydata(list(blender_verts), [], list(volume.faces))\n        me.update()\n\n        me.polygons.foreach_set('use_smooth', [True] * len(me.polygons))\n\n    def select(self, x, *args):\n        \"\"\"Select given neurons.\n\n        Parameters\n        ----------\n        x :     list of neuron IDs | Neuron/List | pd Dataframe\n\n        Returns\n        -------\n        [`navis.interfaces.blender.ObjectList`][] :  containing requested neurons\n\n        Examples\n        --------\n        &gt;&gt;&gt; selection = Handler.select([123456, 7890])\n        &gt;&gt;&gt; # Get only connectors\n        &gt;&gt;&gt; cn = selection.connectors\n        &gt;&gt;&gt; # Hide everything else\n        &gt;&gt;&gt; cn.hide_others()\n        &gt;&gt;&gt; # Change color of presynapses\n        &gt;&gt;&gt; selection.presynapses.color(0, 1, 0)\n\n        \"\"\"\n        ids = utils.eval_id(x)\n\n        if not ids:\n            logger.error('No ids found.')\n\n        names = []\n\n        for ob in bpy.data.objects:\n            ob.select_set(False)\n            if 'id' in ob:\n                if ob['id'] in ids:\n                    ob.select_set(True)\n                    names.append(ob.name)\n        return ObjectList(names, handler=self)\n\n    def color(self, r, g, b):\n        \"\"\"Assign color to all neurons.\n\n        Parameters\n        ----------\n        r :     float\n                Red value, range 0-1\n        g :     float\n                Green value, range 0-1\n        b :     float\n                Blue value, range 0-1\n\n        Notes\n        -----\n        This will only change color of neurons, if you want to change\n        color of e.g. connectors, use:\n\n        &gt;&gt;&gt; handler.connectors.color(r, g, b)\n\n        \"\"\"\n        self.neurons.color(r, g, b)\n\n    def colorize(self):\n        \"\"\"Randomly colorize ALL neurons.\n\n        Notes\n        -----\n        This will only change color of neurons, if you want to change\n        color of e.g. connectors, use:\n\n        &gt;&gt;&gt; handler.connectors.colorize()\n\n        \"\"\"\n        self.neurons.colorize()\n\n    def emit(self, v):\n        \"\"\"Change emit value.\"\"\"\n        self.neurons.emit(v)\n\n    def use_transparency(self, v):\n        \"\"\"Change transparency (True/False).\"\"\"\n        self.neurons.use_transparency(v)\n\n    def alpha(self, v):\n        \"\"\"Change alpha (0-1).\"\"\"\n        self.neurons.alpha(v)\n\n    def bevel(self, r):\n        \"\"\"Change bevel of ALL neurons.\n\n        Parameters\n        ----------\n        r :         float\n                    New bevel radius\n\n        Notes\n        -----\n        This will only change bevel of neurons, if you want to change\n        bevel of e.g. connectors, use:\n\n        &gt;&gt;&gt; handler.connectors.bevel(.02)\n\n        \"\"\"\n        self.neurons.bevel_depth(r)\n\n    def hide(self):\n        \"\"\"Hide all neuron-related objects.\"\"\"\n        self.all.hide()\n\n    def unhide(self):\n        \"\"\"Unide all neuron-related objects.\"\"\"\n        self.all.unhide()\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.Handler.add","title":"<code>add</code>","text":"<p>Add neuron(s) to scene.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Objects to import into Blender.\n</code></pre> <p> TYPE: <code>            TreeNeuron | MeshNeuron | NeuronList | core.Volume</code> </p> <code>neurites</code> <pre><code>        Plot neurites. TreeNeurons only.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> <code>soma</code> <pre><code>        Plot somas. TreeNeurons only.\n</code></pre> <p> TYPE: <code>         bool</code> DEFAULT: <code>True</code> </p> <code>connectors</code> <pre><code>        Plot connectors. Uses a defaults dictionary to set\n        color/type. See Examples on how to change.\n</code></pre> <p> TYPE: <code>   bool</code> DEFAULT: <code>True</code> </p> <code>redraw</code> <pre><code>        If True, will redraw window after each neuron. This\n        will slow down loading!\n</code></pre> <p> TYPE: <code>       bool</code> DEFAULT: <code>False</code> </p> <code>use_radii</code> <pre><code>        If True, will use node radii. For TreeNeurons only.\n</code></pre> <p> TYPE: <code>    bool</code> DEFAULT: <code>False</code> </p> <code>skip_existing</code> <pre><code>        If True, will skip neurons that are already loaded.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>downsample</code> <pre><code>        If integer &lt; 1, will downsample neurites upon import.\n        Preserves branch point/roots. TreeNeurons only.\n</code></pre> <p> TYPE: <code>   False | int</code> DEFAULT: <code>False</code> </p> <code>collection</code> <pre><code>        Only for Blender &gt;2.8: add object(s) to given collection.\n        If collection does not exist, will be created.\n</code></pre> <p> TYPE: <code>   str</code> DEFAULT: <code>None</code> </p> <p>Examples:</p> <p>Add one of the example neurons:</p> <pre><code>&gt;&gt;&gt; h = navis.interfaces.blender.Handler()\n&gt;&gt;&gt; n = navis.example_neurons(1)\n&gt;&gt;&gt; h.add(n, connectors=True)\n</code></pre> <p>Change connector settings:</p> <pre><code>&gt;&gt;&gt; h.cn_dict['display'] = 'sphere'\n&gt;&gt;&gt; h.cn_dict[0]['color'] = (1, 1, 0)\n</code></pre> Source code in <code>navis/interfaces/blender.py</code> <pre><code>def add(self, x, neurites=True, soma=True, connectors=True, redraw=False,\n        use_radii=False, skip_existing=False, downsample=False,\n        collection=None, **kwargs):\n    \"\"\"Add neuron(s) to scene.\n\n    Parameters\n    ----------\n    x :             TreeNeuron | MeshNeuron | NeuronList | core.Volume\n                    Objects to import into Blender.\n    neurites :      bool, optional\n                    Plot neurites. TreeNeurons only.\n    soma :          bool, optional\n                    Plot somas. TreeNeurons only.\n    connectors :    bool, optional\n                    Plot connectors. Uses a defaults dictionary to set\n                    color/type. See Examples on how to change.\n    redraw :        bool, optional\n                    If True, will redraw window after each neuron. This\n                    will slow down loading!\n    use_radii :     bool, optional\n                    If True, will use node radii. For TreeNeurons only.\n    skip_existing : bool, optional\n                    If True, will skip neurons that are already loaded.\n    downsample :    False | int, optional\n                    If integer &lt; 1, will downsample neurites upon import.\n                    Preserves branch point/roots. TreeNeurons only.\n    collection :    str, optional\n                    Only for Blender &gt;2.8: add object(s) to given collection.\n                    If collection does not exist, will be created.\n\n    Examples\n    --------\n    Add one of the example neurons:\n\n    &gt;&gt;&gt; h = navis.interfaces.blender.Handler()\n    &gt;&gt;&gt; n = navis.example_neurons(1)\n    &gt;&gt;&gt; h.add(n, connectors=True)\n\n    Change connector settings:\n\n    &gt;&gt;&gt; h.cn_dict['display'] = 'sphere'\n    &gt;&gt;&gt; h.cn_dict[0]['color'] = (1, 1, 0)\n\n    \"\"\"\n    start = time.time()\n\n    if skip_existing:\n        exists = [ob.get('id', None) for ob in bpy.data.objects]\n\n    if isinstance(x, (core.BaseNeuron, core.NeuronList)):\n        if redraw:\n            print('Set \"redraw=False\" to vastly speed up import!')\n        if isinstance(x, core.BaseNeuron):\n            x = [x]\n        wm = bpy.context.window_manager\n        wm.progress_begin(0, len(x))\n        for i, n in enumerate(x):\n            # Skip existing if applicable\n            if skip_existing and n.id in exists:\n                continue\n            self._create_neuron(n,\n                                neurites=neurites,\n                                soma=soma,\n                                connectors=connectors,\n                                collection=collection,\n                                downsample=downsample,\n                                use_radii=use_radii)\n            if redraw:\n                bpy.ops.wm.redraw_timer(type='DRAW_WIN_SWAP', iterations=1)\n            wm.progress_update(i)\n        wm.progress_end()\n    elif isinstance(x, tm.Trimesh):\n        self._create_volume(x, collection=collection)\n    elif isinstance(x, np.ndarray):\n        self._create_scatter(x, collection=collection, **kwargs)\n    elif isinstance(x, core.Dotprops):\n        self._create_dotprops(x, collection=collection, **kwargs)\n    else:\n        raise AttributeError(f'Unable add data type of type {type(x)}')\n\n    print(f'Import done in {time.time()-start:.2f}s')\n\n    return\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.Handler.alpha","title":"<code>alpha</code>","text":"<p>Change alpha (0-1).</p> Source code in <code>navis/interfaces/blender.py</code> <pre><code>def alpha(self, v):\n    \"\"\"Change alpha (0-1).\"\"\"\n    self.neurons.alpha(v)\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.Handler.bevel","title":"<code>bevel</code>","text":"<p>Change bevel of ALL neurons.</p> PARAMETER DESCRIPTION <code>r</code> <pre><code>    New bevel radius\n</code></pre> <p> TYPE: <code>        float</code> </p> Notes <p>This will only change bevel of neurons, if you want to change bevel of e.g. connectors, use:</p> <p>handler.connectors.bevel(.02)</p> Source code in <code>navis/interfaces/blender.py</code> <pre><code>def bevel(self, r):\n    \"\"\"Change bevel of ALL neurons.\n\n    Parameters\n    ----------\n    r :         float\n                New bevel radius\n\n    Notes\n    -----\n    This will only change bevel of neurons, if you want to change\n    bevel of e.g. connectors, use:\n\n    &gt;&gt;&gt; handler.connectors.bevel(.02)\n\n    \"\"\"\n    self.neurons.bevel_depth(r)\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.Handler.clear","title":"<code>clear</code>","text":"<p>Clear all neurons</p> Source code in <code>navis/interfaces/blender.py</code> <pre><code>def clear(self):\n    \"\"\"Clear all neurons \"\"\"\n    self.all.delete()\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.Handler.color","title":"<code>color</code>","text":"<p>Assign color to all neurons.</p> PARAMETER DESCRIPTION <code>r</code> <pre><code>Red value, range 0-1\n</code></pre> <p> TYPE: <code>    float</code> </p> <code>g</code> <pre><code>Green value, range 0-1\n</code></pre> <p> TYPE: <code>    float</code> </p> <code>b</code> <pre><code>Blue value, range 0-1\n</code></pre> <p> TYPE: <code>    float</code> </p> Notes <p>This will only change color of neurons, if you want to change color of e.g. connectors, use:</p> <p>handler.connectors.color(r, g, b)</p> Source code in <code>navis/interfaces/blender.py</code> <pre><code>def color(self, r, g, b):\n    \"\"\"Assign color to all neurons.\n\n    Parameters\n    ----------\n    r :     float\n            Red value, range 0-1\n    g :     float\n            Green value, range 0-1\n    b :     float\n            Blue value, range 0-1\n\n    Notes\n    -----\n    This will only change color of neurons, if you want to change\n    color of e.g. connectors, use:\n\n    &gt;&gt;&gt; handler.connectors.color(r, g, b)\n\n    \"\"\"\n    self.neurons.color(r, g, b)\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.Handler.colorize","title":"<code>colorize</code>","text":"<p>Randomly colorize ALL neurons.</p> Notes <p>This will only change color of neurons, if you want to change color of e.g. connectors, use:</p> <p>handler.connectors.colorize()</p> Source code in <code>navis/interfaces/blender.py</code> <pre><code>def colorize(self):\n    \"\"\"Randomly colorize ALL neurons.\n\n    Notes\n    -----\n    This will only change color of neurons, if you want to change\n    color of e.g. connectors, use:\n\n    &gt;&gt;&gt; handler.connectors.colorize()\n\n    \"\"\"\n    self.neurons.colorize()\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.Handler.emit","title":"<code>emit</code>","text":"<p>Change emit value.</p> Source code in <code>navis/interfaces/blender.py</code> <pre><code>def emit(self, v):\n    \"\"\"Change emit value.\"\"\"\n    self.neurons.emit(v)\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.Handler.hide","title":"<code>hide</code>","text":"<p>Hide all neuron-related objects.</p> Source code in <code>navis/interfaces/blender.py</code> <pre><code>def hide(self):\n    \"\"\"Hide all neuron-related objects.\"\"\"\n    self.all.hide()\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.Handler.select","title":"<code>select</code>","text":"<p>Select given neurons.</p> PARAMETER DESCRIPTION <code>x</code> <p> TYPE: <code>    list of neuron IDs | Neuron/List | pd Dataframe</code> </p> RETURNS DESCRIPTION <code>[`navis.interfaces.blender.ObjectList`][] :  containing requested neurons</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; selection = Handler.select([123456, 7890])\n&gt;&gt;&gt; # Get only connectors\n&gt;&gt;&gt; cn = selection.connectors\n&gt;&gt;&gt; # Hide everything else\n&gt;&gt;&gt; cn.hide_others()\n&gt;&gt;&gt; # Change color of presynapses\n&gt;&gt;&gt; selection.presynapses.color(0, 1, 0)\n</code></pre> Source code in <code>navis/interfaces/blender.py</code> <pre><code>def select(self, x, *args):\n    \"\"\"Select given neurons.\n\n    Parameters\n    ----------\n    x :     list of neuron IDs | Neuron/List | pd Dataframe\n\n    Returns\n    -------\n    [`navis.interfaces.blender.ObjectList`][] :  containing requested neurons\n\n    Examples\n    --------\n    &gt;&gt;&gt; selection = Handler.select([123456, 7890])\n    &gt;&gt;&gt; # Get only connectors\n    &gt;&gt;&gt; cn = selection.connectors\n    &gt;&gt;&gt; # Hide everything else\n    &gt;&gt;&gt; cn.hide_others()\n    &gt;&gt;&gt; # Change color of presynapses\n    &gt;&gt;&gt; selection.presynapses.color(0, 1, 0)\n\n    \"\"\"\n    ids = utils.eval_id(x)\n\n    if not ids:\n        logger.error('No ids found.')\n\n    names = []\n\n    for ob in bpy.data.objects:\n        ob.select_set(False)\n        if 'id' in ob:\n            if ob['id'] in ids:\n                ob.select_set(True)\n                names.append(ob.name)\n    return ObjectList(names, handler=self)\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.Handler.unhide","title":"<code>unhide</code>","text":"<p>Unide all neuron-related objects.</p> Source code in <code>navis/interfaces/blender.py</code> <pre><code>def unhide(self):\n    \"\"\"Unide all neuron-related objects.\"\"\"\n    self.all.unhide()\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.Handler.use_transparency","title":"<code>use_transparency</code>","text":"<p>Change transparency (True/False).</p> Source code in <code>navis/interfaces/blender.py</code> <pre><code>def use_transparency(self, v):\n    \"\"\"Change transparency (True/False).\"\"\"\n    self.neurons.use_transparency(v)\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.ObjectList","title":"<code>navis.interfaces.blender.ObjectList</code>","text":"<p>Collection of Blender objects.</p> Notes <ol> <li>ObjectLists should normally be constructed via the handler     (see <code>navis.interfaces.blender.Handler</code>)!</li> <li>List works with object NAMES to prevent Blender from crashing when     trying to access neurons that do not exist anymore. This also means     that changing names manually will compromise a object list.</li> <li>Accessing a neuron list's attributes (see below) return another     <code>ObjectList</code> class which you can use to manipulate the new     subselection.</li> </ol> ATTRIBUTE DESCRIPTION <code>neurons</code> <p> TYPE: <code>returns list containing just neurons</code> </p> <code>connectors</code> <p> TYPE: <code>returns list containing all connectors</code> </p> <code>soma</code> <p> TYPE: <code>returns list containing all somata</code> </p> <code>presynapses</code> <p> TYPE: <code>returns list containing all presynapses</code> </p> <code>postsynapses</code> <p> TYPE: <code>returns list containing all postsynapses</code> </p> <code>gapjunctions</code> <p> TYPE: <code>returns list containing all gap junctions</code> </p> <code>abutting</code> <p> TYPE: <code>returns list containing all abutting connectors</code> </p> <code>id</code> <p> TYPE: <code>returns list of IDs</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # b3d module has to be import explicitly\n&gt;&gt;&gt; from navis import b3d\n&gt;&gt;&gt; nl = navis.example_neurons()\n&gt;&gt;&gt; handler = b3d.Handler()\n&gt;&gt;&gt; handler.add(nl)\n&gt;&gt;&gt; # Select only neurons on the right\n&gt;&gt;&gt; right = handler.select('annotation:uPN right')\n&gt;&gt;&gt; # This can be nested to change e.g. color of all right presynases\n&gt;&gt;&gt; handler.select('annotation:uPN right').presynapses.color(0, 1, 0)\n</code></pre> Source code in <code>navis/interfaces/blender.py</code> <pre><code>class ObjectList:\n    \"\"\"Collection of Blender objects.\n\n    Notes\n    -----\n    1.  ObjectLists should normally be constructed via the handler\n        (see [`navis.interfaces.blender.Handler`][])!\n    2.  List works with object NAMES to prevent Blender from crashing when\n        trying to access neurons that do not exist anymore. This also means\n        that changing names manually will compromise a object list.\n    3.  Accessing a neuron list's attributes (see below) return another\n        `ObjectList` class which you can use to manipulate the new\n        subselection.\n\n    Attributes\n    ----------\n    neurons :       returns list containing just neurons\n    connectors :    returns list containing all connectors\n    soma :          returns list containing all somata\n    presynapses :   returns list containing all presynapses\n    postsynapses :  returns list containing all postsynapses\n    gapjunctions :  returns list containing all gap junctions\n    abutting :      returns list containing all abutting connectors\n    id :            returns list of IDs\n\n    Examples\n    --------\n    &gt;&gt;&gt; # b3d module has to be import explicitly\n    &gt;&gt;&gt; from navis import b3d\n    &gt;&gt;&gt; nl = navis.example_neurons()\n    &gt;&gt;&gt; handler = b3d.Handler()\n    &gt;&gt;&gt; handler.add(nl)\n    &gt;&gt;&gt; # Select only neurons on the right\n    &gt;&gt;&gt; right = handler.select('annotation:uPN right')\n    &gt;&gt;&gt; # This can be nested to change e.g. color of all right presynases\n    &gt;&gt;&gt; handler.select('annotation:uPN right').presynapses.color(0, 1, 0)\n\n    \"\"\"\n    def __init__(self, object_names, handler=None):\n        if not isinstance(object_names, list):\n            object_names = [object_names]\n\n        self.object_names = object_names\n        self.handler = handler\n\n    def __getattr__(self, key):\n        if key in ['neurons', 'neuron', 'neurites']:\n            return ObjectList([n for n in self.object_names if n in bpy.data.objects and bpy.data.objects[n]['type'] == 'NEURON'])\n        elif key in ['connectors', 'connector']:\n            return ObjectList([n for n in self.object_names if n in bpy.data.objects and bpy.data.objects[n]['type'] == 'CONNECTORS'])\n        elif key in ['soma', 'somas']:\n            return ObjectList([n for n in self.object_names if n in bpy.data.objects and bpy.data.objects[n]['type'] == 'SOMA'])\n        elif key == 'presynapses':\n            return ObjectList([n for n in self.object_names if n in bpy.data.objects and bpy.data.objects[n]['type'] == 'CONNECTORS' and bpy.data.objects[n]['cn_type'] == 0])\n        elif key == 'postsynapses':\n            return ObjectList([n for n in self.object_names if n in bpy.data.objects and bpy.data.objects[n]['type'] == 'CONNECTORS' and bpy.data.objects[n]['cn_type'] == 1])\n        elif key == 'gapjunctions':\n            return ObjectList([n for n in self.object_names if n in bpy.data.objects and bpy.data.objects[n]['type'] == 'CONNECTORS' and bpy.data.objects[n]['cn_type'] == 2])\n        elif key == 'abutting':\n            return ObjectList([n for n in self.object_names if n in bpy.data.objects and bpy.data.objects[n]['type'] == 'CONNECTORS' and bpy.data.objects[n]['cn_type'] == 3])\n        elif key.lower() in ['id', 'ids']:\n            return [bpy.data.objects[n]['id'] for n in self.object_names if n in bpy.data.objects]\n        else:\n            raise AttributeError('Unknown attribute ' + key)\n\n    def __getitem__(self, key):\n        if isinstance(key, int) or isinstance(key, slice):\n            return ObjectList(self.object_names[key], handler=self.handler)\n        else:\n            raise Exception('Unable to index non-integers.')\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __repr__(self):\n        self._repr = pd.DataFrame([[n, n in bpy.data.objects] for n in self.object_names],\n                                  columns=['name', 'still_exists']\n                                  )\n        return str(self._repr)\n\n    def __len__(self):\n        return len(self.object_names)\n\n    def __add__(self, to_add):\n        if not isinstance(to_add, ObjectList):\n            raise AttributeError('Can only merge other object lists')\n        print(to_add.object_names)\n        return ObjectList(list(set(self.object_names + to_add.object_names)),\n                          handler=self.handler)\n\n    @property\n    def objects(self):\n        \"\"\"Objects in this list.\"\"\"\n        objects = []\n        for n in self.object_names:\n            if n in bpy.data.objects:\n                objects.append(bpy.data.objects[n])\n        return objects\n\n    def add_to_collection(self, collection, unlink_from_other=False):\n        if not collection:\n            col = bpy.context.scene.collection\n        elif collection in bpy.data.collections:\n            col = bpy.data.collections[collection]\n        else:\n            col = bpy.data.collections.new(collection)\n            bpy.context.scene.collection.children.link(col)\n\n        for ob in self.objects:\n            if ob.name not in col.objects:\n                col.objects.link(ob)\n\n            if unlink_from_other:\n                if ob.name in bpy.context.scene.collection.objects:\n                    bpy.context.scene.collection.objects.unlink(ob)\n\n                for col2 in bpy.data.collections:\n                    if col2 == col:\n                        continue\n                    if ob.name in col2.objects:\n                        col2.objects.unlink(ob)\n\n    def select(self, unselect_others=True):\n        \"\"\"Select objects in 3D viewer\n\n        Parameters\n        ----------\n        unselect_others :   bool, optional\n                            If False, will not unselect other objects.\n\n        \"\"\"\n        for ob in bpy.data.objects:\n            if ob.name in self.object_names:\n                ob.select_set(True)\n            elif unselect_others:\n                ob.select_set(False)\n\n    def color(self, r, g, b, a=1):\n        \"\"\"Assign color to all objects in the list.\n\n        Parameters\n        ----------\n        r :     float\n                Red value, range 0-1\n        g :     float\n                Green value, range 0-1\n        b :     float\n                Blue value, range 0-1\n        a :     float\n                Alpha value, range 0-1\n\n        \"\"\"\n        for ob in self.objects:\n            ob.active_material.diffuse_color = eval_color((r, g, b, a),\n                                                          color_range=1,\n                                                          force_alpha=True)\n\n    def colorize(self, groups=None, palette='hls'):\n        \"\"\"Assign colors across the color spectrum.\n\n        Parameters\n        ----------\n        groups :    dict, optional\n                    A dictionary mapping either neuron ID (always str!) or\n                    object name to a group (str). Neurons of the same group will\n                    receive the same color.\n        palette :   str\n                    Name of a seaborn color palette.\n\n        \"\"\"\n        objects = self.objects\n        if isinstance(groups, type(None)):\n            colors = sns.color_palette(palette, len(objects))\n            cmap = dict(zip(objects, colors))\n        elif isinstance(groups, dict):\n            # Make sure keys are strings\n            groups = {str(k): v for k, v in groups.items()}\n\n            # Get unique groups &amp; create a color map\n            groups_uni = list(set(list(groups.values())))\n            colors = sns.color_palette(palette, len(groups_uni))\n            groups_cmap = dict(zip(groups_uni, colors))\n\n            # Make the actual color map\n            cmap = {}\n            for ob in objects:\n                # Get the group either by name or ID\n                g = groups.get(ob.name, groups.get(ob.get('id'), None))\n                cmap[ob] = groups_cmap.get(g, (.1, .1, .1))\n        else:\n            raise TypeError(f'`groups` must be either None or dict, got {type(groups)}')\n\n        for ob in objects:\n            try:\n                ob.active_material.diffuse_color = eval_color(cmap[ob],\n                                                              color_range=1,\n                                                              force_alpha=True)\n            except BaseException:\n                logger.warning(f'Error changing color of object \"{ob}\"')\n\n    def emit(self, e):\n        \"\"\"Change emit value.\"\"\"\n        for ob in self.objects:\n            ob.active_material.emit = e\n\n    def use_transparency(self, t):\n        \"\"\"Change transparency (True/False).\"\"\"\n        for ob in self.objects:\n            ob.active_material.use_transparency = t\n\n    def alpha(self, a):\n        \"\"\"Change alpha (0-1).\"\"\"\n        for ob in self.objects:\n            ob.active_material.alpha = a\n\n    def bevel(self, r):\n        \"\"\"Change bevel radius of objects.\n\n        Parameters\n        ----------\n        r :         float\n                    New bevel radius.\n\n        \"\"\"\n        for ob in self.objects:\n            if ob.type == 'CURVE':\n                ob.data.bevel_depth = r\n\n    def hide(self, viewport=True, render=False):\n        \"\"\"Hide objects.\"\"\"\n        for ob in self.objects:\n            if viewport:\n                ob.hide_set(True)\n            if render:\n                ob.hide_render = True\n\n    def unhide(self, viewport=True, render=False):\n        \"\"\"Unhide objects.\"\"\"\n        for ob in self.objects:\n            if viewport:\n                ob.hide_set(False)\n            if render:\n                ob.hide_render = False\n\n    def hide_others(self):\n        \"\"\"Hide everything BUT these objects.\"\"\"\n        for ob in bpy.data.objects:\n            if ob.name in self.object_names:\n                ob.hide = False\n            else:\n                ob.hide = True\n\n    def delete(self):\n        \"\"\"Delete neurons in the selection.\"\"\"\n        self.select(unselect_others=True)\n        bpy.ops.object.delete()\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.ObjectList.objects","title":"<code>objects</code>  <code>property</code>","text":"<p>Objects in this list.</p>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.ObjectList.alpha","title":"<code>alpha</code>","text":"<p>Change alpha (0-1).</p> Source code in <code>navis/interfaces/blender.py</code> <pre><code>def alpha(self, a):\n    \"\"\"Change alpha (0-1).\"\"\"\n    for ob in self.objects:\n        ob.active_material.alpha = a\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.ObjectList.bevel","title":"<code>bevel</code>","text":"<p>Change bevel radius of objects.</p> PARAMETER DESCRIPTION <code>r</code> <pre><code>    New bevel radius.\n</code></pre> <p> TYPE: <code>        float</code> </p> Source code in <code>navis/interfaces/blender.py</code> <pre><code>def bevel(self, r):\n    \"\"\"Change bevel radius of objects.\n\n    Parameters\n    ----------\n    r :         float\n                New bevel radius.\n\n    \"\"\"\n    for ob in self.objects:\n        if ob.type == 'CURVE':\n            ob.data.bevel_depth = r\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.ObjectList.color","title":"<code>color</code>","text":"<p>Assign color to all objects in the list.</p> PARAMETER DESCRIPTION <code>r</code> <pre><code>Red value, range 0-1\n</code></pre> <p> TYPE: <code>    float</code> </p> <code>g</code> <pre><code>Green value, range 0-1\n</code></pre> <p> TYPE: <code>    float</code> </p> <code>b</code> <pre><code>Blue value, range 0-1\n</code></pre> <p> TYPE: <code>    float</code> </p> <code>a</code> <pre><code>Alpha value, range 0-1\n</code></pre> <p> TYPE: <code>    float</code> DEFAULT: <code>1</code> </p> Source code in <code>navis/interfaces/blender.py</code> <pre><code>def color(self, r, g, b, a=1):\n    \"\"\"Assign color to all objects in the list.\n\n    Parameters\n    ----------\n    r :     float\n            Red value, range 0-1\n    g :     float\n            Green value, range 0-1\n    b :     float\n            Blue value, range 0-1\n    a :     float\n            Alpha value, range 0-1\n\n    \"\"\"\n    for ob in self.objects:\n        ob.active_material.diffuse_color = eval_color((r, g, b, a),\n                                                      color_range=1,\n                                                      force_alpha=True)\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.ObjectList.colorize","title":"<code>colorize</code>","text":"<p>Assign colors across the color spectrum.</p> PARAMETER DESCRIPTION <code>groups</code> <pre><code>    A dictionary mapping either neuron ID (always str!) or\n    object name to a group (str). Neurons of the same group will\n    receive the same color.\n</code></pre> <p> TYPE: <code>   dict</code> DEFAULT: <code>None</code> </p> <code>palette</code> <pre><code>    Name of a seaborn color palette.\n</code></pre> <p> TYPE: <code>  str</code> DEFAULT: <code>'hls'</code> </p> Source code in <code>navis/interfaces/blender.py</code> <pre><code>def colorize(self, groups=None, palette='hls'):\n    \"\"\"Assign colors across the color spectrum.\n\n    Parameters\n    ----------\n    groups :    dict, optional\n                A dictionary mapping either neuron ID (always str!) or\n                object name to a group (str). Neurons of the same group will\n                receive the same color.\n    palette :   str\n                Name of a seaborn color palette.\n\n    \"\"\"\n    objects = self.objects\n    if isinstance(groups, type(None)):\n        colors = sns.color_palette(palette, len(objects))\n        cmap = dict(zip(objects, colors))\n    elif isinstance(groups, dict):\n        # Make sure keys are strings\n        groups = {str(k): v for k, v in groups.items()}\n\n        # Get unique groups &amp; create a color map\n        groups_uni = list(set(list(groups.values())))\n        colors = sns.color_palette(palette, len(groups_uni))\n        groups_cmap = dict(zip(groups_uni, colors))\n\n        # Make the actual color map\n        cmap = {}\n        for ob in objects:\n            # Get the group either by name or ID\n            g = groups.get(ob.name, groups.get(ob.get('id'), None))\n            cmap[ob] = groups_cmap.get(g, (.1, .1, .1))\n    else:\n        raise TypeError(f'`groups` must be either None or dict, got {type(groups)}')\n\n    for ob in objects:\n        try:\n            ob.active_material.diffuse_color = eval_color(cmap[ob],\n                                                          color_range=1,\n                                                          force_alpha=True)\n        except BaseException:\n            logger.warning(f'Error changing color of object \"{ob}\"')\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.ObjectList.delete","title":"<code>delete</code>","text":"<p>Delete neurons in the selection.</p> Source code in <code>navis/interfaces/blender.py</code> <pre><code>def delete(self):\n    \"\"\"Delete neurons in the selection.\"\"\"\n    self.select(unselect_others=True)\n    bpy.ops.object.delete()\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.ObjectList.emit","title":"<code>emit</code>","text":"<p>Change emit value.</p> Source code in <code>navis/interfaces/blender.py</code> <pre><code>def emit(self, e):\n    \"\"\"Change emit value.\"\"\"\n    for ob in self.objects:\n        ob.active_material.emit = e\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.ObjectList.hide","title":"<code>hide</code>","text":"<p>Hide objects.</p> Source code in <code>navis/interfaces/blender.py</code> <pre><code>def hide(self, viewport=True, render=False):\n    \"\"\"Hide objects.\"\"\"\n    for ob in self.objects:\n        if viewport:\n            ob.hide_set(True)\n        if render:\n            ob.hide_render = True\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.ObjectList.hide_others","title":"<code>hide_others</code>","text":"<p>Hide everything BUT these objects.</p> Source code in <code>navis/interfaces/blender.py</code> <pre><code>def hide_others(self):\n    \"\"\"Hide everything BUT these objects.\"\"\"\n    for ob in bpy.data.objects:\n        if ob.name in self.object_names:\n            ob.hide = False\n        else:\n            ob.hide = True\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.ObjectList.select","title":"<code>select</code>","text":"<p>Select objects in 3D viewer</p> PARAMETER DESCRIPTION <code>unselect_others</code> <pre><code>            If False, will not unselect other objects.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>True</code> </p> Source code in <code>navis/interfaces/blender.py</code> <pre><code>def select(self, unselect_others=True):\n    \"\"\"Select objects in 3D viewer\n\n    Parameters\n    ----------\n    unselect_others :   bool, optional\n                        If False, will not unselect other objects.\n\n    \"\"\"\n    for ob in bpy.data.objects:\n        if ob.name in self.object_names:\n            ob.select_set(True)\n        elif unselect_others:\n            ob.select_set(False)\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.ObjectList.unhide","title":"<code>unhide</code>","text":"<p>Unhide objects.</p> Source code in <code>navis/interfaces/blender.py</code> <pre><code>def unhide(self, viewport=True, render=False):\n    \"\"\"Unhide objects.\"\"\"\n    for ob in self.objects:\n        if viewport:\n            ob.hide_set(False)\n        if render:\n            ob.hide_render = False\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.ObjectList.use_transparency","title":"<code>use_transparency</code>","text":"<p>Change transparency (True/False).</p> Source code in <code>navis/interfaces/blender.py</code> <pre><code>def use_transparency(self, t):\n    \"\"\"Change transparency (True/False).\"\"\"\n    for ob in self.objects:\n        ob.active_material.use_transparency = t\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.calc_sphere","title":"<code>navis.interfaces.blender.calc_sphere</code>","text":"<p>Calculate vertices and faces for a sphere.</p> Source code in <code>navis/interfaces/blender.py</code> <pre><code>def calc_sphere(radius, nrPolar, nrAzimuthal):\n    \"\"\"Calculate vertices and faces for a sphere.\"\"\"\n    dPolar = math.pi / (nrPolar - 1)\n    dAzimuthal = 2.0 * math.pi / (nrAzimuthal)\n\n    # 1/2: vertices\n    verts = []\n    currV = mathutils.Vector((0.0, 0.0, radius))        # top vertex\n    verts.append(currV)\n    for iPolar in range(1, nrPolar - 1):                # regular vertices\n        currPolar = dPolar * float(iPolar)\n\n        currCosP = math.cos(currPolar)\n        currSinP = math.sin(currPolar)\n\n        for iAzimuthal in range(nrAzimuthal):\n            currAzimuthal = dAzimuthal * float(iAzimuthal)\n\n            currCosA = math.cos(currAzimuthal)\n            currSinA = math.sin(currAzimuthal)\n\n            currV = mathutils.Vector((currSinP * currCosA,\n                                      currSinP * currSinA,\n                                      currCosP)) * radius\n            verts.append(currV)\n    currV = mathutils.Vector((0.0, 0.0, - radius))        # bottom vertex\n    verts.append(currV)\n\n    # 2/2: faces\n    faces = []\n    for iAzimuthal in range(nrAzimuthal):                # top faces\n        iNextAzimuthal = iAzimuthal + 1\n        if iNextAzimuthal &gt;= nrAzimuthal:\n            iNextAzimuthal -= nrAzimuthal\n        faces.append([0, iAzimuthal + 1, iNextAzimuthal + 1])\n\n    for iPolar in range(nrPolar - 3):                    # regular faces\n        iAzimuthalStart = iPolar * nrAzimuthal + 1\n\n        for iAzimuthal in range(nrAzimuthal):\n            iNextAzimuthal = iAzimuthal + 1\n            if iNextAzimuthal &gt;= nrAzimuthal:\n                iNextAzimuthal -= nrAzimuthal\n            faces.append([iAzimuthalStart + iAzimuthal,\n                          iAzimuthalStart + iAzimuthal + nrAzimuthal,\n                          iAzimuthalStart + iNextAzimuthal + nrAzimuthal,\n                          iAzimuthalStart + iNextAzimuthal])\n\n    iLast = len(verts) - 1\n    iAzimuthalStart = iLast - nrAzimuthal\n    for iAzimuthal in range(nrAzimuthal):                # bottom faces\n        iNextAzimuthal = iAzimuthal + 1\n        if iNextAzimuthal &gt;= nrAzimuthal:\n            iNextAzimuthal -= nrAzimuthal\n        faces.append([iAzimuthalStart + iAzimuthal,\n                      iLast,\n                      iAzimuthalStart + iNextAzimuthal])\n\n    return np.vstack(verts), faces\n</code></pre>"},{"location":"reference/navis/interfaces/blender/#navis.interfaces.blender.eval_color","title":"<code>navis.interfaces.blender.eval_color</code>","text":"<p>Evaluate colors and return tuples.</p> Source code in <code>navis/plotting/colors.py</code> <pre><code>def eval_color(x, color_range=255, force_alpha=False):\n    \"\"\"Evaluate colors and return tuples.\"\"\"\n    if color_range not in (1, 255):\n        raise ValueError('\"color_range\" must be 1 or 255')\n\n    if isinstance(x, str):\n        # Check if named color\n        if mcl.is_color_like(x):\n            c = mcl.to_rgb(x)\n        # Assume it's a matplotlib color map\n        else:\n            try:\n                c = plt.get_cmap(x)\n            except ValueError:\n                raise ValueError(f'Unable to interpret color \"{x}\"')\n            except BaseException:\n                raise\n    elif isinstance(x, dict):\n        return {k: eval_color(v, color_range=color_range) for k, v in x.items()}\n    elif isinstance(x, (list, tuple)):\n        # If is this is not a list of RGB values:\n        if any([not isinstance(elem, numbers.Number) for elem in x]):\n            return [eval_color(c, color_range=color_range) for c in x]\n        # If this is a single RGB color:\n        c = x\n    elif isinstance(x, np.ndarray):\n        # If is this is not a list of RGB values:\n        if any([not isinstance(elem, numbers.Number) for elem in x]):\n            return np.array([eval_color(c, color_range=color_range) for c in x])\n        # If this is a single RGB color:\n        c = x\n    elif isinstance(x, type(None)):\n        return None\n    else:\n        raise TypeError(f'Unable to interpret color of type \"{type(x)}\"')\n\n    if not isinstance(c, mcl.Colormap):\n        # Check if we need to convert\n        if all(v &lt;= 1 for v in c[:3]) and color_range == 255:\n            if len(c) == 4:\n                c = tuple((int(c[0] * 255), int(c[1] * 255), int(c[2] * 255), c[3]))\n            else:\n                c = tuple((int(c[0] * 255), int(c[1] * 255), int(c[2] * 255)))\n        elif any(v &gt; 1 for v in c[:3]) and color_range == 1:\n            if len(c) == 4:\n                c = tuple((c[0] / 255, c[1] / 255, c[2] / 255, c[3]))\n            else:\n                c = tuple((c[0] / 255, c[1] / 255, c[2] / 255))\n\n        c = tuple(c)\n\n    if force_alpha and len(c) == 3:\n        c = (c[0], c[1], c[2], 1)\n\n    return c\n</code></pre>"},{"location":"reference/navis/interfaces/cave_utils/","title":"cave_utils","text":""},{"location":"reference/navis/interfaces/cave_utils/#navis.interfaces.cave_utils.MaterializationMatchError","title":"<code>navis.interfaces.cave_utils.MaterializationMatchError</code>","text":"Source code in <code>navis/interfaces/cave_utils.py</code> <pre><code>class MaterializationMatchError(Exception):\n    pass\n</code></pre>"},{"location":"reference/navis/interfaces/cave_utils/#navis.interfaces.cave_utils.fetch_neurons","title":"<code>navis.interfaces.cave_utils.fetch_neurons</code>","text":"<p>Fetch neuron meshes.</p> Notes <p>Synapses will be attached to the closest vertex on the mesh.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Segment ID(s). Multiple Ids can be provided as list-like.\n</code></pre> <p> TYPE: <code>            str | int | list-like</code> </p> <code>lod</code> <pre><code>        Level of detail. Higher ``lod`` = coarser. This parameter\n        is ignored if the data source does not support multi-level\n        meshes.\n</code></pre> <p> TYPE: <code>          int</code> </p> <code>with_synapses</code> <pre><code>        If True will also attach synapses as ``.connectors``.\n</code></pre> <p> TYPE: <code>bool</code> </p> <code>client</code> <pre><code>        The CAVEclient with which to interact.\n</code></pre> <p> TYPE: <code>       CAVEclient</code> </p> <code>parallel</code> <pre><code>        If True, will use parallel threads to fetch data.\n</code></pre> <p> TYPE: <code>     bool</code> </p> <code>max_threads</code> <pre><code>        Max number of parallel threads to use.\n</code></pre> <p> TYPE: <code>  int</code> </p> <code>materialization</code> <pre><code>        Which materialization version to use to look up somas and synapses\n        (if applicable). If \"auto\" (default) will try to find the most\n        recent version that contains the given root IDs. If an\n        integer is provided will use that version.\n</code></pre> <p> TYPE: <code>auto | int</code> DEFAULT: <code>'auto'</code> </p> <code>**kwargs</code> <pre><code>        Keyword arguments are passed through to the initialization\n        of the ``navis.MeshNeurons``.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>navis.Neuronlist</code> <p>Containing :class:<code>navis.MeshNeuron</code>.</p> Source code in <code>navis/interfaces/cave_utils.py</code> <pre><code>def fetch_neurons(\n    x,\n    lod,\n    with_synapses,\n    client,\n    parallel,\n    max_threads,\n    materialization=\"auto\",\n    **kwargs,\n):\n    \"\"\"Fetch neuron meshes.\n\n    Notes\n    -----\n    Synapses will be attached to the closest vertex on the mesh.\n\n    Parameters\n    ----------\n    x :             str | int | list-like\n                    Segment ID(s). Multiple Ids can be provided as list-like.\n    lod :           int\n                    Level of detail. Higher ``lod`` = coarser. This parameter\n                    is ignored if the data source does not support multi-level\n                    meshes.\n    with_synapses : bool, optional\n                    If True will also attach synapses as ``.connectors``.\n    client :        CAVEclient\n                    The CAVEclient with which to interact.\n    parallel :      bool\n                    If True, will use parallel threads to fetch data.\n    max_threads :   int\n                    Max number of parallel threads to use.\n    materialization : \"auto\" | int\n                    Which materialization version to use to look up somas and synapses\n                    (if applicable). If \"auto\" (default) will try to find the most\n                    recent version that contains the given root IDs. If an\n                    integer is provided will use that version.\n    **kwargs\n                    Keyword arguments are passed through to the initialization\n                    of the ``navis.MeshNeurons``.\n\n    Returns\n    -------\n    navis.Neuronlist\n                    Containing :class:`navis.MeshNeuron`.\n\n    \"\"\"\n    x = utils.make_iterable(x, force_type=int)\n\n    # Let CAVEclient handle the cloudvolume (this should take care of any secrets)\n    vol = _cloudvol_from_cv(client)\n\n    try:\n        somas = _get_somas(x, client=client, materialization=materialization)\n        soma_pos = somas.set_index(\"pt_root_id\").pt_position.to_dict()\n    except BaseException as e:\n        logger.warning(f\"Failed to fetch somas via nucleus segmentation(){e})\")\n        soma_pos = {}\n\n    nl = []\n    if max_threads &gt; 1 and parallel:\n        with ThreadPoolExecutor(max_workers=max_threads) as executor:\n            futures = {}\n            for id in x:\n                f = executor.submit(\n                    _fetch_single_neuron,\n                    id,\n                    vol=vol,\n                    lod=lod,\n                    client=client,\n                    with_synapses=with_synapses,\n                    materialization=materialization,\n                    **kwargs,\n                )\n                futures[f] = id\n\n            with config.tqdm(\n                desc=\"Fetching\",\n                total=len(x),\n                leave=config.pbar_leave,\n                disable=len(x) == 1 or config.pbar_hide,\n            ) as pbar:\n                for f in as_completed(futures):\n                    id = futures[f]\n                    pbar.update(1)\n                    try:\n                        nl.append(f.result())\n                    except Exception as exc:\n                        print(f\"{id} generated an exception:\", exc)\n    else:\n        for id in config.tqdm(\n            x,\n            desc=\"Fetching\",\n            leave=config.pbar_leave,\n            disable=len(x) == 1 or config.pbar_hide,\n        ):\n            n = _fetch_single_neuron(\n                id,\n                vol=vol,\n                lod=lod,\n                client=client,\n                with_synapses=with_synapses,\n                materialization=materialization,\n                **kwargs,\n            )\n            nl.append(n)\n\n    nl = NeuronList(nl)\n\n    for n in nl:\n        if n.id in soma_pos:\n            # For VoxelResolution see client.materialize.get_table_metadata('nucleus_detection_v0')\n            # (attached to df as 'table_voxel_resolution')\n            n.soma_pos = (\n                np.array(soma_pos[n.id]) * somas.attrs[\"table_voxel_resolution\"]\n            )\n        else:\n            n.soma_pos = None\n\n    return nl\n</code></pre>"},{"location":"reference/navis/interfaces/cave_utils/#navis.interfaces.cave_utils.get_voxels","title":"<code>navis.interfaces.cave_utils.get_voxels</code>","text":"<p>Fetch voxels making a up given root ID.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        A single root ID.\n</code></pre> <p> TYPE: <code>            int</code> </p> <code>mip</code> <pre><code>        Scale at which to fetch voxels.\n</code></pre> <p> TYPE: <code>          int</code> </p> <code>bounds</code> <pre><code>        Bounding box [xmin, xmax, ymin, ymax, zmin, zmax] in voxel\n        space. For example, the voxel resolution for mip 0\n        segmentation is 8 x 8 x 40 nm.\n</code></pre> <p> TYPE: <code>       list</code> </p> <code>client</code> <pre><code>        The CAVEclient with which to interact.\n</code></pre> <p> TYPE: <code>       CAVEclient</code> </p> RETURNS DESCRIPTION <code>voxels</code> <p>In voxel space according to <code>mip</code>.</p> <p> TYPE: <code>(N, 3) np.ndarray</code> </p> Source code in <code>navis/interfaces/cave_utils.py</code> <pre><code>def get_voxels(x, mip, bounds, client):\n    \"\"\"Fetch voxels making a up given root ID.\n\n    Parameters\n    ----------\n    x :             int\n                    A single root ID.\n    mip :           int\n                    Scale at which to fetch voxels.\n    bounds :        list, optional\n                    Bounding box [xmin, xmax, ymin, ymax, zmin, zmax] in voxel\n                    space. For example, the voxel resolution for mip 0\n                    segmentation is 8 x 8 x 40 nm.\n    client :        CAVEclient\n                    The CAVEclient with which to interact.\n\n    Returns\n    -------\n    voxels :        (N, 3) np.ndarray\n                    In voxel space according to `mip`.\n\n    \"\"\"\n    # Need to get the graphene (not the precomputed) version of the data\n    vol_graphene = cv.CloudVolume(\n        client.chunkedgraph.cloudvolume_path, use_https=True, progress=False\n    )\n    url = client.info.get_datastack_info()[\"segmentation_source\"]\n    vol_prec = _get_cloudvol(url)\n\n    # Get L2 chunks making up this neuron\n    l2_ids = client.chunkedgraph.get_leaves(x, stop_layer=2)\n\n    # Turn l2_ids into chunk indices\n    l2_ix = [\n        np.array(vol_graphene.mesh.meta.meta.decode_chunk_position(l2)) for l2 in l2_ids\n    ]\n    l2_ix = np.unique(l2_ix, axis=0)\n\n    # Convert to nm\n    l2_nm = np.asarray(_chunks_to_nm(l2_ix, vol=vol_graphene))\n\n    # Convert back to voxel space (according to mip)\n    l2_vxl = l2_nm // vol_prec.meta.scales[mip][\"resolution\"]\n\n    voxels = []\n    ch_size = np.array(vol_graphene.mesh.meta.meta.graph_chunk_size)\n    ch_size = ch_size // (vol_prec.mip_resolution(mip) / vol_prec.mip_resolution(0))\n    ch_size = np.asarray(ch_size).astype(int)\n    old_mip = vol_prec.mip\n\n    if not isinstance(bounds, type(None)):\n        bounds = np.asarray(bounds)\n        if not bounds.ndim == 1 or len(bounds) != 6:\n            raise ValueError(\"`bounds` must be [xmin, xmax, ymin, ymax, zmin, zmax]\")\n        l2_vxl = l2_vxl[np.all(l2_vxl &gt;= bounds[::2], axis=1)]\n        l2_vxl = l2_vxl[np.all(l2_vxl &lt; bounds[1::2] + ch_size, axis=1)]\n\n    try:\n        vol_prec.mip = mip\n        for ch in config.tqdm(l2_vxl, desc=\"Loading\"):\n            ct = vol_prec[\n                ch[0] : ch[0] + ch_size[0],\n                ch[1] : ch[1] + ch_size[1],\n                ch[2] : ch[2] + ch_size[2],\n            ][:, :, :, 0]\n            this_vxl = np.dstack(np.where(ct == x))[0]\n            this_vxl = this_vxl + ch\n            voxels.append(this_vxl)\n    except BaseException:\n        raise\n    finally:\n        vol_prec.mip = old_mip\n    voxels = np.vstack(voxels)\n\n    if not isinstance(bounds, type(None)):\n        voxels = voxels[np.all(voxels &gt;= bounds[::2], axis=1)]\n        voxels = voxels[np.all(voxels &lt; bounds[1::2], axis=1)]\n\n    return voxels\n</code></pre>"},{"location":"reference/navis/interfaces/cave_utils/#navis.interfaces.cave_utils.roots_to_mat","title":"<code>navis.interfaces.cave_utils.roots_to_mat</code>","text":"<p>Find a materialization version (or live) for given root ID(s).</p> PARAMETER DESCRIPTION <code>ids</code> <pre><code>        Root ID(s) to check.\n</code></pre> <p> TYPE: <code>          int | iterable</code> </p> <code>client</code> <pre><code>        The CAVEclient with which to interact.\n</code></pre> <p> TYPE: <code>       CAVEclient</code> </p> <code>verbose</code> <pre><code>        Whether to print results of search.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>True</code> </p> <code>allow_multiple</code> <pre><code>        If True, will track if IDs can be found spread across multiple\n        materialization versions if there is no single one containing\n        all.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>raise_missing</code> <pre><code>        Only relevant if `allow_multiple=True`. If False, will return\n        versions even if some IDs could not be found.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>version</code> <p>A single version (including \"live\") that contains all given root IDs.</p> <p> TYPE: <code>int | live</code> </p> <code>versions</code> <p>If no single version was found and <code>allow_multiple=True</code> will return a vector of <code>len(ids)</code> with the latest version at which the respective ID can be found. Important: \"live\" version will be return as -1! If <code>raise_missing=False</code> and one or more root IDs could not be found in any of the available materialization versions these IDs will be return as version 0.</p> <p> TYPE: <code>np.ndarray</code> </p> Source code in <code>navis/interfaces/cave_utils.py</code> <pre><code>def roots_to_mat(\n    ids,\n    client,\n    verbose=True,\n    allow_multiple=False,\n    raise_missing=True,\n):\n    \"\"\"Find a materialization version (or live) for given root ID(s).\n\n    Parameters\n    ----------\n    ids :           int | iterable\n                    Root ID(s) to check.\n    client :        CAVEclient\n                    The CAVEclient with which to interact.\n    verbose :       bool\n                    Whether to print results of search.\n    allow_multiple : bool\n                    If True, will track if IDs can be found spread across multiple\n                    materialization versions if there is no single one containing\n                    all.\n    raise_missing : bool\n                    Only relevant if `allow_multiple=True`. If False, will return\n                    versions even if some IDs could not be found.\n\n    Returns\n    -------\n    version :       int | \"live\"\n                    A single version (including \"live\") that contains all given\n                    root IDs.\n    versions :      np.ndarray\n                    If no single version was found and `allow_multiple=True` will\n                    return a vector of `len(ids)` with the latest version at which\n                    the respective ID can be found.\n                    Important: \"live\" version will be return as -1!\n                    If `raise_missing=False` and one or more root IDs could not\n                    be found in any of the available materialization versions\n                    these IDs will be return as version 0.\n\n    \"\"\"\n    ids = utils.make_iterable(ids)\n\n    # For each ID track the most recent valid version\n    latest_valid = np.zeros(len(ids), dtype=np.int32)\n\n    # Get the meta data for the available materialization versions\n    # This is a list of dicts where each dict has a \"time_stamp\" key\n    vmeta = client.materialize.get_versions_metadata()\n\n    # Sort by \"time_stamp\"\n    vmeta = sorted(vmeta, key=lambda x: x[\"time_stamp\"], reverse=True)\n\n    # Go over each version (start with the most recent)\n    for i, mat in enumerate(vmeta):\n        ts_m = mat[\"time_stamp\"]\n        version = mat[\"version\"]\n\n        # Check which root IDs were valid at the time\n        is_valid = client.chunkedgraph.is_latest_roots(ids, timestamp=ts_m)\n\n        # Update latest valid versions\n        latest_valid[(latest_valid == 0) &amp; is_valid] = version\n\n        if all(is_valid):\n            if verbose and not SILENCE_FIND_MAT_VERSION:\n                print(f\"Using materialization version {version}.\")\n            return version\n\n    # If no single materialized version can be found, see if we can get\n    # by with the live materialization\n    is_latest = client.chunkedgraph.is_latest_roots(ids, timestamp=None)\n    latest_valid[(latest_valid == 0) &amp; is_latest] = -1  # track \"live\" as -1\n    if all(is_latest) and dataset != \"public\":  # public does not have live\n        if verbose:\n            print(\"Using live materialization\")\n        return \"live\"\n\n    if allow_multiple and any(latest_valid != 0):\n        if all(latest_valid != 0):\n            if verbose and not SILENCE_FIND_MAT_VERSION:\n                print(\n                    f\"Found root IDs spread across {len(np.unique(latest_valid))} \"\n                    \"materialization versions.\"\n                )\n            return latest_valid\n\n        msg = (\n            f\"Found root IDs spread across {len(np.unique(latest_valid)) - 1} \"\n            f\"materialization versions but {(latest_valid == 0).sum()} IDs \"\n            \"do not exist in any of the materialized tables.\"\n        )\n\n        if not raise_missing:\n            if verbose and not SILENCE_FIND_MAT_VERSION:\n                print(msg)\n            return latest_valid\n        else:\n            raise MaterializationMatchError(msg)\n\n    if dataset not in (\"public, \"):\n        raise MaterializationMatchError(\n            \"Given root IDs do not (co-)exist in any of the available \"\n            \"materialization versions (including live). Try updating \"\n            \"root IDs and rerun your query.\"\n        )\n    else:\n        raise MaterializationMatchError(\n            \"Given root IDs do not (co-)exist in any of the available \"\n            \"public materialization versions. Please make sure that \"\n            \"the root IDs do exist and rerun your query.\"\n        )\n</code></pre>"},{"location":"reference/navis/interfaces/h01/","title":"h01","text":""},{"location":"reference/navis/interfaces/h01/#navis.interfaces.h01.fetch_neurons","title":"<code>navis.interfaces.h01.fetch_neurons</code>","text":"<p>Fetch neuron meshes.</p> Notes <p>Synapses will be attached to the closest vertex on the mesh.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Segment ID(s). Multiple IDs can be provided as list-like.\n</code></pre> <p> TYPE: <code>            str | int | list-like</code> </p> <code>lod</code> <pre><code>        Level of detail. Higher ``lod`` = coarser. This parameter\n        is ignored if the data source does not support multi-level\n        meshes.\n</code></pre> <p> TYPE: <code>          int</code> DEFAULT: <code>2</code> </p> <code>with_synapses</code> <pre><code>        If True will also attach synapses as ``.connectors``.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>datastack</code> <pre><code>        Datastack to use. Default to \"h01_c3_flat\".\n</code></pre> <p> TYPE: <code>    str</code> DEFAULT: <code>DATASTACK</code> </p> <code>materialization</code> <pre><code>        Which materialization version to use to look up somas and synapses\n        (if applicable). If \"auto\" (default) will try to find the most\n        recent version that contains the given root IDs. If an\n        integer is provided will use that version.\n</code></pre> <p> TYPE: <code>auto | int</code> DEFAULT: <code>'auto'</code> </p> <code>parallel</code> <pre><code>        If True, will use parallel threads to fetch data.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> <code>max_threads</code> <pre><code>        Max number of parallel threads to use.\n</code></pre> <p> TYPE: <code>  int</code> DEFAULT: <code>4</code> </p> <code>**kwargs</code> <pre><code>        Keyword arguments are passed through to the initialization\n        of the ``navis.MeshNeurons``.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>navis.Neuronlist</code> <p>Containing :class:<code>navis.MeshNeuron</code>.</p> Source code in <code>navis/interfaces/h01.py</code> <pre><code>def fetch_neurons(\n    x,\n    *,\n    lod=2,\n    with_synapses=True,\n    datastack=DATASTACK,\n    materialization=\"auto\",\n    parallel=True,\n    max_threads=4,\n    **kwargs,\n):\n    \"\"\"Fetch neuron meshes.\n\n    Notes\n    -----\n    Synapses will be attached to the closest vertex on the mesh.\n\n    Parameters\n    ----------\n    x :             str | int | list-like\n                    Segment ID(s). Multiple IDs can be provided as list-like.\n    lod :           int\n                    Level of detail. Higher ``lod`` = coarser. This parameter\n                    is ignored if the data source does not support multi-level\n                    meshes.\n    with_synapses : bool, optional\n                    If True will also attach synapses as ``.connectors``.\n    datastack :     str\n                    Datastack to use. Default to \"h01_c3_flat\".\n    materialization : \"auto\" | int\n                    Which materialization version to use to look up somas and synapses\n                    (if applicable). If \"auto\" (default) will try to find the most\n                    recent version that contains the given root IDs. If an\n                    integer is provided will use that version.\n    parallel :      bool\n                    If True, will use parallel threads to fetch data.\n    max_threads :   int\n                    Max number of parallel threads to use.\n    **kwargs\n                    Keyword arguments are passed through to the initialization\n                    of the ``navis.MeshNeurons``.\n\n    Returns\n    -------\n    navis.Neuronlist\n                    Containing :class:`navis.MeshNeuron`.\n\n    \"\"\"\n    client = get_cave_client(datastack)\n    return cave_utils.fetch_neurons(\n        x,\n        lod=lod,\n        with_synapses=with_synapses,\n        client=client,\n        parallel=parallel,\n        max_threads=max_threads,\n        materialization=materialization,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/navis/interfaces/h01/#navis.interfaces.h01.get_cave_client","title":"<code>navis.interfaces.h01.get_cave_client</code>","text":"<p>Get caveclient for H01 dataset.</p> Source code in <code>navis/interfaces/h01.py</code> <pre><code>def get_cave_client(datastack=DATASTACK):\n    \"\"\"Get caveclient for H01 dataset.\"\"\"\n    client = cave_utils.get_cave_client(datastack, SERVER_ADDRESS)\n    client.materialize.nucleus_table = NUCLEUS_TABLE\n    return client\n</code></pre>"},{"location":"reference/navis/interfaces/h01/#navis.interfaces.h01.get_voxels","title":"<code>navis.interfaces.h01.get_voxels</code>","text":"<p>Fetch voxels making a up given root ID.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        A single root ID.\n</code></pre> <p> TYPE: <code>            int</code> </p> <code>mip</code> <pre><code>        Scale at which to fetch voxels.\n</code></pre> <p> TYPE: <code>          int</code> DEFAULT: <code>0</code> </p> <code>bounds</code> <pre><code>        Bounding box [xmin, xmax, ymin, ymax, zmin, zmax] in voxel\n        space. For example, the voxel resolution for mip 0\n        segmentation is 8 x 8 x 40 nm.\n</code></pre> <p> TYPE: <code>       list</code> DEFAULT: <code>None</code> </p> <code>datastack</code> <pre><code>        DATASTACK.\n</code></pre> <p> TYPE: <code>    str</code> DEFAULT: <code>DATASTACK</code> </p> RETURNS DESCRIPTION <code>voxels</code> <p>In voxel space according to <code>mip</code>.</p> <p> TYPE: <code>(N, 3) np.ndarray</code> </p> Source code in <code>navis/interfaces/h01.py</code> <pre><code>def get_voxels(x, mip=0, bounds=None, datastack=DATASTACK):\n    \"\"\"Fetch voxels making a up given root ID.\n\n    Parameters\n    ----------\n    x :             int\n                    A single root ID.\n    mip :           int\n                    Scale at which to fetch voxels.\n    bounds :        list, optional\n                    Bounding box [xmin, xmax, ymin, ymax, zmin, zmax] in voxel\n                    space. For example, the voxel resolution for mip 0\n                    segmentation is 8 x 8 x 40 nm.\n    datastack :     str\n                    DATASTACK.\n\n    Returns\n    -------\n    voxels :        (N, 3) np.ndarray\n                    In voxel space according to `mip`.\n\n    \"\"\"\n    return cave_utils.get_voxels(x, mip=mip, bounds=bounds, client=get_cave_client(datastack))\n</code></pre>"},{"location":"reference/navis/interfaces/insectbrain_db/","title":"insectbrain_db","text":""},{"location":"reference/navis/interfaces/insectbrain_db/#navis.interfaces.insectbrain_db.Session","title":"<code>navis.interfaces.insectbrain_db.Session</code>","text":"<p>Client to manage Insect Brain DB session.</p> PARAMETER DESCRIPTION <code>token</code> <pre><code>        API token. See `authenticate()`.\n</code></pre> <p> TYPE: <code>        str</code> DEFAULT: <code>None</code> </p> <code>created_at</code> <pre><code>        Time and date the token was generated. Iso-formatted.\n</code></pre> <p> TYPE: <code>   str</code> DEFAULT: <code>None</code> </p> Source code in <code>navis/interfaces/insectbrain_db.py</code> <pre><code>class Session:\n    \"\"\"Client to manage Insect Brain DB session.\n\n    Parameters\n    ----------\n    token :         str\n                    API token. See `authenticate()`.\n    created_at :    str\n                    Time and date the token was generated. Iso-formatted.\n\n    \"\"\"\n\n    def __init__(self, username=None, password=None, token=None, created_at=None):\n        self._session = requests.Session()\n\n        self.username = username\n        self.password = password\n\n        self.token = token\n        self.created_at = created_at\n\n    @property\n    def token_expired(self):\n        \"\"\"Check if token is expired.\"\"\"\n        if self._token_created_at:\n            now = dt.datetime.now()\n            expires_in = dt.timedelta(days=1)\n            if now - self._token_created_at &gt;= expires_in:\n                return True\n        return False\n\n    @property\n    def token(self):\n        return self._token\n\n    @token.setter\n    def token(self, token):\n        if token and not token.startswith('Token'):\n            token = f'Token {token}'\n        self._token = token\n        self._session.headers['Authorization'] = token\n\n    @property\n    def token_created_at(self):\n        return self._token_created_at\n\n    @token_created_at.setter\n    def token_created_at(self, value):\n        if value:\n            self._token_created_at = dt.datetime.fromisoformat(value[:-1])\n        else:\n            self._token_created_at = None\n\n    def fetch_token(self):\n        \"\"\"Fetch fresh token.\"\"\"\n        username = self.username\n        if not username:\n            username = os.environ.get('INSECT_BRAIN_DB_USER', None)\n        password = self.password\n        if not password:\n            password = os.environ.get('INSECT_BRAIN_DB_PASSWORD', None)\n\n        if not username or not password:\n            msg = \"\"\"\\\n            You must provide username + password, or an API token. Please see\n            `navis.interfaces.insectbrian_db.authenticate()` for details.\n            \"\"\"\n            raise ValueError(msg)\n\n        creds = {'username': username, 'password': password}\n\n        # Note: do NOT remove the trailing '/' here\n        url = make_url(baseurl, 'api', 'v2', 'token/')\n\n        resp = requests.post(url, data=creds)\n        resp.raise_for_status()\n\n        global session\n        self.token = resp.json()['token']\n        self.token_created_at = resp.json()['created']\n\n        logger.info('Successfully retrieved 24h Insect Brain DB API token!')\n\n    def preflight(self):\n        \"\"\"Check if we're ready to make requests.\"\"\"\n        if self.token and self.token_expired:\n            self.fetch_token()\n\n    def get(self, *args, **kwargs):\n        \"\"\"Make GET request.\"\"\"\n        self.preflight()\n\n        r = self._session.get(*args, **kwargs)\n        r.raise_for_status()\n\n        return r.json()\n\n    def post(self, *args, **kwargs):\n        \"\"\"Make POST request.\"\"\"\n        self.preflight()\n\n        r = self._session.post(*args, **kwargs)\n        r.raise_for_status()\n\n        return r.json()\n</code></pre>"},{"location":"reference/navis/interfaces/insectbrain_db/#navis.interfaces.insectbrain_db.Session.token_expired","title":"<code>token_expired</code>  <code>property</code>","text":"<p>Check if token is expired.</p>"},{"location":"reference/navis/interfaces/insectbrain_db/#navis.interfaces.insectbrain_db.Session.fetch_token","title":"<code>fetch_token</code>","text":"<p>Fetch fresh token.</p> Source code in <code>navis/interfaces/insectbrain_db.py</code> <pre><code>def fetch_token(self):\n    \"\"\"Fetch fresh token.\"\"\"\n    username = self.username\n    if not username:\n        username = os.environ.get('INSECT_BRAIN_DB_USER', None)\n    password = self.password\n    if not password:\n        password = os.environ.get('INSECT_BRAIN_DB_PASSWORD', None)\n\n    if not username or not password:\n        msg = \"\"\"\\\n        You must provide username + password, or an API token. Please see\n        `navis.interfaces.insectbrian_db.authenticate()` for details.\n        \"\"\"\n        raise ValueError(msg)\n\n    creds = {'username': username, 'password': password}\n\n    # Note: do NOT remove the trailing '/' here\n    url = make_url(baseurl, 'api', 'v2', 'token/')\n\n    resp = requests.post(url, data=creds)\n    resp.raise_for_status()\n\n    global session\n    self.token = resp.json()['token']\n    self.token_created_at = resp.json()['created']\n\n    logger.info('Successfully retrieved 24h Insect Brain DB API token!')\n</code></pre>"},{"location":"reference/navis/interfaces/insectbrain_db/#navis.interfaces.insectbrain_db.Session.get","title":"<code>get</code>","text":"<p>Make GET request.</p> Source code in <code>navis/interfaces/insectbrain_db.py</code> <pre><code>def get(self, *args, **kwargs):\n    \"\"\"Make GET request.\"\"\"\n    self.preflight()\n\n    r = self._session.get(*args, **kwargs)\n    r.raise_for_status()\n\n    return r.json()\n</code></pre>"},{"location":"reference/navis/interfaces/insectbrain_db/#navis.interfaces.insectbrain_db.Session.post","title":"<code>post</code>","text":"<p>Make POST request.</p> Source code in <code>navis/interfaces/insectbrain_db.py</code> <pre><code>def post(self, *args, **kwargs):\n    \"\"\"Make POST request.\"\"\"\n    self.preflight()\n\n    r = self._session.post(*args, **kwargs)\n    r.raise_for_status()\n\n    return r.json()\n</code></pre>"},{"location":"reference/navis/interfaces/insectbrain_db/#navis.interfaces.insectbrain_db.Session.preflight","title":"<code>preflight</code>","text":"<p>Check if we're ready to make requests.</p> Source code in <code>navis/interfaces/insectbrain_db.py</code> <pre><code>def preflight(self):\n    \"\"\"Check if we're ready to make requests.\"\"\"\n    if self.token and self.token_expired:\n        self.fetch_token()\n</code></pre>"},{"location":"reference/navis/interfaces/insectbrain_db/#navis.interfaces.insectbrain_db.authenticate","title":"<code>navis.interfaces.insectbrain_db.authenticate</code>","text":"<p>Authenticate against Insect Brain DB.</p> <p>You can either provide username + password, or a token. Each token is only valid for 24h though. The better alternative is to provide your username + password as environment variables: <code>INSECT_BRAIN_DB_USER</code> and <code>INSECT_BRAIN_DB_PASSWORD</code>, respectively. If you are using these environment you don't need to bother with <code>authenticate()</code> at all.</p> PARAMETER DESCRIPTION <code>username</code> <pre><code>        Your username on Insect Brain DB.\n</code></pre> <p> TYPE: <code>     str</code> DEFAULT: <code>None</code> </p> <code>password</code> <pre><code>        Your password on Insect Brain DB.\n</code></pre> <p> TYPE: <code>     str</code> DEFAULT: <code>None</code> </p> <code>token</code> <pre><code>        A token. If provided you don't need to provide username +\n        password.\n</code></pre> <p> TYPE: <code>        str</code> DEFAULT: <code>None</code> </p> Source code in <code>navis/interfaces/insectbrain_db.py</code> <pre><code>def authenticate(username=None, password=None, token=None):\n    \"\"\"Authenticate against Insect Brain DB.\n\n    You can either provide username + password, or a token. Each token is only\n    valid for 24h though. The better alternative is to provide your\n    username + password as environment variables: `INSECT_BRAIN_DB_USER` and\n    `INSECT_BRAIN_DB_PASSWORD`, respectively. If you are using these environment\n    you don't need to bother with `authenticate()` at all.\n\n    Parameters\n    ----------\n    username :      str, optional\n                    Your username on Insect Brain DB.\n    password :      str, optional\n                    Your password on Insect Brain DB.\n    token :         str, optional\n                    A token. If provided you don't need to provide username +\n                    password.\n\n    \"\"\"\n    if not token and (not username and not password):\n        raise ValueError('Must provide either username + password, or token '\n                         '(or both).')\n\n    if username:\n        session.username = username\n    if password:\n        session.password = password\n\n    if token:\n        session.token = token\n    else:\n        session.fetch_token()\n</code></pre>"},{"location":"reference/navis/interfaces/insectbrain_db/#navis.interfaces.insectbrain_db.get_brain_meshes","title":"<code>navis.interfaces.insectbrain_db.get_brain_meshes</code>","text":"<p>Fetch brain meshes for given species.</p> PARAMETER DESCRIPTION <code>species</code> <pre><code>        Species for which to fetch brain volumes. Strings are\n        interpreted as names (scientific or common), integers as IDs.\n</code></pre> <p> TYPE: <code>Union[str, int]</code> </p> <code>combine</code> <pre><code>        If True, will combine subvolumes (i.e. neuropils) into\n        a single navis.Volume - else will return list with volumes.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> <code>max_threads</code> <pre><code>        Number of parallel threads to use for fetching meshes.\n</code></pre> <p> TYPE: <code>  int</code> DEFAULT: <code>4</code> </p> RETURNS DESCRIPTION <code>list of navis.Volume</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; import navis.interfaces.insectbrain_db as ibdb\n&gt;&gt;&gt; v = ibdb.get_brain_meshes('Desert Locust', combine_vols=True)\n&gt;&gt;&gt; navis.plot3d(v)\n</code></pre> Source code in <code>navis/interfaces/insectbrain_db.py</code> <pre><code>def get_brain_meshes(species: Union[str, int],\n                     combine: bool = False,\n                     max_threads: int = 4\n                     ) -&gt; Optional[List[Volume]]:\n    \"\"\"Fetch brain meshes for given species.\n\n    Parameters\n    ----------\n    species:        str | int\n                    Species for which to fetch brain volumes. Strings are\n                    interpreted as names (scientific or common), integers as IDs.\n    combine :       bool, optional\n                    If True, will combine subvolumes (i.e. neuropils) into\n                    a single navis.Volume - else will return list with volumes.\n    max_threads :   int\n                    Number of parallel threads to use for fetching meshes.\n\n    Returns\n    -------\n    list of navis.Volume\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; import navis.interfaces.insectbrain_db as ibdb\n    &gt;&gt;&gt; v = ibdb.get_brain_meshes('Desert Locust', combine_vols=True)\n    &gt;&gt;&gt; navis.plot3d(v)\n\n    \"\"\"\n    # Get info with all available neuropils\n    sp_info = get_species_info(species)\n\n    # Go over all brains\n    n_brains = len(sp_info.reconstructions)  # type: ignore\n    n_reconstr = len([r for r in sp_info.reconstructions if r.get('viewer_files')])  # type: ignore\n    logger.info(f'{n_reconstr} reconstruction(s) from {n_brains} brain(s) found')\n\n    volumes: List[Volume] = []\n    for brain in config.tqdm(sp_info.reconstructions,\n                             disable=config.pbar_hide,\n                             leave=config.pbar_leave,\n                             desc='Brains'):  # type: ignore\n        this_v = []\n        # If no reconstructions, continue\n        if not brain.get('viewer_files'):  # type: ignore\n            continue\n\n        with ThreadPoolExecutor(max_workers=max_threads) as executor:\n            futures = {}\n            for file in brain['viewer_files']:\n                # If no file UUID, continue\n                if not file['p_file']['uuid']:\n                    continue\n                filename = file['p_file']['file_name']\n                f = executor.submit(_get_neuropil_mesh, file,)\n                futures[f] = filename\n\n            with config.tqdm(desc='Fetching',\n                            total=len(futures),\n                            leave=config.pbar_leave,\n                            disable=len(futures) == 1 or config.pbar_hide) as pbar:\n                for f in as_completed(futures):\n                    name = futures[f]\n                    pbar.update(1)\n                    try:\n                        this_v.append(f.result())\n                    except Exception as exc:\n                        print(f'{name} generated an exception:', exc)\n\n        # Combine all volumes in this brain\n        if combine:\n            this_v = [Volume.combine(this_v)]\n            this_v[0].color = (.85, .85, .85, .5)\n            this_v[0].name = sp_info.scientific_name\n\n        volumes += this_v\n\n    return volumes\n</code></pre>"},{"location":"reference/navis/interfaces/insectbrain_db/#navis.interfaces.insectbrain_db.get_meshes_experiment","title":"<code>navis.interfaces.insectbrain_db.get_meshes_experiment</code>","text":"<p>Fetch volumes associated with given experiment.</p> PARAMETER DESCRIPTION <code>id</code> <pre><code>The experiment ID. See e.g. `list_datasets`.\n</code></pre> <p> TYPE: <code>   int</code> </p> RETURNS DESCRIPTION <code>list</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis.interfaces.insectbrain_db as ibdb\n&gt;&gt;&gt; vols = ibdb.get_meshes_experiment(61)\n</code></pre> Source code in <code>navis/interfaces/insectbrain_db.py</code> <pre><code>def get_meshes_experiment(id) -&gt; 'NeuronList':\n    \"\"\"Fetch volumes associated with given experiment.\n\n    Parameters\n    ----------\n    id :    int\n            The experiment ID. See e.g. `list_datasets`.\n\n    Returns\n    -------\n    list\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis.interfaces.insectbrain_db as ibdb\n    &gt;&gt;&gt; vols = ibdb.get_meshes_experiment(61)\n\n    \"\"\"\n    # Make sure ID is integer\n    id = int(id)\n\n    # Get files associated with experiment\n    files = list_experiment_files(id)\n\n    # Figure out which files are skeletons\n    me_files = files[files.file_name.str.endswith('.glb')]\n\n    if me_files.empty:\n        raise ValueError('Did not find any meshes associated with '\n                         f'experiment {id}')\n\n    volumes = []\n    for f in config.tqdm(me_files.itertuples(),\n                         desc='Downloading',\n                         total=me_files.shape[0]):\n        # Load the file\n        r = requests.get(f.url)\n        r.raise_for_status()\n\n        name = '.'.join(f.file_name.split('.')[:-1])\n        ext = f.file_name.split('.')[-1]\n\n        file = io.BytesIO(r.content)\n        scene = tm.load(file, file_type=ext)\n\n        for obj in scene.geometry.values():\n            v = Volume(obj.vertices, obj.faces, name=name)\n            volumes.append(v)\n\n    logger.info(f'Done! Found {len(volumes)} meshes.')\n\n    return volumes\n</code></pre>"},{"location":"reference/navis/interfaces/insectbrain_db/#navis.interfaces.insectbrain_db.get_skeletons","title":"<code>navis.interfaces.insectbrain_db.get_skeletons</code>","text":"<p>Fetch skeletons for given neuron(s).</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Name(s) or ID(s) of neurons you want to fetch.\n</code></pre> <p> TYPE: <code>            str | int | list thereof</code> </p> <code>max_threads</code> <pre><code>        Number of parallel threads to use for fetching skeletons.\n</code></pre> <p> TYPE: <code>  int</code> DEFAULT: <code>4</code> </p> RETURNS DESCRIPTION <code>navis.NeuronList</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis.interfaces.insectbrain_db as ibdb\n&gt;&gt;&gt; neurons = ibdb.get_skeletons('TUps2-2')\n</code></pre> Source code in <code>navis/interfaces/insectbrain_db.py</code> <pre><code>def get_skeletons(x, max_threads=4):\n    \"\"\"Fetch skeletons for given neuron(s).\n\n    Parameters\n    ----------\n    x :             str | int | list thereof\n                    Name(s) or ID(s) of neurons you want to fetch.\n    max_threads :   int\n                    Number of parallel threads to use for fetching skeletons.\n\n    Returns\n    -------\n    navis.NeuronList\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis.interfaces.insectbrain_db as ibdb\n    &gt;&gt;&gt; neurons = ibdb.get_skeletons('TUps2-2')\n\n    \"\"\"\n    if isinstance(x, (int, str, np.int32, np.int64)):\n        neurons = [x]\n    else:\n        neurons = x\n\n    # First fetch URLs for all neurons\n    meta = []\n    for x in neurons:\n        if isinstance(x, str):\n            q = search_neurons(name=x, partial_match=False)\n            if q.empty:\n                raise ValueError(f'No neuron with name \"{x}\" found')\n            ids = q.id.values\n        else:\n            ids = x\n\n        for i, id in enumerate(make_iterable(ids)):\n            url = make_url(baseurl, 'api', 'v2', 'neuron', 'reconstruction',\n                           neuron=id)\n            info = session.get(url)\n\n            if (not info\n                or 'viewer_files' not in info[0]\n                or not info[0]['viewer_files']):\n                raise ValueError(f'Neuron {x} ({id}) has no skeleton.')\n\n            meta.append(info[0])\n\n    return _get_skeletons(meta, max_threads=max_threads)\n</code></pre>"},{"location":"reference/navis/interfaces/insectbrain_db/#navis.interfaces.insectbrain_db.get_skeletons_experiment","title":"<code>navis.interfaces.insectbrain_db.get_skeletons_experiment</code>","text":"<p>Fetch all skeletons for given experiment.</p> PARAMETER DESCRIPTION <code>id</code> <pre><code>The experiment ID. See e.g. `list_datasets`.\n</code></pre> <p> TYPE: <code>   int</code> </p> RETURNS DESCRIPTION <code>NeuronList</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis.interfaces.insectbrain_db as ibdb\n&gt;&gt;&gt; nl = ibdb.get_skeletons_experiment(61)\n</code></pre> Source code in <code>navis/interfaces/insectbrain_db.py</code> <pre><code>def get_skeletons_experiment(id) -&gt; 'NeuronList':\n    \"\"\"Fetch all skeletons for given experiment.\n\n    Parameters\n    ----------\n    id :    int\n            The experiment ID. See e.g. `list_datasets`.\n\n    Returns\n    -------\n    NeuronList\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis.interfaces.insectbrain_db as ibdb\n    &gt;&gt;&gt; nl = ibdb.get_skeletons_experiment(61)\n\n    \"\"\"\n    # Make sure ID is integer\n    id = int(id)\n\n    # Get files associated with experiment\n    files = list_experiment_files(id)\n\n    # Figure out which files are skeletons\n    sk_files = files[files.file_name.str.contains('skeleton') | files.file_name.str.endswith('.gz')]\n\n    if sk_files.empty:\n        raise ValueError('Did not find any skeleton files associated with '\n                         f'experiment {id}')\n\n    skeletons = []\n    for f in sk_files.itertuples():\n        logger.info(f'Downloading {f.file_name}')\n        # Load the file\n        r = requests.get(f.url)\n        r.raise_for_status()\n\n        # Files appear to be json-formatted and not compressed\n        data = r.json()\n\n        for i, neuron in enumerate(data['data']):\n            for sk in neuron['skeletons']:\n                # Load SWC table\n                swc = pd.DataFrame(sk['data'],\n                                   columns=['node_id', 'skeleton_id',\n                                            'x', 'y', 'z', 'radius',\n                                            'parent_id'])\n                # Some cleaning up\n                swc.drop('skeleton_id', axis=1, inplace=True)\n                swc['parent_id'] = swc.parent_id.fillna(-1).astype(int)\n                # Create neuron\n                tn = TreeNeuron(swc,\n                                id=sk.get('id', 1),\n                                name=neuron.get('name', 'NA'),\n                                annotations=neuron.get('annotations', []),\n                                soma=None)\n                skeletons.append(tn)\n    logger.info(f'Done! Found {len(skeletons)} skeletons.')\n\n    return NeuronList(skeletons)\n</code></pre>"},{"location":"reference/navis/interfaces/insectbrain_db/#navis.interfaces.insectbrain_db.get_skeletons_species","title":"<code>navis.interfaces.insectbrain_db.get_skeletons_species</code>","text":"<p>Fetch all skeletons for given species.</p> <p>Note that some neurons might have multiple reconstructions. They will show up with the same ID with different names.</p> PARAMETER DESCRIPTION <code>species</code> <pre><code>        Name or ID of a species to fetch skeletons for.\n</code></pre> <p> TYPE: <code>      str | int</code> </p> <code>max_threads</code> <pre><code>        Number of parallel threads to use for fetching skeletons.\n</code></pre> <p> TYPE: <code>  int</code> DEFAULT: <code>4</code> </p> RETURNS DESCRIPTION <code>navis.NeuronList</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis.interfaces.insectbrain_db as ibdb\n&gt;&gt;&gt; neurons = ibdb.get_skeletons_species('Desert Locust')\n</code></pre> Source code in <code>navis/interfaces/insectbrain_db.py</code> <pre><code>def get_skeletons_species(species, max_threads=4):\n    \"\"\"Fetch all skeletons for given species.\n\n    Note that some neurons might have multiple reconstructions. They will\n    show up with the same ID with different names.\n\n    Parameters\n    ----------\n    species :       str | int\n                    Name or ID of a species to fetch skeletons for.\n    max_threads :   int\n                    Number of parallel threads to use for fetching skeletons.\n\n    Returns\n    -------\n    navis.NeuronList\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis.interfaces.insectbrain_db as ibdb\n    &gt;&gt;&gt; neurons = ibdb.get_skeletons_species('Desert Locust')\n\n    \"\"\"\n    if isinstance(species, str):\n        species = _get_species_id(species)\n\n    # First fetch URLs for all neurons\n    url = make_url(baseurl, 'api', 'v2', 'neuron', 'reconstruction',\n                   neuron__species=species)\n    meta = session.get(url)\n\n    meta = [e for e in meta if e['viewer_files']]\n\n    return _get_skeletons(meta, max_threads=max_threads)\n</code></pre>"},{"location":"reference/navis/interfaces/insectbrain_db/#navis.interfaces.insectbrain_db.list_datasets","title":"<code>navis.interfaces.insectbrain_db.list_datasets</code>","text":"<p>List publication datasets and associated experiments.</p> RETURNS DESCRIPTION <code>pandas.DataFrame</code> <p>DataFrame with available datasets.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis.interfaces.insectbrain_db as ibdb\n&gt;&gt;&gt; datasets = ibdb.list_datasets()\n</code></pre> Source code in <code>navis/interfaces/insectbrain_db.py</code> <pre><code>def list_datasets() -&gt; pd.DataFrame:\n    \"\"\"List publication datasets and associated experiments.\n\n    Returns\n    -------\n    pandas.DataFrame\n            DataFrame with available datasets.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis.interfaces.insectbrain_db as ibdb\n    &gt;&gt;&gt; datasets = ibdb.list_datasets()\n\n    \"\"\"\n    url = make_url(baseurl, 'api', 'publications', 'experiments?offset=0&amp;limit=500')\n\n    return _sort_columns(pd.DataFrame.from_records(session.get(url)['results']))\n</code></pre>"},{"location":"reference/navis/interfaces/insectbrain_db/#navis.interfaces.insectbrain_db.list_experiment_files","title":"<code>navis.interfaces.insectbrain_db.list_experiment_files</code>","text":"<p>List files associated with given experiment.</p> PARAMETER DESCRIPTION <code>id</code> <pre><code>The experiment ID. See e.g. `list_datasets`.\n</code></pre> <p> TYPE: <code>   int</code> </p> RETURNS DESCRIPTION <code>pandas.DataFrame</code> <p>DataFrame with files.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis.interfaces.insectbrain_db as ibdb\n&gt;&gt;&gt; files = ibdb.list_experiment_files(61)\n</code></pre> Source code in <code>navis/interfaces/insectbrain_db.py</code> <pre><code>def list_experiment_files(id) -&gt; pd.DataFrame:\n    \"\"\"List files associated with given experiment.\n\n    Parameters\n    ----------\n    id :    int\n            The experiment ID. See e.g. `list_datasets`.\n\n    Returns\n    -------\n    pandas.DataFrame\n            DataFrame with files.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis.interfaces.insectbrain_db as ibdb\n    &gt;&gt;&gt; files = ibdb.list_experiment_files(61)\n\n    \"\"\"\n    url = make_url(baseurl, 'api', 'v2', 'experiment', id, 'file')\n\n    return _sort_columns(pd.DataFrame.from_records(session.get(url)))\n</code></pre>"},{"location":"reference/navis/interfaces/insectbrain_db/#navis.interfaces.insectbrain_db.search_neurons","title":"<code>navis.interfaces.insectbrain_db.search_neurons</code>","text":"<p>Search for neurons matching given parameters.</p> PARAMETER DESCRIPTION <code>name</code> <pre><code>        Name of the neuron.\n</code></pre> <p> TYPE: <code>         str</code> DEFAULT: <code>None</code> </p> <code>short_name</code> <pre><code>        Short name of the neuron.\n</code></pre> <p> TYPE: <code>   str</code> DEFAULT: <code>None</code> </p> <code>species</code> <pre><code>        Name or ID of the species. Can be common or scientific name.\n</code></pre> <p> TYPE: <code>      str | int</code> DEFAULT: <code>None</code> </p> <code>sex</code> <pre><code>        Sex of the neuron.\n</code></pre> <p> TYPE: <code>          \"FEMALE\" | \"MALE\" | \"UNKNOWN\"</code> DEFAULT: <code>None</code> </p> <code>arborization</code> <pre><code>        Restrict to neurons having arborizations in given neuropil.\n</code></pre> <p> TYPE: <code> str</code> DEFAULT: <code>None</code> </p> <code>partial_match</code> <pre><code>        Whether to allow partial matches (does not apply for species).\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>pandas.DataFrame</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis.interfaces.insectbrain_db as ibdb\n&gt;&gt;&gt; neurons = ibdb.search_neurons(species='Desert Locust')\n</code></pre> Source code in <code>navis/interfaces/insectbrain_db.py</code> <pre><code>def search_neurons(name=None, short_name=None, species=None, sex=None,\n                   arborization=None, partial_match=True) -&gt; pd.DataFrame:\n    \"\"\"Search for neurons matching given parameters.\n\n    Parameters\n    ----------\n    name :          str, optional\n                    Name of the neuron.\n    short_name :    str, optional\n                    Short name of the neuron.\n    species :       str | int, optional\n                    Name or ID of the species. Can be common or scientific name.\n    sex :           \"FEMALE\" | \"MALE\" | \"UNKNOWN\", optional\n                    Sex of the neuron.\n    arborization :  str, optional\n                    Restrict to neurons having arborizations in given neuropil.\n    partial_match : bool\n                    Whether to allow partial matches (does not apply for species).\n\n    Returns\n    -------\n    pandas.DataFrame\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis.interfaces.insectbrain_db as ibdb\n    &gt;&gt;&gt; neurons = ibdb.search_neurons(species='Desert Locust')\n\n    \"\"\"\n    # Construct query\n    options = {}\n    if species:\n        if not isinstance(species, int):\n            species = _get_species_id(species)\n        options['species'] = species\n\n    for key, value in zip(['name', 'short_name', 'sex',\n                           'arborization_region__structure'],\n                          [name, short_name, sex, arborization]):\n        if not value:\n            continue\n        if partial_match:\n            key += '__icontains'\n        options[key] = value\n\n    url = make_url(baseurl, 'api', 'v2', 'neuron', **options)\n\n    resp = requests.get(url)\n\n    resp.raise_for_status()\n\n    return _sort_columns(pd.DataFrame.from_records(resp.json()))\n</code></pre>"},{"location":"reference/navis/interfaces/microns/","title":"microns","text":""},{"location":"reference/navis/interfaces/microns/#navis.interfaces.microns.fetch_neurons","title":"<code>navis.interfaces.microns.fetch_neurons</code>","text":"<p>Fetch neuron meshes.</p> Notes <p>Synapses will be attached to the closest vertex on the mesh.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Segment ID(s). Multiple Ids can be provided as list-like.\n</code></pre> <p> TYPE: <code>            str | int | list-like</code> </p> <code>lod</code> <pre><code>        Level of detail. Higher ``lod`` = coarser. This parameter\n        is ignored if the data source does not support multi-level\n        meshes.\n</code></pre> <p> TYPE: <code>          int</code> DEFAULT: <code>2</code> </p> <code>with_synapses</code> <pre><code>        If True will also attach synapses as ``.connectors``.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>datastack</code> <pre><code>        Which dataset to query. \"cortex65\", \"cortex35\" and \"layer 2/3\"\n        are internally mapped to the corresponding sources: for example,\n        \"minnie65_public\" for \"cortex65\"\n</code></pre> <p> TYPE: <code>    \"cortex65\" | \"cortex35\" | \"layer 2/3\" | str</code> DEFAULT: <code>'cortex65'</code> </p> <code>materialization</code> <pre><code>        Which materialization version to use to look up somas and synapses\n        (if applicable). If \"auto\" (default) will try to find the most\n        recent version that contains the given root IDs. If an\n        integer is provided will use that version.\n</code></pre> <p> TYPE: <code>auto | int</code> DEFAULT: <code>'auto'</code> </p> <code>parallel</code> <pre><code>        If True, will use parallel threads to fetch data.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> <code>max_threads</code> <pre><code>        Max number of parallel threads to use.\n</code></pre> <p> TYPE: <code>  int</code> DEFAULT: <code>4</code> </p> <code>**kwargs</code> <pre><code>        Keyword arguments are passed through to the initialization\n        of the ``navis.MeshNeurons``.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>navis.Neuronlist</code> <p>Containing :class:<code>navis.MeshNeuron</code>.</p> Source code in <code>navis/interfaces/microns.py</code> <pre><code>def fetch_neurons(\n    x,\n    *,\n    lod=2,\n    with_synapses=True,\n    datastack=\"cortex65\",\n    materialization=\"auto\",\n    parallel=True,\n    max_threads=4,\n    **kwargs,\n):\n    \"\"\"Fetch neuron meshes.\n\n    Notes\n    -----\n    Synapses will be attached to the closest vertex on the mesh.\n\n    Parameters\n    ----------\n    x :             str | int | list-like\n                    Segment ID(s). Multiple Ids can be provided as list-like.\n    lod :           int\n                    Level of detail. Higher ``lod`` = coarser. This parameter\n                    is ignored if the data source does not support multi-level\n                    meshes.\n    with_synapses : bool, optional\n                    If True will also attach synapses as ``.connectors``.\n    datastack :     \"cortex65\" | \"cortex35\" | \"layer 2/3\" | str\n                    Which dataset to query. \"cortex65\", \"cortex35\" and \"layer 2/3\"\n                    are internally mapped to the corresponding sources: for example,\n                    \"minnie65_public\" for \"cortex65\"\n    materialization : \"auto\" | int\n                    Which materialization version to use to look up somas and synapses\n                    (if applicable). If \"auto\" (default) will try to find the most\n                    recent version that contains the given root IDs. If an\n                    integer is provided will use that version.\n    parallel :      bool\n                    If True, will use parallel threads to fetch data.\n    max_threads :   int\n                    Max number of parallel threads to use.\n    **kwargs\n                    Keyword arguments are passed through to the initialization\n                    of the ``navis.MeshNeurons``.\n\n    Returns\n    -------\n    navis.Neuronlist\n                    Containing :class:`navis.MeshNeuron`.\n\n    \"\"\"\n    client = get_cave_client(_translate_datastack(datastack))\n    return cave_utils.fetch_neurons(\n        x,\n        lod=lod,\n        with_synapses=with_synapses,\n        client=client,\n        parallel=parallel,\n        max_threads=max_threads,\n        materialization=materialization,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/navis/interfaces/microns/#navis.interfaces.microns.get_cave_client","title":"<code>navis.interfaces.microns.get_cave_client</code>","text":"<p>Get caveclient for given datastack.</p> PARAMETER DESCRIPTION <code>datastack</code> <pre><code>        Which dataset to query. \"cortex65\", \"cortex35\" and \"layer 2/3\"\n        are internally mapped to the corresponding sources: for example,\n        \"minnie65_public\" for \"cortex65\"\n</code></pre> <p> TYPE: <code>    \"cortex65\" | \"cortex35\" | \"layer 2/3\" | str</code> DEFAULT: <code>'cortex65'</code> </p> Source code in <code>navis/interfaces/microns.py</code> <pre><code>def get_cave_client(datastack=\"cortex65\"):\n    \"\"\"Get caveclient for given datastack.\n\n    Parameters\n    ----------\n    datastack :     \"cortex65\" | \"cortex35\" | \"layer 2/3\" | str\n                    Which dataset to query. \"cortex65\", \"cortex35\" and \"layer 2/3\"\n                    are internally mapped to the corresponding sources: for example,\n                    \"minnie65_public\" for \"cortex65\"\n\n    \"\"\"\n    if not CAVEclient:\n        raise ModuleNotFoundError(err_msg)\n\n    # Try mapping, else pass-through\n    datastack = _translate_datastack(datastack)\n    client = cave_utils.get_cave_client(datastack)\n    client.materialize.nucleus_table = NUCLEUS_TABLE\n    return client\n</code></pre>"},{"location":"reference/navis/interfaces/microns/#navis.interfaces.microns.get_voxels","title":"<code>navis.interfaces.microns.get_voxels</code>","text":"<p>Fetch voxels making a up given root ID.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        A single root ID.\n</code></pre> <p> TYPE: <code>            int</code> </p> <code>mip</code> <pre><code>        Scale at which to fetch voxels. Lower = higher resolution.\n</code></pre> <p> TYPE: <code>          int</code> DEFAULT: <code>0</code> </p> <code>bounds</code> <pre><code>        Bounding box [xmin, xmax, ymin, ymax, zmin, zmax] in voxel\n        space. For example, the voxel resolution for mip 0\n        segmentation is 8 x 8 x 40 nm.\n</code></pre> <p> TYPE: <code>       list</code> DEFAULT: <code>None</code> </p> <code>datastack</code> <pre><code>        Which dataset to query. \"cortex65\", \"cortex35\" and \"layer 2/3\"\n        are internally mapped to the corresponding sources: for example,\n        \"minnie65_public\" for \"cortex65\"\n</code></pre> <p> TYPE: <code>    \"cortex65\" | \"cortex35\" | \"layer 2/3\" | str</code> DEFAULT: <code>'cortex65'</code> </p> RETURNS DESCRIPTION <code>voxels</code> <p>In voxel space according to <code>mip</code>.</p> <p> TYPE: <code>(N, 3) np.ndarray</code> </p> Source code in <code>navis/interfaces/microns.py</code> <pre><code>def get_voxels(x, mip=0, bounds=None, datastack=\"cortex65\"):\n    \"\"\"Fetch voxels making a up given root ID.\n\n    Parameters\n    ----------\n    x :             int\n                    A single root ID.\n    mip :           int\n                    Scale at which to fetch voxels. Lower = higher resolution.\n    bounds :        list, optional\n                    Bounding box [xmin, xmax, ymin, ymax, zmin, zmax] in voxel\n                    space. For example, the voxel resolution for mip 0\n                    segmentation is 8 x 8 x 40 nm.\n    datastack :     \"cortex65\" | \"cortex35\" | \"layer 2/3\" | str\n                    Which dataset to query. \"cortex65\", \"cortex35\" and \"layer 2/3\"\n                    are internally mapped to the corresponding sources: for example,\n                    \"minnie65_public\" for \"cortex65\"\n\n    Returns\n    -------\n    voxels :        (N, 3) np.ndarray\n                    In voxel space according to `mip`.\n\n    \"\"\"\n    return cave_utils.get_voxels(\n        x, mip=mip, bounds=bounds, client=get_cave_client(datastack)\n    )\n</code></pre>"},{"location":"reference/navis/interfaces/neuprint/","title":"neuprint","text":""},{"location":"reference/navis/interfaces/neuprint/#navis.interfaces.neuprint.fetch_mesh_neuron","title":"<code>navis.interfaces.neuprint.fetch_mesh_neuron</code>","text":"<p>Fetch neuron meshes as navis.MeshNeuron.</p> <p>Requires additional packages depending on the mesh source.</p> <p>For DVID you need <code>dvid-tools</code>:</p> <pre><code>``` shell\npip3 install dvidtools\n```\n</code></pre> <p>For everything else you need cloudvolume:</p> <pre><code>``` shell\npip3 install cloud-volume\n```\n</code></pre> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Body ID(s). Multiple IDs can be provided as list-like or\n        DataFrame with \"bodyId\" or \"bodyid\" column.\n</code></pre> <p> TYPE: <code>            str | int | list-like | pandas.DataFrame | SegmentCriteria</code> </p> <code>lod</code> <pre><code>        Level of detail. Higher `lod` = coarser. Ignored if mesh\n        source does not support LODs (e.g. for DVID).\n</code></pre> <p> TYPE: <code>          int</code> DEFAULT: <code>1</code> </p> <code>with_synapses</code> <pre><code>        If True will download and attach synapses as `.connectors`.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>missing_mesh</code> <pre><code>        What to do if no mesh is found for a given body ID:\n\n            \"raise\" (default) will raise an exception\n            \"warn\" will throw a warning but continue\n            \"skip\" will skip without any message\n</code></pre> <p> TYPE: <code> 'raise' | 'warn' | 'skip'</code> DEFAULT: <code>'raise'</code> </p> <code>parallel</code> <pre><code>        If True, will use parallel threads to fetch data.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> <code>max_threads</code> <pre><code>        Max number of parallel threads to use.\n</code></pre> <p> TYPE: <code>  int</code> DEFAULT: <code>5</code> </p> <code>seg_source</code> <pre><code>        Use this to override the segmentation source specified by\n        neuPrint.\n</code></pre> <p> TYPE: <code>   str | cloudvolume.CloudVolume</code> DEFAULT: <code>None</code> </p> <code>client</code> <pre><code>        If `None` will try using global client.\n</code></pre> <p> TYPE: <code>       neuprint.Client</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <pre><code>        Will be passed to `cloudvolume.CloudVolume`.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>navis.Neuronlist</code> <p>Containing <code>navis.MeshNeuron</code>. Note that meshes are resized to raw voxel size to match other spatial data from neuprint (synapses, skeletons, etc).</p> Source code in <code>navis/interfaces/neuprint.py</code> <pre><code>@inject_client\ndef fetch_mesh_neuron(x, *, lod=1, with_synapses=False, missing_mesh='raise',\n                      parallel=True, max_threads=5, seg_source=None,\n                      client=None, **kwargs):\n    \"\"\"Fetch neuron meshes as navis.MeshNeuron.\n\n    Requires additional packages depending on the mesh source.\n\n    For DVID you need [`dvid-tools`](https://github.com/flyconnectome/dvid_tools):\n\n        ``` shell\n        pip3 install dvidtools\n        ```\n\n    For everything else you need [cloudvolume](https://github.com/seung-lab/cloud-volume):\n\n        ``` shell\n        pip3 install cloud-volume\n        ```\n\n\n    Parameters\n    ----------\n    x :             str | int | list-like | pandas.DataFrame | SegmentCriteria\n                    Body ID(s). Multiple IDs can be provided as list-like or\n                    DataFrame with \"bodyId\" or \"bodyid\" column.\n    lod :           int\n                    Level of detail. Higher `lod` = coarser. Ignored if mesh\n                    source does not support LODs (e.g. for DVID).\n    with_synapses : bool, optional\n                    If True will download and attach synapses as `.connectors`.\n    missing_mesh :  'raise' | 'warn' | 'skip'\n                    What to do if no mesh is found for a given body ID:\n\n                        \"raise\" (default) will raise an exception\n                        \"warn\" will throw a warning but continue\n                        \"skip\" will skip without any message\n\n    parallel :      bool\n                    If True, will use parallel threads to fetch data.\n    max_threads :   int\n                    Max number of parallel threads to use.\n    seg_source :    str | cloudvolume.CloudVolume, optional\n                    Use this to override the segmentation source specified by\n                    neuPrint.\n    client :        neuprint.Client, optional\n                    If `None` will try using global client.\n    **kwargs\n                    Will be passed to `cloudvolume.CloudVolume`.\n\n    Returns\n    -------\n    navis.Neuronlist\n                    Containing [`navis.MeshNeuron`][]. Note that meshes are\n                    resized to raw voxel size to match other spatial data from\n                    neuprint (synapses, skeletons, etc).\n\n    \"\"\"\n    if isinstance(x, pd.DataFrame):\n        if 'bodyId' in x.columns:\n            x = x['bodyId'].values\n        elif 'bodyid' in x.columns:\n            x = x['bodyid'].values\n        else:\n            raise ValueError('DataFrame must have \"bodyId\" column.')\n\n    # Extract source\n    if not seg_source:\n        seg_source = get_seg_source(client=client)\n\n    if not seg_source:\n        raise ValueError('Segmentation source could not be automatically '\n                         'determined. Please provide via `seg_source`.')\n\n    if isinstance(seg_source, str) and seg_source.startswith('dvid'):\n        try:\n            import dvid as dv\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                'This looks like a DVID mesh source. For this we '\n                'need the `dvid-tools` library:\\n'\n                '  pip3 install dvidtools -U')\n        o = urlparse(seg_source.replace('dvid://', ''))\n        server = f'{o.scheme}://{o.netloc}'\n        node = o.path.split('/')[1]\n\n        if lod is not None:\n                logger.warning(\n                    'This dataset does not support LODs. '\n                    'Will ignore the `lod` argument. '\n                    'You can silence this warning by setting `lod=None`.')\n                lod = None\n    else:\n        try:\n            from cloudvolume import CloudVolume\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                \"You need to install the `cloudvolume` library\"\n                \"to fetch meshes from this mesh source:\\n\"\n                \"  pip3 install cloud-volume -U\")\n        # Initialize volume\n        if isinstance(seg_source, CloudVolume):\n            vol = seg_source\n        else:\n            defaults = dict(use_https=True, progress=False)\n            defaults.update(kwargs)\n            vol = CloudVolume(seg_source, **defaults)\n\n            # Check if vol.mesh.get has a lod argument\n            if lod is not None and 'lod' not in vol.mesh.get.__code__.co_varnames:\n                logger.warning(\n                    'This dataset does not have multi-resolution meshes and '\n                    'the `lod` parameter will be ignored. '\n                    'You can silence this warning by setting `lod=None`.')\n                lod = None\n\n    if isinstance(x, NeuronCriteria):\n        query = x\n        wanted_ids = None\n    else:\n        query = NeuronCriteria(bodyId=x, client=client)\n        wanted_ids = utils.make_iterable(x)\n\n    # Fetch names, etc\n    meta = fetch_neurons(query, client=client, omit_rois=True)\n\n    if meta.empty:\n        raise ValueError('No neurons matching the given criteria found!')\n    elif not isinstance(wanted_ids, type(None)):\n        miss = wanted_ids[~np.isin(wanted_ids, meta.bodyId.values)]\n        if len(miss):\n            logger.warning(f'Skipping {len(miss)} body IDs that were not found: '\n                           f'{\", \".join(miss.astype(str))}')\n\n    # Apply a small number of potential fixes to the meta data\n    meta = _fix_meta(meta)\n\n    if isinstance(seg_source, str) and seg_source.startswith('dvid'):\n        # Fetch the meshes\n        nl = dv.get_meshes(meta.bodyId.values,\n                           on_error=missing_mesh,\n                           output='navis',\n                           progress=meta.shape[0] &gt; 1 and not config.pbar_hide,\n                           max_threads=1 if not parallel else max_threads,\n                           server=server,\n                           node=node)\n    else:\n        nl = []\n        with ThreadPoolExecutor(max_workers=1 if not parallel else max_threads) as executor:\n            futures = {}\n            for r in meta.itertuples():\n                f = executor.submit(__fetch_mesh,\n                                    r.bodyId,\n                                    vol=vol,\n                                    lod=lod,\n                                    missing_mesh=missing_mesh)\n                futures[f] = r.bodyId\n\n            with config.tqdm(desc='Fetching',\n                             total=len(futures),\n                             leave=config.pbar_leave,\n                             disable=meta.shape[0] == 1 or config.pbar_hide) as pbar:\n                for f in as_completed(futures):\n                    bodyId = futures[f]\n                    pbar.update(1)\n                    try:\n                        nl.append(f.result())\n                    except Exception as exc:\n                        print(f'{bodyId} generated an exception:', exc)\n\n    nl = NeuronList(nl)\n\n    # Add meta data\n    instances = meta.set_index('bodyId').instance.to_dict()\n    sizes = meta.set_index('bodyId')['size'].to_dict()\n    status = meta.set_index('bodyId').status.to_dict()\n    statuslabel = meta.set_index('bodyId').statusLabel.to_dict()\n    somalocs = meta.set_index('bodyId').somaLocation.to_dict()\n    radii = meta.set_index('bodyId').somaRadius.to_dict()\n\n    for n in nl:\n        n.name = instances[n.id]\n        n.status = status[n.id]\n        n.statusLabel = statuslabel[n.id]\n        n.n_voxels = sizes[n.id]\n        n.somaLocation = somalocs[n.id]\n\n        # Meshes come out in units (e.g. nanometers) but most other data (synapses,\n        # skeletons, etc) come out in voxels, we will therefore scale meshes to voxels\n        n.vertices /= np.array(client.meta['voxelSize']).reshape(1, 3)\n        n.units=f'{client.meta[\"voxelSize\"][0]} {client.meta[\"voxelUnits\"]}'\n\n        if n.somaLocation:\n            if radii[n.id]:\n                n.soma_radius = radii[n.id] / n.units.to('nm').magnitude\n            else:\n                n.soma_radius = None\n            n.soma_pos = n.somaLocation\n\n    if with_synapses:\n        # Fetch synapses\n        syn = fetch_synapses(meta.bodyId.values,\n                             synapse_criteria=SynapseCriteria(primary_only=True, client=client),\n                             client=client)\n\n        for n in nl:\n            this_syn = syn[syn.bodyId == n.id]\n            if not this_syn.empty:\n                # Keep only relevant columns\n                n.connectors = syn[['type', 'x', 'y', 'z', 'roi', 'confidence']]\n\n    # Make an effort to retain the original order\n    if not isinstance(x, NeuronCriteria) and not nl.empty:\n        nl = nl.idx[np.asarray(x)[np.isin(x, nl.id)]]\n\n    return nl\n</code></pre>"},{"location":"reference/navis/interfaces/neuprint/#navis.interfaces.neuprint.fetch_roi","title":"<code>navis.interfaces.neuprint.fetch_roi</code>","text":"<p>Fetch given ROI.</p> PARAMETER DESCRIPTION <code>roi</code> <pre><code>        Name of an ROI.\n</code></pre> <p> TYPE: <code>          str</code> </p> <code>client</code> <pre><code>        If `None` will try using global client.\n</code></pre> <p> TYPE: <code>       neuprint.Client</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>navis.Volume</code> Source code in <code>navis/interfaces/neuprint.py</code> <pre><code>@inject_client\ndef fetch_roi(roi, *, client=None):\n    \"\"\"Fetch given ROI.\n\n    Parameters\n    ----------\n    roi :           str\n                    Name of an ROI.\n    client :        neuprint.Client, optional\n                    If `None` will try using global client.\n\n    Returns\n    -------\n    navis.Volume\n\n    \"\"\"\n    if not isinstance(roi, str):\n        raise TypeError(f'Expect ROI name as string, got \"{type(roi)}\"')\n\n    # Fetch data\n    data = client.fetch_roi_mesh(roi, export_path=None)\n\n    # Turn into file-like object\n    f = io.StringIO(data.decode())\n\n    # Parse with trimesh\n    ob = trimesh.load_mesh(f, file_type='obj')\n\n    return Volume.from_object(ob, name=roi)\n</code></pre>"},{"location":"reference/navis/interfaces/neuprint/#navis.interfaces.neuprint.fetch_skeletons","title":"<code>navis.interfaces.neuprint.fetch_skeletons</code>","text":"<p>Fetch neuron skeletons as navis.TreeNeurons.</p> Notes <p>Synapses will be attached to the closest node in the skeleton.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Body ID(s). Multiple Ids can be provided as list-like or\n        DataFrame with \"bodyId\"  or \"bodyid\" column.\n</code></pre> <p> TYPE: <code>            str | int | list-like | pandas.DataFrame | SegmentCriteria</code> </p> <code>with_synapses</code> <pre><code>        If True will also attach synapses as `.connectors`.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>heal</code> <pre><code>        If True, will automatically heal fragmented skeletons using\n        neuprint-python's `heal_skeleton` function. Pass a float\n        or an int to limit the max distance at which nodes are\n        allowed to be re-connected (requires neuprint-python &gt;= 0.4.11).\n</code></pre> <p> TYPE: <code>         bool | int | float</code> DEFAULT: <code>False</code> </p> <code>missing_swc</code> <pre><code>        What to do if no skeleton is found for a given body ID:\n\n          - \"raise\" (default) will raise an exception\n          - \"warn\" will throw a warning but continue\n          - \"skip\" will skip without any message\n</code></pre> <p> TYPE: <code>  'raise' | 'warn' | 'skip'</code> DEFAULT: <code>'raise'</code> </p> <code>parallel</code> <pre><code>        If True, will use parallel threads to fetch data.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> <code>max_threads</code> <pre><code>        Max number of parallel threads to use.\n</code></pre> <p> TYPE: <code>  int</code> DEFAULT: <code>5</code> </p> <code>client</code> <pre><code>        If `None` will try using global client.\n</code></pre> <p> TYPE: <code>       neuprint.Client</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>navis.Neuronlist</code> Source code in <code>navis/interfaces/neuprint.py</code> <pre><code>@inject_client\ndef fetch_skeletons(x, *, with_synapses=False, heal=False, missing_swc='raise',\n                    parallel=True, max_threads=5, client=None):\n    \"\"\"Fetch neuron skeletons as navis.TreeNeurons.\n\n    Notes\n    -----\n    Synapses will be attached to the closest node in the skeleton.\n\n    Parameters\n    ----------\n    x :             str | int | list-like | pandas.DataFrame | SegmentCriteria\n                    Body ID(s). Multiple Ids can be provided as list-like or\n                    DataFrame with \"bodyId\"  or \"bodyid\" column.\n    with_synapses : bool, optional\n                    If True will also attach synapses as `.connectors`.\n    heal :          bool | int | float, optional\n                    If True, will automatically heal fragmented skeletons using\n                    neuprint-python's `heal_skeleton` function. Pass a float\n                    or an int to limit the max distance at which nodes are\n                    allowed to be re-connected (requires neuprint-python &gt;= 0.4.11).\n    missing_swc :   'raise' | 'warn' | 'skip'\n                    What to do if no skeleton is found for a given body ID:\n\n                      - \"raise\" (default) will raise an exception\n                      - \"warn\" will throw a warning but continue\n                      - \"skip\" will skip without any message\n\n    parallel :      bool\n                    If True, will use parallel threads to fetch data.\n    max_threads :   int\n                    Max number of parallel threads to use.\n    client :        neuprint.Client, optional\n                    If `None` will try using global client.\n\n    Returns\n    -------\n    navis.Neuronlist\n\n    \"\"\"\n    if isinstance(x, pd.DataFrame):\n        if 'bodyId' in x.columns:\n            x = x['bodyId'].values\n        elif 'bodyid' in x.columns:\n            x = x['bodyid'].values\n        else:\n            raise ValueError('DataFrame must have \"bodyId\" column.')\n\n    if isinstance(x, NeuronCriteria):\n        query = x\n        wanted_ids = None\n    else:\n        query = NeuronCriteria(bodyId=x, client=client)\n        wanted_ids = utils.make_iterable(x)\n\n    # Fetch names, etc\n    meta = fetch_neurons(query, client=client, omit_rois=True)\n\n    if meta.empty:\n        raise ValueError('No neurons matching the given criteria found!')\n    elif not isinstance(wanted_ids, type(None)):\n        miss = wanted_ids[~np.isin(wanted_ids, meta.bodyId.values)]\n        if len(miss):\n            logger.warning(f'Skipping {len(miss)} body IDs that were not found: '\n                           f'{\", \".join(miss.astype(str))}')\n\n    # Apply a small number of potential fixes to the meta data\n    meta = _fix_meta(meta)\n\n    nl = []\n    with ThreadPoolExecutor(max_workers=1 if not parallel else max_threads) as executor:\n        futures = {}\n        for r in meta.itertuples():\n            f = executor.submit(__fetch_skeleton,\n                                r,\n                                client=client,\n                                with_synapses=with_synapses,\n                                missing_swc=missing_swc,\n                                heal=heal)\n            futures[f] = r.bodyId\n\n        with config.tqdm(desc='Fetching',\n                         total=meta.shape[0],\n                         leave=config.pbar_leave,\n                         disable=meta.shape[0] == 1 or config.pbar_hide) as pbar:\n            for f in as_completed(futures):\n                bodyId = futures[f]\n                pbar.update(1)\n                try:\n                    nl.append(f.result())\n                except Exception as exc:\n                    print(f'{bodyId} generated an exception:', exc)\n\n    nl = NeuronList(nl)\n\n    # Make an effort to retain the original order\n    if not isinstance(x, NeuronCriteria) and not nl.empty:\n        nl = nl.idx[np.asarray(x)[np.isin(x, nl.id)]]\n\n    return nl\n</code></pre>"},{"location":"reference/navis/interfaces/neuprint/#navis.interfaces.neuprint.get_seg_source","title":"<code>navis.interfaces.neuprint.get_seg_source</code>","text":"<p>Get segmentation source for given client+dataset.</p> Source code in <code>navis/interfaces/neuprint.py</code> <pre><code>@inject_client\ndef get_seg_source(*, client=None):\n    \"\"\"Get segmentation source for given client+dataset.\"\"\"\n    # First try to fetch the scene for the neuroglancer\n    url = f'{client.server}/api/npexplorer/nglayers/{client.dataset}.json'\n\n    r = client.session.get(url)\n    try:\n        r.raise_for_status()\n        scene = r.json()\n        segs = [s for s in scene['layers'] if s.get('type') == 'segmentation']\n    except BaseException:\n        segs = []\n\n    # If we didn't find a `dataset.json`, will check the client's meta data for a seg source\n    if not segs:\n        segs = [s for s in client.meta['neuroglancerMeta'] if s.get('dataType') == 'segmentation']\n\n    if not len(segs):\n        return None\n\n    # Check if any segmentation source matches our dataset exactly\n    named_segs = [s for s in segs if s.get('name') == client.dataset]\n    if len(named_segs):\n        segs = named_segs\n\n    # If there are multiple segmentation layers, select the first entry\n    seg_source = segs[0]['source']\n\n    # If there are multiple segmentation sources for\n    # the layer we picked, select the first source.\n    if isinstance(seg_source, list):\n        seg_source = seg_source[0]\n\n    # If it's a dict like {'source': url, 'subsources'...},\n    # select the url.\n    if isinstance(seg_source, dict):\n        seg_source = seg_source['url']\n\n    if not isinstance(seg_source, str):\n        e = f\"Could not understand segmentation source: {seg_source}\"\n        raise RuntimeError(e)\n\n    if len(segs) &gt; 1:\n        logger.warning(f'{len(segs)} segmentation sources found. Using the '\n                       f'first entry: \"{seg_source}\"')\n\n    return seg_source\n</code></pre>"},{"location":"reference/navis/interfaces/neuprint/#navis.interfaces.neuprint.remove_soma_hairball","title":"<code>navis.interfaces.neuprint.remove_soma_hairball</code>","text":"<p>Remove hairball around soma.</p> PARAMETER DESCRIPTION <code>x</code> <p> TYPE: <code>        core.TreeNeuron</code> </p> <code>radius</code> <pre><code>    Radius around the soma to check for hairball\n</code></pre> <p> TYPE: <code>   float</code> DEFAULT: <code>500</code> </p> RETURNS DESCRIPTION <code>TreeNeuron</code> <p>If inplace=False.</p> Source code in <code>navis/interfaces/neuprint.py</code> <pre><code>def remove_soma_hairball(x: 'core.TreeNeuron',\n                         radius: float = 500,\n                         inplace: bool = False):\n    \"\"\"Remove hairball around soma.\n\n    Parameters\n    ----------\n    x :         core.TreeNeuron\n    radius :    float\n                Radius around the soma to check for hairball\n\n    Returns\n    -------\n    TreeNeuron\n                If inplace=False.\n    \"\"\"\n    if not inplace:\n        x = x.copy()\n    if not x.soma:\n        if not inplace:\n            return x\n        return\n    # Get all nodes within given radius of soma nodes\n    soma_loc = x.nodes.set_index('node_id').loc[[x.soma],\n                                                ['x', 'y', 'z']].values\n    tree = neuron2KDTree(x)\n    dist, ix = tree.query(soma_loc, k=x.n_nodes, distance_upper_bound=radius)\n\n    # Subset to nodes within range\n    to_check = set(list(ix[0, dist[0, :] &lt;= radius]))\n\n    # Get the segments that have nodes in the soma\n    segs = [seg for seg in x.segments if set(seg) &amp; to_check]\n\n    # Unless these segments end in a root node, we will keep the last node\n    # (which will be a branch point)\n    segs = [s[:-1] if s[-1] not in x.root else s for s in segs]\n\n    # This is already sorted by length -&gt; we will keep the first (i.e. longest)\n    # segment and remove the rest\n    to_remove = [n for s in segs[1:] for n in s]\n\n    to_keep = x.nodes.loc[~x.nodes.node_id.isin(to_remove), 'node_id'].values\n\n    # Move soma if required\n    if x.soma in to_remove:\n        x.soma = list(to_check &amp; set(to_keep))[0]\n\n    subset_neuron(x, to_keep, inplace=True)\n\n    if not inplace:\n        return x\n</code></pre>"},{"location":"reference/navis/interfaces/neuromorpho/","title":"neuromorpho","text":""},{"location":"reference/navis/interfaces/neuromorpho/#navis.interfaces.neuromorpho.find_neurons","title":"<code>navis.interfaces.neuromorpho.find_neurons</code>","text":"<p>Find neurons matching by given criteria.</p> PARAMETER DESCRIPTION <code>page_limit</code> <pre><code>        Use this to limit the results if you are running a big query.\n</code></pre> <p> TYPE: <code>   int | None</code> DEFAULT: <code>None</code> </p> <code>**filters</code> <pre><code>        Search criteria as `field=value`. See\n        [`navis.interfaces.neuromorpho.get_neuron_fields`][] and\n        [`navis.interfaces.neuromorpho.get_available_field_values`][]\n        for available fields and values.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>pandas.DataFrame</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis.interfaces.neuromorpho as nm\n&gt;&gt;&gt; rat_neurons = nm.find_neurons(species='rat')\n&gt;&gt;&gt; rat_or_mouse = nm.find_neurons(species=['rat', 'mouse'])\n</code></pre> Source code in <code>navis/interfaces/neuromorpho.py</code> <pre><code>def find_neurons(page_limit: Optional[int] = None,\n                 parallel: bool = True,\n                 max_threads: int = 4,\n                 **filters) -&gt; pd.DataFrame:\n    \"\"\"Find neurons matching by given criteria.\n\n    Parameters\n    ----------\n    page_limit :    int | None, optional\n                    Use this to limit the results if you are running a big query.\n    **filters\n                    Search criteria as `field=value`. See\n                    [`navis.interfaces.neuromorpho.get_neuron_fields`][] and\n                    [`navis.interfaces.neuromorpho.get_available_field_values`][]\n                    for available fields and values.\n\n    Returns\n    -------\n    pandas.DataFrame\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis.interfaces.neuromorpho as nm\n    &gt;&gt;&gt; rat_neurons = nm.find_neurons(species='rat')\n    &gt;&gt;&gt; rat_or_mouse = nm.find_neurons(species=['rat', 'mouse'])\n\n    \"\"\"\n    if not filters:\n        answer = \"\"\n        while answer not in [\"y\", \"n\"]:\n            answer = input(\"No filters will list all neurons. Continue? [Y/N] \").lower()\n\n        if answer != 'y':\n            return  # type: ignore\n\n    # Turn strings into lists\n    filters = {k: list(utils.make_iterable(v)) for k, v in filters.items()}\n\n    url = utils.make_url(baseurl, 'api', 'neuron', 'select')\n\n    if isinstance(page_limit, type(None)):\n        page_limit = float('inf')\n\n    data: List[str] = []\n\n    # Load the first page to get the total number of pages\n    resp = requests.post(f'{url}?page=0', json=filters)\n    content = resp.json()\n    total_pages = content['page']['totalPages'] - 1\n    page_limit = min(page_limit, total_pages)\n    data += content['_embedded']['neuronResources']\n\n    page = 1   # start with 1 because we already have 0\n\n    with ThreadPoolExecutor(max_workers=1 if not parallel else max_threads) as executor:\n        futures = {}\n        while page &lt; page_limit:\n            f = executor.submit(requests.post, f'{url}?page={page}', json=filters)\n            futures[f] = page\n            page += 1\n\n        with config.tqdm(desc='Fetching',\n                         total=len(futures) + 1,\n                         leave=config.pbar_leave,\n                         disable=len(futures) == 1 or config.pbar_hide) as pbar:\n            pbar.update(1)  # for the first page fetched\n            for f in as_completed(futures):\n                pbar.update(1)\n                try:\n                    resp = f.result()\n                    resp.raise_for_status()\n                    data += resp.json()['_embedded']['neuronResources']\n                except Exception as exc:\n                    print(f'Page {futures[f]} generated an exception:', exc)\n\n    return pd.DataFrame.from_records(data)\n</code></pre>"},{"location":"reference/navis/interfaces/neuromorpho/#navis.interfaces.neuromorpho.get_available_field_values","title":"<code>navis.interfaces.neuromorpho.get_available_field_values</code>","text":"<p>List all possible values for given neuron field.</p> PARAMETER DESCRIPTION <code>field</code> <pre><code>    Field to search for.\n</code></pre> <p> TYPE: <code>    str</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis.interfaces.neuromorpho as nm\n&gt;&gt;&gt; # Get availalbe values for \"species\" field\n&gt;&gt;&gt; species = nm.get_available_field_values('species')\n&gt;&gt;&gt; species\n['rat',\n 'mouse',\n 'drosophila melanogaster',\n 'human',\n 'monkey',\n ...\n</code></pre> Source code in <code>navis/interfaces/neuromorpho.py</code> <pre><code>def get_available_field_values(field: str) -&gt; List[str]:\n    \"\"\"List all possible values for given neuron field.\n\n    Parameters\n    ----------\n    field :     str\n                Field to search for.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis.interfaces.neuromorpho as nm\n    &gt;&gt;&gt; # Get availalbe values for \"species\" field\n    &gt;&gt;&gt; species = nm.get_available_field_values('species')\n    &gt;&gt;&gt; species\n    ['rat',\n     'mouse',\n     'drosophila melanogaster',\n     'human',\n     'monkey',\n     ...\n\n    \"\"\"\n    data: List[str] = []\n    page = 0\n\n    with config.tqdm(total=1,\n                     disable=config.pbar_hide,\n                     leave=config.pbar_leave,\n                     desc='Fetching') as pbar:\n        while True:\n            url = utils.make_url(baseurl, 'api', 'neuron', 'fields', field, page=page)\n\n            resp = requests.get(url)\n\n            resp.raise_for_status()\n\n            content = resp.json()\n\n            data += content['fields']\n\n            if page == content['page']['totalPages']:\n                break\n\n            pbar.total = content['page']['totalPages']\n            pbar.update(1)\n\n            page += 1\n\n    return data\n</code></pre>"},{"location":"reference/navis/interfaces/neuromorpho/#navis.interfaces.neuromorpho.get_neuron","title":"<code>navis.interfaces.neuromorpho.get_neuron</code>","text":"<p>Fetch neuron by ID or by name.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Integer is intepreted as ID, string as neuron name. Dictionary\n        and DataFrame must contain 'archive' (e.g. \"Wearne_Hof\") and\n        'neuron_name' (e.g. \"cnic_001\").\n</code></pre> <p> TYPE: <code>            int | str | dict | pandas.DataFrame</code> </p> <code>parallel</code> <pre><code>        If True, will use threads to fetch data.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> <code>max_threads</code> <pre><code>        Max number of parallel threads to use.\n</code></pre> <p> TYPE: <code>  int</code> DEFAULT: <code>4</code> </p> <code>**kwargs</code> <pre><code>        Keyword arguments passed on to [`navis.read_swc`][].\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>TreeNeuron</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis.interfaces.neuromorpho as nm\n&gt;&gt;&gt; # Get a neuron by its ID\n&gt;&gt;&gt; n = nm.get_neuron(1)\n&gt;&gt;&gt; n\ntype            TreeNeuron\nname                   SWC\nn_nodes               1274\nn_connectors             0\nn_branches              46\nn_leafs                 54\ncable_length       4792.21\nsoma                  None\n</code></pre> Source code in <code>navis/interfaces/neuromorpho.py</code> <pre><code>def get_neuron(x: Union[str, int, Dict[str, str]],\n               parallel: bool = True,\n               max_threads: int = 4,\n               **kwargs) -&gt; TreeNeuron:\n    \"\"\"Fetch neuron by ID or by name.\n\n    Parameters\n    ----------\n    x :             int | str | dict | pandas.DataFrame\n                    Integer is intepreted as ID, string as neuron name. Dictionary\n                    and DataFrame must contain 'archive' (e.g. \"Wearne_Hof\") and\n                    'neuron_name' (e.g. \"cnic_001\").\n    parallel :      bool\n                    If True, will use threads to fetch data.\n    max_threads :   int\n                    Max number of parallel threads to use.\n    **kwargs\n                    Keyword arguments passed on to [`navis.read_swc`][].\n\n    Returns\n    -------\n    TreeNeuron\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis.interfaces.neuromorpho as nm\n    &gt;&gt;&gt; # Get a neuron by its ID\n    &gt;&gt;&gt; n = nm.get_neuron(1)\n    &gt;&gt;&gt; n\n    type            TreeNeuron\n    name                   SWC\n    n_nodes               1274\n    n_connectors             0\n    n_branches              46\n    n_leafs                 54\n    cable_length       4792.21\n    soma                  None\n\n    \"\"\"\n    if isinstance(x, pd.DataFrame):\n        nl = []\n        with ThreadPoolExecutor(max_workers=1 if not parallel else max_threads) as executor:\n            futures = {}\n            for r in x.to_dict(orient='records'):\n                f = executor.submit(get_neuron, r, **kwargs)\n                futures[f] = r.get('neuron_id', r.get('neuron_name', 'NA'))\n\n            with config.tqdm(desc='Fetching',\n                             total=len(x),\n                             leave=config.pbar_leave,\n                             disable=len(x) == 1 or config.pbar_hide) as pbar:\n                for f in as_completed(futures):\n                    id = futures[f]\n                    pbar.update(1)\n                    try:\n                        nl.append(f.result())\n                    except Exception as exc:\n                        print(f'{id} generated an exception:', exc)\n\n        # Turn into neuronlist\n        nl = NeuronList(nl)\n\n        # Make sure we return in same order as input\n        if 'neuron_id' in x.columns:\n            ids = x.neuron_id.values\n            ids = ids[np.isin(ids, nl.id)]  # drop failed IDs\n            nl = nl.idx[ids]\n\n        return nl\n\n    if not isinstance(x, (pd.Series, dict)):\n        info = get_neuron_info(x)\n    else:\n        info = x  # type: ignore\n\n    archive: str = info['archive']\n    name: str = info['neuron_name']\n\n    url = utils.make_url(baseurl, 'dableFiles', archive.lower(), 'CNG version', name + '.CNG.swc')\n\n    n = read_swc(url, **kwargs)\n\n    n.id = info.get('neuron_id', n.id)\n    n.name = info.get('neuron_name', getattr(n, 'name'))\n\n    return n\n</code></pre>"},{"location":"reference/navis/interfaces/neuromorpho/#navis.interfaces.neuromorpho.get_neuron_fields","title":"<code>navis.interfaces.neuromorpho.get_neuron_fields</code>","text":"<p>List all available neuron fields.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis.interfaces.neuromorpho as nm\n&gt;&gt;&gt; fields = nm.get_neuron_fields()\n&gt;&gt;&gt; fields\n['neuron_id',\n 'neuron_name',\n 'archive',\n 'age_scale',\n ...\n</code></pre> Source code in <code>navis/interfaces/neuromorpho.py</code> <pre><code>def get_neuron_fields() -&gt; Dict[str, List[str]]:\n    \"\"\"List all available neuron fields.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis.interfaces.neuromorpho as nm\n    &gt;&gt;&gt; fields = nm.get_neuron_fields()\n    &gt;&gt;&gt; fields\n    ['neuron_id',\n     'neuron_name',\n     'archive',\n     'age_scale',\n     ...\n\n    \"\"\"\n    url = utils.make_url(baseurl, 'api', 'neuron', 'fields')\n    resp = requests.get(url)\n\n    resp.raise_for_status()\n\n    return resp.json().get('Neuron Fields')\n</code></pre>"},{"location":"reference/navis/interfaces/neuromorpho/#navis.interfaces.neuromorpho.get_neuron_info","title":"<code>navis.interfaces.neuromorpho.get_neuron_info</code>","text":"<p>Fetch neuron info by ID or by name.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Integer is intepreted as ID, string as neuron name. Will try\n    to convert strings to integers first.\n</code></pre> <p> TYPE: <code>        int | str</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis.interfaces.neuromorpho as nm\n&gt;&gt;&gt; # Get info by ID\n&gt;&gt;&gt; info = nm.get_neuron_info(1)\n&gt;&gt;&gt; # Get info by Name\n&gt;&gt;&gt; info = nm.get_neuron_info('cnic_001')\n</code></pre> Source code in <code>navis/interfaces/neuromorpho.py</code> <pre><code>def get_neuron_info(x: Union[str, int]) -&gt; pd.Series:\n    \"\"\"Fetch neuron info by ID or by name.\n\n    Parameters\n    ----------\n    x :         int | str\n                Integer is intepreted as ID, string as neuron name. Will try\n                to convert strings to integers first.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis.interfaces.neuromorpho as nm\n    &gt;&gt;&gt; # Get info by ID\n    &gt;&gt;&gt; info = nm.get_neuron_info(1)\n    &gt;&gt;&gt; # Get info by Name\n    &gt;&gt;&gt; info = nm.get_neuron_info('cnic_001')\n\n    \"\"\"\n    try:\n        x = int(x)\n    except BaseException:\n        pass\n\n    if isinstance(x, str):\n        url = utils.make_url(baseurl, 'api', 'neuron', 'name', x)\n    elif isinstance(x, int):\n        url = utils.make_url(baseurl, 'api', 'neuron', 'id', str(x))\n    else:\n        raise TypeError(f'Expected string or int, got {type(x)}')\n\n    resp = requests.get(url)\n\n    resp.raise_for_status()\n\n    return pd.Series(resp.json())\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/","title":"neuron","text":""},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel","title":"<code>navis.interfaces.neuron.CompartmentModel</code>","text":"<p>Compartment model representing a single neuron in NEURON.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Neuron to generate model for. Has to be in microns!\n</code></pre> <p> TYPE: <code>        navis.TreeNeuron</code> </p> <code>res</code> <pre><code>    Approximate length [um] of segments. This guarantees that\n    no section has any segment that is longer than `res` but for\n    small branches (i.e. \"sections\") the segments might be smaller.\n    Lower `res` = more detailed simulation.\n</code></pre> <p> TYPE: <code>      int</code> DEFAULT: <code>10</code> </p> Source code in <code>navis/interfaces/neuron/comp.py</code> <pre><code>class CompartmentModel:\n    \"\"\"Compartment model representing a single neuron in NEURON.\n\n    Parameters\n    ----------\n    x :         navis.TreeNeuron\n                Neuron to generate model for. Has to be in microns!\n    res :       int\n                Approximate length [um] of segments. This guarantees that\n                no section has any segment that is longer than `res` but for\n                small branches (i.e. \"sections\") the segments might be smaller.\n                Lower `res` = more detailed simulation.\n\n    \"\"\"\n\n    def __init__(self, x: 'core.TreeNeuron', res=10):\n        \"\"\"Initialize Neuron.\"\"\"\n        utils.eval_param(x, name='x', allowed_types=(core.TreeNeuron, ))\n\n        # Note that we make a copy to make sure that the data underlying the\n        # model will not accidentally be changed\n        self.skeleton = x.copy()\n\n        # Max section resolution per segment\n        self.res = res\n\n        # Some placeholders\n        self._sections = []\n        self._stimuli = {}\n        self._records = {}\n        self._synapses = {}\n\n        # Generate the actual model\n        self._validate_skeleton()\n        self._generate_sections()\n\n    def __repr__(self):\n        s = (f'CompartmentModel&lt;id={self.skeleton.label},'\n             f'sections={self.n_sections};'\n             f'stimuli={self.n_stimuli};'\n             f'records={self.n_records}&gt;'\n             )\n        return s\n\n    @property\n    def label(self):\n        \"\"\"Name/label of the neuron.\"\"\"\n        return f'CompartmentModel[{self.skeleton.label}]'\n\n    @property\n    def n_records(self):\n        \"\"\"Number of records (across all types) active on this model.\"\"\"\n        return len([r for t in self.records.values() for r in t])\n\n    @property\n    def n_sections(self):\n        \"\"\"Number of sections in this model.\"\"\"\n        return len(self.sections)\n\n    @property\n    def n_stimuli(self):\n        \"\"\"Number of stimuli active on this model.\"\"\"\n        return len(self.stimuli)\n\n    @property\n    def nodes(self) -&gt; pd.DataFrame:\n        \"\"\"Node table of the skeleton.\"\"\"\n        return self.skeleton.nodes\n\n    @property\n    def cm(self) -&gt; float:\n        \"\"\"Membran capacity [micro Farads / cm^2] of all sections.\"\"\"\n        return np.array([s.cm for s in self.sections])\n\n    @cm.setter\n    def cm(self, value: float):\n        \"\"\"Membran capacity [micro Farads / cm^2] for all sections.\"\"\"\n        for s in self.sections:\n            s.cm = value\n\n    @property\n    def Ra(self) -&gt; float:\n        \"\"\"Axial resistance [Ohm * cm] of all sections.\"\"\"\n        return np.array([s.Ra for s in self.sections])\n\n    @Ra.setter\n    def Ra(self, value: float):\n        \"\"\"Set axial resistance [Ohm * cm] for all sections.\"\"\"\n        for s in self.sections:\n            s.Ra = value\n\n    @property\n    def records(self) -&gt; dict:\n        \"\"\"Return mapping of node ID(s) to recordings.\"\"\"\n        return self._records\n\n    @property\n    def sections(self) -&gt; np.ndarray:\n        \"\"\"List of sections making up this model.\"\"\"\n        return self._sections\n\n    @property\n    def stimuli(self) -&gt; dict:\n        \"\"\"Return mapping of node ID(s) to stimuli.\"\"\"\n        return self._stimuli\n\n    @property\n    def synapses(self) -&gt; dict:\n        \"\"\"Return mapping of node ID(s) to synapses.\"\"\"\n        return self._synapses\n\n    @property\n    def t(self) -&gt; np.ndarray:\n        \"\"\"The global time. Should be the same for all neurons.\"\"\"\n        return main_t\n\n    def _generate_sections(self):\n        \"\"\"Generate sections from the neuron.\n\n        This will automatically be called at initialization and should not be\n        called again.\n\n        \"\"\"\n        # First generate sections\n        self._sections = []\n        nodes = self.skeleton.nodes.set_index('node_id')\n        roots = self.skeleton.root\n        bp = self.skeleton.branch_points.node_id.values\n        G = self.skeleton.graph\n        node2sec = {}\n        node2pos = {}\n        for i, seg in enumerate(self.skeleton.small_segments):\n            # Get child -&gt; parent distances in this segment\n            dists = np.array([G.edges[(c, p)]['weight']\n                              for c, p in zip(seg[:-1], seg[1:])])\n\n            # Invert the sections\n            # That's because in navis sections go from tip -&gt; root (i.e.\n            # child -&gt; parent) but in neuron section(0) is the base and\n            # section(1) is the tip.\n            seg = np.asarray(seg)[::-1]\n            dists = dists[::-1]\n\n            # Grab the coordinates and radii\n            seg_nodes = nodes.loc[seg]\n            locs = seg_nodes[['x', 'y', 'z']].values\n            radii = seg_nodes.radius.values\n\n            # Generate section\n            sec = neuron.h.Section(name=f'segment_{i}')\n\n            # Set 3D points -&gt; this automatically sets length L\n            xvec = neuron.h.Vector(locs[:, 0])\n            yvec = neuron.h.Vector(locs[:, 1])\n            zvec = neuron.h.Vector(locs[:, 2])\n            dvec = neuron.h.Vector(radii * 2)\n            neuron.h.pt3dadd(xvec, yvec, zvec, dvec, sec=sec)\n\n            # Set number of segments for this section\n            # We also will make sure that each section has an odd\n            # number of segments\n            sec.nseg = 1 + 2 * int(sec.L / (self.res * 2))\n            # Keep track of section\n            self.sections.append(sec)\n\n            # While we're at it: for each point (except the root of this\n            # section) find the relative position within the section\n\n            # Get normalized positions within this segment\n            norm_pos = dists.cumsum() / dists.sum()\n\n            # Update positional dictionaries (required for connecting the\n            # segments in the next step)\n            node2pos.update(dict(zip(seg[1:], norm_pos)))\n            node2sec.update({n: i for n in seg[1:]})\n\n            # If this happens to be the segment with the skeleton's root, keep\n            # track of it too\n            if seg[0] in roots:\n                node2pos[seg[0]] = 0\n                node2sec[seg[0]] = i\n\n        self._sections = np.array(self.sections)\n        self.skeleton.nodes['sec_ix'] = self.skeleton.nodes.node_id.map(node2sec)\n        self.skeleton.nodes['sec_pos'] = self.skeleton.nodes.node_id.map(node2pos)\n\n        # Need to grab nodes again after adding `sec_ix` and `sec_pos`\n        nodes = self.skeleton.nodes.set_index('node_id')\n\n        # Connect segments\n        for i, seg in enumerate(self.skeleton.small_segments):\n            # Root is special in that it only needs to be connected if it's also\n            # a branch point\n            if seg[-1] in roots:\n                # Skip if root is not a branch point\n                if seg[-1] not in bp:\n                    continue\n                # If root is also a branch point, it will be part of more than\n                # one section but in the positional dicts we will have kept track\n                # of only one of them. That's the one we pick as base segment\n                if node2sec[seg[-1]] == i:\n                    continue\n\n            parent = nodes.loc[seg[-1]]\n            parent_sec = self.sections[parent.sec_ix]\n            self.sections[i].connect(parent_sec(1))\n\n    def _validate_skeleton(self):\n        \"\"\"Validate skeleton.\"\"\"\n        if self.skeleton.units and not self.skeleton.units.dimensionless:\n            not_um = self.skeleton.units.units != config.ureg.Unit('um')\n            not_microns = self.skeleton.units.units != config.ureg.Unit('microns')\n            if not_um and not_microns:\n                logger.warning('Model expects coordinates in microns but '\n                               f'neuron has units \"{self.skeleton.units}\"!')\n\n        if len(self.skeleton.root) &gt; 1:\n            logger.warning('Neuron has multiple roots and hence consists of '\n                           'multiple disconnected fragments!')\n\n        if 'radius' not in self.skeleton.nodes.columns:\n            raise ValueError('Neuron node table must have `radius` column')\n\n        if np.any(self.skeleton.nodes.radius.values &lt;= 0):\n            raise ValueError('Neuron node table contains radii &lt;= 0.')\n\n    def add_synaptic_input(self, where, start=5 * ms,\n                           spike_no=1, spike_int=10 * ms, spike_noise=0,\n                           syn_tau1=.1 * ms, syn_tau2=10 * ms, syn_rev_pot=0,\n                           cn_thresh=10, cn_delay=1 * ms, cn_weight=0.05):\n        \"\"\"Add synaptic input to model.\n\n        This uses the Exp2Syn synapse. All targets in `where` are triggered\n        by the same NetStim - i.e. they will all receive their spike(s) at the\n        same time.\n\n        Parameters\n        ----------\n        where :         int | list of int\n                        Node IDs at which to simulate synaptic input.\n\n        Properties for presynaptic spikes:\n\n        start :         int\n                        Onset [ms] of first spike from beginning of simulation.\n        spike_no :      int\n                        Number of presynaptic spikes to produce.\n        spike_int :     int\n                        Interval [ms] between consecutive spikes.\n        spike_noise :   float [0-1]\n                        Fractional randomness in spike timing.\n\n        Synapse properties:\n\n        syn_tau1 :      int\n                        Rise time constant [ms].\n        syn_tau2 :      int\n                        Decay time constant [ms].\n        syn_rev_pot :   int\n                        Reversal potential (e) [mV].\n\n        Connection properties:\n\n        cn_thresh :     int\n                        Presynaptic membrane potential [mV] at which synaptic\n                        event is triggered.\n        cn_delay :      int\n                        Delay [ms] between presynaptic trigger and postsynaptic\n                        event.\n        cn_weight :     float\n                        Weight variable. This bundles a couple of synaptic\n                        properties such as e.g. how much transmitter is released\n                        or binding affinity at postsynaptic receptors.\n\n        \"\"\"\n        where = utils.make_iterable(where)\n\n        # Make a new stimulator\n        stim = neuron.h.NetStim()\n        stim.number = spike_no\n        stim.start = start\n        stim.noise = spike_noise\n        stim.interval = spike_int\n\n        # Connect\n        self.connect(stim, where, syn_tau1=syn_tau1, syn_tau2=syn_tau2,\n                     syn_rev_pot=syn_rev_pot, cn_thresh=cn_thresh,\n                     cn_delay=cn_delay, cn_weight=cn_weight)\n\n    def inject_current_pulse(self, where, start=5,\n                             duration=1, current=0.1):\n        \"\"\"Add current injection (IClamp) stimulation to model.\n\n        Parameters\n        ----------\n        where :     int | list of int\n                    Node ID(s) at which to stimulate.\n        start :     int\n                    Onset (delay) [ms] from beginning of simulation.\n        duration :  int\n                    Duration (dur) [ms] of injection.\n        current :   float\n                    Amount (i) [nA] of injected current.\n\n        \"\"\"\n        self._add_stimulus('IClamp', where=where, delay=start,\n                           dur=duration, amp=current)\n\n    def add_synaptic_current(self, where, start=5, tau=0.1, rev_pot=0,\n                             max_syn_cond=0.1):\n        \"\"\"Add synaptic current(s) (AlphaSynapse) to model.\n\n        Parameters\n        ----------\n        where :         int | list of int\n                        Node ID(s) at which to stimulate.\n        start :         int\n                        Onset [ms] from beginning of simulation.\n        tau :           int\n                        Decay time constant [ms].\n        rev_pot :       int\n                        Reverse potential (e) [mV].\n        max_syn_cond :  float\n                        Max synaptic conductance (gmax) [uS].\n\n        \"\"\"\n        self._add_stimulus('AlphaSynapse', where=where, onset=start,\n                           tau=tau, e=rev_pot, gmax=max_syn_cond)\n\n    def _add_stimulus(self, stimulus, where, **kwargs):\n        \"\"\"Add generic stimulus.\"\"\"\n        if not callable(stimulus):\n            stimulus = getattr(neuron.h, stimulus)\n\n        where = utils.make_iterable(where)\n\n        nodes = self.nodes.set_index('node_id')\n        for node in nodes.loc[where].itertuples():\n            sec = self.sections[node.sec_ix](node.sec_pos)\n            stim = stimulus(sec)\n\n            for k, v in kwargs.items():\n                setattr(stim, k, v)\n\n            self.stimuli[node.Index] = self.stimuli.get(node.Index, []) + [stim]\n\n    def add_voltage_record(self, where, label=None):\n        \"\"\"Add voltage recording to model.\n\n        Parameters\n        ----------\n        where :     int | list of int\n                    Node ID(s) at which to record.\n        label :     str, optional\n                    If label is given, this recording will be added as\n                    `self.records['v'][label]` else  `self.records['v'][node_id]`.\n\n        \"\"\"\n        self._add_record(where, what='v', label=label)\n\n    def add_current_record(self, where, label=None):\n        \"\"\"Add current recording to model.\n\n        This only works if nodes map to sections that have point processes.\n\n        Parameters\n        ----------\n        where :     int | list of int\n                    Node ID(s) at which to record.\n        label :     str, optional\n                    If label is given, this recording will be added as\n                    `self.records['i'][label]` else  `self.records['i'][node_id]`.\n\n        \"\"\"\n        nodes = utils.make_iterable(where)\n\n        # Map nodes to point processes\n        secs = self.get_node_segment(nodes)\n        where = []\n        for n, sec in zip(nodes, secs):\n            pp = sec.point_processes()\n            if not pp:\n                raise TypeError(f'Section for node {n} has no point process '\n                                '- unable to add current record')\n            elif len(pp) &gt; 1:\n                logger.warning(f'Section for node {n} has more than on point '\n                               'process. Recording current at first.')\n                pp = pp[:1]\n            where += pp\n\n        self._add_record(where, what='i', label=label)\n\n    def add_spike_detector(self, where, threshold=20, label=None):\n        \"\"\"Add a spike detector at given node(s).\n\n        Parameters\n        ----------\n        where :     int | list of int\n                    Node ID(s) at which to record.\n        threshold : float\n                    Threshold in mV for a spike to be counted.\n        label :     str, optional\n                    If label is given, this recording will be added as\n                    `self.records[label]` else  `self.records[node_id]`.\n\n        \"\"\"\n        where = utils.make_iterable(where)\n\n        self.records['spikes'] = self.records.get('spikes', {})\n        self._spike_det = getattr(self, '_spike_det', [])\n        segments = self.get_node_segment(where)\n        sections = self.get_node_section(where)\n        for n, sec, seg in zip(where, sections, segments):\n            # Generate a NetCon object that has no target\n            sp_det = neuron.h.NetCon(seg._ref_v, None, sec=sec)\n\n            # Set threshold\n            if threshold:\n                sp_det.threshold = threshold\n\n            # Keeping track of this to save it from garbage collector\n            self._spike_det.append(sp_det)\n\n            # Create a vector for the spike timings\n            vec = neuron.h.Vector()\n            # Tell the NetCon object to record into that vector\n            sp_det.record(vec)\n\n            if label:\n                self.records['spikes'][label] = vec\n            else:\n                self.records['spikes'][n] = vec\n\n    def _add_record(self, where, what, label=None):\n        \"\"\"Add a recording to given node.\n\n        Parameters\n        ----------\n        where :     int | list of int | point process | section\n                    Node ID(s) (or a section) at which to record.\n        what :      str\n                    What to record. Can be e.g. `v` or `_ref_v` for Voltage.\n        label :     str, optional\n                    If label is given, this recording will be added as\n                    `self.records[label]` else  `self.records[node_id]`.\n\n        \"\"\"\n        where = utils.make_iterable(where)\n\n        if not isinstance(what, str):\n            raise TypeError(f'Required str e.g. \"v\", got {type(what)}')\n\n        if not what.startswith('_ref_'):\n            what = f'_ref_{what}'\n\n        rec_type = what.split('_')[-1]\n        if rec_type not in self.records:\n            self.records[rec_type] = {}\n\n        # # Get node segments only for nodes\n        is_node = ~np.array([is_NEURON_object(w) for w in where])\n        node_segs = np.zeros(len(where), dtype=object)\n        node_segs[is_node] = self.get_node_segment(where[is_node])\n\n        for i, w in enumerate(where):\n            # If this is a neuron object (e.g. segment, section or point\n            # process) we assume this does not need mapping\n            if is_NEURON_object(w):\n                seg = w\n            else:\n                seg = node_segs[i]\n\n            rec = neuron.h.Vector().record(getattr(seg, what))\n\n            if label:\n                self.records[rec_type][label] = rec\n            else:\n                self.records[rec_type][w] = rec\n\n    def connect(self, pre, where, syn_tau1=.1 * ms, syn_tau2=10 * ms,\n                syn_rev_pot=0, cn_thresh=10, cn_delay=1 * ms, cn_weight=0):\n        \"\"\"Connect object to model.\n\n        This uses the Exp2Syn synapse and treats `pre` as the presynaptic\n        object.\n\n        Parameters\n        ----------\n        pre :           NetStim | section\n                        The presynaptic object to connect to this neuron.\n        where :         int | list of int\n                        Node IDs at which to simulate synaptic input.\n\n        Synapse properties:\n\n        syn_tau1 :      int\n                        Rise time constant [ms].\n        syn_tau2 :      int\n                        Decay time constant [ms].\n        syn_rev_pot :   int\n                        Reversal potential (e) [mV].\n\n        Connection properties:\n\n        cn_thresh :     int\n                        Presynaptic membrane potential [mV] at which synaptic\n                        event is triggered.\n        cn_delay :      int\n                        Delay [ms] between presynaptic trigger and postsynaptic\n                        event.\n        cn_weight :     int\n                        Weight variable. This bundles a couple of synaptic\n                        properties such as e.g. how much transmitter is released\n                        or binding affinity at postsynaptic receptors.\n\n        \"\"\"\n        where = utils.make_iterable(where)\n\n        if not is_NEURON_object(pre):\n            raise ValueError(f'Expected NEURON object, got {type(pre)}')\n\n        # Turn section into segment\n        if isinstance(pre, neuron.nrn.Section):\n            pre = pre()\n\n        # Go over the nodes\n        nodes = self.nodes.set_index('node_id')\n        for node in nodes.loc[where].itertuples():\n            # Generate synapses for the nodes in question\n            # Note that we are not reusing existing synapses\n            # in case the properties are different\n            sec = self.sections[node.sec_ix](node.sec_pos)\n            syn = neuron.h.Exp2Syn(sec)\n            syn.tau1 = syn_tau1\n            syn.tau2 = syn_tau2\n            syn.e = syn_rev_pot\n\n            self.synapses[node.Index] = self.synapses.get(node.Index, []) + [syn]\n\n            # Connect spike stimulus and synapse\n            if isinstance(pre, neuron.nrn.Segment):\n                nc = neuron.h.NetCon(pre._ref_v, syn, sec=pre.sec)\n            else:\n                nc = neuron.h.NetCon(pre, syn)\n\n            # Set connection parameters\n            nc.threshold = cn_thresh\n            nc.delay = cn_delay\n            nc.weight[0] = cn_weight\n\n            self.stimuli[node.Index] = self.stimuli.get(node.Index, []) + [nc, pre]\n\n    def clear_records(self):\n        \"\"\"Clear records.\"\"\"\n        self._records = {}\n\n    def clear_stimuli(self):\n        \"\"\"Clear stimuli.\"\"\"\n        self._stimuli = {}\n\n    def clear_synapses(self):\n        \"\"\"Clear synapses.\"\"\"\n        self._synapses = {}\n\n    def clear(self):\n        \"\"\"Attempt to remove model from NEURON space.\n\n        This is not guaranteed to work. Check `neuron.h.topology()` to inspect.\n\n        \"\"\"\n        # Basically we have to bring the reference count to zero\n        self.clear_records()\n        self.clear_stimuli()\n        self.clear_synapses()\n        for s in self._sections:\n            del s\n        self._sections = []\n\n    def get_node_section(self, node_ids):\n        \"\"\"Return section(s) for given node(s).\n\n        Parameters\n        ----------\n        node_ids :  int | list of int\n                    Node IDs.\n\n        Returns\n        -------\n        section(s) :    segment or list of segments\n                        Depends on input.\n\n        \"\"\"\n        nodes = self.nodes.set_index('node_id')\n        if not utils.is_iterable(node_ids):\n            n = nodes.loc[node_ids]\n            return self.sections[n.sec_ix]\n        else:\n            segs = []\n            for node in nodes.loc[node_ids].itertuples():\n                segs.append(self.sections[node.sec_ix])\n            return segs\n\n    def get_node_segment(self, node_ids):\n        \"\"\"Return segment(s) for given node(s).\n\n        Parameters\n        ----------\n        node_ids :  int | list of int\n                    Node IDs.\n\n        Returns\n        -------\n        segment(s) :    segment or list of segments\n                        Depends on input.\n\n        \"\"\"\n        nodes = self.nodes.set_index('node_id')\n        if not utils.is_iterable(node_ids):\n            n = nodes.loc[node_ids]\n            return self.sections[n.sec_ix](n.sec_pos)\n        else:\n            segs = []\n            for node in nodes.loc[node_ids].itertuples():\n                segs.append(self.sections[node.sec_ix](node.sec_pos))\n            return segs\n\n    def insert(self, mechanism, subset=None, **kwargs):\n        \"\"\"Insert biophysical mechanism for model.\n\n        Parameters\n        ----------\n        mechanism : str\n                    Mechanism to insert - e.g. \"hh\" for Hodgkin-Huxley kinetics.\n        subset :    list of sections | list of int\n                    Sections (or indices thereof) to set mechanism for.\n                    If `None` will add mechanism to all sections.\n        **kwargs\n                    Use to set properties for mechanism.\n\n        \"\"\"\n        if isinstance(subset, type(None)):\n            sections = self.sections\n        else:\n            subset = utils.make_iterable(subset)\n\n            if all([is_section(s) for s in subset]):\n                sections = subset\n            elif all([isinstance(s, Number) for s in subset]):\n                sections = self.sections[subset]\n            else:\n                raise TypeError('`subset` must be None, a list of sections or '\n                                'a list of section indices')\n\n        for sec in np.unique(sections):\n            _ = sec.insert(mechanism)\n            for seg in sec:\n                mech = getattr(seg, mechanism)\n                for k, v in kwargs.items():\n                    setattr(mech, k, v)\n\n    def uninsert(self, mechanism, subset=None):\n        \"\"\"Remove biophysical mechanism from model.\n\n        Parameters\n        ----------\n        mechanism : str\n                    Mechanism to remove - e.g. \"hh\" for Hodgkin-Huxley kinetics.\n        subset :    list of sections | list of int\n                    Sections (or indices thereof) to set mechanism for.\n                    If `None` will add mechanism to all sections.\n\n        \"\"\"\n        if isinstance(subset, type(None)):\n            sections = self.sections\n        else:\n            subset = utils.make_iterable(subset)\n\n            if all([is_section(s) for s in subset]):\n                sections = subset\n            elif all([isinstance(s, Number) for s in subset]):\n                sections = self.sections[subset]\n            else:\n                raise TypeError('`subset` must be None, a list of sections or '\n                                'a list of section indices')\n\n        for sec in np.unique(sections):\n            if hasattr(sec, mechanism):\n                _ = sec.uninsert(mechanism)\n\n    def plot_structure(self):\n        \"\"\"Visualize structure in 3D using matplotlib.\"\"\"\n        _ = neuron.h.PlotShape().plot(plt)\n\n    def run_simulation(self, duration=25 * ms, v_init=-65 * mV):\n        \"\"\"Run the simulation.\"\"\"\n        # Add recording of time\n        global main_t\n        main_t = neuron.h.Vector().record(neuron.h._ref_t)\n\n        # This resets the entire model space not just this neuron!\n        neuron.h.finitialize(v_init)\n        neuron.h.continuerun(duration)\n\n    def plot_results(self, axes=None):\n        \"\"\"Plot results.\n\n        Parameters\n        ----------\n        axes :      matplotlib axes\n                    Axes to plot onto. Must have one ax for each recording\n                    type (mV, spike count, etc) in `self.records`.\n\n        Returns\n        -------\n        axes\n\n        \"\"\"\n        if isinstance(self.t, type(None)) or not len(self.t):\n            logger.warning('Looks like the simulation has not yet been run.')\n            return\n        if not self.records:\n            logger.warning('Nothing to plot: no recordings found.')\n            return\n\n        if not axes:\n            fig, axes = plt.subplots(len(self.records), sharex=True)\n\n        # Make sure that even a single ax is a list\n        if not isinstance(axes, (np.ndarray, list)):\n            axes = [axes] * len(self.records)\n\n        for t, ax in zip(self.records, axes):\n            for i, (k, v) in enumerate(self.records[t].items()):\n                if not len(v):\n                    continue\n                v = v.as_numpy()\n                # For spikes the vector contains the times\n                if t == 'spikes':\n                    # Calculate spike rate\n                    bins = np.linspace(0, max(self.t), 10)\n                    hist, _ = np.histogram(v, bins=bins)\n                    width = bins[1] - bins[0]\n                    rate = hist * (1000 / width)\n                    ax.plot(bins[:-1] + (width / 2), rate, label=k)\n\n                    ax.scatter(v, [-i] * len(v), marker='|', s=100)\n                else:\n                    ax.plot(self.t, v, label=k)\n\n            ax.set_xlabel('time [ms]')\n            ax.set_ylabel(f'{t}')\n\n            ax.legend()\n        return axes\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.Ra","title":"<code>Ra: float</code>  <code>property</code> <code>writable</code>","text":"<p>Axial resistance [Ohm * cm] of all sections.</p>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.cm","title":"<code>cm: float</code>  <code>property</code> <code>writable</code>","text":"<p>Membran capacity [micro Farads / cm^2] of all sections.</p>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.label","title":"<code>label</code>  <code>property</code>","text":"<p>Name/label of the neuron.</p>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.n_records","title":"<code>n_records</code>  <code>property</code>","text":"<p>Number of records (across all types) active on this model.</p>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.n_sections","title":"<code>n_sections</code>  <code>property</code>","text":"<p>Number of sections in this model.</p>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.n_stimuli","title":"<code>n_stimuli</code>  <code>property</code>","text":"<p>Number of stimuli active on this model.</p>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.nodes","title":"<code>nodes: pd.DataFrame</code>  <code>property</code>","text":"<p>Node table of the skeleton.</p>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.records","title":"<code>records: dict</code>  <code>property</code>","text":"<p>Return mapping of node ID(s) to recordings.</p>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.sections","title":"<code>sections: np.ndarray</code>  <code>property</code>","text":"<p>List of sections making up this model.</p>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.stimuli","title":"<code>stimuli: dict</code>  <code>property</code>","text":"<p>Return mapping of node ID(s) to stimuli.</p>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.synapses","title":"<code>synapses: dict</code>  <code>property</code>","text":"<p>Return mapping of node ID(s) to synapses.</p>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.t","title":"<code>t: np.ndarray</code>  <code>property</code>","text":"<p>The global time. Should be the same for all neurons.</p>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.__init__","title":"<code>__init__</code>","text":"<p>Initialize Neuron.</p> Source code in <code>navis/interfaces/neuron/comp.py</code> <pre><code>def __init__(self, x: 'core.TreeNeuron', res=10):\n    \"\"\"Initialize Neuron.\"\"\"\n    utils.eval_param(x, name='x', allowed_types=(core.TreeNeuron, ))\n\n    # Note that we make a copy to make sure that the data underlying the\n    # model will not accidentally be changed\n    self.skeleton = x.copy()\n\n    # Max section resolution per segment\n    self.res = res\n\n    # Some placeholders\n    self._sections = []\n    self._stimuli = {}\n    self._records = {}\n    self._synapses = {}\n\n    # Generate the actual model\n    self._validate_skeleton()\n    self._generate_sections()\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.add_current_record","title":"<code>add_current_record</code>","text":"<p>Add current recording to model.</p> <p>This only works if nodes map to sections that have point processes.</p> PARAMETER DESCRIPTION <code>where</code> <pre><code>    Node ID(s) at which to record.\n</code></pre> <p> TYPE: <code>    int | list of int</code> </p> <code>label</code> <pre><code>    If label is given, this recording will be added as\n    `self.records['i'][label]` else  `self.records['i'][node_id]`.\n</code></pre> <p> TYPE: <code>    str</code> DEFAULT: <code>None</code> </p> Source code in <code>navis/interfaces/neuron/comp.py</code> <pre><code>def add_current_record(self, where, label=None):\n    \"\"\"Add current recording to model.\n\n    This only works if nodes map to sections that have point processes.\n\n    Parameters\n    ----------\n    where :     int | list of int\n                Node ID(s) at which to record.\n    label :     str, optional\n                If label is given, this recording will be added as\n                `self.records['i'][label]` else  `self.records['i'][node_id]`.\n\n    \"\"\"\n    nodes = utils.make_iterable(where)\n\n    # Map nodes to point processes\n    secs = self.get_node_segment(nodes)\n    where = []\n    for n, sec in zip(nodes, secs):\n        pp = sec.point_processes()\n        if not pp:\n            raise TypeError(f'Section for node {n} has no point process '\n                            '- unable to add current record')\n        elif len(pp) &gt; 1:\n            logger.warning(f'Section for node {n} has more than on point '\n                           'process. Recording current at first.')\n            pp = pp[:1]\n        where += pp\n\n    self._add_record(where, what='i', label=label)\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.add_spike_detector","title":"<code>add_spike_detector</code>","text":"<p>Add a spike detector at given node(s).</p> PARAMETER DESCRIPTION <code>where</code> <pre><code>    Node ID(s) at which to record.\n</code></pre> <p> TYPE: <code>    int | list of int</code> </p> <code>threshold</code> <pre><code>    Threshold in mV for a spike to be counted.\n</code></pre> <p> TYPE: <code>float</code> DEFAULT: <code>20</code> </p> <code>label</code> <pre><code>    If label is given, this recording will be added as\n    `self.records[label]` else  `self.records[node_id]`.\n</code></pre> <p> TYPE: <code>    str</code> DEFAULT: <code>None</code> </p> Source code in <code>navis/interfaces/neuron/comp.py</code> <pre><code>def add_spike_detector(self, where, threshold=20, label=None):\n    \"\"\"Add a spike detector at given node(s).\n\n    Parameters\n    ----------\n    where :     int | list of int\n                Node ID(s) at which to record.\n    threshold : float\n                Threshold in mV for a spike to be counted.\n    label :     str, optional\n                If label is given, this recording will be added as\n                `self.records[label]` else  `self.records[node_id]`.\n\n    \"\"\"\n    where = utils.make_iterable(where)\n\n    self.records['spikes'] = self.records.get('spikes', {})\n    self._spike_det = getattr(self, '_spike_det', [])\n    segments = self.get_node_segment(where)\n    sections = self.get_node_section(where)\n    for n, sec, seg in zip(where, sections, segments):\n        # Generate a NetCon object that has no target\n        sp_det = neuron.h.NetCon(seg._ref_v, None, sec=sec)\n\n        # Set threshold\n        if threshold:\n            sp_det.threshold = threshold\n\n        # Keeping track of this to save it from garbage collector\n        self._spike_det.append(sp_det)\n\n        # Create a vector for the spike timings\n        vec = neuron.h.Vector()\n        # Tell the NetCon object to record into that vector\n        sp_det.record(vec)\n\n        if label:\n            self.records['spikes'][label] = vec\n        else:\n            self.records['spikes'][n] = vec\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.add_synaptic_current","title":"<code>add_synaptic_current</code>","text":"<p>Add synaptic current(s) (AlphaSynapse) to model.</p> PARAMETER DESCRIPTION <code>where</code> <pre><code>        Node ID(s) at which to stimulate.\n</code></pre> <p> TYPE: <code>        int | list of int</code> </p> <code>start</code> <pre><code>        Onset [ms] from beginning of simulation.\n</code></pre> <p> TYPE: <code>        int</code> DEFAULT: <code>5</code> </p> <code>tau</code> <pre><code>        Decay time constant [ms].\n</code></pre> <p> TYPE: <code>          int</code> DEFAULT: <code>0.1</code> </p> <code>rev_pot</code> <pre><code>        Reverse potential (e) [mV].\n</code></pre> <p> TYPE: <code>      int</code> DEFAULT: <code>0</code> </p> <code>max_syn_cond</code> <pre><code>        Max synaptic conductance (gmax) [uS].\n</code></pre> <p> TYPE: <code> float</code> DEFAULT: <code>0.1</code> </p> Source code in <code>navis/interfaces/neuron/comp.py</code> <pre><code>def add_synaptic_current(self, where, start=5, tau=0.1, rev_pot=0,\n                         max_syn_cond=0.1):\n    \"\"\"Add synaptic current(s) (AlphaSynapse) to model.\n\n    Parameters\n    ----------\n    where :         int | list of int\n                    Node ID(s) at which to stimulate.\n    start :         int\n                    Onset [ms] from beginning of simulation.\n    tau :           int\n                    Decay time constant [ms].\n    rev_pot :       int\n                    Reverse potential (e) [mV].\n    max_syn_cond :  float\n                    Max synaptic conductance (gmax) [uS].\n\n    \"\"\"\n    self._add_stimulus('AlphaSynapse', where=where, onset=start,\n                       tau=tau, e=rev_pot, gmax=max_syn_cond)\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.add_synaptic_input","title":"<code>add_synaptic_input</code>","text":"<p>Add synaptic input to model.</p> <p>This uses the Exp2Syn synapse. All targets in <code>where</code> are triggered by the same NetStim - i.e. they will all receive their spike(s) at the same time.</p> PARAMETER DESCRIPTION <code>where</code> <pre><code>        Node IDs at which to simulate synaptic input.\n</code></pre> <p> TYPE: <code>        int | list of int</code> </p> <code>Properties</code> <p> </p> <code>start</code> <pre><code>        Onset [ms] of first spike from beginning of simulation.\n</code></pre> <p> TYPE: <code>        int</code> DEFAULT: <code>5 * ms</code> </p> <code>spike_no</code> <pre><code>        Number of presynaptic spikes to produce.\n</code></pre> <p> TYPE: <code>     int</code> DEFAULT: <code>1</code> </p> <code>spike_int</code> <pre><code>        Interval [ms] between consecutive spikes.\n</code></pre> <p> TYPE: <code>    int</code> DEFAULT: <code>10 * ms</code> </p> <code>spike_noise</code> <pre><code>        Fractional randomness in spike timing.\n</code></pre> <p> TYPE: <code>  float [0-1]</code> DEFAULT: <code>0</code> </p> <code>Synapse</code> <p> </p> <code>syn_tau1</code> <pre><code>        Rise time constant [ms].\n</code></pre> <p> TYPE: <code>     int</code> DEFAULT: <code>0.1 * ms</code> </p> <code>syn_tau2</code> <pre><code>        Decay time constant [ms].\n</code></pre> <p> TYPE: <code>     int</code> DEFAULT: <code>10 * ms</code> </p> <code>syn_rev_pot</code> <pre><code>        Reversal potential (e) [mV].\n</code></pre> <p> TYPE: <code>  int</code> DEFAULT: <code>0</code> </p> <code>Connection</code> <p> </p> <code>cn_thresh</code> <pre><code>        Presynaptic membrane potential [mV] at which synaptic\n        event is triggered.\n</code></pre> <p> TYPE: <code>    int</code> DEFAULT: <code>10</code> </p> <code>cn_delay</code> <pre><code>        Delay [ms] between presynaptic trigger and postsynaptic\n        event.\n</code></pre> <p> TYPE: <code>     int</code> DEFAULT: <code>1 * ms</code> </p> <code>cn_weight</code> <pre><code>        Weight variable. This bundles a couple of synaptic\n        properties such as e.g. how much transmitter is released\n        or binding affinity at postsynaptic receptors.\n</code></pre> <p> TYPE: <code>    float</code> DEFAULT: <code>0.05</code> </p> Source code in <code>navis/interfaces/neuron/comp.py</code> <pre><code>def add_synaptic_input(self, where, start=5 * ms,\n                       spike_no=1, spike_int=10 * ms, spike_noise=0,\n                       syn_tau1=.1 * ms, syn_tau2=10 * ms, syn_rev_pot=0,\n                       cn_thresh=10, cn_delay=1 * ms, cn_weight=0.05):\n    \"\"\"Add synaptic input to model.\n\n    This uses the Exp2Syn synapse. All targets in `where` are triggered\n    by the same NetStim - i.e. they will all receive their spike(s) at the\n    same time.\n\n    Parameters\n    ----------\n    where :         int | list of int\n                    Node IDs at which to simulate synaptic input.\n\n    Properties for presynaptic spikes:\n\n    start :         int\n                    Onset [ms] of first spike from beginning of simulation.\n    spike_no :      int\n                    Number of presynaptic spikes to produce.\n    spike_int :     int\n                    Interval [ms] between consecutive spikes.\n    spike_noise :   float [0-1]\n                    Fractional randomness in spike timing.\n\n    Synapse properties:\n\n    syn_tau1 :      int\n                    Rise time constant [ms].\n    syn_tau2 :      int\n                    Decay time constant [ms].\n    syn_rev_pot :   int\n                    Reversal potential (e) [mV].\n\n    Connection properties:\n\n    cn_thresh :     int\n                    Presynaptic membrane potential [mV] at which synaptic\n                    event is triggered.\n    cn_delay :      int\n                    Delay [ms] between presynaptic trigger and postsynaptic\n                    event.\n    cn_weight :     float\n                    Weight variable. This bundles a couple of synaptic\n                    properties such as e.g. how much transmitter is released\n                    or binding affinity at postsynaptic receptors.\n\n    \"\"\"\n    where = utils.make_iterable(where)\n\n    # Make a new stimulator\n    stim = neuron.h.NetStim()\n    stim.number = spike_no\n    stim.start = start\n    stim.noise = spike_noise\n    stim.interval = spike_int\n\n    # Connect\n    self.connect(stim, where, syn_tau1=syn_tau1, syn_tau2=syn_tau2,\n                 syn_rev_pot=syn_rev_pot, cn_thresh=cn_thresh,\n                 cn_delay=cn_delay, cn_weight=cn_weight)\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.add_voltage_record","title":"<code>add_voltage_record</code>","text":"<p>Add voltage recording to model.</p> PARAMETER DESCRIPTION <code>where</code> <pre><code>    Node ID(s) at which to record.\n</code></pre> <p> TYPE: <code>    int | list of int</code> </p> <code>label</code> <pre><code>    If label is given, this recording will be added as\n    `self.records['v'][label]` else  `self.records['v'][node_id]`.\n</code></pre> <p> TYPE: <code>    str</code> DEFAULT: <code>None</code> </p> Source code in <code>navis/interfaces/neuron/comp.py</code> <pre><code>def add_voltage_record(self, where, label=None):\n    \"\"\"Add voltage recording to model.\n\n    Parameters\n    ----------\n    where :     int | list of int\n                Node ID(s) at which to record.\n    label :     str, optional\n                If label is given, this recording will be added as\n                `self.records['v'][label]` else  `self.records['v'][node_id]`.\n\n    \"\"\"\n    self._add_record(where, what='v', label=label)\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.clear","title":"<code>clear</code>","text":"<p>Attempt to remove model from NEURON space.</p> <p>This is not guaranteed to work. Check <code>neuron.h.topology()</code> to inspect.</p> Source code in <code>navis/interfaces/neuron/comp.py</code> <pre><code>def clear(self):\n    \"\"\"Attempt to remove model from NEURON space.\n\n    This is not guaranteed to work. Check `neuron.h.topology()` to inspect.\n\n    \"\"\"\n    # Basically we have to bring the reference count to zero\n    self.clear_records()\n    self.clear_stimuli()\n    self.clear_synapses()\n    for s in self._sections:\n        del s\n    self._sections = []\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.clear_records","title":"<code>clear_records</code>","text":"<p>Clear records.</p> Source code in <code>navis/interfaces/neuron/comp.py</code> <pre><code>def clear_records(self):\n    \"\"\"Clear records.\"\"\"\n    self._records = {}\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.clear_stimuli","title":"<code>clear_stimuli</code>","text":"<p>Clear stimuli.</p> Source code in <code>navis/interfaces/neuron/comp.py</code> <pre><code>def clear_stimuli(self):\n    \"\"\"Clear stimuli.\"\"\"\n    self._stimuli = {}\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.clear_synapses","title":"<code>clear_synapses</code>","text":"<p>Clear synapses.</p> Source code in <code>navis/interfaces/neuron/comp.py</code> <pre><code>def clear_synapses(self):\n    \"\"\"Clear synapses.\"\"\"\n    self._synapses = {}\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.connect","title":"<code>connect</code>","text":"<p>Connect object to model.</p> <p>This uses the Exp2Syn synapse and treats <code>pre</code> as the presynaptic object.</p> PARAMETER DESCRIPTION <code>pre</code> <pre><code>        The presynaptic object to connect to this neuron.\n</code></pre> <p> TYPE: <code>          NetStim | section</code> </p> <code>where</code> <pre><code>        Node IDs at which to simulate synaptic input.\n</code></pre> <p> TYPE: <code>        int | list of int</code> </p> <code>Synapse</code> <p> </p> <code>syn_tau1</code> <pre><code>        Rise time constant [ms].\n</code></pre> <p> TYPE: <code>     int</code> DEFAULT: <code>0.1 * ms</code> </p> <code>syn_tau2</code> <pre><code>        Decay time constant [ms].\n</code></pre> <p> TYPE: <code>     int</code> DEFAULT: <code>10 * ms</code> </p> <code>syn_rev_pot</code> <pre><code>        Reversal potential (e) [mV].\n</code></pre> <p> TYPE: <code>  int</code> DEFAULT: <code>0</code> </p> <code>Connection</code> <p> </p> <code>cn_thresh</code> <pre><code>        Presynaptic membrane potential [mV] at which synaptic\n        event is triggered.\n</code></pre> <p> TYPE: <code>    int</code> DEFAULT: <code>10</code> </p> <code>cn_delay</code> <pre><code>        Delay [ms] between presynaptic trigger and postsynaptic\n        event.\n</code></pre> <p> TYPE: <code>     int</code> DEFAULT: <code>1 * ms</code> </p> <code>cn_weight</code> <pre><code>        Weight variable. This bundles a couple of synaptic\n        properties such as e.g. how much transmitter is released\n        or binding affinity at postsynaptic receptors.\n</code></pre> <p> TYPE: <code>    int</code> DEFAULT: <code>0</code> </p> Source code in <code>navis/interfaces/neuron/comp.py</code> <pre><code>def connect(self, pre, where, syn_tau1=.1 * ms, syn_tau2=10 * ms,\n            syn_rev_pot=0, cn_thresh=10, cn_delay=1 * ms, cn_weight=0):\n    \"\"\"Connect object to model.\n\n    This uses the Exp2Syn synapse and treats `pre` as the presynaptic\n    object.\n\n    Parameters\n    ----------\n    pre :           NetStim | section\n                    The presynaptic object to connect to this neuron.\n    where :         int | list of int\n                    Node IDs at which to simulate synaptic input.\n\n    Synapse properties:\n\n    syn_tau1 :      int\n                    Rise time constant [ms].\n    syn_tau2 :      int\n                    Decay time constant [ms].\n    syn_rev_pot :   int\n                    Reversal potential (e) [mV].\n\n    Connection properties:\n\n    cn_thresh :     int\n                    Presynaptic membrane potential [mV] at which synaptic\n                    event is triggered.\n    cn_delay :      int\n                    Delay [ms] between presynaptic trigger and postsynaptic\n                    event.\n    cn_weight :     int\n                    Weight variable. This bundles a couple of synaptic\n                    properties such as e.g. how much transmitter is released\n                    or binding affinity at postsynaptic receptors.\n\n    \"\"\"\n    where = utils.make_iterable(where)\n\n    if not is_NEURON_object(pre):\n        raise ValueError(f'Expected NEURON object, got {type(pre)}')\n\n    # Turn section into segment\n    if isinstance(pre, neuron.nrn.Section):\n        pre = pre()\n\n    # Go over the nodes\n    nodes = self.nodes.set_index('node_id')\n    for node in nodes.loc[where].itertuples():\n        # Generate synapses for the nodes in question\n        # Note that we are not reusing existing synapses\n        # in case the properties are different\n        sec = self.sections[node.sec_ix](node.sec_pos)\n        syn = neuron.h.Exp2Syn(sec)\n        syn.tau1 = syn_tau1\n        syn.tau2 = syn_tau2\n        syn.e = syn_rev_pot\n\n        self.synapses[node.Index] = self.synapses.get(node.Index, []) + [syn]\n\n        # Connect spike stimulus and synapse\n        if isinstance(pre, neuron.nrn.Segment):\n            nc = neuron.h.NetCon(pre._ref_v, syn, sec=pre.sec)\n        else:\n            nc = neuron.h.NetCon(pre, syn)\n\n        # Set connection parameters\n        nc.threshold = cn_thresh\n        nc.delay = cn_delay\n        nc.weight[0] = cn_weight\n\n        self.stimuli[node.Index] = self.stimuli.get(node.Index, []) + [nc, pre]\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.get_node_section","title":"<code>get_node_section</code>","text":"<p>Return section(s) for given node(s).</p> PARAMETER DESCRIPTION <code>node_ids</code> <pre><code>    Node IDs.\n</code></pre> <p> TYPE: <code> int | list of int</code> </p> RETURNS DESCRIPTION <code>section(s) :    segment or list of segments</code> <p>Depends on input.</p> Source code in <code>navis/interfaces/neuron/comp.py</code> <pre><code>def get_node_section(self, node_ids):\n    \"\"\"Return section(s) for given node(s).\n\n    Parameters\n    ----------\n    node_ids :  int | list of int\n                Node IDs.\n\n    Returns\n    -------\n    section(s) :    segment or list of segments\n                    Depends on input.\n\n    \"\"\"\n    nodes = self.nodes.set_index('node_id')\n    if not utils.is_iterable(node_ids):\n        n = nodes.loc[node_ids]\n        return self.sections[n.sec_ix]\n    else:\n        segs = []\n        for node in nodes.loc[node_ids].itertuples():\n            segs.append(self.sections[node.sec_ix])\n        return segs\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.get_node_segment","title":"<code>get_node_segment</code>","text":"<p>Return segment(s) for given node(s).</p> PARAMETER DESCRIPTION <code>node_ids</code> <pre><code>    Node IDs.\n</code></pre> <p> TYPE: <code> int | list of int</code> </p> RETURNS DESCRIPTION <code>segment(s) :    segment or list of segments</code> <p>Depends on input.</p> Source code in <code>navis/interfaces/neuron/comp.py</code> <pre><code>def get_node_segment(self, node_ids):\n    \"\"\"Return segment(s) for given node(s).\n\n    Parameters\n    ----------\n    node_ids :  int | list of int\n                Node IDs.\n\n    Returns\n    -------\n    segment(s) :    segment or list of segments\n                    Depends on input.\n\n    \"\"\"\n    nodes = self.nodes.set_index('node_id')\n    if not utils.is_iterable(node_ids):\n        n = nodes.loc[node_ids]\n        return self.sections[n.sec_ix](n.sec_pos)\n    else:\n        segs = []\n        for node in nodes.loc[node_ids].itertuples():\n            segs.append(self.sections[node.sec_ix](node.sec_pos))\n        return segs\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.inject_current_pulse","title":"<code>inject_current_pulse</code>","text":"<p>Add current injection (IClamp) stimulation to model.</p> PARAMETER DESCRIPTION <code>where</code> <pre><code>    Node ID(s) at which to stimulate.\n</code></pre> <p> TYPE: <code>    int | list of int</code> </p> <code>start</code> <pre><code>    Onset (delay) [ms] from beginning of simulation.\n</code></pre> <p> TYPE: <code>    int</code> DEFAULT: <code>5</code> </p> <code>duration</code> <pre><code>    Duration (dur) [ms] of injection.\n</code></pre> <p> TYPE: <code> int</code> DEFAULT: <code>1</code> </p> <code>current</code> <pre><code>    Amount (i) [nA] of injected current.\n</code></pre> <p> TYPE: <code>  float</code> DEFAULT: <code>0.1</code> </p> Source code in <code>navis/interfaces/neuron/comp.py</code> <pre><code>def inject_current_pulse(self, where, start=5,\n                         duration=1, current=0.1):\n    \"\"\"Add current injection (IClamp) stimulation to model.\n\n    Parameters\n    ----------\n    where :     int | list of int\n                Node ID(s) at which to stimulate.\n    start :     int\n                Onset (delay) [ms] from beginning of simulation.\n    duration :  int\n                Duration (dur) [ms] of injection.\n    current :   float\n                Amount (i) [nA] of injected current.\n\n    \"\"\"\n    self._add_stimulus('IClamp', where=where, delay=start,\n                       dur=duration, amp=current)\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.insert","title":"<code>insert</code>","text":"<p>Insert biophysical mechanism for model.</p> PARAMETER DESCRIPTION <code>mechanism</code> <pre><code>    Mechanism to insert - e.g. \"hh\" for Hodgkin-Huxley kinetics.\n</code></pre> <p> TYPE: <code>str</code> </p> <code>subset</code> <pre><code>    Sections (or indices thereof) to set mechanism for.\n    If `None` will add mechanism to all sections.\n</code></pre> <p> TYPE: <code>   list of sections | list of int</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <pre><code>    Use to set properties for mechanism.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> Source code in <code>navis/interfaces/neuron/comp.py</code> <pre><code>def insert(self, mechanism, subset=None, **kwargs):\n    \"\"\"Insert biophysical mechanism for model.\n\n    Parameters\n    ----------\n    mechanism : str\n                Mechanism to insert - e.g. \"hh\" for Hodgkin-Huxley kinetics.\n    subset :    list of sections | list of int\n                Sections (or indices thereof) to set mechanism for.\n                If `None` will add mechanism to all sections.\n    **kwargs\n                Use to set properties for mechanism.\n\n    \"\"\"\n    if isinstance(subset, type(None)):\n        sections = self.sections\n    else:\n        subset = utils.make_iterable(subset)\n\n        if all([is_section(s) for s in subset]):\n            sections = subset\n        elif all([isinstance(s, Number) for s in subset]):\n            sections = self.sections[subset]\n        else:\n            raise TypeError('`subset` must be None, a list of sections or '\n                            'a list of section indices')\n\n    for sec in np.unique(sections):\n        _ = sec.insert(mechanism)\n        for seg in sec:\n            mech = getattr(seg, mechanism)\n            for k, v in kwargs.items():\n                setattr(mech, k, v)\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.plot_results","title":"<code>plot_results</code>","text":"<p>Plot results.</p> PARAMETER DESCRIPTION <code>axes</code> <pre><code>    Axes to plot onto. Must have one ax for each recording\n    type (mV, spike count, etc) in `self.records`.\n</code></pre> <p> TYPE: <code>     matplotlib axes</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>axes</code> Source code in <code>navis/interfaces/neuron/comp.py</code> <pre><code>def plot_results(self, axes=None):\n    \"\"\"Plot results.\n\n    Parameters\n    ----------\n    axes :      matplotlib axes\n                Axes to plot onto. Must have one ax for each recording\n                type (mV, spike count, etc) in `self.records`.\n\n    Returns\n    -------\n    axes\n\n    \"\"\"\n    if isinstance(self.t, type(None)) or not len(self.t):\n        logger.warning('Looks like the simulation has not yet been run.')\n        return\n    if not self.records:\n        logger.warning('Nothing to plot: no recordings found.')\n        return\n\n    if not axes:\n        fig, axes = plt.subplots(len(self.records), sharex=True)\n\n    # Make sure that even a single ax is a list\n    if not isinstance(axes, (np.ndarray, list)):\n        axes = [axes] * len(self.records)\n\n    for t, ax in zip(self.records, axes):\n        for i, (k, v) in enumerate(self.records[t].items()):\n            if not len(v):\n                continue\n            v = v.as_numpy()\n            # For spikes the vector contains the times\n            if t == 'spikes':\n                # Calculate spike rate\n                bins = np.linspace(0, max(self.t), 10)\n                hist, _ = np.histogram(v, bins=bins)\n                width = bins[1] - bins[0]\n                rate = hist * (1000 / width)\n                ax.plot(bins[:-1] + (width / 2), rate, label=k)\n\n                ax.scatter(v, [-i] * len(v), marker='|', s=100)\n            else:\n                ax.plot(self.t, v, label=k)\n\n        ax.set_xlabel('time [ms]')\n        ax.set_ylabel(f'{t}')\n\n        ax.legend()\n    return axes\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.plot_structure","title":"<code>plot_structure</code>","text":"<p>Visualize structure in 3D using matplotlib.</p> Source code in <code>navis/interfaces/neuron/comp.py</code> <pre><code>def plot_structure(self):\n    \"\"\"Visualize structure in 3D using matplotlib.\"\"\"\n    _ = neuron.h.PlotShape().plot(plt)\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.run_simulation","title":"<code>run_simulation</code>","text":"<p>Run the simulation.</p> Source code in <code>navis/interfaces/neuron/comp.py</code> <pre><code>def run_simulation(self, duration=25 * ms, v_init=-65 * mV):\n    \"\"\"Run the simulation.\"\"\"\n    # Add recording of time\n    global main_t\n    main_t = neuron.h.Vector().record(neuron.h._ref_t)\n\n    # This resets the entire model space not just this neuron!\n    neuron.h.finitialize(v_init)\n    neuron.h.continuerun(duration)\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.CompartmentModel.uninsert","title":"<code>uninsert</code>","text":"<p>Remove biophysical mechanism from model.</p> PARAMETER DESCRIPTION <code>mechanism</code> <pre><code>    Mechanism to remove - e.g. \"hh\" for Hodgkin-Huxley kinetics.\n</code></pre> <p> TYPE: <code>str</code> </p> <code>subset</code> <pre><code>    Sections (or indices thereof) to set mechanism for.\n    If `None` will add mechanism to all sections.\n</code></pre> <p> TYPE: <code>   list of sections | list of int</code> DEFAULT: <code>None</code> </p> Source code in <code>navis/interfaces/neuron/comp.py</code> <pre><code>def uninsert(self, mechanism, subset=None):\n    \"\"\"Remove biophysical mechanism from model.\n\n    Parameters\n    ----------\n    mechanism : str\n                Mechanism to remove - e.g. \"hh\" for Hodgkin-Huxley kinetics.\n    subset :    list of sections | list of int\n                Sections (or indices thereof) to set mechanism for.\n                If `None` will add mechanism to all sections.\n\n    \"\"\"\n    if isinstance(subset, type(None)):\n        sections = self.sections\n    else:\n        subset = utils.make_iterable(subset)\n\n        if all([is_section(s) for s in subset]):\n            sections = subset\n        elif all([isinstance(s, Number) for s in subset]):\n            sections = self.sections[subset]\n        else:\n            raise TypeError('`subset` must be None, a list of sections or '\n                            'a list of section indices')\n\n    for sec in np.unique(sections):\n        if hasattr(sec, mechanism):\n            _ = sec.uninsert(mechanism)\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.DrosophilaPN","title":"<code>navis.interfaces.neuron.DrosophilaPN</code>","text":"<p>Compartment model of an olfactory projection neuron in Drosophila.</p> <p>This is a <code>CompartmentModel</code> that uses passive membrane properties from Tobin et al. (2017) as presets:</p> <ul> <li>specific axial resistivity (<code>Ra</code>) of 266.1 Ohm / cm</li> <li>specific membrane capacitance (<code>cm</code>) of 0.8 mF / cm**2</li> <li>specific leakage conductance (<code>g</code>) of 1/Rm</li> <li>Rm = specific membran resistance of 20800 Ohm cm**2</li> <li>leakage reverse potential of -60 mV</li> </ul> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Neuron to generate model for. Has to be in microns!\n</code></pre> <p> TYPE: <code>        navis.TreeNeuron</code> </p> <code>res</code> <pre><code>    Approximate length [um] of segments. This guarantees that\n    no section has any segment that is longer than `res` but for\n    small branches (i.e. \"sections\") the segments might be smaller.\n    Lower `res` = more detailed simulation.\n</code></pre> <p> TYPE: <code>      int</code> DEFAULT: <code>10</code> </p> Source code in <code>navis/interfaces/neuron/comp.py</code> <pre><code>class DrosophilaPN(CompartmentModel):\n    \"\"\"Compartment model of an olfactory projection neuron in Drosophila.\n\n    This is a `CompartmentModel` that uses passive membrane properties\n    from Tobin et al. (2017) as presets:\n\n    - specific axial resistivity (`Ra`) of 266.1 Ohm / cm\n    - specific membrane capacitance (`cm`) of 0.8 mF / cm**2\n    - specific leakage conductance (`g`) of 1/Rm\n    - Rm = specific membran resistance of 20800 Ohm cm**2\n    - leakage reverse potential of -60 mV\n\n    Parameters\n    ----------\n    x :         navis.TreeNeuron\n                Neuron to generate model for. Has to be in microns!\n    res :       int\n                Approximate length [um] of segments. This guarantees that\n                no section has any segment that is longer than `res` but for\n                small branches (i.e. \"sections\") the segments might be smaller.\n                Lower `res` = more detailed simulation.\n\n    \"\"\"\n\n    def __init__(self, x, res=10):\n        super().__init__(x, res=res)\n\n        self.Ra = 266.1  # specific axial resistivity in Ohm cm\n        self.cm = 0.8    # specific membrane capacitance in mF / cm**2\n\n        # Add passive membran properties\n        self.insert('pas',\n                    g=1/20800,  # specific leakage conductance = 1/Rm; Rm = specific membran resistance in Ohm cm**2\n                    e=-60,      # leakage reverse potential\n                    )\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.PointNetwork","title":"<code>navis.interfaces.neuron.PointNetwork</code>","text":"<p>A Network of Leaky-Integrate-and-Fire (LIF) point processes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis.interfaces.neuron as nrn\n&gt;&gt;&gt; N = nrn.PointNetwork()\n&gt;&gt;&gt;\n</code></pre> Source code in <code>navis/interfaces/neuron/network.py</code> <pre><code>class PointNetwork:\n    \"\"\"A Network of Leaky-Integrate-and-Fire (LIF) point processes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis.interfaces.neuron as nrn\n    &gt;&gt;&gt; N = nrn.PointNetwork()\n    &gt;&gt;&gt;\n    \"\"\"\n\n    def __init__(self):\n        self._neurons = []\n        self._neurons_dict = {}\n        self._edges = []\n        self._stimuli = []\n        self._ids = []\n        self._labels = []\n        self.idx = NetworkIdIndexer(self)\n\n    def __str__(self):\n        return f'{type(self).__name__}&lt;neurons={len(self)},edges={len(self.edges)}&gt;'\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __contains__(self, id):\n        return id in self._ids\n\n    def __getitem__(self, ix):\n        \"\"\"Get point process with given ID.\"\"\"\n        return np.asarray(self.neurons)[ix]\n\n    def __len__(self):\n        return len(self._neurons)\n\n    @property\n    def edges(self):\n        \"\"\"Edges between nodes of the network.\n\n        Returns\n        -------\n        list of tuples\n\n                `[(source_ix, target_ix, weight, NetCon object), ...]`\n\n        \"\"\"\n        return self._edges\n\n    @property\n    def neurons(self):\n        \"\"\"Neurons in the network.\n\n        Returns\n        -------\n        list\n                List of `PointNeurons`.\n\n        \"\"\"\n        return self._neurons\n\n    @property\n    def ids(self):\n        \"\"\"IDs of neurons in the network.\"\"\"\n        return self._ids\n\n    @property\n    def labels(self):\n        \"\"\"Labels of neurons in the network.\"\"\"\n        return self._labels\n\n    @classmethod\n    def from_edge_list(cls, edges, model='IntFire1', source_col='source',\n                       target_col='target', weight_col='weight', **props):\n        \"\"\"Generate network from edge list.\n\n        Parameters\n        ----------\n        edges :         pd.DataFrame\n                        Edge list. Must contain 'source', 'target' and 'weight'\n                        columns.\n        model :         \"IntFire1\" | \"IntFire2\" | \"IntFire4\"\n                        The model to use for the integrate-and-fire point processes.\n        source_col :    str\n                        Name of the column with the source IDs.\n        target_col :    str\n                        Name of the column with the target IDs.\n        weight_col :    str\n                        Name of the column with the weights. The important thing\n                        to note here is that weight is expected to be in the 0-1\n                        range with 1 effectively guaranteeing that a presynaptic\n                        spike triggers a postsynaptic spike.\n        **props\n                        Keyword arguments are passed through to `add_neurons`.\n                        Use to set e.g. labels, threshold or additional\n                        model parameters.\n\n        \"\"\"\n        assert isinstance(edges, pd.DataFrame)\n        miss = {source_col, target_col, weight_col} - set(edges.columns)\n        if miss:\n            raise ValueError(f'edge list is missing columns: {miss}')\n\n        # Instantiate\n        net = cls()\n\n        # Add neurons\n        ids = np.unique(edges[[source_col, target_col]].values)\n        net.add_neurons(ids, model=model, **props)\n\n        # Connect neurons\n        for (s, t, w) in edges[[source_col, target_col, weight_col]].values:\n            net.connect(s, t, w)\n\n        return net\n\n    def add_neurons(self, ids, model='IntFire1', labels=None,\n                    skip_existing=False, **props):\n        \"\"\"Add neurons to network.\n\n        Parameters\n        ----------\n        ids :           list-like\n                        Iterable of IDs for which to create neurons.\n        model :         \"IntFire1\" | \"IntFire2\" | \"IntFire4\"\n                        The model to use for the integrate-and-fire point processes.\n        labels :        str | list | dict, optional\n                        Labels for neurons. If str will apply the same label to\n                        all neurons. If list, must be same length as `ids`.\n                        Dictionary must be ID -&gt; label map.\n        skip_existing : bool\n                        If True, will skip existing IDs.\n        **props\n                        Additional parameters used when initializing the point\n                        processes. Depends on which model type you are using:\n                        e.g. `refrac` and `tau` for \"IntFire1\".\n\n        \"\"\"\n        assert model in [\"IntFire1\", \"IntFire2\", \"IntFire4\"]\n        model = getattr(neuron.h, model)\n\n        ids = utils.make_iterable(ids)\n\n        if isinstance(labels, dict):\n            labels = [labels.get(i, 'NA') for i in ids]\n        elif labels:\n            labels = utils.make_iterable(labels)\n        else:\n            labels = ids\n\n        if len(labels) != len(ids):\n            raise ValueError('Must provide a label for each neuron')\n\n        for i, l in zip(ids, labels):\n            if i in self:\n                if skip_existing:\n                    continue\n                else:\n                    raise ValueError(f'Neuron with id {i} already exists. '\n                                     'Try using `skip_existing=True`.')\n\n            # Create neuron\n            n = PointNeuron(id=i, model=model, label=l, **props)\n\n            self._neurons.append(n)\n            self._ids.append(i)\n            self._labels.append(l)\n            self._neurons_dict[i] = n\n\n    def add_background_noise(self, ids, frequency, randomness=.5,\n                             independent=True):\n        \"\"\"Add background noise to given neurons.\n\n        Parameters\n        ----------\n        ids :           hashable  | iterable\n                        IDs of neurons to add noise to.\n        frequency :     int | float | iterable\n                        Frequency [Hz] of background noise. If iterable must\n                        match length of `ids`.\n        randomness :    float [0-1]\n                        Randomness of spike timings.\n        independent :   bool\n                        If True (default), each neuron will get its own\n                        independent noise stimulus.\n\n        \"\"\"\n        self.add_stimulus(ids=ids, start=0, stop=9999999999,\n                          frequency=frequency, randomness=randomness,\n                          independent=independent)\n\n    def add_stimulus(self, ids, start, frequency, stop=None, duration=None,\n                     randomness=.5, independent=True, label=None, weight=1):\n        \"\"\"Add stimulus to given neurons.\n\n        Important\n        ---------\n        Stimuli are implemented via a NetStim object which provides the\n        specified stimulation (i.e. start, stop, frequency). This NetStim is\n        then connected to the neuron(s) via a NetCon. The response of the\n        neuron to the stimulus depends heavily on the `weight` of that\n        connection: too low and you won't elicit any spikes, too high and you\n        will produce higher frequencies than expected. The \"correct\" weight\n        depends on the model &amp; parameters you use for your point processes, and\n        I highly recommend you check if you get the expected stimulation.\n\n        Parameters\n        ----------\n        ids :           int | iterable\n                        IDs of neurons to add stimulus to.\n        start :         int\n                        Start time [ms] for the stimulus. Note that\n                        the exact start and stop time will vary depending on\n                        `randomness`.\n        stop/duration : int\n                        Either stop time or duration of stimulus [ms]. Must\n                        provide one or the other but not both.\n        frequency :     int | iterable\n                        Frequency [Hz] of background noise . If iterable must\n                        match length of `ids`. Values &lt;= 0 are silently skipped.\n        randomness :    float [0-1]\n                        Randomness of spike timings.\n        independent :   bool\n                        If True (default), each neuron will get its own\n                        independent stimulus.\n        label :         str, optional\n                        A label to identify the stimulus.\n        weight :        float\n                        Weight for the connection between the stimulator and\n                        the neuron. This really should be 1 to make sure each\n                        spike in the stimulus elicits a spike in the target.\n\n        \"\"\"\n        ids = utils.make_iterable(ids)\n        if not utils.is_iterable(frequency):\n            frequency = [frequency] * len(ids)\n        elif not independent:\n            raise ValueError('Stimuli/noises must be independent when '\n                             'providing individual frequencies')\n\n        if len(frequency) != len(ids):\n            raise ValueError('Must provide either a single frequency for all '\n                             'neurons or a frequency for each neuron.')\n\n        if (duration and stop) or (not duration and not stop):\n            raise ValueError('Must provide either duration or stop (but not both).')\n\n        if stop:\n            duration = stop - start\n        else:\n            stop = start + duration\n\n        if duration &lt;= 0:\n            raise ValueError(f'Duration must be greater than zero.')\n\n        if not independent:\n            ns = neuron.h.NetStim()\n            ns.interval = 1000 / frequency[0]\n            ns.noise = randomness\n            ns.number = int(duration / 1000 * frequency[0])\n            ns.start = start\n\n        for i, f in zip(ids, frequency):\n            # Skip frequencies lower than 0\n            if f &lt;= 0:\n                continue\n\n            interval = 1000 / f\n            proc = self.idx[i].process\n\n            if independent:\n                ns = neuron.h.NetStim()\n                ns.interval = interval\n                ns.noise = randomness\n                ns.number = int(duration / 1000 * f)\n                ns.start = start\n\n            nc = neuron.h.NetCon(ns, proc)\n            nc.weight[0] = weight\n            nc.delay = 0\n\n            self._stimuli.append(Stimulus(start, stop, f, randomness, i, ns, nc, label))\n\n    def clear_stimuli(self):\n        \"\"\"Clear stimuli.\"\"\"\n        self._stimuli = []\n\n    def connect(self, source, target, weight, delay=5):\n        \"\"\"Connect two neurons.\n\n        Parameters\n        ----------\n        source :    int | str\n                    ID of the source.\n        target :    int | str\n                    ID of the target\n        weight :    float\n                    Weight of the edge. The important thing to note here is that\n                    the weight is expected to be in the 0-1 range with 1\n                    effectively guaranteeing that a presynaptic spike triggers\n                    a postsynaptic spike.\n        delay :     int\n                    Delay in ms between a pre- and a postsynaptic spike.\n\n        \"\"\"\n        # Get the point processes corresponding to source and target\n        pre = self.idx[source]\n        post = self.idx[target]\n\n        # Connect\n        nc = neuron.h.NetCon(pre.process, post.process)\n        nc.weight[0] = weight\n        nc.delay = delay\n\n        # Keep track\n        self._edges.append([source, target, weight, nc])\n\n    def plot_raster(self, subset=None, group=False, stimuli=True, ax=None, label=False,\n                    backend='auto', **kwargs):\n        \"\"\"Raster plot of spike timings.\n\n        Parameters\n        ----------\n        subset :        list-like\n                        Subset of IDs to plot. You can also use this to\n                        determine the order of appearance.\n        ax :            matplotlib axis |  plotly figure, optional\n                        Axis/figure to plot onto.\n        label :         bool\n                        Whether to label individual neurons.\n        backend :       \"auto\" | \"plotly\" | \"matplotlib\"\n                        Which backend to use. If \"auto\" will use plotly in\n                        Jupyter environments and matplotlib elsewhere.\n\n        \"\"\"\n        if not isinstance(subset, type(None)):\n            ids = utils.make_iterable(subset)\n            if not len(ids):\n                raise ValueError('`ids` must not be empty')\n        else:\n            ids = self._ids\n\n        # Collect spike timings\n        x = []\n        y = []\n        i = 0\n        for id in ids:\n            pp = self.idx[id]\n            x += list(pp.spk_timings)\n            y += [i] * len(pp.spk_timings)\n            i += 1\n\n        if not x:\n            raise ValueError('No spikes detected.')\n\n        if label:\n            ld = dict(zip(self._ids, self._labels))\n            labels = [ld[i] for i in ids]\n        else:\n            labels = None\n\n        # Turn into lines\n        x = np.vstack((x, x, [None] * len(x))).T.flatten()\n        y = np.array(y)\n        y = np.vstack((y, y + .9, [None] * len(y))).T.flatten()\n\n        if backend == 'auto':\n            if utils.is_jupyter():\n                backend = 'plotly'\n            else:\n                backend = 'matplotlib'\n\n        if backend == 'plotly':\n            return _plot_raster_plotly(x, y, ids, fig=ax, labels=labels, **kwargs)\n        elif backend == 'matplotlib':\n            ax = _plot_raster_mpl(x, y, ids, ax=ax, labels=labels,\n                                  stimuli=self._stimuli if stimuli else None,\n                                  **kwargs)\n            ax.set_xlim(0, neuron.h.t)\n            return ax\n        else:\n            raise ValueError(f'Unknown backend \"{backend}\"')\n\n    def plot_traces(self, bin_size=100, subset=None, rolling_window=None,\n                    group=False, stimuli=True, ax=None, backend='auto', **kwargs):\n        \"\"\"Plot mean firing rate.\n\n        Parameters\n        ----------\n        bin_size :          int\n                            Size [ms] of the bins over which to average spike\n                            frequency.\n        subset :            list-like\n                            Subset of IDs to plot. You can also use this to\n                            determine the order of appearance.\n        rolling_window :    int\n                            Average firing rates over a given rolling window.\n        group :             bool | array\n                            Set to True to group traces by label, showing mean\n                            firing rate and standard error as envelope. Pass\n                            an array of labels for each neuron to group by\n                            arbitrary labels.\n        ax :                matplotlib axis | plotly figure, optional\n                            Axis/figure to plot onto.\n        stimuli :           bool\n                            Whether to plot stimuli.\n        backend :           \"auto\" | \"plotly\" | \"matplotlib\"\n                            Which backend to use. If \"auto\" will use plotly in\n                            Jupyter environments and matplotlib elsewhere.\n\n        \"\"\"\n        if not isinstance(subset, type(None)):\n            ids = utils.make_iterable(subset)\n            if not len(ids):\n                raise ValueError('`ids` must not be empty')\n        else:\n            ids = self._ids\n\n        # Collect spike frequencies\n        spks = self.get_spike_counts(bin_size=bin_size, subset=subset,\n                                     rolling_window=rolling_window)\n        freq = spks * 1000 / bin_size\n\n        if self.labels:\n            ld = dict(zip(self._ids, self._labels))\n            labels = [f'{i} ({ld[i]})' for i in ids]\n        else:\n            labels = None\n\n        if backend == 'auto':\n            if utils.is_jupyter():\n                backend = 'plotly'\n            else:\n                backend = 'matplotlib'\n\n        if isinstance(group, bool) and group:\n            sem = freq.groupby(dict(zip(self._ids, self._labels))).sem()\n            freq = freq.groupby(dict(zip(self._ids, self._labels))).mean()\n            labels = freq.index.values.tolist()\n        elif not isinstance(group, bool):\n            sem = freq.groupby(group).sem()\n            freq = freq.groupby(group).mean()\n            labels = freq.index.values.tolist()\n        else:\n            sem = None\n\n        if backend == 'plotly':\n            return _plot_traces_plotly(freq, fig=ax, labels=labels,\n                                       stimuli=self._stimuli if stimuli else None,\n                                       env=sem, **kwargs)\n        elif backend == 'matplotlib':\n            return _plot_traces_mpl(freq, ax=ax, env=sem,\n                                    stimuli=self._stimuli if stimuli else None,\n                                    **kwargs)\n        else:\n            raise ValueError(f'Unknown backend \"{backend}\"')\n\n    def set_labels(self, labels):\n        \"\"\"Set labels for neurons.\n\n        Parameters\n        ----------\n        labels :    dict | list-like\n                    If list, must provide a label for every neuron.\n\n        \"\"\"\n        if isinstance(labels, dict):\n            for i, n in enumerate(self.neurons):\n                n.label = self._labels[i] = labels.get(n.id, n.id)\n        elif utils.is_iterable(labels):\n            if len(labels) != len(self.neurons):\n                raise ValueError(f'Got {len(labels)} labels for {len(self)} neurons.')\n            for i, n in enumerate(self.neurons):\n                n.label = self._labels[i] = labels[i]\n        else:\n            raise TypeError(f'`labels` must be dict or list-like, got \"{type(labels)}\"')\n\n    def run_simulation(self, duration=25 * ms, v_init=-65 * mV):\n        \"\"\"Run the simulation.\"\"\"\n\n        # This resets the entire model space not just this neuron!\n        neuron.h.finitialize(v_init)\n        neuron.h.continuerun(duration)\n\n    def get_spike_counts(self, bin_size=50, subset=None, rolling_window=None,\n                         group=False):\n        \"\"\"Get matrix of spike counts.\n\n        Parameters\n        ----------\n        bin_size :          int | None\n                            Size [ms] of the bins over which to count spikes.\n                            If None, will simply return total counts.\n        rolling_window :    int, optional\n                            Average spike counts in a rolling window.\n        group :             bool\n                            If True, will return the spike counts per unique\n                            label.\n\n        Returns\n        -------\n        pd.DataFrame\n\n        \"\"\"\n        if not isinstance(subset, type(None)):\n            ids = utils.make_iterable(subset)\n        else:\n            ids = self._ids\n\n        end_time = neuron.h.t\n\n        if not end_time:\n            raise ValueError('Looks like simulation has not yet been run.')\n\n        if not bin_size:\n            bin_size = end_time\n\n        bins = np.arange(0, end_time + bin_size, bin_size)\n        counts = np.zeros((len(ids), len(bins) - 1))\n        # Collect spike counts\n        for i, id in enumerate(ids):\n            pp = self.idx[id]\n            timings = list(pp.spk_timings)\n            if timings:\n                hist, _ = np.histogram(timings, bins)\n                counts[i, :] = hist\n\n        counts = pd.DataFrame(counts, index=ids, columns=bins[1:])\n\n        if group:\n            if not self._labels:\n                raise ValueError('Unable to group: Network has no labels.')\n            counts = counts.groupby(counts.index.map(dict(zip(self.ids, self._labels)))).sum()\n\n        if rolling_window:\n            avg = sliding_window_view(counts, rolling_window, axis=1).mean(axis=2)\n            counts.iloc[:, :-(rolling_window - 1)] = avg\n\n        return counts\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.PointNetwork.edges","title":"<code>edges</code>  <code>property</code>","text":"<p>Edges between nodes of the network.</p> RETURNS DESCRIPTION <code>list of tuples</code> <p><code>[(source_ix, target_ix, weight, NetCon object), ...]</code></p>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.PointNetwork.ids","title":"<code>ids</code>  <code>property</code>","text":"<p>IDs of neurons in the network.</p>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.PointNetwork.labels","title":"<code>labels</code>  <code>property</code>","text":"<p>Labels of neurons in the network.</p>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.PointNetwork.neurons","title":"<code>neurons</code>  <code>property</code>","text":"<p>Neurons in the network.</p> RETURNS DESCRIPTION <code>list</code> <p>List of <code>PointNeurons</code>.</p>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.PointNetwork.add_background_noise","title":"<code>add_background_noise</code>","text":"<p>Add background noise to given neurons.</p> PARAMETER DESCRIPTION <code>ids</code> <pre><code>        IDs of neurons to add noise to.\n</code></pre> <p> TYPE: <code>          hashable  | iterable</code> </p> <code>frequency</code> <pre><code>        Frequency [Hz] of background noise. If iterable must\n        match length of `ids`.\n</code></pre> <p> TYPE: <code>    int | float | iterable</code> </p> <code>randomness</code> <pre><code>        Randomness of spike timings.\n</code></pre> <p> TYPE: <code>   float [0-1]</code> DEFAULT: <code>0.5</code> </p> <code>independent</code> <pre><code>        If True (default), each neuron will get its own\n        independent noise stimulus.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>True</code> </p> Source code in <code>navis/interfaces/neuron/network.py</code> <pre><code>def add_background_noise(self, ids, frequency, randomness=.5,\n                         independent=True):\n    \"\"\"Add background noise to given neurons.\n\n    Parameters\n    ----------\n    ids :           hashable  | iterable\n                    IDs of neurons to add noise to.\n    frequency :     int | float | iterable\n                    Frequency [Hz] of background noise. If iterable must\n                    match length of `ids`.\n    randomness :    float [0-1]\n                    Randomness of spike timings.\n    independent :   bool\n                    If True (default), each neuron will get its own\n                    independent noise stimulus.\n\n    \"\"\"\n    self.add_stimulus(ids=ids, start=0, stop=9999999999,\n                      frequency=frequency, randomness=randomness,\n                      independent=independent)\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.PointNetwork.add_neurons","title":"<code>add_neurons</code>","text":"<p>Add neurons to network.</p> PARAMETER DESCRIPTION <code>ids</code> <pre><code>        Iterable of IDs for which to create neurons.\n</code></pre> <p> TYPE: <code>          list-like</code> </p> <code>model</code> <pre><code>        The model to use for the integrate-and-fire point processes.\n</code></pre> <p> TYPE: <code>        \"IntFire1\" | \"IntFire2\" | \"IntFire4\"</code> DEFAULT: <code>'IntFire1'</code> </p> <code>labels</code> <pre><code>        Labels for neurons. If str will apply the same label to\n        all neurons. If list, must be same length as `ids`.\n        Dictionary must be ID -&gt; label map.\n</code></pre> <p> TYPE: <code>       str | list | dict</code> DEFAULT: <code>None</code> </p> <code>skip_existing</code> <pre><code>        If True, will skip existing IDs.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>**props</code> <pre><code>        Additional parameters used when initializing the point\n        processes. Depends on which model type you are using:\n        e.g. `refrac` and `tau` for \"IntFire1\".\n</code></pre> <p> DEFAULT: <code>{}</code> </p> Source code in <code>navis/interfaces/neuron/network.py</code> <pre><code>def add_neurons(self, ids, model='IntFire1', labels=None,\n                skip_existing=False, **props):\n    \"\"\"Add neurons to network.\n\n    Parameters\n    ----------\n    ids :           list-like\n                    Iterable of IDs for which to create neurons.\n    model :         \"IntFire1\" | \"IntFire2\" | \"IntFire4\"\n                    The model to use for the integrate-and-fire point processes.\n    labels :        str | list | dict, optional\n                    Labels for neurons. If str will apply the same label to\n                    all neurons. If list, must be same length as `ids`.\n                    Dictionary must be ID -&gt; label map.\n    skip_existing : bool\n                    If True, will skip existing IDs.\n    **props\n                    Additional parameters used when initializing the point\n                    processes. Depends on which model type you are using:\n                    e.g. `refrac` and `tau` for \"IntFire1\".\n\n    \"\"\"\n    assert model in [\"IntFire1\", \"IntFire2\", \"IntFire4\"]\n    model = getattr(neuron.h, model)\n\n    ids = utils.make_iterable(ids)\n\n    if isinstance(labels, dict):\n        labels = [labels.get(i, 'NA') for i in ids]\n    elif labels:\n        labels = utils.make_iterable(labels)\n    else:\n        labels = ids\n\n    if len(labels) != len(ids):\n        raise ValueError('Must provide a label for each neuron')\n\n    for i, l in zip(ids, labels):\n        if i in self:\n            if skip_existing:\n                continue\n            else:\n                raise ValueError(f'Neuron with id {i} already exists. '\n                                 'Try using `skip_existing=True`.')\n\n        # Create neuron\n        n = PointNeuron(id=i, model=model, label=l, **props)\n\n        self._neurons.append(n)\n        self._ids.append(i)\n        self._labels.append(l)\n        self._neurons_dict[i] = n\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.PointNetwork.add_stimulus","title":"<code>add_stimulus</code>","text":"<p>Add stimulus to given neurons.</p> Important <p>Stimuli are implemented via a NetStim object which provides the specified stimulation (i.e. start, stop, frequency). This NetStim is then connected to the neuron(s) via a NetCon. The response of the neuron to the stimulus depends heavily on the <code>weight</code> of that connection: too low and you won't elicit any spikes, too high and you will produce higher frequencies than expected. The \"correct\" weight depends on the model &amp; parameters you use for your point processes, and I highly recommend you check if you get the expected stimulation.</p> PARAMETER DESCRIPTION <code>ids</code> <pre><code>        IDs of neurons to add stimulus to.\n</code></pre> <p> TYPE: <code>          int | iterable</code> </p> <code>start</code> <pre><code>        Start time [ms] for the stimulus. Note that\n        the exact start and stop time will vary depending on\n        `randomness`.\n</code></pre> <p> TYPE: <code>        int</code> </p> <code>stop</code> <pre><code>        Either stop time or duration of stimulus [ms]. Must\n        provide one or the other but not both.\n</code></pre> <p> DEFAULT: <code>None</code> </p> <code>frequency</code> <pre><code>        Frequency [Hz] of background noise . If iterable must\n        match length of `ids`. Values &lt;= 0 are silently skipped.\n</code></pre> <p> TYPE: <code>    int | iterable</code> </p> <code>randomness</code> <pre><code>        Randomness of spike timings.\n</code></pre> <p> TYPE: <code>   float [0-1]</code> DEFAULT: <code>0.5</code> </p> <code>independent</code> <pre><code>        If True (default), each neuron will get its own\n        independent stimulus.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>True</code> </p> <code>label</code> <pre><code>        A label to identify the stimulus.\n</code></pre> <p> TYPE: <code>        str</code> DEFAULT: <code>None</code> </p> <code>weight</code> <pre><code>        Weight for the connection between the stimulator and\n        the neuron. This really should be 1 to make sure each\n        spike in the stimulus elicits a spike in the target.\n</code></pre> <p> TYPE: <code>       float</code> DEFAULT: <code>1</code> </p> Source code in <code>navis/interfaces/neuron/network.py</code> <pre><code>def add_stimulus(self, ids, start, frequency, stop=None, duration=None,\n                 randomness=.5, independent=True, label=None, weight=1):\n    \"\"\"Add stimulus to given neurons.\n\n    Important\n    ---------\n    Stimuli are implemented via a NetStim object which provides the\n    specified stimulation (i.e. start, stop, frequency). This NetStim is\n    then connected to the neuron(s) via a NetCon. The response of the\n    neuron to the stimulus depends heavily on the `weight` of that\n    connection: too low and you won't elicit any spikes, too high and you\n    will produce higher frequencies than expected. The \"correct\" weight\n    depends on the model &amp; parameters you use for your point processes, and\n    I highly recommend you check if you get the expected stimulation.\n\n    Parameters\n    ----------\n    ids :           int | iterable\n                    IDs of neurons to add stimulus to.\n    start :         int\n                    Start time [ms] for the stimulus. Note that\n                    the exact start and stop time will vary depending on\n                    `randomness`.\n    stop/duration : int\n                    Either stop time or duration of stimulus [ms]. Must\n                    provide one or the other but not both.\n    frequency :     int | iterable\n                    Frequency [Hz] of background noise . If iterable must\n                    match length of `ids`. Values &lt;= 0 are silently skipped.\n    randomness :    float [0-1]\n                    Randomness of spike timings.\n    independent :   bool\n                    If True (default), each neuron will get its own\n                    independent stimulus.\n    label :         str, optional\n                    A label to identify the stimulus.\n    weight :        float\n                    Weight for the connection between the stimulator and\n                    the neuron. This really should be 1 to make sure each\n                    spike in the stimulus elicits a spike in the target.\n\n    \"\"\"\n    ids = utils.make_iterable(ids)\n    if not utils.is_iterable(frequency):\n        frequency = [frequency] * len(ids)\n    elif not independent:\n        raise ValueError('Stimuli/noises must be independent when '\n                         'providing individual frequencies')\n\n    if len(frequency) != len(ids):\n        raise ValueError('Must provide either a single frequency for all '\n                         'neurons or a frequency for each neuron.')\n\n    if (duration and stop) or (not duration and not stop):\n        raise ValueError('Must provide either duration or stop (but not both).')\n\n    if stop:\n        duration = stop - start\n    else:\n        stop = start + duration\n\n    if duration &lt;= 0:\n        raise ValueError(f'Duration must be greater than zero.')\n\n    if not independent:\n        ns = neuron.h.NetStim()\n        ns.interval = 1000 / frequency[0]\n        ns.noise = randomness\n        ns.number = int(duration / 1000 * frequency[0])\n        ns.start = start\n\n    for i, f in zip(ids, frequency):\n        # Skip frequencies lower than 0\n        if f &lt;= 0:\n            continue\n\n        interval = 1000 / f\n        proc = self.idx[i].process\n\n        if independent:\n            ns = neuron.h.NetStim()\n            ns.interval = interval\n            ns.noise = randomness\n            ns.number = int(duration / 1000 * f)\n            ns.start = start\n\n        nc = neuron.h.NetCon(ns, proc)\n        nc.weight[0] = weight\n        nc.delay = 0\n\n        self._stimuli.append(Stimulus(start, stop, f, randomness, i, ns, nc, label))\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.PointNetwork.clear_stimuli","title":"<code>clear_stimuli</code>","text":"<p>Clear stimuli.</p> Source code in <code>navis/interfaces/neuron/network.py</code> <pre><code>def clear_stimuli(self):\n    \"\"\"Clear stimuli.\"\"\"\n    self._stimuli = []\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.PointNetwork.connect","title":"<code>connect</code>","text":"<p>Connect two neurons.</p> PARAMETER DESCRIPTION <code>source</code> <pre><code>    ID of the source.\n</code></pre> <p> TYPE: <code>   int | str</code> </p> <code>target</code> <pre><code>    ID of the target\n</code></pre> <p> TYPE: <code>   int | str</code> </p> <code>weight</code> <pre><code>    Weight of the edge. The important thing to note here is that\n    the weight is expected to be in the 0-1 range with 1\n    effectively guaranteeing that a presynaptic spike triggers\n    a postsynaptic spike.\n</code></pre> <p> TYPE: <code>   float</code> </p> <code>delay</code> <pre><code>    Delay in ms between a pre- and a postsynaptic spike.\n</code></pre> <p> TYPE: <code>    int</code> DEFAULT: <code>5</code> </p> Source code in <code>navis/interfaces/neuron/network.py</code> <pre><code>def connect(self, source, target, weight, delay=5):\n    \"\"\"Connect two neurons.\n\n    Parameters\n    ----------\n    source :    int | str\n                ID of the source.\n    target :    int | str\n                ID of the target\n    weight :    float\n                Weight of the edge. The important thing to note here is that\n                the weight is expected to be in the 0-1 range with 1\n                effectively guaranteeing that a presynaptic spike triggers\n                a postsynaptic spike.\n    delay :     int\n                Delay in ms between a pre- and a postsynaptic spike.\n\n    \"\"\"\n    # Get the point processes corresponding to source and target\n    pre = self.idx[source]\n    post = self.idx[target]\n\n    # Connect\n    nc = neuron.h.NetCon(pre.process, post.process)\n    nc.weight[0] = weight\n    nc.delay = delay\n\n    # Keep track\n    self._edges.append([source, target, weight, nc])\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.PointNetwork.from_edge_list","title":"<code>from_edge_list</code>  <code>classmethod</code>","text":"<p>Generate network from edge list.</p> PARAMETER DESCRIPTION <code>edges</code> <pre><code>        Edge list. Must contain 'source', 'target' and 'weight'\n        columns.\n</code></pre> <p> TYPE: <code>        pd.DataFrame</code> </p> <code>model</code> <pre><code>        The model to use for the integrate-and-fire point processes.\n</code></pre> <p> TYPE: <code>        \"IntFire1\" | \"IntFire2\" | \"IntFire4\"</code> DEFAULT: <code>'IntFire1'</code> </p> <code>source_col</code> <pre><code>        Name of the column with the source IDs.\n</code></pre> <p> TYPE: <code>   str</code> DEFAULT: <code>'source'</code> </p> <code>target_col</code> <pre><code>        Name of the column with the target IDs.\n</code></pre> <p> TYPE: <code>   str</code> DEFAULT: <code>'target'</code> </p> <code>weight_col</code> <pre><code>        Name of the column with the weights. The important thing\n        to note here is that weight is expected to be in the 0-1\n        range with 1 effectively guaranteeing that a presynaptic\n        spike triggers a postsynaptic spike.\n</code></pre> <p> TYPE: <code>   str</code> DEFAULT: <code>'weight'</code> </p> <code>**props</code> <pre><code>        Keyword arguments are passed through to `add_neurons`.\n        Use to set e.g. labels, threshold or additional\n        model parameters.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> Source code in <code>navis/interfaces/neuron/network.py</code> <pre><code>@classmethod\ndef from_edge_list(cls, edges, model='IntFire1', source_col='source',\n                   target_col='target', weight_col='weight', **props):\n    \"\"\"Generate network from edge list.\n\n    Parameters\n    ----------\n    edges :         pd.DataFrame\n                    Edge list. Must contain 'source', 'target' and 'weight'\n                    columns.\n    model :         \"IntFire1\" | \"IntFire2\" | \"IntFire4\"\n                    The model to use for the integrate-and-fire point processes.\n    source_col :    str\n                    Name of the column with the source IDs.\n    target_col :    str\n                    Name of the column with the target IDs.\n    weight_col :    str\n                    Name of the column with the weights. The important thing\n                    to note here is that weight is expected to be in the 0-1\n                    range with 1 effectively guaranteeing that a presynaptic\n                    spike triggers a postsynaptic spike.\n    **props\n                    Keyword arguments are passed through to `add_neurons`.\n                    Use to set e.g. labels, threshold or additional\n                    model parameters.\n\n    \"\"\"\n    assert isinstance(edges, pd.DataFrame)\n    miss = {source_col, target_col, weight_col} - set(edges.columns)\n    if miss:\n        raise ValueError(f'edge list is missing columns: {miss}')\n\n    # Instantiate\n    net = cls()\n\n    # Add neurons\n    ids = np.unique(edges[[source_col, target_col]].values)\n    net.add_neurons(ids, model=model, **props)\n\n    # Connect neurons\n    for (s, t, w) in edges[[source_col, target_col, weight_col]].values:\n        net.connect(s, t, w)\n\n    return net\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.PointNetwork.get_spike_counts","title":"<code>get_spike_counts</code>","text":"<p>Get matrix of spike counts.</p> PARAMETER DESCRIPTION <code>bin_size</code> <pre><code>            Size [ms] of the bins over which to count spikes.\n            If None, will simply return total counts.\n</code></pre> <p> TYPE: <code>         int | None</code> DEFAULT: <code>50</code> </p> <code>rolling_window</code> <pre><code>            Average spike counts in a rolling window.\n</code></pre> <p> TYPE: <code>   int</code> DEFAULT: <code>None</code> </p> <code>group</code> <pre><code>            If True, will return the spike counts per unique\n            label.\n</code></pre> <p> TYPE: <code>            bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>pd.DataFrame</code> Source code in <code>navis/interfaces/neuron/network.py</code> <pre><code>def get_spike_counts(self, bin_size=50, subset=None, rolling_window=None,\n                     group=False):\n    \"\"\"Get matrix of spike counts.\n\n    Parameters\n    ----------\n    bin_size :          int | None\n                        Size [ms] of the bins over which to count spikes.\n                        If None, will simply return total counts.\n    rolling_window :    int, optional\n                        Average spike counts in a rolling window.\n    group :             bool\n                        If True, will return the spike counts per unique\n                        label.\n\n    Returns\n    -------\n    pd.DataFrame\n\n    \"\"\"\n    if not isinstance(subset, type(None)):\n        ids = utils.make_iterable(subset)\n    else:\n        ids = self._ids\n\n    end_time = neuron.h.t\n\n    if not end_time:\n        raise ValueError('Looks like simulation has not yet been run.')\n\n    if not bin_size:\n        bin_size = end_time\n\n    bins = np.arange(0, end_time + bin_size, bin_size)\n    counts = np.zeros((len(ids), len(bins) - 1))\n    # Collect spike counts\n    for i, id in enumerate(ids):\n        pp = self.idx[id]\n        timings = list(pp.spk_timings)\n        if timings:\n            hist, _ = np.histogram(timings, bins)\n            counts[i, :] = hist\n\n    counts = pd.DataFrame(counts, index=ids, columns=bins[1:])\n\n    if group:\n        if not self._labels:\n            raise ValueError('Unable to group: Network has no labels.')\n        counts = counts.groupby(counts.index.map(dict(zip(self.ids, self._labels)))).sum()\n\n    if rolling_window:\n        avg = sliding_window_view(counts, rolling_window, axis=1).mean(axis=2)\n        counts.iloc[:, :-(rolling_window - 1)] = avg\n\n    return counts\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.PointNetwork.plot_raster","title":"<code>plot_raster</code>","text":"<p>Raster plot of spike timings.</p> PARAMETER DESCRIPTION <code>subset</code> <pre><code>        Subset of IDs to plot. You can also use this to\n        determine the order of appearance.\n</code></pre> <p> TYPE: <code>       list-like</code> DEFAULT: <code>None</code> </p> <code>ax</code> <pre><code>        Axis/figure to plot onto.\n</code></pre> <p> TYPE: <code>           matplotlib axis |  plotly figure</code> DEFAULT: <code>None</code> </p> <code>label</code> <pre><code>        Whether to label individual neurons.\n</code></pre> <p> TYPE: <code>        bool</code> DEFAULT: <code>False</code> </p> <code>backend</code> <pre><code>        Which backend to use. If \"auto\" will use plotly in\n        Jupyter environments and matplotlib elsewhere.\n</code></pre> <p> TYPE: <code>      \"auto\" | \"plotly\" | \"matplotlib\"</code> DEFAULT: <code>'auto'</code> </p> Source code in <code>navis/interfaces/neuron/network.py</code> <pre><code>def plot_raster(self, subset=None, group=False, stimuli=True, ax=None, label=False,\n                backend='auto', **kwargs):\n    \"\"\"Raster plot of spike timings.\n\n    Parameters\n    ----------\n    subset :        list-like\n                    Subset of IDs to plot. You can also use this to\n                    determine the order of appearance.\n    ax :            matplotlib axis |  plotly figure, optional\n                    Axis/figure to plot onto.\n    label :         bool\n                    Whether to label individual neurons.\n    backend :       \"auto\" | \"plotly\" | \"matplotlib\"\n                    Which backend to use. If \"auto\" will use plotly in\n                    Jupyter environments and matplotlib elsewhere.\n\n    \"\"\"\n    if not isinstance(subset, type(None)):\n        ids = utils.make_iterable(subset)\n        if not len(ids):\n            raise ValueError('`ids` must not be empty')\n    else:\n        ids = self._ids\n\n    # Collect spike timings\n    x = []\n    y = []\n    i = 0\n    for id in ids:\n        pp = self.idx[id]\n        x += list(pp.spk_timings)\n        y += [i] * len(pp.spk_timings)\n        i += 1\n\n    if not x:\n        raise ValueError('No spikes detected.')\n\n    if label:\n        ld = dict(zip(self._ids, self._labels))\n        labels = [ld[i] for i in ids]\n    else:\n        labels = None\n\n    # Turn into lines\n    x = np.vstack((x, x, [None] * len(x))).T.flatten()\n    y = np.array(y)\n    y = np.vstack((y, y + .9, [None] * len(y))).T.flatten()\n\n    if backend == 'auto':\n        if utils.is_jupyter():\n            backend = 'plotly'\n        else:\n            backend = 'matplotlib'\n\n    if backend == 'plotly':\n        return _plot_raster_plotly(x, y, ids, fig=ax, labels=labels, **kwargs)\n    elif backend == 'matplotlib':\n        ax = _plot_raster_mpl(x, y, ids, ax=ax, labels=labels,\n                              stimuli=self._stimuli if stimuli else None,\n                              **kwargs)\n        ax.set_xlim(0, neuron.h.t)\n        return ax\n    else:\n        raise ValueError(f'Unknown backend \"{backend}\"')\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.PointNetwork.plot_traces","title":"<code>plot_traces</code>","text":"<p>Plot mean firing rate.</p> PARAMETER DESCRIPTION <code>bin_size</code> <pre><code>            Size [ms] of the bins over which to average spike\n            frequency.\n</code></pre> <p> TYPE: <code>         int</code> DEFAULT: <code>100</code> </p> <code>subset</code> <pre><code>            Subset of IDs to plot. You can also use this to\n            determine the order of appearance.\n</code></pre> <p> TYPE: <code>           list-like</code> DEFAULT: <code>None</code> </p> <code>rolling_window</code> <pre><code>            Average firing rates over a given rolling window.\n</code></pre> <p> TYPE: <code>   int</code> DEFAULT: <code>None</code> </p> <code>group</code> <pre><code>            Set to True to group traces by label, showing mean\n            firing rate and standard error as envelope. Pass\n            an array of labels for each neuron to group by\n            arbitrary labels.\n</code></pre> <p> TYPE: <code>            bool | array</code> DEFAULT: <code>False</code> </p> <code>ax</code> <pre><code>            Axis/figure to plot onto.\n</code></pre> <p> TYPE: <code>               matplotlib axis | plotly figure</code> DEFAULT: <code>None</code> </p> <code>stimuli</code> <pre><code>            Whether to plot stimuli.\n</code></pre> <p> TYPE: <code>          bool</code> DEFAULT: <code>True</code> </p> <code>backend</code> <pre><code>            Which backend to use. If \"auto\" will use plotly in\n            Jupyter environments and matplotlib elsewhere.\n</code></pre> <p> TYPE: <code>          \"auto\" | \"plotly\" | \"matplotlib\"</code> DEFAULT: <code>'auto'</code> </p> Source code in <code>navis/interfaces/neuron/network.py</code> <pre><code>def plot_traces(self, bin_size=100, subset=None, rolling_window=None,\n                group=False, stimuli=True, ax=None, backend='auto', **kwargs):\n    \"\"\"Plot mean firing rate.\n\n    Parameters\n    ----------\n    bin_size :          int\n                        Size [ms] of the bins over which to average spike\n                        frequency.\n    subset :            list-like\n                        Subset of IDs to plot. You can also use this to\n                        determine the order of appearance.\n    rolling_window :    int\n                        Average firing rates over a given rolling window.\n    group :             bool | array\n                        Set to True to group traces by label, showing mean\n                        firing rate and standard error as envelope. Pass\n                        an array of labels for each neuron to group by\n                        arbitrary labels.\n    ax :                matplotlib axis | plotly figure, optional\n                        Axis/figure to plot onto.\n    stimuli :           bool\n                        Whether to plot stimuli.\n    backend :           \"auto\" | \"plotly\" | \"matplotlib\"\n                        Which backend to use. If \"auto\" will use plotly in\n                        Jupyter environments and matplotlib elsewhere.\n\n    \"\"\"\n    if not isinstance(subset, type(None)):\n        ids = utils.make_iterable(subset)\n        if not len(ids):\n            raise ValueError('`ids` must not be empty')\n    else:\n        ids = self._ids\n\n    # Collect spike frequencies\n    spks = self.get_spike_counts(bin_size=bin_size, subset=subset,\n                                 rolling_window=rolling_window)\n    freq = spks * 1000 / bin_size\n\n    if self.labels:\n        ld = dict(zip(self._ids, self._labels))\n        labels = [f'{i} ({ld[i]})' for i in ids]\n    else:\n        labels = None\n\n    if backend == 'auto':\n        if utils.is_jupyter():\n            backend = 'plotly'\n        else:\n            backend = 'matplotlib'\n\n    if isinstance(group, bool) and group:\n        sem = freq.groupby(dict(zip(self._ids, self._labels))).sem()\n        freq = freq.groupby(dict(zip(self._ids, self._labels))).mean()\n        labels = freq.index.values.tolist()\n    elif not isinstance(group, bool):\n        sem = freq.groupby(group).sem()\n        freq = freq.groupby(group).mean()\n        labels = freq.index.values.tolist()\n    else:\n        sem = None\n\n    if backend == 'plotly':\n        return _plot_traces_plotly(freq, fig=ax, labels=labels,\n                                   stimuli=self._stimuli if stimuli else None,\n                                   env=sem, **kwargs)\n    elif backend == 'matplotlib':\n        return _plot_traces_mpl(freq, ax=ax, env=sem,\n                                stimuli=self._stimuli if stimuli else None,\n                                **kwargs)\n    else:\n        raise ValueError(f'Unknown backend \"{backend}\"')\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.PointNetwork.run_simulation","title":"<code>run_simulation</code>","text":"<p>Run the simulation.</p> Source code in <code>navis/interfaces/neuron/network.py</code> <pre><code>def run_simulation(self, duration=25 * ms, v_init=-65 * mV):\n    \"\"\"Run the simulation.\"\"\"\n\n    # This resets the entire model space not just this neuron!\n    neuron.h.finitialize(v_init)\n    neuron.h.continuerun(duration)\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/#navis.interfaces.neuron.PointNetwork.set_labels","title":"<code>set_labels</code>","text":"<p>Set labels for neurons.</p> PARAMETER DESCRIPTION <code>labels</code> <pre><code>    If list, must provide a label for every neuron.\n</code></pre> <p> TYPE: <code>   dict | list-like</code> </p> Source code in <code>navis/interfaces/neuron/network.py</code> <pre><code>def set_labels(self, labels):\n    \"\"\"Set labels for neurons.\n\n    Parameters\n    ----------\n    labels :    dict | list-like\n                If list, must provide a label for every neuron.\n\n    \"\"\"\n    if isinstance(labels, dict):\n        for i, n in enumerate(self.neurons):\n            n.label = self._labels[i] = labels.get(n.id, n.id)\n    elif utils.is_iterable(labels):\n        if len(labels) != len(self.neurons):\n            raise ValueError(f'Got {len(labels)} labels for {len(self)} neurons.')\n        for i, n in enumerate(self.neurons):\n            n.label = self._labels[i] = labels[i]\n    else:\n        raise TypeError(f'`labels` must be dict or list-like, got \"{type(labels)}\"')\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/comp/","title":"comp","text":""},{"location":"reference/navis/interfaces/neuron/comp/#navis.interfaces.neuron.comp.is_NEURON_object","title":"<code>navis.interfaces.neuron.comp.is_NEURON_object</code>","text":"<p>Best guess whether object comes from NEURON.</p> Source code in <code>navis/interfaces/neuron/utils.py</code> <pre><code>def is_NEURON_object(x):\n    \"\"\"Best guess whether object comes from NEURON.\"\"\"\n    # Note:\n    # NEURON objects also have a `.hname()` method that\n    # might be useful\n    if not hasattr(x, '__module__'):\n        return False\n    if x.__module__ == 'nrn' or x.__module__ == 'hoc':\n        return True\n    return False\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/comp/#navis.interfaces.neuron.comp.is_section","title":"<code>navis.interfaces.neuron.comp.is_section</code>","text":"<p>Check if object is a section.</p> Source code in <code>navis/interfaces/neuron/utils.py</code> <pre><code>def is_section(x):\n    \"\"\"Check if object is a section.\"\"\"\n    if 'nrn.Section' in str(type(x)):\n        return True\n    return False\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/comp/#navis.interfaces.neuron.comp.is_segment","title":"<code>navis.interfaces.neuron.comp.is_segment</code>","text":"<p>Check if object is a segment.</p> Source code in <code>navis/interfaces/neuron/utils.py</code> <pre><code>def is_segment(x):\n    \"\"\"Check if object is a segment.\"\"\"\n    if 'nrn.Segment' in str(type(x)):\n        return True\n    return False\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/network/","title":"network","text":""},{"location":"reference/navis/interfaces/neuron/network/#navis.interfaces.neuron.network.NetworkIdIndexer","title":"<code>navis.interfaces.neuron.network.NetworkIdIndexer</code>","text":"Source code in <code>navis/interfaces/neuron/network.py</code> <pre><code>class NetworkIdIndexer:\n    def __init__(self, network):\n        self.network = network\n\n    def __getitem__(self, id):\n        neurons = self.network._neurons_dict\n        if utils.is_iterable(id):\n            return [neurons[i] for i in id]\n        else:\n            return neurons[id]\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/network/#navis.interfaces.neuron.network.PointNeuron","title":"<code>navis.interfaces.neuron.network.PointNeuron</code>","text":"Source code in <code>navis/interfaces/neuron/network.py</code> <pre><code>class PointNeuron:\n    def __init__(self, id, model, label=None, **props):\n        self.process = model()\n        self.id = id\n        self.label = label\n        for p, v in props.items():\n            setattr(self.process, p, v)\n\n        self.record_spikes()\n\n    def __str__(self):\n        return f'{type(self).__name__}&lt;id={self.id},label={self.label}&gt;'\n\n    def __repr__(self):\n        return self.__str__()\n\n    def record_spikes(self, threshold=10):\n        \"\"\"Set up spike recording for this neuron.\"\"\"\n        self.spk_timings = neuron.h.Vector()\n        self.sp_det = neuron.h.NetCon(self.process, None)\n        self.sp_det.record(self.spk_timings)\n\n    def record_state(self):\n        \"\"\"Set up recording of state for this neuron.\"\"\"\n        self.state = neuron.h.Vector()\n        self.state.record(self.process._ref_m)\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/network/#navis.interfaces.neuron.network.PointNeuron.record_spikes","title":"<code>record_spikes</code>","text":"<p>Set up spike recording for this neuron.</p> Source code in <code>navis/interfaces/neuron/network.py</code> <pre><code>def record_spikes(self, threshold=10):\n    \"\"\"Set up spike recording for this neuron.\"\"\"\n    self.spk_timings = neuron.h.Vector()\n    self.sp_det = neuron.h.NetCon(self.process, None)\n    self.sp_det.record(self.spk_timings)\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/network/#navis.interfaces.neuron.network.PointNeuron.record_state","title":"<code>record_state</code>","text":"<p>Set up recording of state for this neuron.</p> Source code in <code>navis/interfaces/neuron/network.py</code> <pre><code>def record_state(self):\n    \"\"\"Set up recording of state for this neuron.\"\"\"\n    self.state = neuron.h.Vector()\n    self.state.record(self.process._ref_m)\n</code></pre>"},{"location":"reference/navis/interfaces/neuron/network/#navis.interfaces.neuron.network.Stimulus","title":"<code>navis.interfaces.neuron.network.Stimulus = namedtuple('Stimulus', ['start', 'stop', 'frequency', 'randomness', 'neurons', 'netstim', 'netcon', 'label'])</code>  <code>module-attribute</code>","text":"<p>Note to self: it would best best if PointNetwork would not actually build the model until it's executed. That way we can run the entire simulation in a separate thread (or multiple cores).</p>"},{"location":"reference/navis/intersection/intersect/","title":"intersect","text":""},{"location":"reference/navis/intersection/intersect/#navis.intersection.intersect.in_volume_convex","title":"<code>navis.intersection.intersect.in_volume_convex</code>","text":"<p>Use scipy to test if points are within a given volume.</p> <p>The idea is to test if adding the point to the pointcloud changes the convex hull -&gt; if yes, that point is outside the convex hull.</p> Source code in <code>navis/intersection/convex.py</code> <pre><code>def in_volume_convex(points: np.ndarray,\n                     volume: Volume,\n                     approximate: bool = False,\n                     ignore_axis: list = []) -&gt; np.ndarray:\n    \"\"\"Use scipy to test if points are within a given volume.\n\n    The idea is to test if adding the point to the pointcloud changes the\n    convex hull -&gt; if yes, that point is outside the convex hull.\n\n    \"\"\"\n    verts = volume.vertices\n\n    if not approximate:\n        intact_hull = ConvexHull(verts)\n        intact_verts = list(intact_hull.vertices)\n\n        if isinstance(points, list):\n            points = np.array(points)\n        elif isinstance(points, pd.DataFrame):\n            points = points.to_matrix()\n\n        return [list(ConvexHull(np.append(verts, list([p]), axis=0)).vertices) == intact_verts for p in points]\n    else:\n        bbox = [(min([v[0] for v in verts]), max([v[0] for v in verts])),\n                (min([v[1] for v in verts]), max([v[1] for v in verts])),\n                (min([v[2] for v in verts]), max([v[2] for v in verts]))\n                ]\n\n        for a in ignore_axis:\n            bbox[a] = (float('-inf'), float('inf'))\n\n        return [False not in [bbox[0][0] &lt; p.x &lt; bbox[0][1], bbox[1][0] &lt; p.y &lt; bbox[1][1], bbox[2][0] &lt; p.z &lt; bbox[2][1], ] for p in points]\n</code></pre>"},{"location":"reference/navis/intersection/intersect/#navis.intersection.intersect.in_volume_ncoll","title":"<code>navis.intersection.intersect.in_volume_ncoll</code>","text":"<p>Use ncollpyde to test if points are within a given volume.</p> Source code in <code>navis/intersection/ray.py</code> <pre><code>def in_volume_ncoll(points: np.ndarray,\n                    volume: Volume,\n                    n_rays: Optional[int] = 3) -&gt; Sequence[bool]:\n    \"\"\"Use ncollpyde to test if points are within a given volume.\"\"\"\n    if isinstance(n_rays, type(None)):\n        n_rays = 3\n\n    if not isinstance(n_rays, (int, np.integer)):\n        raise TypeError(f'n_rays must be integer, got \"{type(n_rays)}\"')\n\n    if n_rays &lt;= 0:\n        raise ValueError('n_rays must be &gt; 0')\n\n    coll = ncollpyde.Volume(volume.vertices, volume.faces, n_rays=n_rays)\n\n    return coll.contains(points)\n</code></pre>"},{"location":"reference/navis/intersection/intersect/#navis.intersection.intersect.in_volume_pyoc","title":"<code>navis.intersection.intersect.in_volume_pyoc</code>","text":"<p>Use pyoctree's raycasting to test if points are within a given volume.</p> Source code in <code>navis/intersection/ray.py</code> <pre><code>def in_volume_pyoc(points: np.ndarray,\n                   volume: Volume,\n                   n_rays: Optional[int] = 1) -&gt; Sequence[bool]:\n    \"\"\"Use pyoctree's raycasting to test if points are within a given volume.\"\"\"\n    if isinstance(n_rays, type(None)):\n        n_rays = 1\n\n    if not isinstance(n_rays, (int, np.integer)):\n        raise TypeError(f'n_rays must be integer, got \"{type(n_rays)}\"')\n\n    if n_rays &lt;= 0:\n        raise ValueError('n_rays must be &gt; 0')\n\n    tree = getattr(volume, 'pyoctree', None)\n\n    if not tree:\n        # Create octree from scratch\n        tree = pyoctree.PyOctree(np.array(volume.vertices, dtype=float, order='C'),\n                                 np.array(volume.faces, dtype=np.int32, order='C')\n                                 )\n        volume.pyoctree = tree  # type: ignore\n\n    # Get min max of volume\n    mx = np.array(volume.vertices).max(axis=0)\n    mn = np.array(volume.vertices).min(axis=0)\n    dm = mx - mn\n\n    # Remove points outside of bounding box\n    is_out = (points &gt; mx).any(axis=1) | (points &lt; mn).any(axis=1)\n\n    # Cast rays\n    # There is no point of vectorizing this because pyoctree's rayIntersection\n    # does only take a single ray at a time...\n    for i in range(n_rays):\n        # Process only point that we think could be in\n        in_points = points[~is_out]\n\n        # If no in points left, break out\n        if in_points.size == 0:\n            break\n\n        # Pick a random point inside the volumes bounding box as origin\n        origin = np.random.rand(3) * dm + mn\n\n        # Generate ray point list:\n        rayPointList = np.array([[origin, p] for p in in_points],\n                                dtype=np.float32)\n\n        # Get intersections and extract coordinates of intersection\n        intersections = [np.array([i.p for i in tree.rayIntersection(ray)]) for ray in rayPointList]\n\n        # In a few odd cases we can get the multiple intersections at the exact\n        # same coordinate (something funny with the faces).\n        unique_int = [np.unique(np.round(i), axis=0) if np.any(i) else i for i in intersections]\n\n        # Unfortunately rays are bidirectional -&gt; we have to filter intersections\n        # to those that occur \"above\" the point we are querying\n        unilat_int = [i[i[:, 2] &gt;= p] if np.any(i) else i for i, p in zip(unique_int, in_points[:, 2])]\n\n        # Count intersections\n        int_count = [i.shape[0] for i in unilat_int]\n\n        # Get even (= outside volume) numbers of intersections\n        is_even = np.remainder(int_count, 2) == 0\n\n        # Set outside points\n        is_out[~is_out] = is_even\n\n    return ~is_out\n</code></pre>"},{"location":"reference/navis/io/base/","title":"base","text":""},{"location":"reference/navis/io/base/#navis.io.base.BaseReader","title":"<code>navis.io.base.BaseReader</code>","text":"<p>Abstract reader to parse various inputs into neurons.</p> <p>Any subclass should implement at least one of <code>read_buffer</code> or <code>read_dataframe</code>. Entry methods such as <code>read_any</code> will pass and parse an input through to the appropriate method.</p> PARAMETER DESCRIPTION <code>fmt</code> <pre><code>        A string describing how to parse filenames into neuron\n        properties. For example '{id}.swc'.\n</code></pre> <p> TYPE: <code>          str</code> </p> <code>file_ext</code> <pre><code>        The file extension(s) to look for when searching folders.\n        For example '.swc'. Alternatively, you can re-implement\n        the `is_valid_file` method for more complex filters. That\n        method needs to be able to deal with: Path objects, ZipInfo\n        objects and strings.\n</code></pre> <p> TYPE: <code>     str | tuple</code> </p> <code>name_fallback</code> <pre><code>        Fallback for name when reading from e.g. string.\n</code></pre> <p> TYPE: <code>str</code> DEFAULT: <code>'NA'</code> </p> <code>attrs</code> <pre><code>        Additional attributes to use when creating the neuron.\n        Will be overwritten by later additions (e.g. from `fmt`).\n</code></pre> <p> TYPE: <code>        dict</code> DEFAULT: <code>None</code> </p> <code>ignore_hidden</code> <pre><code>        Whether to ignore files that start with \"._\".\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>errors</code> <pre><code>        What to do when an error is encountered:\n         - \"raise\" (default) will raise an error\n         - \"log\" will log a warning and return `None`\n         - \"ignore\" will return `None`\n        Applies only to errors in parsing file contents into neurons\n        not to errors in reading files, archives, URLs, etc.\n</code></pre> <p> TYPE: <code>       \"raise\" | \"log\" | \"ignore\"</code> DEFAULT: <code>'raise'</code> </p> Source code in <code>navis/io/base.py</code> <pre><code>class BaseReader(ABC):\n    \"\"\"Abstract reader to parse various inputs into neurons.\n\n    Any subclass should implement at least one of `read_buffer` or\n    `read_dataframe`. Entry methods such as `read_any` will pass\n    and parse an input through to the appropriate method.\n\n    Parameters\n    ----------\n    fmt :           str\n                    A string describing how to parse filenames into neuron\n                    properties. For example '{id}.swc'.\n    file_ext :      str | tuple\n                    The file extension(s) to look for when searching folders.\n                    For example '.swc'. Alternatively, you can re-implement\n                    the `is_valid_file` method for more complex filters. That\n                    method needs to be able to deal with: Path objects, ZipInfo\n                    objects and strings.\n    name_fallback : str\n                    Fallback for name when reading from e.g. string.\n    attrs :         dict\n                    Additional attributes to use when creating the neuron.\n                    Will be overwritten by later additions (e.g. from `fmt`).\n    ignore_hidden : bool\n                    Whether to ignore files that start with \"._\".\n    errors :        \"raise\" | \"log\" | \"ignore\"\n                    What to do when an error is encountered:\n                     - \"raise\" (default) will raise an error\n                     - \"log\" will log a warning and return `None`\n                     - \"ignore\" will return `None`\n                    Applies only to errors in parsing file contents into neurons\n                    not to errors in reading files, archives, URLs, etc.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        fmt: str,\n        file_ext: str,\n        name_fallback: str = \"NA\",\n        read_binary: bool = False,\n        attrs: Optional[Dict[str, Any]] = None,\n        ignore_hidden=True,\n        errors=\"raise\",\n    ):\n        self.attrs = attrs\n        self.fmt = fmt\n        self.file_ext = file_ext\n        self.name_fallback = name_fallback\n        self.read_binary = read_binary\n        self.ignore_hidden = ignore_hidden\n        self.errors = errors\n\n        assert errors in (\"raise\", \"log\", \"ignore\")\n\n    @property\n    def file_ext(self):\n        return self._file_ext\n\n    @file_ext.setter\n    def file_ext(self, value):\n        \"\"\"Makes sure file_ext is always a tuple.\"\"\"\n        if isinstance(value, str):\n            value = (value,)\n\n        if any((ext.startswith(\"*\") for ext in value)):\n            raise ValueError('File extension must be \".ext\", not \"*.ext\"')\n\n        self._file_ext = value\n\n    def format_output(self, x):\n        \"\"\"Format output into NeuronList.\n\n        Replace this method if output is not (always) a NeuronList.\n        See for example NrrdReader in nrrd_io.py.\n        \"\"\"\n        if not x:\n            return core.NeuronList([])\n        else:\n            return core.NeuronList([n for n in x if n])\n\n    def files_in_dir(\n        self, dpath: Path, include_subdirs: bool = DEFAULT_INCLUDE_SUBDIRS\n    ) -&gt; Iterable[Path]:\n        \"\"\"List files to read in directory.\"\"\"\n        if not isinstance(dpath, Path):\n            dpath = Path(dpath)\n\n        if \"*\" in str(dpath):\n            pattern = str(dpath.name)\n            dpath = dpath.parent\n        else:\n            pattern = \"*\"\n\n        dpath = dpath.expanduser()\n        if include_subdirs:\n            pattern = os.path.join(\"**\", pattern)\n\n        yield from (f for f in dpath.glob(pattern) if self.is_valid_file(f))\n\n    def is_valid_file(self, file):\n        \"\"\"Return true if file should be considered for reading.\"\"\"\n        if isinstance(file, ZipInfo):\n            file = file.filename\n        elif isinstance(file, tarfile.TarInfo):\n            file = file.name\n        elif isinstance(file, Path):\n            file = file.name\n\n        if self.ignore_hidden and str(file).startswith(\"._\"):\n            return False\n\n        for ext in self.file_ext:\n            if str(file).endswith(ext):\n                return True\n        return False\n\n    def _make_attributes(\n        self, *dicts: Optional[Dict[str, Any]], **kwargs\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Combine attributes with a timestamp and those defined on the object.\n\n        Later additions take precedence:\n\n        - created_at (now)\n        - object-defined attributes\n        - additional dicts given as *args\n        - additional attributes given as **kwargs\n\n        Returns\n        -------\n        Dict[str, Any]\n            Arbitrary string-keyed attributes.\n        \"\"\"\n        return merge_dicts(\n            dict(created_at=str(datetime.datetime.now())),\n            self.attrs,\n            *dicts,\n            **kwargs,\n        )\n\n    def read_file_path(\n        self, fpath: os.PathLike, attrs: Optional[Dict[str, Any]] = None\n    ) -&gt; \"core.BaseNeuron\":\n        \"\"\"Read single file from path into a neuron.\n\n        Parameters\n        ----------\n        fpath :     str | os.PathLike\n                    Path to files.\n        attrs :     dict or None\n                    Arbitrary attributes to include in the neuron.\n\n        Returns\n        -------\n        core.BaseNeuron\n        \"\"\"\n        p = Path(fpath)\n        with open(p, \"rb\" if self.read_binary else \"r\") as f:\n            props = self.parse_filename(f.name)\n            props[\"origin\"] = str(p)\n            return self.read_buffer(f, attrs=merge_dicts(props, attrs))\n\n    def read_from_zip(\n        self,\n        files: Union[str, List[str]],\n        zippath: os.PathLike,\n        attrs: Optional[Dict[str, Any]] = None,\n    ) -&gt; \"core.NeuronList\":\n        \"\"\"Read given files from a zip into a NeuronList.\n\n        Typically not used directly but via `read_zip()` dispatcher.\n\n        Parameters\n        ----------\n        files :     zipfile.ZipInfo | list thereof\n                    Files inside the ZIP file to read.\n        zippath :   str | os.PathLike\n                    Path to zip file.\n        attrs :     dict or None\n                    Arbitrary attributes to include in the TreeNeuron.\n\n        Returns\n        -------\n        core.NeuronList\n\n        \"\"\"\n        p = Path(zippath)\n        files = utils.make_iterable(files)\n\n        neurons = []\n        with ZipFile(p, \"r\") as zip:\n            for file in files:\n                # Note the `file` is of type zipfile.ZipInfo here\n                props = self.parse_filename(file.orig_filename)\n                props[\"origin\"] = str(p)\n                try:\n                    n = self.read_bytes(zip.read(file), attrs=merge_dicts(props, attrs))\n                    neurons.append(n)\n                except BaseException:\n                    if self.errors == \"ignore\":\n                        logger.warning(f'Failed to read \"{file.filename}\" from zip.')\n                    else:\n                        raise\n\n        return self.format_output(neurons)\n\n    def read_zip(\n        self,\n        fpath: os.PathLike,\n        parallel=\"auto\",\n        limit: Optional[int] = None,\n        attrs: Optional[Dict[str, Any]] = None,\n    ) -&gt; \"core.NeuronList\":\n        \"\"\"Read files from a zip into a NeuronList.\n\n        This is a dispatcher for `.read_from_zip`.\n\n        Parameters\n        ----------\n        fpath :     str | os.PathLike\n                    Path to zip file.\n        limit :     int, optional\n                    Limit the number of files read from this directory.\n        attrs :     dict or None\n                    Arbitrary attributes to include in the TreeNeuron.\n\n        Returns\n        -------\n        core.NeuronList\n\n        \"\"\"\n        fpath = Path(fpath).expanduser()\n        read_fn = partial(self.read_from_zip, zippath=fpath, attrs=attrs)\n        neurons = parallel_read_archive(\n            read_fn=read_fn,\n            fpath=fpath,\n            file_ext=self.is_valid_file,\n            limit=limit,\n            parallel=parallel,\n        )\n        return self.format_output(neurons)\n\n    def read_tar(\n        self,\n        fpath: os.PathLike,\n        limit: Optional[int] = None,\n        attrs: Optional[Dict[str, Any]] = None,\n        ignore_hidden: bool = True,\n    ) -&gt; \"core.NeuronList\":\n        \"\"\"Read files from a tar archive into a NeuronList.\n\n        Parameters\n        ----------\n        fpath :     str | os.PathLike\n                    Path to tar file.\n        limit :     int, optional\n                    Limit the number of files read from this directory.\n        attrs :     dict or None\n                    Arbitrary attributes to include in the TreeNeuron.\n\n        Returns\n        -------\n        core.NeuronList\n\n        \"\"\"\n        p = Path(fpath).expanduser()\n        file_ext = self.is_valid_file\n\n        # Check the content of the tar file\n        # N.B. the TarInfo objects are hashable but the hash changes\n        # when the archive is re-opened. Therefore, we track the\n        # filenames and not the TarInfo objects.\n        to_read = []\n        with tarfile.open(p, \"r\") as tf:\n            for i, file in enumerate(tf):\n                fpath = file.name  # full path inside the tar\n                fname = fpath.split(\"/\")[-1]  # just the filename\n                if ignore_hidden and fname.startswith(\"._\"):\n                    continue\n                if callable(file_ext):\n                    if self.is_valid_file(file):\n                        to_read.append(fpath)\n                elif file_ext == \"*\":\n                    to_read.append(fpath)\n                elif file_ext and fname.endswith(file_ext):\n                    to_read.append(fpath)\n                elif \".\" not in file.filename:\n                    to_read.append(fpath)\n\n                if isinstance(limit, int) and i &gt;= limit:\n                    break\n\n        if isinstance(limit, list):\n            to_read = [f for f in to_read if f in limit]\n        elif isinstance(limit, slice):\n            to_read = to_read[limit]\n        elif isinstance(limit, str):\n            # Check if limit is a regex\n            if rgx.search(limit):\n                to_read = [f for f in to_read if re.search(limit, f.split(\"/\")[-1])]\n            else:\n                to_read = [f for f in to_read if limit in f.split(\"/\")[-1]]\n\n        # Wrapper for progess bar\n        prog = partial(\n            config.tqdm,\n            desc=\"Importing\",\n            total=len(to_read),\n            disable=config.pbar_hide,\n            leave=config.pbar_leave,\n        )\n\n        # N.B. tar.gz is a bunch of files concatenated and then compressed!\n        # In consequence, random access is very slow because we may have to seek\n        # through the whole archive to find the start of the requested file.\n        # The workaround is to open the archive in streaming (e.g. \"r|gz\") mode,\n        # iterate through the files in sequence and exract if the file is requested.\n        # This is also why we are not using parallel processing here.\n        # See also https://tinyurl.com/5n8wz54m (links to StackOverflow)\n        neurons = []\n        to_read = set(to_read)  # faster lookup\n        with prog() as pbar:\n            # Open the tar file in streaming mode with transparent compression\n            with tarfile.open(p, \"r|*\") as tf:\n                for t in tf:\n                    # Skip files we don't want to read\n                    if t.name not in to_read:\n                        continue\n                    try:\n                        props = self.parse_filename(t.name.split(\"/\")[-1])\n                        props[\"origin\"] = str(p)\n                        n = self.read_bytes(\n                            tf.extractfile(t).read(),\n                            attrs=merge_dicts(props, attrs),\n                        )\n                        neurons.append(n)\n                        to_read.remove(t.name)\n                        pbar.update()\n                    except BaseException as e:\n                        if self.errors == \"ignore\":\n                            logger.warning(f'Failed to read \"{t.name}\" from tar.')\n                        else:\n                            raise\n\n                    # If we have read all (requested) files we can stop\n                    if not len(to_read):\n                        break\n\n        return self.format_output(neurons)\n\n    def read_ftp(\n        self,\n        url,\n        parallel=\"auto\",\n        limit: Optional[int] = None,\n        attrs: Optional[Dict[str, Any]] = None,\n    ) -&gt; \"core.NeuronList\":\n        \"\"\"Read files from an FTP server.\n\n        This is a dispatcher for `.read_from_ftp`.\n\n        Parameters\n        ----------\n        url :       str\n                    Can be the path to a single file or a directory.\n        limit :     int, optional\n                    Limit the number of files read from this directory.\n        attrs :     dict or None\n                    Arbitrary attributes to include in the TreeNeuron.\n\n        Returns\n        -------\n        core.NeuronList\n\n        \"\"\"\n        # Remove the ftp:// prefix\n        url = url.replace(\"ftp://\", \"\")\n\n        # Split into server and path\n        server, path = url.split(\"/\", 1)\n\n        # Check if server contains a port\n        if \":\" in server:\n            server, port = server.split(\":\")\n            port = int(port)\n        else:\n            port = 21  # default port\n\n        read_fn = partial(self.read_from_ftp, attrs=attrs)\n        neurons = parallel_read_ftp(\n            read_fn=read_fn,\n            server=server,\n            port=port,\n            path=path,\n            file_ext=self.is_valid_file,\n            limit=limit,\n            parallel=parallel,\n        )\n        return self.format_output(neurons)\n\n    def read_from_ftp(\n        self,\n        files: Union[str, List[str]],\n        ftp: FTP,\n        attrs: Optional[Dict[str, Any]] = None,\n    ) -&gt; \"core.NeuronList\":\n        \"\"\"Read given files from an FTP server into a NeuronList.\n\n        Typically not used directly but via `read_ftp()` dispatcher.\n\n        Parameters\n        ----------\n        files :     tarfile.TarInfo | list thereof\n                    Files inside the tar file to read.\n        ftp :       ftplib.FTP | \"GLOBAL\"\n                    The FTP client. This should already be connected, logged in\n                    and in the correct directory. If \"GLOBAL\", we will look for a\n                    `_FTP` global variable.\n        attrs :     dict or None\n                    Arbitrary attributes to include in the TreeNeuron.\n\n        Returns\n        -------\n        core.NeuronList\n\n        \"\"\"\n        # When reading in parallel, we expect there to be a global FTP connection\n        # that was initialized once for each worker process.\n        if ftp == \"GLOBAL\":\n            if \"_FTP\" not in globals():\n                raise ValueError(\"No global FTP connection found.\")\n            ftp = _FTP\n\n        files = utils.make_iterable(files)\n\n        neurons = []\n        for file in files:\n            # Read the file into a bytes\n            with io.BytesIO() as f:\n                ftp.retrbinary(\"RETR \" + file, f.write)\n                f.seek(0)\n                props = self.parse_filename(file)\n                props[\"origin\"] = f\"{ftp.host}:{ftp.port}{ftp.pwd()}/{file}\"\n                try:\n                    n = self.read_buffer(f, attrs=merge_dicts(props, attrs))\n                    neurons.append(n)\n                except BaseException:\n                    if self.errors == \"ignore\":\n                        logger.warning(f'Failed to read \"{file}\" from FTP.')\n                    else:\n                        raise\n\n        return self.format_output(neurons)\n\n    def read_gs(\n        self,\n        path,\n        parallel=\"auto\",\n        limit: Optional[int] = None,\n        attrs: Optional[Dict[str, Any]] = None,\n    ) -&gt; \"core.NeuronList\":\n        \"\"\"Read files from an Google bucket.\n\n        This is a dispatcher for `.read_from_gs`.\n\n        Parameters\n        ----------\n        path :      str | list thereof\n                    Can be the path to a directory, a single file or\n                    a list of files inside the bucket.\n        limit :     int, optional\n                    Limit the number of files read from this directory.\n        attrs :     dict or None\n                    Arbitrary attributes to include in the TreeNeuron.\n\n        Returns\n        -------\n        core.NeuronList\n\n        \"\"\"\n        # This will initialize the global gcsfs filesystem if necessary\n        gcsfs = get_gs_filesystem()\n\n        read_fn = partial(self.read_from_gs, gcsfs=gcsfs, attrs=attrs)\n        neurons = parallel_read_gs(\n            read_fn=read_fn,\n            gcsfs=gcsfs,\n            path=path,\n            file_ext=self.is_valid_file,\n            limit=limit,\n            parallel=parallel,\n        )\n        return self.format_output(neurons)\n\n    def read_from_gs(\n        self,\n        files: Union[str, List[str]],\n        gcsfs,\n        attrs: Optional[Dict[str, Any]] = None,\n    ) -&gt; \"core.NeuronList\":\n        \"\"\"Read given file(s) from an Google Storage bucket.\n\n        Typically not used directly but via `read_gs()` dispatcher.\n\n        Parameters\n        ----------\n        files :     str | list thereof\n                    Filepaths(s) to read.\n        gcsfs :\n                    The Google Cloud Storage filesystem client.\n        attrs :     dict or None\n                    Arbitrary attributes to include in the TreeNeuron.\n\n        Returns\n        -------\n        core.NeuronList\n\n        \"\"\"\n        files = utils.make_iterable(files)\n\n        neurons = []\n        for file in files:\n            # Remove leading gs://\n            if file.startswith(\"gs://\"):\n                file = file[5:]\n\n            # Read the file into a bytes\n            props = self.parse_filename(file)\n            props[\"origin\"] = f\"gs://{file}\"\n            try:\n                with gcsfs.open(file, \"rb\") as f:\n                    n = self.read_buffer(f, attrs=merge_dicts(props, attrs))\n                    neurons.append(n)\n            except BaseException:\n                if self.errors == \"ignore\":\n                    path, f = file.rsplit(\"/\", 1)\n                    logger.warning(f'Failed to read \"{f}\" from GS bucket {path}.')\n                else:\n                    raise\n\n        return self.format_output(neurons)\n\n    def read_directory(\n        self,\n        path: os.PathLike,\n        include_subdirs=DEFAULT_INCLUDE_SUBDIRS,\n        parallel=\"auto\",\n        limit: Optional[int] = None,\n        attrs: Optional[Dict[str, Any]] = None,\n    ) -&gt; \"core.NeuronList\":\n        \"\"\"Read directory of files into a NeuronList.\n\n        Parameters\n        ----------\n        fpath :             str | os.PathLike\n                            Path to directory containing files.\n        include_subdirs :   bool, optional\n                            Whether to descend into subdirectories, default False.\n        parallel :          str | bool | \"auto\"\n        limit :             int, optional\n                            Limit the number of files read from this directory.\n        attrs :             dict or None\n                            Arbitrary attributes to include in the TreeNeurons\n                            of the NeuronList\n\n        Returns\n        -------\n        core.NeuronList\n\n        \"\"\"\n        files = list(self.files_in_dir(Path(path), include_subdirs))\n\n        if isinstance(limit, int):\n            files = files[:limit]\n        elif isinstance(limit, list):\n            files = [f for f in files if f in limit]\n        elif isinstance(limit, slice):\n            files = files[limit]\n        elif isinstance(limit, str):\n            # Check if limit is a regex\n            if rgx.search(limit):\n                files = [f for f in files if re.search(limit, str(f.name))]\n            else:\n                files = [f for f in files if limit in str(f)]\n\n        read_fn = partial(self.read_file_path, attrs=attrs)\n        neurons = parallel_read(read_fn, files, parallel)\n        return self.format_output(neurons)\n\n    def read_url(\n        self, url: str, attrs: Optional[Dict[str, Any]] = None\n    ) -&gt; \"core.BaseNeuron\":\n        \"\"\"Read file from URL into a neuron.\n\n        Parameters\n        ----------\n        url :       str\n                    URL to file.\n        attrs :     dict or None\n                    Arbitrary attributes to include in the neuron.\n\n        Returns\n        -------\n        core.BaseNeuron\n        \"\"\"\n        # Note: originally, we used stream=True and passed `r.raw` to the\n        # read_buffer function but that caused issue when there was more\n        # than one chunk which would require us to concatenate the chunks\n        # `via r.raw.iter_content()`.\n        # Instead, we will simply read the whole content, wrap it in a BytesIO\n        # and pass that to the read_buffer function. This is not ideal as it\n        # will load the whole file into memory while the streaming solution\n        # may have raised an exception earlier if the file was corrupted or\n        # the wrong format.\n        with requests.get(url, stream=False) as r:\n            r.raise_for_status()\n            props = self.parse_filename(url.split(\"/\")[-1])\n            props[\"origin\"] = url\n            return self.read_buffer(\n                io.BytesIO(r.content), attrs=merge_dicts(props, attrs)\n            )\n\n    def read_string(\n        self, s: str, attrs: Optional[Dict[str, Any]] = None\n    ) -&gt; \"core.BaseNeuron\":\n        \"\"\"Read single string into a Neuron.\n\n        Parameters\n        ----------\n        s :         str\n                    String.\n        attrs :     dict or None\n                    Arbitrary attributes to include in the neuron.\n\n        Returns\n        -------\n        core.BaseNeuron\n        \"\"\"\n        sio = io.StringIO(s)\n        return self.read_buffer(\n            sio,\n            attrs=merge_dicts({\"name\": self.name_fallback, \"origin\": \"string\"}, attrs),\n        )\n\n    def read_bytes(\n        self, s: str, attrs: Optional[Dict[str, Any]] = None\n    ) -&gt; \"core.BaseNeuron\":\n        \"\"\"Read bytes into a Neuron.\n\n        Parameters\n        ----------\n        s :         bytes\n                    Bytes.\n        attrs :     dict or None\n                    Arbitrary attributes to include in the neuron.\n\n        Returns\n        -------\n        core.BaseNeuron\n        \"\"\"\n        sio = io.BytesIO(s)\n        return self.read_buffer(\n            sio,\n            attrs=merge_dicts({\"name\": self.name_fallback, \"origin\": \"string\"}, attrs),\n        )\n\n    @handle_errors\n    def read_dataframe(\n        self, nodes: pd.DataFrame, attrs: Optional[Dict[str, Any]] = None\n    ) -&gt; \"core.BaseNeuron\":\n        \"\"\"Convert a DataFrame into a neuron.\n\n        Parameters\n        ----------\n        nodes :     pandas.DataFrame\n        attrs :     dict or None\n                    Arbitrary attributes to include in the neuron.\n\n        Returns\n        -------\n        core.BaseNeuron\n        \"\"\"\n        raise NotImplementedError(\n            f\"Reading DataFrames not implemented for {type(self)}\"\n        )\n\n    @handle_errors\n    def read_buffer(\n        self, f: IO, attrs: Optional[Dict[str, Any]] = None\n    ) -&gt; \"core.BaseNeuron\":\n        \"\"\"Read buffer into a single neuron.\n\n\n\n        Parameters\n        ----------\n        f :         IO\n                    Readable buffer.\n        attrs :     dict | None\n                    Arbitrary attributes to include in the neuron.\n\n        Returns\n        -------\n        core.NeuronObject\n        \"\"\"\n        raise NotImplementedError(\n            f\"Reading from buffer not implemented for {type(self)}\"\n        )\n\n    def read_any_single(\n        self, obj, attrs: Optional[Dict[str, Any]] = None\n    ) -&gt; \"core.BaseNeuron\":\n        \"\"\"Attempt to convert an arbitrary object into a neuron.\n\n        Parameters\n        ----------\n        obj :       typing.IO | pandas.DataFrame | str | os.PathLike\n                    Readable buffer, dataframe, path or URL to single file,\n                    or parsable string.\n        attrs :     dict or None\n                    Arbitrary attributes to include in the neuron.\n\n        Returns\n        -------\n        core.BaseNeuron\n        \"\"\"\n        if hasattr(obj, \"read\"):\n            return self.read_buffer(obj, attrs=attrs)\n        if isinstance(obj, pd.DataFrame):\n            return self.read_dataframe(obj, attrs=attrs)\n        if isinstance(obj, os.PathLike):\n            if str(obj).endswith(\".zip\"):\n                return self.read_zip(obj, attrs=attrs)\n            elif \".tar\" in str(obj):\n                return self.read_tar(obj, attrs=attrs)\n            return self.read_file_path(obj, attrs=attrs)\n        if isinstance(obj, str):\n            # See if this might be a file (make sure to expand user)\n            if os.path.isfile(os.path.expanduser(obj)):\n                p = Path(obj).expanduser()\n                if p.suffix == \".zip\":\n                    return self.read_zip(p, attrs=attrs)\n                elif any(\n                    str(p).endswith(f) for f in (\".tar\", \"tar.gz\", \"tar.bz\", \"tar.bz2\")\n                ):\n                    return self.read_tar(p, attrs=attrs)\n                return self.read_file_path(p, attrs=attrs)\n            if obj.startswith(\"http://\") or obj.startswith(\"https://\"):\n                return self.read_url(obj, attrs=attrs)\n            if obj.startswith(\"ftp://\"):\n                return self.read_ftp(obj, attrs=attrs)\n            if obj.startswith(\"gs://\"):\n                return self.read_gs(obj, attrs=attrs)\n            return self.read_string(obj, attrs=attrs)\n        if isinstance(obj, bytes):\n            return self.read_bytes(obj, attrs=attrs)\n        raise ValueError(f\"Could not read neuron from object of type '{type(obj)}'\")\n\n    def read_any_multi(\n        self,\n        objs,\n        include_subdirs=DEFAULT_INCLUDE_SUBDIRS,\n        parallel=\"auto\",\n        attrs: Optional[Dict[str, Any]] = None,\n    ) -&gt; \"core.NeuronList\":\n        \"\"\"Attempt to convert an arbitrary object into a NeuronList,\n        potentially in parallel.\n\n        Parameters\n        ----------\n        obj :               sequence\n                            Sequence of anything readable by read_any_single or\n                            directory path(s).\n        include_subdirs :   bool\n                            Whether to include subdirectories of a given\n                            directory.\n        parallel :          str | bool | int | None\n                            \"auto\" or True for n_cores // 2, otherwise int for\n                            number of jobs, or False for serial.\n        attrs :             dict or None\n                            Arbitrary attributes to include in each TreeNeuron\n                            of the NeuronList.\n\n        Returns\n        -------\n        core.NeuronList\n\n        \"\"\"\n        if not utils.is_iterable(objs):\n            objs = [objs]\n\n        if not objs:\n            logger.warning(\"No files found, returning empty NeuronList\")\n            return self.format_output(objs)\n\n        new_objs = []\n        new_objs_gs = []\n        for obj in objs:\n            try:\n                if is_dir(obj):\n                    new_objs.extend(self.files_in_dir(obj, include_subdirs))\n                    continue\n            except TypeError:\n                pass\n\n            if isinstance(obj, str) and obj.startswith(\"gs://\"):\n                new_objs_gs.append(obj)\n            else:\n                new_objs.append(obj)\n\n        neurons = []\n        if new_objs_gs:\n            neurons += self.read_gs(new_objs_gs, parallel=parallel, attrs=attrs)\n\n        if new_objs:\n            read_fn = partial(self.read_any_single, attrs=attrs)\n            neurons += parallel_read(read_fn, new_objs, parallel)\n\n        return self.format_output(neurons)\n\n    def read_any(\n        self,\n        obj,\n        include_subdirs=DEFAULT_INCLUDE_SUBDIRS,\n        parallel=\"auto\",\n        limit=None,\n        attrs: Optional[Dict[str, Any]] = None,\n    ) -&gt; \"core.NeuronObject\":\n        \"\"\"Attempt to read an arbitrary object into a neuron.\n\n        Parameters\n        ----------\n        obj :       typing.IO | str | os.PathLike | pandas.DataFrame\n                    Buffer, path to file or directory, URL, string, or\n                    dataframe.\n\n        Returns\n        -------\n        core.NeuronObject\n        \"\"\"\n        if utils.is_iterable(obj) and not hasattr(obj, \"read\"):\n            return self.read_any_multi(\n                obj, parallel=parallel, include_subdirs=include_subdirs, attrs=attrs\n            )\n        else:\n            try:\n                if is_dir(obj):\n                    return self.read_directory(\n                        obj,\n                        include_subdirs,\n                        parallel=parallel,\n                        limit=limit,\n                        attrs=attrs,\n                    )\n            except TypeError:\n                pass\n            try:\n                if os.path.isfile(os.path.expanduser(obj)) and str(obj).endswith(\n                    \".zip\"\n                ):\n                    return self.read_zip(\n                        obj, parallel=parallel, limit=limit, attrs=attrs\n                    )\n                if os.path.isfile(os.path.expanduser(obj)) and \".tar\" in str(obj):\n                    return self.read_tar(obj, limit=limit, attrs=attrs)\n                if isinstance(obj, str) and obj.startswith(\"ftp://\"):\n                    return self.read_ftp(\n                        obj, parallel=parallel, limit=limit, attrs=attrs\n                    )\n                elif isinstance(obj, str) and obj.startswith(\"gs://\"):\n                    return self.read_gs(\n                        obj, parallel=parallel, limit=limit, attrs=attrs\n                    )\n            except TypeError:\n                pass\n            return self.read_any_single(obj, attrs=attrs)\n\n    def parse_filename(self, filename: str) -&gt; dict:\n        \"\"\"Extract properties from filename according to specified formatter.\n\n        Parameters\n        ----------\n        filename : str\n\n        Returns\n        -------\n        props :     dict\n                    Properties extracted from filename.\n\n        \"\"\"\n        # Make sure we are working with the filename not the whole path\n        filename = Path(filename).name\n\n        # Escape all special characters\n        fmt = re.escape(self.fmt)\n\n        # Unescape { and }\n        fmt = fmt.replace(\"\\\\{\", \"{\").replace(\"\\\\}\", \"}\")\n\n        # Replace all e.g. {name} with {.*}\n        prop_names = []\n        for prop in re.findall(\"{.*?}\", fmt):\n            prop_names.append(prop[1:-1].replace(\" \", \"\"))\n            fmt = fmt.replace(prop, \"(.*)\")\n\n        # Match\n        match = re.search(fmt, filename)\n\n        if not match:\n            raise ValueError(f'Unable to match \"{self.fmt}\" to filename \"{filename}\"')\n\n        props = {\"file\": filename}\n        for i, prop in enumerate(prop_names):\n            for p in prop.split(\",\"):\n                # Ignore empty (\"{}\")\n                if not p:\n                    continue\n\n                # If datatype was specified\n                if \":\" in p:\n                    p, dt = p.split(\":\")\n\n                    props[p] = match.group(i + 1)\n\n                    if dt == \"int\":\n                        props[p] = int(props[p])\n                    elif dt == \"float\":\n                        props[p] = float(props[p])\n                    elif dt == \"bool\":\n                        props[p] = bool(props[p])\n                    elif dt == \"str\":\n                        props[p] = str(props[p])\n                    else:\n                        raise ValueError(\n                            f'Unable to interpret datatype \"{dt}\" for property {p}'\n                        )\n                else:\n                    props[p] = match.group(i + 1)\n        return props\n\n    def _extract_connectors(self, nodes: pd.DataFrame) -&gt; Optional[pd.DataFrame]:\n        \"\"\"Infer outgoing/incoming connectors from data.\n\n        Parameters\n        ----------\n        nodes :     pd.DataFrame\n\n        Returns\n        -------\n        Optional[pd.DataFrame]\n                    With columns `[\"node_id\", \"x\", \"y\", \"z\", \"connector_id\", \"type\"]`\n        \"\"\"\n        return\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.BaseReader.files_in_dir","title":"<code>files_in_dir</code>","text":"<p>List files to read in directory.</p> Source code in <code>navis/io/base.py</code> <pre><code>def files_in_dir(\n    self, dpath: Path, include_subdirs: bool = DEFAULT_INCLUDE_SUBDIRS\n) -&gt; Iterable[Path]:\n    \"\"\"List files to read in directory.\"\"\"\n    if not isinstance(dpath, Path):\n        dpath = Path(dpath)\n\n    if \"*\" in str(dpath):\n        pattern = str(dpath.name)\n        dpath = dpath.parent\n    else:\n        pattern = \"*\"\n\n    dpath = dpath.expanduser()\n    if include_subdirs:\n        pattern = os.path.join(\"**\", pattern)\n\n    yield from (f for f in dpath.glob(pattern) if self.is_valid_file(f))\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.BaseReader.format_output","title":"<code>format_output</code>","text":"<p>Format output into NeuronList.</p> <p>Replace this method if output is not (always) a NeuronList. See for example NrrdReader in nrrd_io.py.</p> Source code in <code>navis/io/base.py</code> <pre><code>def format_output(self, x):\n    \"\"\"Format output into NeuronList.\n\n    Replace this method if output is not (always) a NeuronList.\n    See for example NrrdReader in nrrd_io.py.\n    \"\"\"\n    if not x:\n        return core.NeuronList([])\n    else:\n        return core.NeuronList([n for n in x if n])\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.BaseReader.is_valid_file","title":"<code>is_valid_file</code>","text":"<p>Return true if file should be considered for reading.</p> Source code in <code>navis/io/base.py</code> <pre><code>def is_valid_file(self, file):\n    \"\"\"Return true if file should be considered for reading.\"\"\"\n    if isinstance(file, ZipInfo):\n        file = file.filename\n    elif isinstance(file, tarfile.TarInfo):\n        file = file.name\n    elif isinstance(file, Path):\n        file = file.name\n\n    if self.ignore_hidden and str(file).startswith(\"._\"):\n        return False\n\n    for ext in self.file_ext:\n        if str(file).endswith(ext):\n            return True\n    return False\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.BaseReader.parse_filename","title":"<code>parse_filename</code>","text":"<p>Extract properties from filename according to specified formatter.</p> PARAMETER DESCRIPTION <code>filename</code> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>props</code> <p>Properties extracted from filename.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>navis/io/base.py</code> <pre><code>def parse_filename(self, filename: str) -&gt; dict:\n    \"\"\"Extract properties from filename according to specified formatter.\n\n    Parameters\n    ----------\n    filename : str\n\n    Returns\n    -------\n    props :     dict\n                Properties extracted from filename.\n\n    \"\"\"\n    # Make sure we are working with the filename not the whole path\n    filename = Path(filename).name\n\n    # Escape all special characters\n    fmt = re.escape(self.fmt)\n\n    # Unescape { and }\n    fmt = fmt.replace(\"\\\\{\", \"{\").replace(\"\\\\}\", \"}\")\n\n    # Replace all e.g. {name} with {.*}\n    prop_names = []\n    for prop in re.findall(\"{.*?}\", fmt):\n        prop_names.append(prop[1:-1].replace(\" \", \"\"))\n        fmt = fmt.replace(prop, \"(.*)\")\n\n    # Match\n    match = re.search(fmt, filename)\n\n    if not match:\n        raise ValueError(f'Unable to match \"{self.fmt}\" to filename \"{filename}\"')\n\n    props = {\"file\": filename}\n    for i, prop in enumerate(prop_names):\n        for p in prop.split(\",\"):\n            # Ignore empty (\"{}\")\n            if not p:\n                continue\n\n            # If datatype was specified\n            if \":\" in p:\n                p, dt = p.split(\":\")\n\n                props[p] = match.group(i + 1)\n\n                if dt == \"int\":\n                    props[p] = int(props[p])\n                elif dt == \"float\":\n                    props[p] = float(props[p])\n                elif dt == \"bool\":\n                    props[p] = bool(props[p])\n                elif dt == \"str\":\n                    props[p] = str(props[p])\n                else:\n                    raise ValueError(\n                        f'Unable to interpret datatype \"{dt}\" for property {p}'\n                    )\n            else:\n                props[p] = match.group(i + 1)\n    return props\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.BaseReader.read_any","title":"<code>read_any</code>","text":"<p>Attempt to read an arbitrary object into a neuron.</p> PARAMETER DESCRIPTION <code>obj</code> <pre><code>    Buffer, path to file or directory, URL, string, or\n    dataframe.\n</code></pre> <p> TYPE: <code>      typing.IO | str | os.PathLike | pandas.DataFrame</code> </p> RETURNS DESCRIPTION <code>core.NeuronObject</code> Source code in <code>navis/io/base.py</code> <pre><code>def read_any(\n    self,\n    obj,\n    include_subdirs=DEFAULT_INCLUDE_SUBDIRS,\n    parallel=\"auto\",\n    limit=None,\n    attrs: Optional[Dict[str, Any]] = None,\n) -&gt; \"core.NeuronObject\":\n    \"\"\"Attempt to read an arbitrary object into a neuron.\n\n    Parameters\n    ----------\n    obj :       typing.IO | str | os.PathLike | pandas.DataFrame\n                Buffer, path to file or directory, URL, string, or\n                dataframe.\n\n    Returns\n    -------\n    core.NeuronObject\n    \"\"\"\n    if utils.is_iterable(obj) and not hasattr(obj, \"read\"):\n        return self.read_any_multi(\n            obj, parallel=parallel, include_subdirs=include_subdirs, attrs=attrs\n        )\n    else:\n        try:\n            if is_dir(obj):\n                return self.read_directory(\n                    obj,\n                    include_subdirs,\n                    parallel=parallel,\n                    limit=limit,\n                    attrs=attrs,\n                )\n        except TypeError:\n            pass\n        try:\n            if os.path.isfile(os.path.expanduser(obj)) and str(obj).endswith(\n                \".zip\"\n            ):\n                return self.read_zip(\n                    obj, parallel=parallel, limit=limit, attrs=attrs\n                )\n            if os.path.isfile(os.path.expanduser(obj)) and \".tar\" in str(obj):\n                return self.read_tar(obj, limit=limit, attrs=attrs)\n            if isinstance(obj, str) and obj.startswith(\"ftp://\"):\n                return self.read_ftp(\n                    obj, parallel=parallel, limit=limit, attrs=attrs\n                )\n            elif isinstance(obj, str) and obj.startswith(\"gs://\"):\n                return self.read_gs(\n                    obj, parallel=parallel, limit=limit, attrs=attrs\n                )\n        except TypeError:\n            pass\n        return self.read_any_single(obj, attrs=attrs)\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.BaseReader.read_any_multi","title":"<code>read_any_multi</code>","text":"<p>Attempt to convert an arbitrary object into a NeuronList, potentially in parallel.</p> PARAMETER DESCRIPTION <code>obj</code> <pre><code>            Sequence of anything readable by read_any_single or\n            directory path(s).\n</code></pre> <p> TYPE: <code>              sequence</code> </p> <code>include_subdirs</code> <pre><code>            Whether to include subdirectories of a given\n            directory.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>DEFAULT_INCLUDE_SUBDIRS</code> </p> <code>parallel</code> <pre><code>            \"auto\" or True for n_cores // 2, otherwise int for\n            number of jobs, or False for serial.\n</code></pre> <p> TYPE: <code>         str | bool | int | None</code> DEFAULT: <code>'auto'</code> </p> <code>attrs</code> <pre><code>            Arbitrary attributes to include in each TreeNeuron\n            of the NeuronList.\n</code></pre> <p> TYPE: <code>            dict or None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.NeuronList</code> Source code in <code>navis/io/base.py</code> <pre><code>def read_any_multi(\n    self,\n    objs,\n    include_subdirs=DEFAULT_INCLUDE_SUBDIRS,\n    parallel=\"auto\",\n    attrs: Optional[Dict[str, Any]] = None,\n) -&gt; \"core.NeuronList\":\n    \"\"\"Attempt to convert an arbitrary object into a NeuronList,\n    potentially in parallel.\n\n    Parameters\n    ----------\n    obj :               sequence\n                        Sequence of anything readable by read_any_single or\n                        directory path(s).\n    include_subdirs :   bool\n                        Whether to include subdirectories of a given\n                        directory.\n    parallel :          str | bool | int | None\n                        \"auto\" or True for n_cores // 2, otherwise int for\n                        number of jobs, or False for serial.\n    attrs :             dict or None\n                        Arbitrary attributes to include in each TreeNeuron\n                        of the NeuronList.\n\n    Returns\n    -------\n    core.NeuronList\n\n    \"\"\"\n    if not utils.is_iterable(objs):\n        objs = [objs]\n\n    if not objs:\n        logger.warning(\"No files found, returning empty NeuronList\")\n        return self.format_output(objs)\n\n    new_objs = []\n    new_objs_gs = []\n    for obj in objs:\n        try:\n            if is_dir(obj):\n                new_objs.extend(self.files_in_dir(obj, include_subdirs))\n                continue\n        except TypeError:\n            pass\n\n        if isinstance(obj, str) and obj.startswith(\"gs://\"):\n            new_objs_gs.append(obj)\n        else:\n            new_objs.append(obj)\n\n    neurons = []\n    if new_objs_gs:\n        neurons += self.read_gs(new_objs_gs, parallel=parallel, attrs=attrs)\n\n    if new_objs:\n        read_fn = partial(self.read_any_single, attrs=attrs)\n        neurons += parallel_read(read_fn, new_objs, parallel)\n\n    return self.format_output(neurons)\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.BaseReader.read_any_single","title":"<code>read_any_single</code>","text":"<p>Attempt to convert an arbitrary object into a neuron.</p> PARAMETER DESCRIPTION <code>obj</code> <pre><code>    Readable buffer, dataframe, path or URL to single file,\n    or parsable string.\n</code></pre> <p> TYPE: <code>      typing.IO | pandas.DataFrame | str | os.PathLike</code> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the neuron.\n</code></pre> <p> TYPE: <code>    dict or None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.BaseNeuron</code> Source code in <code>navis/io/base.py</code> <pre><code>def read_any_single(\n    self, obj, attrs: Optional[Dict[str, Any]] = None\n) -&gt; \"core.BaseNeuron\":\n    \"\"\"Attempt to convert an arbitrary object into a neuron.\n\n    Parameters\n    ----------\n    obj :       typing.IO | pandas.DataFrame | str | os.PathLike\n                Readable buffer, dataframe, path or URL to single file,\n                or parsable string.\n    attrs :     dict or None\n                Arbitrary attributes to include in the neuron.\n\n    Returns\n    -------\n    core.BaseNeuron\n    \"\"\"\n    if hasattr(obj, \"read\"):\n        return self.read_buffer(obj, attrs=attrs)\n    if isinstance(obj, pd.DataFrame):\n        return self.read_dataframe(obj, attrs=attrs)\n    if isinstance(obj, os.PathLike):\n        if str(obj).endswith(\".zip\"):\n            return self.read_zip(obj, attrs=attrs)\n        elif \".tar\" in str(obj):\n            return self.read_tar(obj, attrs=attrs)\n        return self.read_file_path(obj, attrs=attrs)\n    if isinstance(obj, str):\n        # See if this might be a file (make sure to expand user)\n        if os.path.isfile(os.path.expanduser(obj)):\n            p = Path(obj).expanduser()\n            if p.suffix == \".zip\":\n                return self.read_zip(p, attrs=attrs)\n            elif any(\n                str(p).endswith(f) for f in (\".tar\", \"tar.gz\", \"tar.bz\", \"tar.bz2\")\n            ):\n                return self.read_tar(p, attrs=attrs)\n            return self.read_file_path(p, attrs=attrs)\n        if obj.startswith(\"http://\") or obj.startswith(\"https://\"):\n            return self.read_url(obj, attrs=attrs)\n        if obj.startswith(\"ftp://\"):\n            return self.read_ftp(obj, attrs=attrs)\n        if obj.startswith(\"gs://\"):\n            return self.read_gs(obj, attrs=attrs)\n        return self.read_string(obj, attrs=attrs)\n    if isinstance(obj, bytes):\n        return self.read_bytes(obj, attrs=attrs)\n    raise ValueError(f\"Could not read neuron from object of type '{type(obj)}'\")\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.BaseReader.read_buffer","title":"<code>read_buffer</code>","text":"<p>Read buffer into a single neuron.</p> PARAMETER DESCRIPTION <code>f</code> <pre><code>    Readable buffer.\n</code></pre> <p> TYPE: <code>        IO</code> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the neuron.\n</code></pre> <p> TYPE: <code>    dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.NeuronObject</code> Source code in <code>navis/io/base.py</code> <pre><code>@handle_errors\ndef read_buffer(\n    self, f: IO, attrs: Optional[Dict[str, Any]] = None\n) -&gt; \"core.BaseNeuron\":\n    \"\"\"Read buffer into a single neuron.\n\n\n\n    Parameters\n    ----------\n    f :         IO\n                Readable buffer.\n    attrs :     dict | None\n                Arbitrary attributes to include in the neuron.\n\n    Returns\n    -------\n    core.NeuronObject\n    \"\"\"\n    raise NotImplementedError(\n        f\"Reading from buffer not implemented for {type(self)}\"\n    )\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.BaseReader.read_bytes","title":"<code>read_bytes</code>","text":"<p>Read bytes into a Neuron.</p> PARAMETER DESCRIPTION <code>s</code> <pre><code>    Bytes.\n</code></pre> <p> TYPE: <code>        bytes</code> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the neuron.\n</code></pre> <p> TYPE: <code>    dict or None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.BaseNeuron</code> Source code in <code>navis/io/base.py</code> <pre><code>def read_bytes(\n    self, s: str, attrs: Optional[Dict[str, Any]] = None\n) -&gt; \"core.BaseNeuron\":\n    \"\"\"Read bytes into a Neuron.\n\n    Parameters\n    ----------\n    s :         bytes\n                Bytes.\n    attrs :     dict or None\n                Arbitrary attributes to include in the neuron.\n\n    Returns\n    -------\n    core.BaseNeuron\n    \"\"\"\n    sio = io.BytesIO(s)\n    return self.read_buffer(\n        sio,\n        attrs=merge_dicts({\"name\": self.name_fallback, \"origin\": \"string\"}, attrs),\n    )\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.BaseReader.read_dataframe","title":"<code>read_dataframe</code>","text":"<p>Convert a DataFrame into a neuron.</p> PARAMETER DESCRIPTION <code>nodes</code> <p> TYPE: <code>    pandas.DataFrame</code> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the neuron.\n</code></pre> <p> TYPE: <code>    dict or None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.BaseNeuron</code> Source code in <code>navis/io/base.py</code> <pre><code>@handle_errors\ndef read_dataframe(\n    self, nodes: pd.DataFrame, attrs: Optional[Dict[str, Any]] = None\n) -&gt; \"core.BaseNeuron\":\n    \"\"\"Convert a DataFrame into a neuron.\n\n    Parameters\n    ----------\n    nodes :     pandas.DataFrame\n    attrs :     dict or None\n                Arbitrary attributes to include in the neuron.\n\n    Returns\n    -------\n    core.BaseNeuron\n    \"\"\"\n    raise NotImplementedError(\n        f\"Reading DataFrames not implemented for {type(self)}\"\n    )\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.BaseReader.read_directory","title":"<code>read_directory</code>","text":"<p>Read directory of files into a NeuronList.</p> PARAMETER DESCRIPTION <code>fpath</code> <pre><code>            Path to directory containing files.\n</code></pre> <p> TYPE: <code>            str | os.PathLike</code> </p> <code>include_subdirs</code> <pre><code>            Whether to descend into subdirectories, default False.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>DEFAULT_INCLUDE_SUBDIRS</code> </p> <code>parallel</code> <p> TYPE: <code>         str | bool | \"auto\"</code> DEFAULT: <code>'auto'</code> </p> <code>limit</code> <pre><code>            Limit the number of files read from this directory.\n</code></pre> <p> TYPE: <code>            int</code> DEFAULT: <code>None</code> </p> <code>attrs</code> <pre><code>            Arbitrary attributes to include in the TreeNeurons\n            of the NeuronList\n</code></pre> <p> TYPE: <code>            dict or None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.NeuronList</code> Source code in <code>navis/io/base.py</code> <pre><code>def read_directory(\n    self,\n    path: os.PathLike,\n    include_subdirs=DEFAULT_INCLUDE_SUBDIRS,\n    parallel=\"auto\",\n    limit: Optional[int] = None,\n    attrs: Optional[Dict[str, Any]] = None,\n) -&gt; \"core.NeuronList\":\n    \"\"\"Read directory of files into a NeuronList.\n\n    Parameters\n    ----------\n    fpath :             str | os.PathLike\n                        Path to directory containing files.\n    include_subdirs :   bool, optional\n                        Whether to descend into subdirectories, default False.\n    parallel :          str | bool | \"auto\"\n    limit :             int, optional\n                        Limit the number of files read from this directory.\n    attrs :             dict or None\n                        Arbitrary attributes to include in the TreeNeurons\n                        of the NeuronList\n\n    Returns\n    -------\n    core.NeuronList\n\n    \"\"\"\n    files = list(self.files_in_dir(Path(path), include_subdirs))\n\n    if isinstance(limit, int):\n        files = files[:limit]\n    elif isinstance(limit, list):\n        files = [f for f in files if f in limit]\n    elif isinstance(limit, slice):\n        files = files[limit]\n    elif isinstance(limit, str):\n        # Check if limit is a regex\n        if rgx.search(limit):\n            files = [f for f in files if re.search(limit, str(f.name))]\n        else:\n            files = [f for f in files if limit in str(f)]\n\n    read_fn = partial(self.read_file_path, attrs=attrs)\n    neurons = parallel_read(read_fn, files, parallel)\n    return self.format_output(neurons)\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.BaseReader.read_file_path","title":"<code>read_file_path</code>","text":"<p>Read single file from path into a neuron.</p> PARAMETER DESCRIPTION <code>fpath</code> <pre><code>    Path to files.\n</code></pre> <p> TYPE: <code>    str | os.PathLike</code> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the neuron.\n</code></pre> <p> TYPE: <code>    dict or None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.BaseNeuron</code> Source code in <code>navis/io/base.py</code> <pre><code>def read_file_path(\n    self, fpath: os.PathLike, attrs: Optional[Dict[str, Any]] = None\n) -&gt; \"core.BaseNeuron\":\n    \"\"\"Read single file from path into a neuron.\n\n    Parameters\n    ----------\n    fpath :     str | os.PathLike\n                Path to files.\n    attrs :     dict or None\n                Arbitrary attributes to include in the neuron.\n\n    Returns\n    -------\n    core.BaseNeuron\n    \"\"\"\n    p = Path(fpath)\n    with open(p, \"rb\" if self.read_binary else \"r\") as f:\n        props = self.parse_filename(f.name)\n        props[\"origin\"] = str(p)\n        return self.read_buffer(f, attrs=merge_dicts(props, attrs))\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.BaseReader.read_from_ftp","title":"<code>read_from_ftp</code>","text":"<p>Read given files from an FTP server into a NeuronList.</p> <p>Typically not used directly but via <code>read_ftp()</code> dispatcher.</p> PARAMETER DESCRIPTION <code>files</code> <pre><code>    Files inside the tar file to read.\n</code></pre> <p> TYPE: <code>    tarfile.TarInfo | list thereof</code> </p> <code>ftp</code> <pre><code>    The FTP client. This should already be connected, logged in\n    and in the correct directory. If \"GLOBAL\", we will look for a\n    `_FTP` global variable.\n</code></pre> <p> TYPE: <code>      ftplib.FTP | \"GLOBAL\"</code> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the TreeNeuron.\n</code></pre> <p> TYPE: <code>    dict or None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.NeuronList</code> Source code in <code>navis/io/base.py</code> <pre><code>def read_from_ftp(\n    self,\n    files: Union[str, List[str]],\n    ftp: FTP,\n    attrs: Optional[Dict[str, Any]] = None,\n) -&gt; \"core.NeuronList\":\n    \"\"\"Read given files from an FTP server into a NeuronList.\n\n    Typically not used directly but via `read_ftp()` dispatcher.\n\n    Parameters\n    ----------\n    files :     tarfile.TarInfo | list thereof\n                Files inside the tar file to read.\n    ftp :       ftplib.FTP | \"GLOBAL\"\n                The FTP client. This should already be connected, logged in\n                and in the correct directory. If \"GLOBAL\", we will look for a\n                `_FTP` global variable.\n    attrs :     dict or None\n                Arbitrary attributes to include in the TreeNeuron.\n\n    Returns\n    -------\n    core.NeuronList\n\n    \"\"\"\n    # When reading in parallel, we expect there to be a global FTP connection\n    # that was initialized once for each worker process.\n    if ftp == \"GLOBAL\":\n        if \"_FTP\" not in globals():\n            raise ValueError(\"No global FTP connection found.\")\n        ftp = _FTP\n\n    files = utils.make_iterable(files)\n\n    neurons = []\n    for file in files:\n        # Read the file into a bytes\n        with io.BytesIO() as f:\n            ftp.retrbinary(\"RETR \" + file, f.write)\n            f.seek(0)\n            props = self.parse_filename(file)\n            props[\"origin\"] = f\"{ftp.host}:{ftp.port}{ftp.pwd()}/{file}\"\n            try:\n                n = self.read_buffer(f, attrs=merge_dicts(props, attrs))\n                neurons.append(n)\n            except BaseException:\n                if self.errors == \"ignore\":\n                    logger.warning(f'Failed to read \"{file}\" from FTP.')\n                else:\n                    raise\n\n    return self.format_output(neurons)\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.BaseReader.read_from_gs","title":"<code>read_from_gs</code>","text":"<p>Read given file(s) from an Google Storage bucket.</p> <p>Typically not used directly but via <code>read_gs()</code> dispatcher.</p> PARAMETER DESCRIPTION <code>files</code> <pre><code>    Filepaths(s) to read.\n</code></pre> <p> TYPE: <code>    str | list thereof</code> </p> <code>gcsfs</code> <pre><code>    The Google Cloud Storage filesystem client.\n</code></pre> <p> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the TreeNeuron.\n</code></pre> <p> TYPE: <code>    dict or None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.NeuronList</code> Source code in <code>navis/io/base.py</code> <pre><code>def read_from_gs(\n    self,\n    files: Union[str, List[str]],\n    gcsfs,\n    attrs: Optional[Dict[str, Any]] = None,\n) -&gt; \"core.NeuronList\":\n    \"\"\"Read given file(s) from an Google Storage bucket.\n\n    Typically not used directly but via `read_gs()` dispatcher.\n\n    Parameters\n    ----------\n    files :     str | list thereof\n                Filepaths(s) to read.\n    gcsfs :\n                The Google Cloud Storage filesystem client.\n    attrs :     dict or None\n                Arbitrary attributes to include in the TreeNeuron.\n\n    Returns\n    -------\n    core.NeuronList\n\n    \"\"\"\n    files = utils.make_iterable(files)\n\n    neurons = []\n    for file in files:\n        # Remove leading gs://\n        if file.startswith(\"gs://\"):\n            file = file[5:]\n\n        # Read the file into a bytes\n        props = self.parse_filename(file)\n        props[\"origin\"] = f\"gs://{file}\"\n        try:\n            with gcsfs.open(file, \"rb\") as f:\n                n = self.read_buffer(f, attrs=merge_dicts(props, attrs))\n                neurons.append(n)\n        except BaseException:\n            if self.errors == \"ignore\":\n                path, f = file.rsplit(\"/\", 1)\n                logger.warning(f'Failed to read \"{f}\" from GS bucket {path}.')\n            else:\n                raise\n\n    return self.format_output(neurons)\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.BaseReader.read_from_zip","title":"<code>read_from_zip</code>","text":"<p>Read given files from a zip into a NeuronList.</p> <p>Typically not used directly but via <code>read_zip()</code> dispatcher.</p> PARAMETER DESCRIPTION <code>files</code> <pre><code>    Files inside the ZIP file to read.\n</code></pre> <p> TYPE: <code>    zipfile.ZipInfo | list thereof</code> </p> <code>zippath</code> <pre><code>    Path to zip file.\n</code></pre> <p> TYPE: <code>  str | os.PathLike</code> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the TreeNeuron.\n</code></pre> <p> TYPE: <code>    dict or None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.NeuronList</code> Source code in <code>navis/io/base.py</code> <pre><code>def read_from_zip(\n    self,\n    files: Union[str, List[str]],\n    zippath: os.PathLike,\n    attrs: Optional[Dict[str, Any]] = None,\n) -&gt; \"core.NeuronList\":\n    \"\"\"Read given files from a zip into a NeuronList.\n\n    Typically not used directly but via `read_zip()` dispatcher.\n\n    Parameters\n    ----------\n    files :     zipfile.ZipInfo | list thereof\n                Files inside the ZIP file to read.\n    zippath :   str | os.PathLike\n                Path to zip file.\n    attrs :     dict or None\n                Arbitrary attributes to include in the TreeNeuron.\n\n    Returns\n    -------\n    core.NeuronList\n\n    \"\"\"\n    p = Path(zippath)\n    files = utils.make_iterable(files)\n\n    neurons = []\n    with ZipFile(p, \"r\") as zip:\n        for file in files:\n            # Note the `file` is of type zipfile.ZipInfo here\n            props = self.parse_filename(file.orig_filename)\n            props[\"origin\"] = str(p)\n            try:\n                n = self.read_bytes(zip.read(file), attrs=merge_dicts(props, attrs))\n                neurons.append(n)\n            except BaseException:\n                if self.errors == \"ignore\":\n                    logger.warning(f'Failed to read \"{file.filename}\" from zip.')\n                else:\n                    raise\n\n    return self.format_output(neurons)\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.BaseReader.read_ftp","title":"<code>read_ftp</code>","text":"<p>Read files from an FTP server.</p> <p>This is a dispatcher for <code>.read_from_ftp</code>.</p> PARAMETER DESCRIPTION <code>url</code> <pre><code>    Can be the path to a single file or a directory.\n</code></pre> <p> TYPE: <code>      str</code> </p> <code>limit</code> <pre><code>    Limit the number of files read from this directory.\n</code></pre> <p> TYPE: <code>    int</code> DEFAULT: <code>None</code> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the TreeNeuron.\n</code></pre> <p> TYPE: <code>    dict or None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.NeuronList</code> Source code in <code>navis/io/base.py</code> <pre><code>def read_ftp(\n    self,\n    url,\n    parallel=\"auto\",\n    limit: Optional[int] = None,\n    attrs: Optional[Dict[str, Any]] = None,\n) -&gt; \"core.NeuronList\":\n    \"\"\"Read files from an FTP server.\n\n    This is a dispatcher for `.read_from_ftp`.\n\n    Parameters\n    ----------\n    url :       str\n                Can be the path to a single file or a directory.\n    limit :     int, optional\n                Limit the number of files read from this directory.\n    attrs :     dict or None\n                Arbitrary attributes to include in the TreeNeuron.\n\n    Returns\n    -------\n    core.NeuronList\n\n    \"\"\"\n    # Remove the ftp:// prefix\n    url = url.replace(\"ftp://\", \"\")\n\n    # Split into server and path\n    server, path = url.split(\"/\", 1)\n\n    # Check if server contains a port\n    if \":\" in server:\n        server, port = server.split(\":\")\n        port = int(port)\n    else:\n        port = 21  # default port\n\n    read_fn = partial(self.read_from_ftp, attrs=attrs)\n    neurons = parallel_read_ftp(\n        read_fn=read_fn,\n        server=server,\n        port=port,\n        path=path,\n        file_ext=self.is_valid_file,\n        limit=limit,\n        parallel=parallel,\n    )\n    return self.format_output(neurons)\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.BaseReader.read_gs","title":"<code>read_gs</code>","text":"<p>Read files from an Google bucket.</p> <p>This is a dispatcher for <code>.read_from_gs</code>.</p> PARAMETER DESCRIPTION <code>path</code> <pre><code>    Can be the path to a directory, a single file or\n    a list of files inside the bucket.\n</code></pre> <p> TYPE: <code>     str | list thereof</code> </p> <code>limit</code> <pre><code>    Limit the number of files read from this directory.\n</code></pre> <p> TYPE: <code>    int</code> DEFAULT: <code>None</code> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the TreeNeuron.\n</code></pre> <p> TYPE: <code>    dict or None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.NeuronList</code> Source code in <code>navis/io/base.py</code> <pre><code>def read_gs(\n    self,\n    path,\n    parallel=\"auto\",\n    limit: Optional[int] = None,\n    attrs: Optional[Dict[str, Any]] = None,\n) -&gt; \"core.NeuronList\":\n    \"\"\"Read files from an Google bucket.\n\n    This is a dispatcher for `.read_from_gs`.\n\n    Parameters\n    ----------\n    path :      str | list thereof\n                Can be the path to a directory, a single file or\n                a list of files inside the bucket.\n    limit :     int, optional\n                Limit the number of files read from this directory.\n    attrs :     dict or None\n                Arbitrary attributes to include in the TreeNeuron.\n\n    Returns\n    -------\n    core.NeuronList\n\n    \"\"\"\n    # This will initialize the global gcsfs filesystem if necessary\n    gcsfs = get_gs_filesystem()\n\n    read_fn = partial(self.read_from_gs, gcsfs=gcsfs, attrs=attrs)\n    neurons = parallel_read_gs(\n        read_fn=read_fn,\n        gcsfs=gcsfs,\n        path=path,\n        file_ext=self.is_valid_file,\n        limit=limit,\n        parallel=parallel,\n    )\n    return self.format_output(neurons)\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.BaseReader.read_string","title":"<code>read_string</code>","text":"<p>Read single string into a Neuron.</p> PARAMETER DESCRIPTION <code>s</code> <pre><code>    String.\n</code></pre> <p> TYPE: <code>        str</code> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the neuron.\n</code></pre> <p> TYPE: <code>    dict or None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.BaseNeuron</code> Source code in <code>navis/io/base.py</code> <pre><code>def read_string(\n    self, s: str, attrs: Optional[Dict[str, Any]] = None\n) -&gt; \"core.BaseNeuron\":\n    \"\"\"Read single string into a Neuron.\n\n    Parameters\n    ----------\n    s :         str\n                String.\n    attrs :     dict or None\n                Arbitrary attributes to include in the neuron.\n\n    Returns\n    -------\n    core.BaseNeuron\n    \"\"\"\n    sio = io.StringIO(s)\n    return self.read_buffer(\n        sio,\n        attrs=merge_dicts({\"name\": self.name_fallback, \"origin\": \"string\"}, attrs),\n    )\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.BaseReader.read_tar","title":"<code>read_tar</code>","text":"<p>Read files from a tar archive into a NeuronList.</p> PARAMETER DESCRIPTION <code>fpath</code> <pre><code>    Path to tar file.\n</code></pre> <p> TYPE: <code>    str | os.PathLike</code> </p> <code>limit</code> <pre><code>    Limit the number of files read from this directory.\n</code></pre> <p> TYPE: <code>    int</code> DEFAULT: <code>None</code> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the TreeNeuron.\n</code></pre> <p> TYPE: <code>    dict or None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.NeuronList</code> Source code in <code>navis/io/base.py</code> <pre><code>def read_tar(\n    self,\n    fpath: os.PathLike,\n    limit: Optional[int] = None,\n    attrs: Optional[Dict[str, Any]] = None,\n    ignore_hidden: bool = True,\n) -&gt; \"core.NeuronList\":\n    \"\"\"Read files from a tar archive into a NeuronList.\n\n    Parameters\n    ----------\n    fpath :     str | os.PathLike\n                Path to tar file.\n    limit :     int, optional\n                Limit the number of files read from this directory.\n    attrs :     dict or None\n                Arbitrary attributes to include in the TreeNeuron.\n\n    Returns\n    -------\n    core.NeuronList\n\n    \"\"\"\n    p = Path(fpath).expanduser()\n    file_ext = self.is_valid_file\n\n    # Check the content of the tar file\n    # N.B. the TarInfo objects are hashable but the hash changes\n    # when the archive is re-opened. Therefore, we track the\n    # filenames and not the TarInfo objects.\n    to_read = []\n    with tarfile.open(p, \"r\") as tf:\n        for i, file in enumerate(tf):\n            fpath = file.name  # full path inside the tar\n            fname = fpath.split(\"/\")[-1]  # just the filename\n            if ignore_hidden and fname.startswith(\"._\"):\n                continue\n            if callable(file_ext):\n                if self.is_valid_file(file):\n                    to_read.append(fpath)\n            elif file_ext == \"*\":\n                to_read.append(fpath)\n            elif file_ext and fname.endswith(file_ext):\n                to_read.append(fpath)\n            elif \".\" not in file.filename:\n                to_read.append(fpath)\n\n            if isinstance(limit, int) and i &gt;= limit:\n                break\n\n    if isinstance(limit, list):\n        to_read = [f for f in to_read if f in limit]\n    elif isinstance(limit, slice):\n        to_read = to_read[limit]\n    elif isinstance(limit, str):\n        # Check if limit is a regex\n        if rgx.search(limit):\n            to_read = [f for f in to_read if re.search(limit, f.split(\"/\")[-1])]\n        else:\n            to_read = [f for f in to_read if limit in f.split(\"/\")[-1]]\n\n    # Wrapper for progess bar\n    prog = partial(\n        config.tqdm,\n        desc=\"Importing\",\n        total=len(to_read),\n        disable=config.pbar_hide,\n        leave=config.pbar_leave,\n    )\n\n    # N.B. tar.gz is a bunch of files concatenated and then compressed!\n    # In consequence, random access is very slow because we may have to seek\n    # through the whole archive to find the start of the requested file.\n    # The workaround is to open the archive in streaming (e.g. \"r|gz\") mode,\n    # iterate through the files in sequence and exract if the file is requested.\n    # This is also why we are not using parallel processing here.\n    # See also https://tinyurl.com/5n8wz54m (links to StackOverflow)\n    neurons = []\n    to_read = set(to_read)  # faster lookup\n    with prog() as pbar:\n        # Open the tar file in streaming mode with transparent compression\n        with tarfile.open(p, \"r|*\") as tf:\n            for t in tf:\n                # Skip files we don't want to read\n                if t.name not in to_read:\n                    continue\n                try:\n                    props = self.parse_filename(t.name.split(\"/\")[-1])\n                    props[\"origin\"] = str(p)\n                    n = self.read_bytes(\n                        tf.extractfile(t).read(),\n                        attrs=merge_dicts(props, attrs),\n                    )\n                    neurons.append(n)\n                    to_read.remove(t.name)\n                    pbar.update()\n                except BaseException as e:\n                    if self.errors == \"ignore\":\n                        logger.warning(f'Failed to read \"{t.name}\" from tar.')\n                    else:\n                        raise\n\n                # If we have read all (requested) files we can stop\n                if not len(to_read):\n                    break\n\n    return self.format_output(neurons)\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.BaseReader.read_url","title":"<code>read_url</code>","text":"<p>Read file from URL into a neuron.</p> PARAMETER DESCRIPTION <code>url</code> <pre><code>    URL to file.\n</code></pre> <p> TYPE: <code>      str</code> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the neuron.\n</code></pre> <p> TYPE: <code>    dict or None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.BaseNeuron</code> Source code in <code>navis/io/base.py</code> <pre><code>def read_url(\n    self, url: str, attrs: Optional[Dict[str, Any]] = None\n) -&gt; \"core.BaseNeuron\":\n    \"\"\"Read file from URL into a neuron.\n\n    Parameters\n    ----------\n    url :       str\n                URL to file.\n    attrs :     dict or None\n                Arbitrary attributes to include in the neuron.\n\n    Returns\n    -------\n    core.BaseNeuron\n    \"\"\"\n    # Note: originally, we used stream=True and passed `r.raw` to the\n    # read_buffer function but that caused issue when there was more\n    # than one chunk which would require us to concatenate the chunks\n    # `via r.raw.iter_content()`.\n    # Instead, we will simply read the whole content, wrap it in a BytesIO\n    # and pass that to the read_buffer function. This is not ideal as it\n    # will load the whole file into memory while the streaming solution\n    # may have raised an exception earlier if the file was corrupted or\n    # the wrong format.\n    with requests.get(url, stream=False) as r:\n        r.raise_for_status()\n        props = self.parse_filename(url.split(\"/\")[-1])\n        props[\"origin\"] = url\n        return self.read_buffer(\n            io.BytesIO(r.content), attrs=merge_dicts(props, attrs)\n        )\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.BaseReader.read_zip","title":"<code>read_zip</code>","text":"<p>Read files from a zip into a NeuronList.</p> <p>This is a dispatcher for <code>.read_from_zip</code>.</p> PARAMETER DESCRIPTION <code>fpath</code> <pre><code>    Path to zip file.\n</code></pre> <p> TYPE: <code>    str | os.PathLike</code> </p> <code>limit</code> <pre><code>    Limit the number of files read from this directory.\n</code></pre> <p> TYPE: <code>    int</code> DEFAULT: <code>None</code> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the TreeNeuron.\n</code></pre> <p> TYPE: <code>    dict or None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.NeuronList</code> Source code in <code>navis/io/base.py</code> <pre><code>def read_zip(\n    self,\n    fpath: os.PathLike,\n    parallel=\"auto\",\n    limit: Optional[int] = None,\n    attrs: Optional[Dict[str, Any]] = None,\n) -&gt; \"core.NeuronList\":\n    \"\"\"Read files from a zip into a NeuronList.\n\n    This is a dispatcher for `.read_from_zip`.\n\n    Parameters\n    ----------\n    fpath :     str | os.PathLike\n                Path to zip file.\n    limit :     int, optional\n                Limit the number of files read from this directory.\n    attrs :     dict or None\n                Arbitrary attributes to include in the TreeNeuron.\n\n    Returns\n    -------\n    core.NeuronList\n\n    \"\"\"\n    fpath = Path(fpath).expanduser()\n    read_fn = partial(self.read_from_zip, zippath=fpath, attrs=attrs)\n    neurons = parallel_read_archive(\n        read_fn=read_fn,\n        fpath=fpath,\n        file_ext=self.is_valid_file,\n        limit=limit,\n        parallel=parallel,\n    )\n    return self.format_output(neurons)\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.ImageReader","title":"<code>navis.io.base.ImageReader</code>","text":"<p>Reader for image data.</p> Source code in <code>navis/io/base.py</code> <pre><code>class ImageReader(BaseReader):\n    \"\"\"Reader for image data.\"\"\"\n\n    def __init__(self, output, thin, threshold, dotprop_kwargs, **kwargs):\n        super().__init__(**kwargs)\n        self.output = output\n        self.thin = thin\n        self.threshold = threshold\n        self.dotprop_kwargs = dotprop_kwargs\n\n    def convert_image(self, data, attrs, header, voxdim, units, space_units):\n        \"\"\"Convert image data to desired output.\n\n        Parameters\n        ----------\n        data :          np.ndarray\n                        Image/Voxel data.\n        attrs :         dict\n                        Additional attributes to associate with the neuron.\n        header :        dict\n                        Header information.\n        voxdim :        list of numbers\n                        Voxel dimensions.\n        units :         str | list, optional\n                        Units for the neuron (e.g. \"1um\" or [\"4um\", \"4um\", \"40um\"]).\n        space_units :   str, optional\n                        Space units (e.g. \"um\").\n\n        \"\"\"\n        if self.output == \"dotprops\":\n            # If we're trying to get voxels from an image\n            if data.ndim == 3:\n                if self.threshold:\n                    if self.threshold &gt;= 1:\n                        data = data &gt;= self.threshold\n                    elif self.threshold &lt; 1 and self.threshold &gt; 0:\n                        data = data &gt;= (self.threshold * data.max())\n                    else:\n                        raise ValueError(\n                            f\"Threshold must be either &gt;=1 or 0-1, got {self.threshold}\"\n                        )\n\n                if self.thin:\n                    from skimage.morphology import skeletonize\n\n                    data = skeletonize(data)\n\n                # Convert data to x/y/z coordinates\n                # Note we need to multiply units before creating the Dotprops\n                # - otherwise the KNN will be wrong\n                x, y, z = np.where(data)\n                points = np.vstack((x, y, z)).T\n                points = points * voxdim\n\n                if not len(points):\n                    raise ValueError(\n                        f\"No points extracted from {self.name_fallback} file. Try lowering the threshold?\"\n                    )\n\n                x = core.make_dotprops(points, **self.dotprop_kwargs)\n            elif data.ndim == 2:\n                if data.shape[1] == 3:\n                    points, vect, alpha = data, None, None\n                elif data.shape[1] == 6:\n                    points, vect, alpha = data[:, :3], data[:, 3:6], None\n                elif data.shape[1] == 7:\n                    points, vect, alpha = data[:, :3], data[:, 3:6], data[:, 6]\n                else:\n                    raise ValueError(\n                        \"Expected data to be either (N, 3), (N, 6) \"\n                        f\"or (N, 7) but {self.name_fallback} file contains {data.shape}\"\n                    )\n                # Get `k` either from provided kwargs or the file's header\n                k = self.dotprop_kwargs.pop(\"k\", header.get(\"k\", 20))\n\n                x = core.Dotprops(\n                    points, k=k, vect=vect, alpha=alpha, **self.dotprop_kwargs\n                )\n            else:\n                raise ValueError(\n                    \"Data must be 2- or 3-dimensional to extract \"\n                    f\"Dotprops, got {data.ndim}\"\n                )\n\n            # Set units from space_units (points are already in physical space)\n            if space_units:\n                if isinstance(space_units, str):\n                    x.units = f\"1 {space_units}\"\n                elif len(space_units) == 3:\n                    x.units = [f\"1 {s}\" for s in space_units]\n        else:\n            if data.ndim == 2:\n                logger.warning(\n                    f\"Data in {self.name_fallback} file is of shape {data.shape} - \"\n                    \"i.e. 2D. Could this be a point cloud or dotprops \"\n                    \"instead of voxels?\"\n                )\n            x = core.VoxelNeuron(data, units=units)\n\n        # Header is special - we do not want to register it\n        setattr(x, f\"{self.name_fallback.lower()}_header\", header)\n\n        # Try adding properties one-by-one. If one fails, we'll keep track of it\n        # in the `.meta` attribute\n        meta = {}\n        for k, v in attrs.items():\n            try:\n                x._register_attr(k, v)\n            except (AttributeError, ValueError, TypeError):\n                meta[k] = v\n\n        if meta:\n            x.meta = meta\n\n        return x\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.ImageReader.convert_image","title":"<code>convert_image</code>","text":"<p>Convert image data to desired output.</p> PARAMETER DESCRIPTION <code>data</code> <pre><code>        Image/Voxel data.\n</code></pre> <p> TYPE: <code>         np.ndarray</code> </p> <code>attrs</code> <pre><code>        Additional attributes to associate with the neuron.\n</code></pre> <p> TYPE: <code>        dict</code> </p> <code>header</code> <pre><code>        Header information.\n</code></pre> <p> TYPE: <code>       dict</code> </p> <code>voxdim</code> <pre><code>        Voxel dimensions.\n</code></pre> <p> TYPE: <code>       list of numbers</code> </p> <code>units</code> <pre><code>        Units for the neuron (e.g. \"1um\" or [\"4um\", \"4um\", \"40um\"]).\n</code></pre> <p> TYPE: <code>        str | list</code> </p> <code>space_units</code> <pre><code>        Space units (e.g. \"um\").\n</code></pre> <p> TYPE: <code>  str</code> </p> Source code in <code>navis/io/base.py</code> <pre><code>def convert_image(self, data, attrs, header, voxdim, units, space_units):\n    \"\"\"Convert image data to desired output.\n\n    Parameters\n    ----------\n    data :          np.ndarray\n                    Image/Voxel data.\n    attrs :         dict\n                    Additional attributes to associate with the neuron.\n    header :        dict\n                    Header information.\n    voxdim :        list of numbers\n                    Voxel dimensions.\n    units :         str | list, optional\n                    Units for the neuron (e.g. \"1um\" or [\"4um\", \"4um\", \"40um\"]).\n    space_units :   str, optional\n                    Space units (e.g. \"um\").\n\n    \"\"\"\n    if self.output == \"dotprops\":\n        # If we're trying to get voxels from an image\n        if data.ndim == 3:\n            if self.threshold:\n                if self.threshold &gt;= 1:\n                    data = data &gt;= self.threshold\n                elif self.threshold &lt; 1 and self.threshold &gt; 0:\n                    data = data &gt;= (self.threshold * data.max())\n                else:\n                    raise ValueError(\n                        f\"Threshold must be either &gt;=1 or 0-1, got {self.threshold}\"\n                    )\n\n            if self.thin:\n                from skimage.morphology import skeletonize\n\n                data = skeletonize(data)\n\n            # Convert data to x/y/z coordinates\n            # Note we need to multiply units before creating the Dotprops\n            # - otherwise the KNN will be wrong\n            x, y, z = np.where(data)\n            points = np.vstack((x, y, z)).T\n            points = points * voxdim\n\n            if not len(points):\n                raise ValueError(\n                    f\"No points extracted from {self.name_fallback} file. Try lowering the threshold?\"\n                )\n\n            x = core.make_dotprops(points, **self.dotprop_kwargs)\n        elif data.ndim == 2:\n            if data.shape[1] == 3:\n                points, vect, alpha = data, None, None\n            elif data.shape[1] == 6:\n                points, vect, alpha = data[:, :3], data[:, 3:6], None\n            elif data.shape[1] == 7:\n                points, vect, alpha = data[:, :3], data[:, 3:6], data[:, 6]\n            else:\n                raise ValueError(\n                    \"Expected data to be either (N, 3), (N, 6) \"\n                    f\"or (N, 7) but {self.name_fallback} file contains {data.shape}\"\n                )\n            # Get `k` either from provided kwargs or the file's header\n            k = self.dotprop_kwargs.pop(\"k\", header.get(\"k\", 20))\n\n            x = core.Dotprops(\n                points, k=k, vect=vect, alpha=alpha, **self.dotprop_kwargs\n            )\n        else:\n            raise ValueError(\n                \"Data must be 2- or 3-dimensional to extract \"\n                f\"Dotprops, got {data.ndim}\"\n            )\n\n        # Set units from space_units (points are already in physical space)\n        if space_units:\n            if isinstance(space_units, str):\n                x.units = f\"1 {space_units}\"\n            elif len(space_units) == 3:\n                x.units = [f\"1 {s}\" for s in space_units]\n    else:\n        if data.ndim == 2:\n            logger.warning(\n                f\"Data in {self.name_fallback} file is of shape {data.shape} - \"\n                \"i.e. 2D. Could this be a point cloud or dotprops \"\n                \"instead of voxels?\"\n            )\n        x = core.VoxelNeuron(data, units=units)\n\n    # Header is special - we do not want to register it\n    setattr(x, f\"{self.name_fallback.lower()}_header\", header)\n\n    # Try adding properties one-by-one. If one fails, we'll keep track of it\n    # in the `.meta` attribute\n    meta = {}\n    for k, v in attrs.items():\n        try:\n            x._register_attr(k, v)\n        except (AttributeError, ValueError, TypeError):\n            meta[k] = v\n\n    if meta:\n        x.meta = meta\n\n    return x\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.ReadError","title":"<code>navis.io.base.ReadError</code>","text":"<p>Error raised when reading a file fails.</p> Source code in <code>navis/io/base.py</code> <pre><code>class ReadError(Exception):\n    \"\"\"Error raised when reading a file fails.\"\"\"\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.Writer","title":"<code>navis.io.base.Writer</code>","text":"<p>Writer class that takes care of things like filenames, archives, etc.</p> PARAMETER DESCRIPTION <code>write_func</code> <pre><code>        Writer function to write a single file to disk. Must\n        accept a `filepath` parameter.\n</code></pre> <p> TYPE: <code>   callable</code> </p> <code>ext</code> <pre><code>        File extension - e.g. '.swc'.\n</code></pre> <p> TYPE: <code>          str</code> </p> Source code in <code>navis/io/base.py</code> <pre><code>class Writer:\n    \"\"\"Writer class that takes care of things like filenames, archives, etc.\n\n    Parameters\n    ----------\n    write_func :    callable\n                    Writer function to write a single file to disk. Must\n                    accept a `filepath` parameter.\n    ext :           str, optional\n                    File extension - e.g. '.swc'.\n\n    \"\"\"\n\n    def __init__(self, write_func, ext):\n        assert callable(write_func)\n        if ext:\n            assert isinstance(ext, str) and ext.startswith(\".\")\n        self.write_func = write_func\n        self.ext = ext\n\n    def write_single(self, x, filepath, **kwargs):\n        \"\"\"Write single object to file.\"\"\"\n        # try to str.format any path-like\n        try:\n            as_str = os.fspath(filepath)\n        except TypeError:\n            raise ValueError(\n                f'`filepath` must be str or pathlib.Path, got \"{type(filepath)}\"'\n            )\n\n        # Format filename (e.g. \"{neuron.name}.swc\")\n        formatted_str = as_str.format(neuron=x)\n\n        # If it was formatted, make sure it has correct extension\n        if self.ext and formatted_str != as_str and not as_str.endswith(self.ext):\n            raise ValueError(f\"Formattable filepaths must end with '{self.ext}'\")\n\n        filepath = Path(formatted_str)\n\n        # Expand user - otherwise .exists() might fail\n        filepath = filepath.expanduser()\n\n        # If not specified, generate filename\n        if self.ext and not str(filepath).endswith(self.ext):\n            filepath = filepath / f\"{x.id}{self.ext}\"\n\n        # Make sure the parent directory exists\n        if not filepath.parent.exists():\n            raise ValueError(f\"Parent folder {filepath.parent} must exist.\")\n\n        # Track the path we put this (and presumably all other files in)\n        self.path = Path(filepath)\n        while not self.path.is_dir():\n            self.path = self.path.parent\n\n        return self.write_func(x, filepath=filepath, **kwargs)\n\n    def write_many(self, x, filepath, **kwargs):\n        \"\"\"Write multiple files to folder.\"\"\"\n        if not utils.is_iterable(filepath):\n            # Assume this is a folder if it doesn't end with e.g. '.swc'\n            is_filename = str(filepath).endswith(self.ext) if self.ext else False\n            is_single = len(x) == 1\n            is_formattable = \"{\" in str(filepath) and \"}\" in str(filepath)\n            if not is_filename or is_single or is_formattable:\n                filepath = [filepath] * len(x)\n            else:\n                raise ValueError(\n                    \"`filepath` must either be a folder, a \"\n                    \"formattable filepath or a list of filepaths\"\n                    \"when saving multiple neurons.\"\n                )\n\n        if len(filepath) != len(x):\n            raise ValueError(f\"Got {len(filepath)} file names for {len(x)} neurons.\")\n\n        # At this point filepath is iterable\n        filepath: Iterable[str]\n        for n, f in config.tqdm(\n            zip(x, filepath),\n            disable=config.pbar_hide,\n            leave=config.pbar_leave,\n            total=len(x),\n            desc=\"Writing\",\n        ):\n            self.write_single(n, filepath=f, **kwargs)\n\n    def write_zip(self, x, filepath, **kwargs):\n        \"\"\"Write files to zip.\"\"\"\n        filepath = Path(filepath).expanduser()\n        # Parse pattern, if given\n        pattern = \"{neuron.id}\" + (self.ext if self.ext else \"\")\n        if \"@\" in str(filepath):\n            pattern, filename = filepath.name.split(\"@\")\n            filepath = filepath.parent / filename\n\n        # Make sure we have an iterable\n        x = core.NeuronList(x)\n\n        with ZipFile(filepath, mode=\"w\") as zf:\n            # Context-manager will remove temporary directory and its contents\n            with tempfile.TemporaryDirectory() as tempdir:\n                for n in config.tqdm(\n                    x,\n                    disable=config.pbar_hide,\n                    leave=config.pbar_leave,\n                    total=len(x),\n                    desc=\"Writing\",\n                ):\n                    # Save to temporary file\n                    f = None\n                    try:\n                        # Generate temporary filename\n                        f = os.path.join(tempdir, pattern.format(neuron=n))\n                        # Write to temporary file\n                        self.write_single(n, filepath=f, **kwargs)\n                        # Add file to zip\n                        zf.write(\n                            f,\n                            arcname=pattern.format(neuron=n),\n                            compress_type=compression,\n                        )\n                    except BaseException:\n                        raise\n                    finally:\n                        # Remove temporary file - we do this inside the loop\n                        # to avoid unnecessarily occupying space as we write\n                        if f:\n                            os.remove(f)\n\n        # Set filepath to zipfile -&gt; this overwrite filepath set in write_single\n        # (which would be the temporary file)\n        self.path = Path(filepath)\n\n    def write_any(self, x, filepath, **kwargs):\n        \"\"\"Write any to file. Default entry point.\"\"\"\n        # If target is a zipfile\n        if isinstance(filepath, (str, Path)) and str(filepath).endswith(\".zip\"):\n            return self.write_zip(x, filepath=filepath, **kwargs)\n        elif isinstance(x, core.NeuronList):\n            return self.write_many(x, filepath=filepath, **kwargs)\n        else:\n            return self.write_single(x, filepath=filepath, **kwargs)\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.Writer.write_any","title":"<code>write_any</code>","text":"<p>Write any to file. Default entry point.</p> Source code in <code>navis/io/base.py</code> <pre><code>def write_any(self, x, filepath, **kwargs):\n    \"\"\"Write any to file. Default entry point.\"\"\"\n    # If target is a zipfile\n    if isinstance(filepath, (str, Path)) and str(filepath).endswith(\".zip\"):\n        return self.write_zip(x, filepath=filepath, **kwargs)\n    elif isinstance(x, core.NeuronList):\n        return self.write_many(x, filepath=filepath, **kwargs)\n    else:\n        return self.write_single(x, filepath=filepath, **kwargs)\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.Writer.write_many","title":"<code>write_many</code>","text":"<p>Write multiple files to folder.</p> Source code in <code>navis/io/base.py</code> <pre><code>def write_many(self, x, filepath, **kwargs):\n    \"\"\"Write multiple files to folder.\"\"\"\n    if not utils.is_iterable(filepath):\n        # Assume this is a folder if it doesn't end with e.g. '.swc'\n        is_filename = str(filepath).endswith(self.ext) if self.ext else False\n        is_single = len(x) == 1\n        is_formattable = \"{\" in str(filepath) and \"}\" in str(filepath)\n        if not is_filename or is_single or is_formattable:\n            filepath = [filepath] * len(x)\n        else:\n            raise ValueError(\n                \"`filepath` must either be a folder, a \"\n                \"formattable filepath or a list of filepaths\"\n                \"when saving multiple neurons.\"\n            )\n\n    if len(filepath) != len(x):\n        raise ValueError(f\"Got {len(filepath)} file names for {len(x)} neurons.\")\n\n    # At this point filepath is iterable\n    filepath: Iterable[str]\n    for n, f in config.tqdm(\n        zip(x, filepath),\n        disable=config.pbar_hide,\n        leave=config.pbar_leave,\n        total=len(x),\n        desc=\"Writing\",\n    ):\n        self.write_single(n, filepath=f, **kwargs)\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.Writer.write_single","title":"<code>write_single</code>","text":"<p>Write single object to file.</p> Source code in <code>navis/io/base.py</code> <pre><code>def write_single(self, x, filepath, **kwargs):\n    \"\"\"Write single object to file.\"\"\"\n    # try to str.format any path-like\n    try:\n        as_str = os.fspath(filepath)\n    except TypeError:\n        raise ValueError(\n            f'`filepath` must be str or pathlib.Path, got \"{type(filepath)}\"'\n        )\n\n    # Format filename (e.g. \"{neuron.name}.swc\")\n    formatted_str = as_str.format(neuron=x)\n\n    # If it was formatted, make sure it has correct extension\n    if self.ext and formatted_str != as_str and not as_str.endswith(self.ext):\n        raise ValueError(f\"Formattable filepaths must end with '{self.ext}'\")\n\n    filepath = Path(formatted_str)\n\n    # Expand user - otherwise .exists() might fail\n    filepath = filepath.expanduser()\n\n    # If not specified, generate filename\n    if self.ext and not str(filepath).endswith(self.ext):\n        filepath = filepath / f\"{x.id}{self.ext}\"\n\n    # Make sure the parent directory exists\n    if not filepath.parent.exists():\n        raise ValueError(f\"Parent folder {filepath.parent} must exist.\")\n\n    # Track the path we put this (and presumably all other files in)\n    self.path = Path(filepath)\n    while not self.path.is_dir():\n        self.path = self.path.parent\n\n    return self.write_func(x, filepath=filepath, **kwargs)\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.Writer.write_zip","title":"<code>write_zip</code>","text":"<p>Write files to zip.</p> Source code in <code>navis/io/base.py</code> <pre><code>def write_zip(self, x, filepath, **kwargs):\n    \"\"\"Write files to zip.\"\"\"\n    filepath = Path(filepath).expanduser()\n    # Parse pattern, if given\n    pattern = \"{neuron.id}\" + (self.ext if self.ext else \"\")\n    if \"@\" in str(filepath):\n        pattern, filename = filepath.name.split(\"@\")\n        filepath = filepath.parent / filename\n\n    # Make sure we have an iterable\n    x = core.NeuronList(x)\n\n    with ZipFile(filepath, mode=\"w\") as zf:\n        # Context-manager will remove temporary directory and its contents\n        with tempfile.TemporaryDirectory() as tempdir:\n            for n in config.tqdm(\n                x,\n                disable=config.pbar_hide,\n                leave=config.pbar_leave,\n                total=len(x),\n                desc=\"Writing\",\n            ):\n                # Save to temporary file\n                f = None\n                try:\n                    # Generate temporary filename\n                    f = os.path.join(tempdir, pattern.format(neuron=n))\n                    # Write to temporary file\n                    self.write_single(n, filepath=f, **kwargs)\n                    # Add file to zip\n                    zf.write(\n                        f,\n                        arcname=pattern.format(neuron=n),\n                        compress_type=compression,\n                    )\n                except BaseException:\n                    raise\n                finally:\n                    # Remove temporary file - we do this inside the loop\n                    # to avoid unnecessarily occupying space as we write\n                    if f:\n                        os.remove(f)\n\n    # Set filepath to zipfile -&gt; this overwrite filepath set in write_single\n    # (which would be the temporary file)\n    self.path = Path(filepath)\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.get_gs_filesystem","title":"<code>navis.io.base.get_gs_filesystem</code>","text":"<p>Get global gcsfs filesystem, initializing if necessary.</p> Source code in <code>navis/io/base.py</code> <pre><code>def get_gs_filesystem():\n    \"\"\"Get global gcsfs filesystem, initializing if necessary.\"\"\"\n    try:\n        import gcsfs\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"The `gcsfs` package is required to read from Google \"\n            \"storage buckets. Please install it via `pip install gcsfs`.\"\n        )\n\n    # Initialise only once\n    global GCSFS_GLOBAL\n    if GCSFS_GLOBAL is None:\n        GCSFS_GLOBAL = gcsfs.GCSFileSystem(asynchronous=False)\n\n    return GCSFS_GLOBAL\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.handle_errors","title":"<code>navis.io.base.handle_errors</code>","text":"<p>Decorator for read_buffer and read_dataframe methods to handle errors.</p> <p>Catches exceptions, logs/raises and potentially return <code>None</code>.</p> <p>Note: various other BaseReader methods have their own error handling.</p> PARAMETER DESCRIPTION <code>func</code> <p>Function to wrap.</p> <p> TYPE: <code>callable</code> </p> RETURNS DESCRIPTION <code>callable</code> <p>Wrapped function.</p> Source code in <code>navis/io/base.py</code> <pre><code>def handle_errors(func):\n    \"\"\"Decorator for read_buffer and read_dataframe methods to handle errors.\n\n    Catches exceptions, logs/raises and potentially return `None`.\n\n    Note: various other BaseReader methods have their own error handling.\n\n    Parameters\n    ----------\n    func : callable\n        Function to wrap.\n\n    Returns\n    -------\n    callable\n        Wrapped function.\n\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        self = args[0]\n        attrs = kwargs.get(\"attrs\", {})  # we rely on this being a keyword argument!\n        try:\n            return func(*args, **kwargs)\n        except BaseException as e:\n            # Check if we can provide any hint as to which file failed\n            id = self.name_fallback\n            for a in (\"file\", \"origin\", \"name\"):\n                if a in attrs:\n                    id = attrs[a]\n                    break\n\n            if self.errors == \"raise\":\n                raise ReadError(\n                    f\"Error reading {id}. See above traceback for details.\"\n                ) from e\n            elif self.errors == \"log\":\n                logger.exception(f\"Failed to read {id}\", exc_info=True)\n\n            return None\n\n    return wrapper\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.is_dir","title":"<code>navis.io.base.is_dir</code>","text":"<p>Check if path is a directory.</p> <p>The main purpose of this function is to catch *.file_ext at the end of the path.</p> PARAMETER DESCRIPTION <code>path</code> <p>Path to check.</p> <p> TYPE: <code>os.PathLike</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if path is a directory.</p> Source code in <code>navis/io/base.py</code> <pre><code>def is_dir(path: os.PathLike) -&gt; bool:\n    \"\"\"Check if path is a directory.\n\n    The main purpose of this function is to catch\n    *.file_ext at the end of the path.\n\n    Parameters\n    ----------\n    path : os.PathLike\n        Path to check.\n\n    Returns\n    -------\n    bool\n        True if path is a directory.\n\n    \"\"\"\n    # Expand user\n    path = os.path.expanduser(path)\n\n    # Remove any trailing *.file_ext\n    path = path.split(\"*\")[0]\n\n    return os.path.isdir(path)\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.merge_dicts","title":"<code>navis.io.base.merge_dicts</code>","text":"<p>Merge dicts and kwargs left to right.</p> <p>Ignores None arguments.</p> Source code in <code>navis/io/base.py</code> <pre><code>def merge_dicts(*dicts: Optional[Dict], **kwargs) -&gt; Dict:\n    \"\"\"Merge dicts and kwargs left to right.\n\n    Ignores None arguments.\n    \"\"\"\n    # handles nones\n    out = dict()\n    for d in dicts:\n        if d:\n            out.update(d)\n    out.update(kwargs)\n    return out\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.parallel_read","title":"<code>navis.io.base.parallel_read</code>","text":"<p>Read neurons from some objects with the given reader function, potentially in parallel.</p> <p>Reader function must be picklable.</p> PARAMETER DESCRIPTION <code>read_fn</code> <p> TYPE: <code>      Callable</code> </p> <code>objs</code> <p> TYPE: <code>         Iterable</code> </p> <code>parallel</code> <pre><code>        \"auto\" or True for `n_cores` // 2, otherwise int for number\n        of jobs, or false for serial.\n</code></pre> <p> TYPE: <code>     str | bool | int</code> DEFAULT: <code>'auto'</code> </p> RETURNS DESCRIPTION <code>core.NeuronList</code> Source code in <code>navis/io/base.py</code> <pre><code>def parallel_read(read_fn, objs, parallel=\"auto\") -&gt; List[\"core.NeuronList\"]:\n    \"\"\"Read neurons from some objects with the given reader function,\n    potentially in parallel.\n\n    Reader function must be picklable.\n\n    Parameters\n    ----------\n    read_fn :       Callable\n    objs :          Iterable\n    parallel :      str | bool | int\n                    \"auto\" or True for `n_cores` // 2, otherwise int for number\n                    of jobs, or false for serial.\n\n    Returns\n    -------\n    core.NeuronList\n\n    \"\"\"\n    try:\n        length = len(objs)\n    except TypeError:\n        length = None\n\n    prog = partial(\n        config.tqdm,\n        desc=\"Importing\",\n        total=length,\n        disable=config.pbar_hide,\n        leave=config.pbar_leave,\n    )\n\n    # `parallel` can be (\"auto\", threshold) in which case `threshold`\n    # determines at what length we use parallel processing\n    if isinstance(parallel, tuple):\n        parallel, threshold = parallel\n    else:\n        threshold = 200\n\n    if (\n        isinstance(parallel, str)\n        and parallel.lower() == \"auto\"\n        and not isinstance(length, type(None))\n        and length &lt; threshold\n    ):\n        parallel = False\n\n    if parallel:\n        # Do not swap this as `isinstance(True, int)` returns `True`\n        if isinstance(parallel, (bool, str)):\n            n_cores = max(1, os.cpu_count() // 2)\n        else:\n            n_cores = int(parallel)\n\n        # If all objects are URLs, we will use threads instead of processes\n        if all(\n            isinstance(obj, str)\n            and (obj.startswith(\"http://\") or obj.startswith(\"https://\"))\n            for obj in objs\n        ):\n            session = FuturesSession(max_workers=n_cores)\n            futures = {session.get(url): url for url in objs}\n            neurons = []\n            for fut, url in prog(futures.items()):\n                r = fut.result()\n                r.raise_for_status()\n                neurons.append(read_fn(r.content))\n        else:\n            with mp.Pool(processes=n_cores) as pool:\n                results = pool.imap(read_fn, objs)\n                neurons = list(prog(results))\n    else:\n        neurons = [read_fn(obj) for obj in prog(objs)]\n\n    return neurons\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.parallel_read_archive","title":"<code>navis.io.base.parallel_read_archive</code>","text":"<p>Read neurons from a ZIP archive, potentially in parallel.</p> <p>Reader function must be picklable.</p> PARAMETER DESCRIPTION <code>read_fn</code> <p> TYPE: <code>      Callable</code> </p> <code>fpath</code> <p> TYPE: <code>        str | Path</code> </p> <code>file_ext</code> <pre><code>        File extension to search for - e.g. \".swc\". `None` or `''`\n        are interpreted as looking for filenames without extension.\n        To include all files use `'*'`. Can also be callable that\n        accepts a filename and returns True or False depending on\n        if it should be included.\n</code></pre> <p> TYPE: <code>     str | callable</code> </p> <code>limit</code> <pre><code>        Limit the number of files read from this directory.\n</code></pre> <p> TYPE: <code>        int</code> DEFAULT: <code>None</code> </p> <code>parallel</code> <pre><code>        \"auto\" or True for n_cores // 2, otherwise int for number of\n        jobs, or false for serial.\n</code></pre> <p> TYPE: <code>     str | bool | int</code> DEFAULT: <code>'auto'</code> </p> <code>ignore_hidden</code> <pre><code>        Archives zipped on OSX can end up containing a\n        `__MACOSX` folder with files that mirror the name of other\n        files. For example if there is a `123456.swc` in the archive\n        you might also find a `__MACOSX/._123456.swc`. Reading the\n        latter will result in an error. If ignore_hidden=True\n        we will simply ignore all file that starts with \"._\".\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>core.NeuronList</code> Source code in <code>navis/io/base.py</code> <pre><code>def parallel_read_archive(\n    read_fn,\n    fpath,\n    file_ext,\n    reader=None,\n    limit=None,\n    parallel=\"auto\",\n    ignore_hidden=True,\n) -&gt; List[\"core.NeuronList\"]:\n    \"\"\"Read neurons from a ZIP archive, potentially in parallel.\n\n    Reader function must be picklable.\n\n    Parameters\n    ----------\n    read_fn :       Callable\n    fpath :         str | Path\n    file_ext :      str | callable\n                    File extension to search for - e.g. \".swc\". `None` or `''`\n                    are interpreted as looking for filenames without extension.\n                    To include all files use `'*'`. Can also be callable that\n                    accepts a filename and returns True or False depending on\n                    if it should be included.\n    limit :         int, optional\n                    Limit the number of files read from this directory.\n    parallel :      str | bool | int\n                    \"auto\" or True for n_cores // 2, otherwise int for number of\n                    jobs, or false for serial.\n    ignore_hidden : bool\n                    Archives zipped on OSX can end up containing a\n                    `__MACOSX` folder with files that mirror the name of other\n                    files. For example if there is a `123456.swc` in the archive\n                    you might also find a `__MACOSX/._123456.swc`. Reading the\n                    latter will result in an error. If ignore_hidden=True\n                    we will simply ignore all file that starts with \"._\".\n\n    Returns\n    -------\n    core.NeuronList\n\n    \"\"\"\n    # Check zip content\n    p = Path(fpath)\n    to_read = []\n    with ZipFile(p, \"r\") as zip:\n        for i, file in enumerate(zip.filelist):\n            fname = file.filename.split(\"/\")[-1]\n            if ignore_hidden and fname.startswith(\"._\"):\n                continue\n            if callable(file_ext):\n                if file_ext(file):\n                    to_read.append(file)\n            elif file_ext == \"*\":\n                to_read.append(file)\n            elif file_ext and fname.endswith(file_ext):\n                to_read.append(file)\n            elif \".\" not in file.filename:\n                to_read.append(file)\n\n            if isinstance(limit, int) and i &gt;= limit:\n                break\n\n    if isinstance(limit, list):\n        to_read = [f for f in to_read if f in limit]\n    elif isinstance(limit, slice):\n        to_read = to_read[limit]\n    elif isinstance(limit, str):\n        # Check if limit is a regex\n        if rgx.search(limit):\n            to_read = [f for f in to_read if re.search(limit, f.filename)]\n        else:\n            to_read = [f for f in to_read if limit in f.filename]\n\n    prog = partial(\n        config.tqdm,\n        desc=\"Importing\",\n        total=len(to_read),\n        disable=config.pbar_hide,\n        leave=config.pbar_leave,\n    )\n\n    # `parallel` can be (\"auto\", threshold) in which case `threshold`\n    # determines at what length we use parallel processing\n    if isinstance(parallel, tuple):\n        parallel, threshold = parallel\n    else:\n        threshold = 200\n\n    if (\n        isinstance(parallel, str)\n        and parallel.lower() == \"auto\"\n        and len(to_read) &lt; threshold\n    ):\n        parallel = False\n\n    if parallel:\n        # Do not swap this as `isinstance(True, int)` returns `True`\n        if isinstance(parallel, (bool, str)):\n            n_cores = max(1, os.cpu_count() // 2)\n        else:\n            n_cores = int(parallel)\n\n        with mp.Pool(processes=n_cores) as pool:\n            results = pool.imap(read_fn, to_read)\n            neurons = list(prog(results))\n    else:\n        neurons = [read_fn(obj) for obj in prog(to_read)]\n\n    return neurons\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.parallel_read_ftp","title":"<code>navis.io.base.parallel_read_ftp</code>","text":"<p>Read neurons from an FTP server, potentially in parallel.</p> <p>Reader function must be picklable.</p> PARAMETER DESCRIPTION <code>read_fn</code> <p> TYPE: <code>      Callable</code> </p> <code>server</code> <pre><code>        FTP server address.\n</code></pre> <p> TYPE: <code>       str</code> </p> <code>port</code> <pre><code>        FTP server port.\n</code></pre> <p> TYPE: <code>         int</code> </p> <code>path</code> <pre><code>        Path to directory containing files or single file.\n</code></pre> <p> TYPE: <code>         str</code> </p> <code>file_ext</code> <pre><code>        File extension to search for - e.g. \".swc\". `None` or `''`\n        are interpreted as looking for filenames without extension.\n        To include all files use `'*'`. Can also be callable that\n        accepts a filename and returns True or False depending on\n        if it should be included.\n</code></pre> <p> TYPE: <code>     str | callable</code> </p> <code>limit</code> <pre><code>        Limit the number of files read from this directory.\n</code></pre> <p> TYPE: <code>        int</code> DEFAULT: <code>None</code> </p> <code>parallel</code> <pre><code>        \"auto\" or True for n_cores // 2, otherwise int for number of\n        jobs, or false for serial.\n</code></pre> <p> TYPE: <code>     str | bool | int</code> DEFAULT: <code>'auto'</code> </p> RETURNS DESCRIPTION <code>core.NeuronList</code> Source code in <code>navis/io/base.py</code> <pre><code>def parallel_read_ftp(\n    read_fn,\n    server,\n    port,\n    path,\n    file_ext,\n    limit=None,\n    parallel=\"auto\",\n) -&gt; List[\"core.NeuronList\"]:\n    \"\"\"Read neurons from an FTP server, potentially in parallel.\n\n    Reader function must be picklable.\n\n    Parameters\n    ----------\n    read_fn :       Callable\n    server :        str\n                    FTP server address.\n    port :          int\n                    FTP server port.\n    path :          str\n                    Path to directory containing files or single file.\n    file_ext :      str | callable\n                    File extension to search for - e.g. \".swc\". `None` or `''`\n                    are interpreted as looking for filenames without extension.\n                    To include all files use `'*'`. Can also be callable that\n                    accepts a filename and returns True or False depending on\n                    if it should be included.\n    limit :         int, optional\n                    Limit the number of files read from this directory.\n    parallel :      str | bool | int\n                    \"auto\" or True for n_cores // 2, otherwise int for number of\n                    jobs, or false for serial.\n\n    Returns\n    -------\n    core.NeuronList\n\n    \"\"\"\n    # Check if this is a single file\n    is_single_file = False\n    if \"*\" not in path:\n        if isinstance(file_ext, str) and path.endswith(file_ext):\n            is_single_file = True\n        elif callable(file_ext) and file_ext(path.rsplit(\"/\", 1)[1]):\n            is_single_file = True\n\n    if is_single_file:\n        path, fname = path.rsplit(\"/\", 1)\n        to_read = [fname]\n    else:\n        pattern = \"\"\n        # Check if path contains a \"*.\" pattern - e.g. something like \"*_raw.swc\"\n        if \"*\" in path:\n            path, fname = path.rsplit(\"/\", 1)\n            pattern = fname\n\n        # Remove leading /\n        if path.startswith(\"/\"):\n            path = path[1:]\n\n        # First check content\n        with FTP() as ftp:\n            ftp.connect(server, port)  # connect to server\n            ftp.login()  # anonymous login\n            ftp.cwd(path)  # change to path\n\n            # Read content\n            content = []\n            ftp.retrlines(f\"LIST {pattern}\", content.append)\n\n        # Parse content into filenames\n        to_read = []\n        for line in content:\n            if not line:\n                continue\n            file = line.split()[-1].strip()\n\n            if callable(file_ext):\n                if file_ext(file):\n                    to_read.append(file)\n            elif file_ext == \"*\":\n                to_read.append(file)\n            elif file_ext and fname.endswith(file_ext):\n                to_read.append(file)\n\n    if isinstance(limit, int):\n        to_read = to_read[:limit]\n    elif isinstance(limit, list):\n        to_read = [f for f in to_read if f in limit]\n    elif isinstance(limit, slice):\n        to_read = to_read[limit]\n    elif isinstance(limit, str):\n        # Check if limit is a regex\n        if rgx.search(limit):\n            to_read = [f for f in to_read if re.search(limit, f)]\n        else:\n            to_read = [f for f in to_read if limit in f]\n\n    if not to_read:\n        return []\n\n    prog = partial(\n        config.tqdm,\n        desc=\"Loading\",\n        total=len(to_read),\n        disable=config.pbar_hide,\n        leave=config.pbar_leave,\n    )\n\n    # `parallel` can be (\"auto\", threshold) in which case `threshold`\n    # determines at what length we use parallel processing\n    if isinstance(parallel, tuple):\n        parallel, threshold = parallel\n    else:\n        threshold = 200\n\n    if (\n        isinstance(parallel, str)\n        and parallel.lower() == \"auto\"\n        and len(to_read) &lt; threshold\n    ):\n        parallel = False\n\n    if parallel:\n        # Do not swap this as `isinstance(True, int)` returns `True`\n        if isinstance(parallel, (bool, str)):\n            n_cores = max(1, os.cpu_count() // 2)\n        else:\n            n_cores = int(parallel)\n\n        # We can't send the FTP object to the process (because its socket is not pickleable)\n        # Instead, we need to initialize a new FTP connection in each process via a global variable\n        with mp.Pool(\n            processes=n_cores, initializer=_ftp_pool_init, initargs=(server, port, path)\n        ) as pool:\n            results = pool.imap(partial(read_fn, ftp=\"GLOBAL\"), to_read)\n            neurons = list(prog(results))\n    else:\n        with FTP() as ftp:\n            ftp.connect(server, port)\n            ftp.login()\n            ftp.cwd(path)\n\n            neurons = [read_fn(file, ftp=ftp) for file in prog(to_read)]\n\n    return neurons\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.parallel_read_gs","title":"<code>navis.io.base.parallel_read_gs</code>","text":"<p>Read neurons from an Google bucket, potentially in parallel.</p> <p>Reader function must be picklable.</p> PARAMETER DESCRIPTION <code>read_fn</code> <p> TYPE: <code>      Callable</code> </p> <code>gcsfs</code> <p> TYPE: <code>        gcsfs.GCSFileSystem</code> </p> <code>path</code> <pre><code>        Path to directory, single file, or list of files inside\n        the bucket.\n</code></pre> <p> TYPE: <code>         str | list thereof</code> </p> <code>file_ext</code> <pre><code>        File extension to search for - e.g. \".swc\". `None` or `''`\n        are interpreted as looking for filenames without extension.\n        To include all files use `'*'`. Can also be callable that\n        accepts a filename and returns True or False depending on\n        if it should be included.\n</code></pre> <p> TYPE: <code>     str | callable</code> </p> <code>limit</code> <pre><code>        Limit the number of files read from this directory.\n</code></pre> <p> TYPE: <code>        int | list | slice | str | None</code> DEFAULT: <code>None</code> </p> <code>parallel</code> <pre><code>        Number of threads:\n         - \"auto\" or True for 20\n         - int for number of threads\n         - False for serial\n</code></pre> <p> TYPE: <code>     str | bool | int</code> DEFAULT: <code>'auto'</code> </p> RETURNS DESCRIPTION <code>neurons</code> <p>List of neurons read from the bucket.</p> <p> TYPE: <code>list</code> </p> Source code in <code>navis/io/base.py</code> <pre><code>def parallel_read_gs(\n    read_fn,\n    gcsfs,\n    path,\n    file_ext,\n    limit=None,\n    parallel=\"auto\",\n) -&gt; List[\"core.NeuronList\"]:\n    \"\"\"Read neurons from an Google bucket, potentially in parallel.\n\n    Reader function must be picklable.\n\n    Parameters\n    ----------\n    read_fn :       Callable\n    gcsfs :         gcsfs.GCSFileSystem\n    path :          str | list thereof\n                    Path to directory, single file, or list of files inside\n                    the bucket.\n    file_ext :      str | callable\n                    File extension to search for - e.g. \".swc\". `None` or `''`\n                    are interpreted as looking for filenames without extension.\n                    To include all files use `'*'`. Can also be callable that\n                    accepts a filename and returns True or False depending on\n                    if it should be included.\n    limit :         int | list | slice | str | None, optional\n                    Limit the number of files read from this directory.\n    parallel :      str | bool | int\n                    Number of threads:\n                     - \"auto\" or True for 20\n                     - int for number of threads\n                     - False for serial\n\n    Returns\n    -------\n    neurons :       list\n                    List of neurons read from the bucket.\n\n    \"\"\"\n    if isinstance(path, str):\n        path = [path]\n\n    # Check if this is a single file\n    is_single_file = False\n    if isinstance(path, str) and \"*\" not in path:\n        if isinstance(file_ext, str) and path.endswith(file_ext):\n            is_single_file = True\n        elif callable(file_ext) and file_ext(path.rsplit(\"/\", 1)[1]):\n            is_single_file = True\n\n    # Parse all paths\n    to_read = []\n    for p in path:\n        if not isinstance(p, str):\n            raise ValueError(\"All paths must be strings\")\n\n        # If this is a file, add and continue\n        if \"*\" not in p:\n            if isinstance(file_ext, str) and p.endswith(file_ext):\n                to_read.append(p)\n                continue\n            elif callable(file_ext) and file_ext(p.rsplit(\"/\", 1)[1]):\n                to_read.append(p)\n                continue\n\n        # Check if path contains a \"*.\" pattern - e.g. something like \"*_raw.swc\"\n        pattern = None\n        if \"*\" in p:\n            p, pattern = p.rsplit(\"/\", 1)\n\n        if not gcsfs.isdir(p):\n            raise ValueError(f\"Path '{p}' is not a directory in the bucket\")\n\n        content = gcsfs.ls(p.replace('gs://', ''), detail=False)\n\n        # Parse content into filenames\n        to_read = []\n        for line in content:\n            if not line:\n                continue\n            file = line.rsplit(\"/\", 1)[-1].strip()\n            fp = f\"{p}/{file}\"\n\n            # If no pattern, just check extension\n            if not pattern:\n                if callable(file_ext):\n                    if file_ext(file):\n                        to_read.append(fp)\n                elif file_ext == \"*\":\n                    to_read.append(fp)\n                elif file_ext and file.endswith(file_ext):\n                    to_read.append(fp)\n            elif re.match(file, pattern):\n                to_read.append(fp)\n\n    if isinstance(limit, int):\n        to_read = to_read[:limit]\n    elif isinstance(limit, list):\n        to_read = [f for f in to_read if f in limit]\n    elif isinstance(limit, slice):\n        to_read = to_read[limit]\n    elif isinstance(limit, str):\n        # Check if limit is a regex\n        if rgx.search(limit):\n            to_read = [f for f in to_read if re.search(limit, f)]\n        else:\n            to_read = [f for f in to_read if limit in f]\n\n    if not to_read:\n        return []\n\n    prog = partial(\n        config.tqdm,\n        desc=\"Reading\",\n        total=len(to_read),\n        disable=config.pbar_hide,\n        leave=config.pbar_leave,\n    )\n\n    # `parallel` can be (\"auto\", threshold) in which case `threshold`\n    # determines at what length we use parallel processing\n    if isinstance(parallel, tuple):\n        parallel, threshold = parallel\n    else:\n        threshold = 5 # GS reading is fast, so lower threshold\n\n    if (\n        isinstance(parallel, str)\n        and parallel.lower() == \"auto\"\n        and len(to_read) &lt; threshold\n    ):\n        parallel = False\n\n    if parallel:\n        # Do not swap this as `isinstance(True, int)` returns `True`\n        if isinstance(parallel, (bool, str)):\n            n_threads = 20\n        else:\n            n_threads = int(parallel)\n\n        with ThreadPoolExecutor(max_workers=n_threads) as executor:\n            results = executor.map(partial(read_fn, gcsfs=gcsfs), to_read)\n            neurons = list(prog(results))\n    else:\n        neurons = [read_fn(file, gcsfs=gcsfs) for file in prog(to_read)]\n\n    return neurons\n</code></pre>"},{"location":"reference/navis/io/base/#navis.io.base.parse_precision","title":"<code>navis.io.base.parse_precision</code>","text":"<p>Convert bit width into int and float dtypes.</p> PARAMETER DESCRIPTION <code>precision</code> <p>16, 32, 64, or None</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>Integer numpy dtype, float numpy dtype</p> RAISES DESCRIPTION <code>ValueError</code> <p>Unknown precision.</p> Source code in <code>navis/io/base.py</code> <pre><code>def parse_precision(precision: Optional[int]):\n    \"\"\"Convert bit width into int and float dtypes.\n\n    Parameters\n    ----------\n    precision : int\n        16, 32, 64, or None\n\n    Returns\n    -------\n    tuple\n        Integer numpy dtype, float numpy dtype\n\n    Raises\n    ------\n    ValueError\n        Unknown precision.\n    \"\"\"\n    INT_DTYPES = {16: np.int16, 32: np.int32, 64: np.int64, None: None}\n    FLOAT_DTYPES = {16: np.float16, 32: np.float32, 64: np.float64, None: None}\n\n    try:\n        return (INT_DTYPES[precision], FLOAT_DTYPES[precision])\n    except KeyError:\n        raise ValueError(\n            f\"Unknown precision {precision}. Expected on of the following: 16, 32 (default), 64 or None\"\n        )\n</code></pre>"},{"location":"reference/navis/io/hdf_io/","title":"hdf_io","text":""},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.BaseH5Reader","title":"<code>navis.io.hdf_io.BaseH5Reader</code>","text":"<p>Reads neurons from HDF5 files.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>class BaseH5Reader(ABC):\n    \"\"\"Reads neurons from HDF5 files.\"\"\"\n\n    def __init__(self, filepath: str):\n        \"\"\"Initialize.\"\"\"\n        self.filepath = filepath\n\n    def __enter__(self):\n        \"\"\"Open file on enter.\"\"\"\n        self.f = h5py.File(self.filepath, 'r')\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Close file on exit.\"\"\"\n        self.f.close()\n\n    def check_compatible(self):\n        \"\"\"Check if this reader is compatible with existing data.\"\"\"\n        fmt = self.f.attrs.get('format_spec')\n        if not isinstance(fmt, str):\n            raise ValueError('No format specifier found for file '\n                             f'{self.filepath}')\n        elif fmt != f'hnf_v{self.version}':\n            raise ValueError('Unexpected format specifier for file '\n                             f'{self.filepath}: \"{fmt}\"')\n\n    @abstractmethod\n    def list_neurons(self):\n        \"\"\"List neurons in file.\"\"\"\n        pass\n\n    @abstractmethod\n    def read_neurons(self):\n        \"\"\"Read neurons from file.\"\"\"\n        pass\n\n    def read_dataframe(self, group, subset=None, exclude=None,\n                       include_attrs=False, skip_hidden=True):\n        \"\"\"Read dataset within a group into a single pandas DataFrame.\"\"\"\n        # List datasets contained in this group\n        datasets = [k for k, v in group.items() if isinstance(v, h5py.Dataset)]\n\n        if isinstance(subset, type(None)):\n            subset = datasets\n        else:\n            subset = utils.make_iterable(subset)\n\n        if isinstance(exclude, type(None)):\n            exclude = []\n        else:\n            exclude = utils.make_iterable(exclude)\n\n        # Read the datasets\n        df = pd.DataFrame()\n        for col in datasets:\n            if col not in subset or col in exclude:\n                continue\n            # Skip hidden datasets\n            if col.startswith('.') and skip_hidden:\n                continue\n\n            # This is the dataset\n            ds = group[col]\n\n            # Easy-peasy if dataset has 1 dimension\n            if ds.ndim == 1:\n                df[col] = ds[:]\n            # 2-dimensions should not happen but we will simply enumerate cols\n            elif ds.ndim == 2:\n                for i in range(ds.shape[1]):\n                    df[f'{col}_{i}'] = ds[:, i]\n            else:\n                raise ValueError(f'Dataset {col} has more than two dimensions.')\n\n        if include_attrs:\n            for k in group.attrs:\n                df.attrs[k] = group.attrs[k]\n\n        return df\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.BaseH5Reader.__init__","title":"<code>__init__</code>","text":"<p>Initialize.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def __init__(self, filepath: str):\n    \"\"\"Initialize.\"\"\"\n    self.filepath = filepath\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.BaseH5Reader.check_compatible","title":"<code>check_compatible</code>","text":"<p>Check if this reader is compatible with existing data.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def check_compatible(self):\n    \"\"\"Check if this reader is compatible with existing data.\"\"\"\n    fmt = self.f.attrs.get('format_spec')\n    if not isinstance(fmt, str):\n        raise ValueError('No format specifier found for file '\n                         f'{self.filepath}')\n    elif fmt != f'hnf_v{self.version}':\n        raise ValueError('Unexpected format specifier for file '\n                         f'{self.filepath}: \"{fmt}\"')\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.BaseH5Reader.list_neurons","title":"<code>list_neurons</code>  <code>abstractmethod</code>","text":"<p>List neurons in file.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>@abstractmethod\ndef list_neurons(self):\n    \"\"\"List neurons in file.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.BaseH5Reader.read_dataframe","title":"<code>read_dataframe</code>","text":"<p>Read dataset within a group into a single pandas DataFrame.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def read_dataframe(self, group, subset=None, exclude=None,\n                   include_attrs=False, skip_hidden=True):\n    \"\"\"Read dataset within a group into a single pandas DataFrame.\"\"\"\n    # List datasets contained in this group\n    datasets = [k for k, v in group.items() if isinstance(v, h5py.Dataset)]\n\n    if isinstance(subset, type(None)):\n        subset = datasets\n    else:\n        subset = utils.make_iterable(subset)\n\n    if isinstance(exclude, type(None)):\n        exclude = []\n    else:\n        exclude = utils.make_iterable(exclude)\n\n    # Read the datasets\n    df = pd.DataFrame()\n    for col in datasets:\n        if col not in subset or col in exclude:\n            continue\n        # Skip hidden datasets\n        if col.startswith('.') and skip_hidden:\n            continue\n\n        # This is the dataset\n        ds = group[col]\n\n        # Easy-peasy if dataset has 1 dimension\n        if ds.ndim == 1:\n            df[col] = ds[:]\n        # 2-dimensions should not happen but we will simply enumerate cols\n        elif ds.ndim == 2:\n            for i in range(ds.shape[1]):\n                df[f'{col}_{i}'] = ds[:, i]\n        else:\n            raise ValueError(f'Dataset {col} has more than two dimensions.')\n\n    if include_attrs:\n        for k in group.attrs:\n            df.attrs[k] = group.attrs[k]\n\n    return df\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.BaseH5Reader.read_neurons","title":"<code>read_neurons</code>  <code>abstractmethod</code>","text":"<p>Read neurons from file.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>@abstractmethod\ndef read_neurons(self):\n    \"\"\"Read neurons from file.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.BaseH5Writer","title":"<code>navis.io.hdf_io.BaseH5Writer</code>","text":"<p>Writes neurons to HDF5 files.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>class BaseH5Writer(ABC):\n    \"\"\"Writes neurons to HDF5 files.\"\"\"\n\n    version = None\n\n    def __init__(self, filepath: str,  mode='a', **kwargs):\n        \"\"\"Initialize.\n\n        Parameters\n        ----------\n        filepath :      str\n                        Path to HDF5 file.\n        mode :          \"a\" | \"w\" |\n                        Mode in which to open H5 file::\n\n                            w\tCreate file, truncate if exists\n                            a\tRead/write if exists, create otherwise\n\n        **kwargs\n                        Passed to `h5py.File`.\n        \"\"\"\n        self.filepath = filepath\n        self.mode = mode\n        self.kwargs = kwargs\n\n    def __enter__(self):\n        \"\"\"Open file on enter.\"\"\"\n        self.f = h5py.File(self.filepath, self.mode, **self.kwargs)\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Close file on exit.\"\"\"\n        self.f.close()\n\n    def check_compatible(self):\n        \"\"\"Check if this writer is compatible with existing data.\"\"\"\n        fmt = self.f.attrs.get('format_spec')\n        if fmt:\n            if not fmt.startswith('hnf_'):\n                raise ValueError('Unexpected format specifier for file '\n                                 f'{self.filepath}: \"{fmt}\"')\n\n            ver = fmt.split('_')[-1]\n            if ver != 'v1':\n                raise ValueError(f'File {self.filepath} appears to contain '\n                                 f'data from an incompatible version: \"{ver}\"')\n\n    def write_dataframe(self, df, group, subset=None, exclude=None, overwrite=False):\n        \"\"\"Write dataframe to group.\"\"\"\n        assert isinstance(df, pd.DataFrame)\n\n        if isinstance(subset, type(None)):\n            subset = df.columns\n        else:\n            subset = utils.make_iterable(subset)\n\n        if isinstance(exclude, type(None)):\n            exclude = []\n        else:\n            exclude = utils.make_iterable(exclude)\n\n        # Remove datasets if it already exists\n        for c in df.columns:\n            if c not in subset or c in exclude:\n                continue\n\n            if c in group:\n                if not overwrite:\n                    raise ValueError(f'Dataset {c} already exists in group \"{group}\"')\n                del group[c]\n\n            # Convert categoricals\n            if isinstance(df[c].dtype, pd.CategoricalDtype):\n                data = np.asarray(df[c])\n            else:\n                data = df[c].values\n\n            # HDF5 does not like numpy strings (\"&lt;U4\") or object\n            if data.dtype.type in (np.str_, np.object_):\n                data = data.astype(\"S\")\n\n            group.create_dataset(c, data=data, compression='gzip')\n\n    def write_base_info(self):\n        \"\"\"Add version and url to root path.\"\"\"\n        self.f.attrs['format_spec'] = f'hnf_v{self.version}'\n        self.f.attrs['format_url'] = 'https://github.com/navis-org/navis'\n\n    @abstractmethod\n    def write_neurons(self, neuron: 'core.NeuronObject'):\n        \"\"\"Write Neuron to file.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.BaseH5Writer.__init__","title":"<code>__init__</code>","text":"<p>Initialize.</p> PARAMETER DESCRIPTION <code>filepath</code> <pre><code>        Path to HDF5 file.\n</code></pre> <p> TYPE: <code>     str</code> </p> <code>mode</code> <pre><code>        Mode in which to open H5 file::\n\n            w   Create file, truncate if exists\n            a   Read/write if exists, create otherwise\n</code></pre> <p> TYPE: <code>         \"a\" | \"w\" |</code> DEFAULT: <code>'a'</code> </p> <code>**kwargs</code> <pre><code>        Passed to `h5py.File`.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def __init__(self, filepath: str,  mode='a', **kwargs):\n    \"\"\"Initialize.\n\n    Parameters\n    ----------\n    filepath :      str\n                    Path to HDF5 file.\n    mode :          \"a\" | \"w\" |\n                    Mode in which to open H5 file::\n\n                        w\tCreate file, truncate if exists\n                        a\tRead/write if exists, create otherwise\n\n    **kwargs\n                    Passed to `h5py.File`.\n    \"\"\"\n    self.filepath = filepath\n    self.mode = mode\n    self.kwargs = kwargs\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.BaseH5Writer.check_compatible","title":"<code>check_compatible</code>","text":"<p>Check if this writer is compatible with existing data.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def check_compatible(self):\n    \"\"\"Check if this writer is compatible with existing data.\"\"\"\n    fmt = self.f.attrs.get('format_spec')\n    if fmt:\n        if not fmt.startswith('hnf_'):\n            raise ValueError('Unexpected format specifier for file '\n                             f'{self.filepath}: \"{fmt}\"')\n\n        ver = fmt.split('_')[-1]\n        if ver != 'v1':\n            raise ValueError(f'File {self.filepath} appears to contain '\n                             f'data from an incompatible version: \"{ver}\"')\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.BaseH5Writer.write_base_info","title":"<code>write_base_info</code>","text":"<p>Add version and url to root path.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def write_base_info(self):\n    \"\"\"Add version and url to root path.\"\"\"\n    self.f.attrs['format_spec'] = f'hnf_v{self.version}'\n    self.f.attrs['format_url'] = 'https://github.com/navis-org/navis'\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.BaseH5Writer.write_dataframe","title":"<code>write_dataframe</code>","text":"<p>Write dataframe to group.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def write_dataframe(self, df, group, subset=None, exclude=None, overwrite=False):\n    \"\"\"Write dataframe to group.\"\"\"\n    assert isinstance(df, pd.DataFrame)\n\n    if isinstance(subset, type(None)):\n        subset = df.columns\n    else:\n        subset = utils.make_iterable(subset)\n\n    if isinstance(exclude, type(None)):\n        exclude = []\n    else:\n        exclude = utils.make_iterable(exclude)\n\n    # Remove datasets if it already exists\n    for c in df.columns:\n        if c not in subset or c in exclude:\n            continue\n\n        if c in group:\n            if not overwrite:\n                raise ValueError(f'Dataset {c} already exists in group \"{group}\"')\n            del group[c]\n\n        # Convert categoricals\n        if isinstance(df[c].dtype, pd.CategoricalDtype):\n            data = np.asarray(df[c])\n        else:\n            data = df[c].values\n\n        # HDF5 does not like numpy strings (\"&lt;U4\") or object\n        if data.dtype.type in (np.str_, np.object_):\n            data = data.astype(\"S\")\n\n        group.create_dataset(c, data=data, compression='gzip')\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.BaseH5Writer.write_neurons","title":"<code>write_neurons</code>  <code>abstractmethod</code>","text":"<p>Write Neuron to file.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>@abstractmethod\ndef write_neurons(self, neuron: 'core.NeuronObject'):\n    \"\"\"Write Neuron to file.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.H5ReaderV1","title":"<code>navis.io.hdf_io.H5ReaderV1</code>","text":"<p>Reads neurons from HDF5 files.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>class H5ReaderV1(BaseH5Reader):\n    \"\"\"Reads neurons from HDF5 files.\"\"\"\n\n    version = 1\n\n    def list_neurons(self, from_cache=True):\n        \"\"\"List all neurons in file.\"\"\"\n        if from_cache and hasattr(self, 'neurons'):\n            return self.neurons\n\n        # Go over all top level groups\n        self.neurons = []\n        for id, grp in self.f.items():\n            # Skip if not a group\n            if not isinstance(grp, h5py.Group):\n                continue\n\n            self.neurons.append(id)\n\n        return self.neurons\n\n    def read_neurons(self,\n                     subset=None,\n                     read='mesh-&gt;skeleton-&gt;dotprops',\n                     strict=False,\n                     prefer_raw=False,\n                     on_error='stop',\n                     progress=True,\n                     annotations=False,\n                     **kwargs):\n        \"\"\"Read neurons from file.\"\"\"\n        assert isinstance(read, str)\n\n        readers = {'mesh': self.read_meshneuron,\n                   'skeleton': self.read_treeneuron,\n                   'dotprops': self.read_dotprops}\n\n        # If no subset specified, load all neurons\n        if isinstance(subset, type(None)):\n            subset = self.list_neurons()\n        else:\n            subset = [id for id in subset if id in self.f]\n\n        neurons = []\n        errors = {}\n\n        # We can not simply use disable=True here because of some odd\n        # interactions when this is run in multiple processes\n        if progress:\n            pbar = config.tqdm(desc='Reading',\n                               leave=False,\n                               disable=config.pbar_hide,\n                               total=len(subset))\n\n        try:\n            for id in subset:\n                # Go over the requested neuron representations\n                for rep in read.split(','):\n                    # Strip potential whitespaces\n                    rep = rep.strip()\n\n                    # Go over priorities\n                    for prio in rep.split('-&gt;'):\n                        # Strip potential whitespaces\n                        prio = prio.strip()\n                        # If that neuron type is present\n                        if prio in self.f[id]:\n                            try:\n                                # Read neuron\n                                read_func = readers[prio]\n                                n = read_func(id,\n                                              strict=strict,\n                                              prefer_raw=prefer_raw,\n                                              **kwargs)\n\n                                # Read annotations\n                                if not isinstance(annotations, type(None)):\n                                    an = self.read_annotations(id, annotations, **kwargs)\n                                    for k, v in an.items():\n                                        setattr(n, k, v)\n\n                                # Append neuron and break prio loop\n                                neurons.append(n)\n                            except BaseException as e:\n                                errors[id] = str(e)\n                                if on_error in ('stop', 'raise'):\n                                    raise e\n                                elif on_error == 'warn':\n                                    warnings.warn(f'Error parsing {prio} for '\n                                                  f'neuron {id}: {e}')\n\n                            break\n\n                if progress:\n                    pbar.update()\n        except BaseException:\n            raise\n        finally:\n            if progress:\n                pbar.close()\n\n        return neurons, errors\n\n    def parse_add_attributes(self, grp, base_grp, neuron,\n                             subset=None, exclude=['units_nm']):\n        \"\"\"Parse attributes and associate with neuron.\"\"\"\n        if not isinstance(subset, type(None)):\n            subset = utils.make_iterable(subset)\n\n        # First parse base (neuron-level) attributes\n        attrs = {}\n        for k, v in base_grp.attrs.items():\n            if not isinstance(subset, type(None)) and k not in subset:\n                continue\n            elif k in exclude:\n                continue\n            attrs[k] = v\n\n        # Now parse attributes specific for this representation\n        for k, v in grp.attrs.items():\n            if not isinstance(subset, type(None)) and k not in subset:\n                continue\n            elif k in exclude:\n                continue\n            attrs[k] = v\n\n        # Now add attributes\n        for k, v in attrs.items():\n            setattr(neuron, k, v)\n\n    def parse_add_datasets(self, grp, neuron, subset=None, exclude=None,\n                           skip_hidden=True):\n        \"\"\"Parse datasets and associate with neuron.\"\"\"\n        if not isinstance(subset, type(None)):\n            subset = utils.make_iterable(subset)\n        if not isinstance(exclude, type(None)):\n            exclude = utils.make_iterable(exclude)\n\n        # Now parse attributes specific for this representation\n        for k in grp.keys():\n            if not isinstance(subset, type(None)) and k not in subset:\n                continue\n            elif k.startswith('.') and skip_hidden:\n                continue\n            elif k in exclude:\n                continue\n            # Do not remove the [:] as it ensures that we get a numpy array\n            setattr(neuron, k, grp[k][:])\n\n    def parse_add_units(self, grp, base_grp, neuron):\n        \"\"\"Parse units and associate with neuron.\"\"\"\n        # Check if we have units\n        units = grp.attrs.get('units_nm',\n                              base_grp.attrs.get('units_nm',\n                                                 None))\n\n        if isinstance(units, str):\n            # If string, see if pint can parse it\n            neuron.units = units\n        elif isinstance(units, np.ndarray):\n            # Currently navis .units does support x/y/z units\n            # We will use only the first entry for now\n            neuron.units = f'{units[0]} nm'\n        elif not isinstance(units, type(None)):\n            neuron.units = f'{units} nm'\n\n    def read_annotations(self, id, annotations, **kwargs):\n        \"\"\"Read annotations for given neuron from file.\"\"\"\n        # Get the group for this neuron\n        neuron_grp = self.f[id]\n\n        an_grp = neuron_grp.get('annotations')\n        if not an_grp:\n            return {}\n\n        if isinstance(annotations, bool):\n            annotations = [k for k, grp in an_grp.items() if isinstance(grp, type(h5py.Group))]\n        else:\n            annotations = utils.make_iterable(annotations)\n\n        parsed_an = {}\n        for an in annotations:\n            if an in an_grp:\n                parsed_an[an] = self.read_dataframe(an_grp[an],\n                                                    include_attrs=True)\n\n        return parsed_an\n\n    def read_treeneuron(self, id, strict=False, prefer_raw=False, **kwargs):\n        \"\"\"Read given TreeNeuron from file.\"\"\"\n        # Get the group for this neuron\n        neuron_grp = self.f[id]\n\n        # See if this neuron has a skeleton\n        if 'skeleton' not in neuron_grp:\n            raise ValueError(f'Neuron {id} has no skeleton')\n        sk_grp = neuron_grp['skeleton']\n\n        if '.serialized_navis' in sk_grp and not prefer_raw:\n            return pickle.loads(sk_grp['.serialized_navis'][()])\n\n        # Parse node table\n        nodes = self.read_dataframe(sk_grp,\n                                    subset=['node_id', 'parent_id',\n                                            'x', 'y', 'z',\n                                            'radius'] if strict else None)\n\n        n = core.TreeNeuron(nodes, id=id)\n\n        # Check if we have units\n        self.parse_add_units(sk_grp, neuron_grp, n)\n\n        # Parse attributes\n        self.parse_add_attributes(sk_grp, neuron_grp, n,\n                                  subset=['soma'] if strict else None)\n\n        return n\n\n    def read_dotprops(self, id, strict=False, prefer_raw=False, **kwargs):\n        \"\"\"Read given Dotprops from file.\"\"\"\n        # Get the group for this neuron\n        neuron_grp = self.f[id]\n\n        # See if this neuron has dotprops\n        if 'dotprops' not in neuron_grp:\n            raise ValueError(f'Neuron {id} has no dotprops')\n        dp_grp = neuron_grp['dotprops']\n\n        if '.serialized_navis' in dp_grp and not prefer_raw:\n            return pickle.loads(dp_grp['.serialized_navis'][()])\n\n        # Parse dotprop arrays\n        points = dp_grp['points']\n        vect = dp_grp['vect']\n        alpha = dp_grp['alpha']\n        k = dp_grp.attrs['k']\n\n        n = core.Dotprops(points=points,\n                          k=k,\n                          vect=vect,\n                          alpha=alpha,\n                          id=id,\n                          name=neuron_grp.attrs.get('neuron_name'))\n\n        # Check if we have units\n        self.parse_add_units(dp_grp, neuron_grp, n)\n\n        # Parse additional attributes\n        self.parse_add_attributes(dp_grp, neuron_grp, n,\n                                  subset=['soma'] if strict else None,\n                                  exclude=['units_nm', 'k'])\n\n        # Parse additional datatsets\n        if not strict:\n            self.parse_add_datasets(dp_grp, n,\n                                    exclude=['points', 'vect', 'alpha'])\n\n        return n\n\n    def read_meshneuron(self, id, strict=False, prefer_raw=False, **kwargs):\n        \"\"\"Read given MeshNeuron from file.\"\"\"\n        # Get the group for this neuron\n        neuron_grp = self.f[id]\n\n        # See if this neuron has a mesh\n        if 'mesh' not in neuron_grp:\n            raise ValueError(f'Neuron {id} has no mesh')\n        me_grp = neuron_grp['mesh']\n\n        if '.serialized_navis' in me_grp and not prefer_raw:\n            return pickle.loads(me_grp['.serialized_navis'][()])\n\n        # Parse node table -&gt; do not remove the [:] as it ensures that we get\n        # a numpy array\n        verts = me_grp['vertices'][:]\n        faces = me_grp['faces'][:]\n\n        n = core.MeshNeuron({'vertices': verts, 'faces': faces},\n                            id=id,\n                            name=neuron_grp.attrs.get('neuron_name'))\n\n        if 'skeleton_map' in me_grp:\n            n.skeleton_map = me_grp['skeleton_map']\n\n        # Check if we have units\n        self.parse_add_units(me_grp, neuron_grp, n)\n\n        # Parse additional attributes\n        self.parse_add_attributes(me_grp, neuron_grp, n,\n                                  subset=['soma'] if strict else None,\n                                  exclude=['units_nm'])\n\n        # Parse additional datatsets\n        if not strict:\n            self.parse_add_datasets(me_grp, n,\n                                    exclude=['vertices',\n                                             'faces',\n                                             'skeleton_map'])\n\n        return n\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.H5ReaderV1.list_neurons","title":"<code>list_neurons</code>","text":"<p>List all neurons in file.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def list_neurons(self, from_cache=True):\n    \"\"\"List all neurons in file.\"\"\"\n    if from_cache and hasattr(self, 'neurons'):\n        return self.neurons\n\n    # Go over all top level groups\n    self.neurons = []\n    for id, grp in self.f.items():\n        # Skip if not a group\n        if not isinstance(grp, h5py.Group):\n            continue\n\n        self.neurons.append(id)\n\n    return self.neurons\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.H5ReaderV1.parse_add_attributes","title":"<code>parse_add_attributes</code>","text":"<p>Parse attributes and associate with neuron.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def parse_add_attributes(self, grp, base_grp, neuron,\n                         subset=None, exclude=['units_nm']):\n    \"\"\"Parse attributes and associate with neuron.\"\"\"\n    if not isinstance(subset, type(None)):\n        subset = utils.make_iterable(subset)\n\n    # First parse base (neuron-level) attributes\n    attrs = {}\n    for k, v in base_grp.attrs.items():\n        if not isinstance(subset, type(None)) and k not in subset:\n            continue\n        elif k in exclude:\n            continue\n        attrs[k] = v\n\n    # Now parse attributes specific for this representation\n    for k, v in grp.attrs.items():\n        if not isinstance(subset, type(None)) and k not in subset:\n            continue\n        elif k in exclude:\n            continue\n        attrs[k] = v\n\n    # Now add attributes\n    for k, v in attrs.items():\n        setattr(neuron, k, v)\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.H5ReaderV1.parse_add_datasets","title":"<code>parse_add_datasets</code>","text":"<p>Parse datasets and associate with neuron.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def parse_add_datasets(self, grp, neuron, subset=None, exclude=None,\n                       skip_hidden=True):\n    \"\"\"Parse datasets and associate with neuron.\"\"\"\n    if not isinstance(subset, type(None)):\n        subset = utils.make_iterable(subset)\n    if not isinstance(exclude, type(None)):\n        exclude = utils.make_iterable(exclude)\n\n    # Now parse attributes specific for this representation\n    for k in grp.keys():\n        if not isinstance(subset, type(None)) and k not in subset:\n            continue\n        elif k.startswith('.') and skip_hidden:\n            continue\n        elif k in exclude:\n            continue\n        # Do not remove the [:] as it ensures that we get a numpy array\n        setattr(neuron, k, grp[k][:])\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.H5ReaderV1.parse_add_units","title":"<code>parse_add_units</code>","text":"<p>Parse units and associate with neuron.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def parse_add_units(self, grp, base_grp, neuron):\n    \"\"\"Parse units and associate with neuron.\"\"\"\n    # Check if we have units\n    units = grp.attrs.get('units_nm',\n                          base_grp.attrs.get('units_nm',\n                                             None))\n\n    if isinstance(units, str):\n        # If string, see if pint can parse it\n        neuron.units = units\n    elif isinstance(units, np.ndarray):\n        # Currently navis .units does support x/y/z units\n        # We will use only the first entry for now\n        neuron.units = f'{units[0]} nm'\n    elif not isinstance(units, type(None)):\n        neuron.units = f'{units} nm'\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.H5ReaderV1.read_annotations","title":"<code>read_annotations</code>","text":"<p>Read annotations for given neuron from file.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def read_annotations(self, id, annotations, **kwargs):\n    \"\"\"Read annotations for given neuron from file.\"\"\"\n    # Get the group for this neuron\n    neuron_grp = self.f[id]\n\n    an_grp = neuron_grp.get('annotations')\n    if not an_grp:\n        return {}\n\n    if isinstance(annotations, bool):\n        annotations = [k for k, grp in an_grp.items() if isinstance(grp, type(h5py.Group))]\n    else:\n        annotations = utils.make_iterable(annotations)\n\n    parsed_an = {}\n    for an in annotations:\n        if an in an_grp:\n            parsed_an[an] = self.read_dataframe(an_grp[an],\n                                                include_attrs=True)\n\n    return parsed_an\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.H5ReaderV1.read_dotprops","title":"<code>read_dotprops</code>","text":"<p>Read given Dotprops from file.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def read_dotprops(self, id, strict=False, prefer_raw=False, **kwargs):\n    \"\"\"Read given Dotprops from file.\"\"\"\n    # Get the group for this neuron\n    neuron_grp = self.f[id]\n\n    # See if this neuron has dotprops\n    if 'dotprops' not in neuron_grp:\n        raise ValueError(f'Neuron {id} has no dotprops')\n    dp_grp = neuron_grp['dotprops']\n\n    if '.serialized_navis' in dp_grp and not prefer_raw:\n        return pickle.loads(dp_grp['.serialized_navis'][()])\n\n    # Parse dotprop arrays\n    points = dp_grp['points']\n    vect = dp_grp['vect']\n    alpha = dp_grp['alpha']\n    k = dp_grp.attrs['k']\n\n    n = core.Dotprops(points=points,\n                      k=k,\n                      vect=vect,\n                      alpha=alpha,\n                      id=id,\n                      name=neuron_grp.attrs.get('neuron_name'))\n\n    # Check if we have units\n    self.parse_add_units(dp_grp, neuron_grp, n)\n\n    # Parse additional attributes\n    self.parse_add_attributes(dp_grp, neuron_grp, n,\n                              subset=['soma'] if strict else None,\n                              exclude=['units_nm', 'k'])\n\n    # Parse additional datatsets\n    if not strict:\n        self.parse_add_datasets(dp_grp, n,\n                                exclude=['points', 'vect', 'alpha'])\n\n    return n\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.H5ReaderV1.read_meshneuron","title":"<code>read_meshneuron</code>","text":"<p>Read given MeshNeuron from file.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def read_meshneuron(self, id, strict=False, prefer_raw=False, **kwargs):\n    \"\"\"Read given MeshNeuron from file.\"\"\"\n    # Get the group for this neuron\n    neuron_grp = self.f[id]\n\n    # See if this neuron has a mesh\n    if 'mesh' not in neuron_grp:\n        raise ValueError(f'Neuron {id} has no mesh')\n    me_grp = neuron_grp['mesh']\n\n    if '.serialized_navis' in me_grp and not prefer_raw:\n        return pickle.loads(me_grp['.serialized_navis'][()])\n\n    # Parse node table -&gt; do not remove the [:] as it ensures that we get\n    # a numpy array\n    verts = me_grp['vertices'][:]\n    faces = me_grp['faces'][:]\n\n    n = core.MeshNeuron({'vertices': verts, 'faces': faces},\n                        id=id,\n                        name=neuron_grp.attrs.get('neuron_name'))\n\n    if 'skeleton_map' in me_grp:\n        n.skeleton_map = me_grp['skeleton_map']\n\n    # Check if we have units\n    self.parse_add_units(me_grp, neuron_grp, n)\n\n    # Parse additional attributes\n    self.parse_add_attributes(me_grp, neuron_grp, n,\n                              subset=['soma'] if strict else None,\n                              exclude=['units_nm'])\n\n    # Parse additional datatsets\n    if not strict:\n        self.parse_add_datasets(me_grp, n,\n                                exclude=['vertices',\n                                         'faces',\n                                         'skeleton_map'])\n\n    return n\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.H5ReaderV1.read_neurons","title":"<code>read_neurons</code>","text":"<p>Read neurons from file.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def read_neurons(self,\n                 subset=None,\n                 read='mesh-&gt;skeleton-&gt;dotprops',\n                 strict=False,\n                 prefer_raw=False,\n                 on_error='stop',\n                 progress=True,\n                 annotations=False,\n                 **kwargs):\n    \"\"\"Read neurons from file.\"\"\"\n    assert isinstance(read, str)\n\n    readers = {'mesh': self.read_meshneuron,\n               'skeleton': self.read_treeneuron,\n               'dotprops': self.read_dotprops}\n\n    # If no subset specified, load all neurons\n    if isinstance(subset, type(None)):\n        subset = self.list_neurons()\n    else:\n        subset = [id for id in subset if id in self.f]\n\n    neurons = []\n    errors = {}\n\n    # We can not simply use disable=True here because of some odd\n    # interactions when this is run in multiple processes\n    if progress:\n        pbar = config.tqdm(desc='Reading',\n                           leave=False,\n                           disable=config.pbar_hide,\n                           total=len(subset))\n\n    try:\n        for id in subset:\n            # Go over the requested neuron representations\n            for rep in read.split(','):\n                # Strip potential whitespaces\n                rep = rep.strip()\n\n                # Go over priorities\n                for prio in rep.split('-&gt;'):\n                    # Strip potential whitespaces\n                    prio = prio.strip()\n                    # If that neuron type is present\n                    if prio in self.f[id]:\n                        try:\n                            # Read neuron\n                            read_func = readers[prio]\n                            n = read_func(id,\n                                          strict=strict,\n                                          prefer_raw=prefer_raw,\n                                          **kwargs)\n\n                            # Read annotations\n                            if not isinstance(annotations, type(None)):\n                                an = self.read_annotations(id, annotations, **kwargs)\n                                for k, v in an.items():\n                                    setattr(n, k, v)\n\n                            # Append neuron and break prio loop\n                            neurons.append(n)\n                        except BaseException as e:\n                            errors[id] = str(e)\n                            if on_error in ('stop', 'raise'):\n                                raise e\n                            elif on_error == 'warn':\n                                warnings.warn(f'Error parsing {prio} for '\n                                              f'neuron {id}: {e}')\n\n                        break\n\n            if progress:\n                pbar.update()\n    except BaseException:\n        raise\n    finally:\n        if progress:\n            pbar.close()\n\n    return neurons, errors\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.H5ReaderV1.read_treeneuron","title":"<code>read_treeneuron</code>","text":"<p>Read given TreeNeuron from file.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def read_treeneuron(self, id, strict=False, prefer_raw=False, **kwargs):\n    \"\"\"Read given TreeNeuron from file.\"\"\"\n    # Get the group for this neuron\n    neuron_grp = self.f[id]\n\n    # See if this neuron has a skeleton\n    if 'skeleton' not in neuron_grp:\n        raise ValueError(f'Neuron {id} has no skeleton')\n    sk_grp = neuron_grp['skeleton']\n\n    if '.serialized_navis' in sk_grp and not prefer_raw:\n        return pickle.loads(sk_grp['.serialized_navis'][()])\n\n    # Parse node table\n    nodes = self.read_dataframe(sk_grp,\n                                subset=['node_id', 'parent_id',\n                                        'x', 'y', 'z',\n                                        'radius'] if strict else None)\n\n    n = core.TreeNeuron(nodes, id=id)\n\n    # Check if we have units\n    self.parse_add_units(sk_grp, neuron_grp, n)\n\n    # Parse attributes\n    self.parse_add_attributes(sk_grp, neuron_grp, n,\n                              subset=['soma'] if strict else None)\n\n    return n\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.H5WriterV1","title":"<code>navis.io.hdf_io.H5WriterV1</code>","text":"<p>Implements v1 of the HDF schema.</p> <p>See  for format specs.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>class H5WriterV1(BaseH5Writer):\n    \"\"\"Implements v1 of the HDF schema.\n\n    See &lt;url&gt; for format specs.\n    \"\"\"\n\n    version = 1\n\n    def write_neurons(self, neuron, serialized=True, raw=False,\n                      overwrite=True, annotations=None, **kwargs):\n        \"\"\"Write neuron to file.\"\"\"\n        if isinstance(neuron, core.NeuronList):\n            for n in config.tqdm(neuron, desc='Writing',\n                                 leave=False,\n                                 disable=config.pbar_hide):\n                self.write_neurons(n, overwrite=overwrite,\n                                   annotations=annotations, **kwargs)\n            return\n\n        if isinstance(neuron, core.TreeNeuron):\n            self.write_treeneuron(neuron, serialized=serialized, raw=raw,\n                                  overwrite=overwrite, **kwargs)\n        elif isinstance(neuron, core.MeshNeuron):\n            self.write_meshneuron(neuron, serialized=serialized, raw=raw,\n                                  overwrite=overwrite, **kwargs)\n        elif isinstance(neuron, core.Dotprops):\n            self.write_dotprops(neuron, serialized=serialized, raw=raw,\n                                overwrite=overwrite, **kwargs)\n        else:\n            raise TypeError(f'Unable to write object of type \"{type(neuron)}\"'\n                            'to HDF5 file.')\n\n        # Write annotations\n        if not isinstance(annotations, type(None)):\n            _ = self.write_annotations(neuron, annotations,\n                                       overwrite=overwrite,\n                                       **kwargs)\n\n    def write_annotations(self, neuron, annotations, overwrite=True, **kwargs):\n        \"\"\"Write annotations for given neuron to file.\"\"\"\n        # Get this neuron's group\n        neuron_grp = self.get_neuron_group(neuron)\n        # Create annotation group if it does not exist\n        an_grp = neuron_grp.require_group('annotations')\n\n        annotations = utils.make_iterable(annotations)\n\n        for an in annotations:\n            data = getattr(neuron, an)\n            if isinstance(data, pd.DataFrame):\n                data_grp = an_grp.require_group(an)\n                self.write_dataframe(data, data_grp, overwrite=overwrite)\n            else:\n                raise ValueError(f'Unable to write \"{an}\" of type '\n                                 f'\"({type(data)})\" to HDF5 file.')\n\n    def get_neuron_group(self, neuron):\n        \"\"\"Get group for neuron or create if it does not exists.\"\"\"\n        # Can only use strings as group names\n        id = str(neuron.id)\n\n        grp = self.f.require_group(id)\n\n        # Write some basic info\n        if hasattr(neuron, 'name'):\n            grp.attrs['neuron_name'] = neuron.name\n\n        return grp\n\n    def write_treeneuron(self, neuron,\n                         serialized=True, raw=False,\n                         overwrite=True, **kwargs):\n        \"\"\"Write TreeNeuron to file.\"\"\"\n        assert isinstance(neuron, core.TreeNeuron)\n\n        # Get the group for this neuron -&gt; this also write BaseInfo\n        neuron_grp = self.get_neuron_group(neuron)\n\n        # See if this neuron already has a skeleton\n        if 'skeleton' in neuron_grp:\n            if not overwrite:\n                raise ValueError('File already contains a skeleton for neuron '\n                                 f'{neuron.id}')\n            del neuron_grp['skeleton']\n\n        sk_grp = neuron_grp.require_group('skeleton')\n\n        if serialized:\n            sk_grp.create_dataset('.serialized_navis',\n                                  data=np.void(pickle.dumps(neuron)))\n\n        if raw:\n            # Write info\n            units = neuron_nm_units(neuron)\n            if units:\n                sk_grp.attrs['units_nm'] = units\n\n            if neuron.has_soma:\n                sk_grp.attrs['soma'] = neuron.soma\n\n            # Write node table\n            self.write_dataframe(neuron.nodes,\n                                 group=sk_grp,\n                                 exclude=['type'],\n                                 overwrite=overwrite)\n\n    def write_dotprops(self, neuron,\n                       serialized=True, raw=False,\n                       overwrite=True, **kwargs):\n        \"\"\"Write Dotprops to file.\"\"\"\n        assert isinstance(neuron, core.Dotprops)\n\n        # Get the group for this neuron -&gt; this also write BaseInfo\n        neuron_grp = self.get_neuron_group(neuron)\n\n        # See if this neuron already has dotprops\n        if 'dotprops' in neuron_grp:\n            if not overwrite:\n                raise ValueError('File already contains dotprops for neuron '\n                                 f'{neuron.id}')\n            del neuron_grp['dotprops']\n\n        dp_grp = neuron_grp.require_group('dotprops')\n\n        if serialized:\n            dp_grp.create_dataset('.serialized_navis',\n                                  data=np.void(pickle.dumps(neuron)))\n\n        if raw:\n            # Write info\n            dp_grp.attrs['k'] = neuron.k\n            units = neuron_nm_units(neuron)\n            if units:\n                dp_grp.attrs['units_nm'] = units\n            if neuron.has_soma:\n                dp_grp.attrs['soma'] = neuron.soma\n\n            # Write data\n            for d in ['points', 'vect', 'alpha']:\n                # The neuron really ought to have these but just in case\n                if not hasattr(neuron, d):\n                    continue\n\n                data = getattr(neuron, d)\n                dp_grp.create_dataset(d, data=data, compression='gzip')\n\n    def write_meshneuron(self, neuron,\n                         serialized=True, raw=False,\n                         overwrite=True, **kwargs):\n        \"\"\"Write MeshNeuron to file.\"\"\"\n        assert isinstance(neuron, core.MeshNeuron)\n\n        # Get the group for this neuron -&gt; this also write BaseInfo\n        neuron_grp = self.get_neuron_group(neuron)\n\n        # See if this neuron already has a mesh\n        if 'mesh' in neuron_grp:\n            if not overwrite:\n                raise ValueError('File already contains a mesh for neuron '\n                                 f'{neuron.id}')\n            del neuron_grp['mesh']\n\n        me_grp = neuron_grp.require_group('mesh')\n\n        if serialized:\n            me_grp.create_dataset('.serialized_navis',\n                                  data=np.void(pickle.dumps(neuron)))\n\n        if raw:\n            # Write info\n            units = neuron_nm_units(neuron)\n            if units:\n                me_grp.attrs['units_nm'] = units\n            if neuron.has_soma:\n                me_grp.attrs['soma'] = neuron.soma\n\n            # Write data\n            for d in ['vertices', 'faces', 'skeleton_map']:\n                if not hasattr(neuron, d):\n                    continue\n\n                data = getattr(neuron, d)\n                me_grp.create_dataset(d, data=data, compression='gzip')\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.H5WriterV1.get_neuron_group","title":"<code>get_neuron_group</code>","text":"<p>Get group for neuron or create if it does not exists.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def get_neuron_group(self, neuron):\n    \"\"\"Get group for neuron or create if it does not exists.\"\"\"\n    # Can only use strings as group names\n    id = str(neuron.id)\n\n    grp = self.f.require_group(id)\n\n    # Write some basic info\n    if hasattr(neuron, 'name'):\n        grp.attrs['neuron_name'] = neuron.name\n\n    return grp\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.H5WriterV1.write_annotations","title":"<code>write_annotations</code>","text":"<p>Write annotations for given neuron to file.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def write_annotations(self, neuron, annotations, overwrite=True, **kwargs):\n    \"\"\"Write annotations for given neuron to file.\"\"\"\n    # Get this neuron's group\n    neuron_grp = self.get_neuron_group(neuron)\n    # Create annotation group if it does not exist\n    an_grp = neuron_grp.require_group('annotations')\n\n    annotations = utils.make_iterable(annotations)\n\n    for an in annotations:\n        data = getattr(neuron, an)\n        if isinstance(data, pd.DataFrame):\n            data_grp = an_grp.require_group(an)\n            self.write_dataframe(data, data_grp, overwrite=overwrite)\n        else:\n            raise ValueError(f'Unable to write \"{an}\" of type '\n                             f'\"({type(data)})\" to HDF5 file.')\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.H5WriterV1.write_dotprops","title":"<code>write_dotprops</code>","text":"<p>Write Dotprops to file.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def write_dotprops(self, neuron,\n                   serialized=True, raw=False,\n                   overwrite=True, **kwargs):\n    \"\"\"Write Dotprops to file.\"\"\"\n    assert isinstance(neuron, core.Dotprops)\n\n    # Get the group for this neuron -&gt; this also write BaseInfo\n    neuron_grp = self.get_neuron_group(neuron)\n\n    # See if this neuron already has dotprops\n    if 'dotprops' in neuron_grp:\n        if not overwrite:\n            raise ValueError('File already contains dotprops for neuron '\n                             f'{neuron.id}')\n        del neuron_grp['dotprops']\n\n    dp_grp = neuron_grp.require_group('dotprops')\n\n    if serialized:\n        dp_grp.create_dataset('.serialized_navis',\n                              data=np.void(pickle.dumps(neuron)))\n\n    if raw:\n        # Write info\n        dp_grp.attrs['k'] = neuron.k\n        units = neuron_nm_units(neuron)\n        if units:\n            dp_grp.attrs['units_nm'] = units\n        if neuron.has_soma:\n            dp_grp.attrs['soma'] = neuron.soma\n\n        # Write data\n        for d in ['points', 'vect', 'alpha']:\n            # The neuron really ought to have these but just in case\n            if not hasattr(neuron, d):\n                continue\n\n            data = getattr(neuron, d)\n            dp_grp.create_dataset(d, data=data, compression='gzip')\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.H5WriterV1.write_meshneuron","title":"<code>write_meshneuron</code>","text":"<p>Write MeshNeuron to file.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def write_meshneuron(self, neuron,\n                     serialized=True, raw=False,\n                     overwrite=True, **kwargs):\n    \"\"\"Write MeshNeuron to file.\"\"\"\n    assert isinstance(neuron, core.MeshNeuron)\n\n    # Get the group for this neuron -&gt; this also write BaseInfo\n    neuron_grp = self.get_neuron_group(neuron)\n\n    # See if this neuron already has a mesh\n    if 'mesh' in neuron_grp:\n        if not overwrite:\n            raise ValueError('File already contains a mesh for neuron '\n                             f'{neuron.id}')\n        del neuron_grp['mesh']\n\n    me_grp = neuron_grp.require_group('mesh')\n\n    if serialized:\n        me_grp.create_dataset('.serialized_navis',\n                              data=np.void(pickle.dumps(neuron)))\n\n    if raw:\n        # Write info\n        units = neuron_nm_units(neuron)\n        if units:\n            me_grp.attrs['units_nm'] = units\n        if neuron.has_soma:\n            me_grp.attrs['soma'] = neuron.soma\n\n        # Write data\n        for d in ['vertices', 'faces', 'skeleton_map']:\n            if not hasattr(neuron, d):\n                continue\n\n            data = getattr(neuron, d)\n            me_grp.create_dataset(d, data=data, compression='gzip')\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.H5WriterV1.write_neurons","title":"<code>write_neurons</code>","text":"<p>Write neuron to file.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def write_neurons(self, neuron, serialized=True, raw=False,\n                  overwrite=True, annotations=None, **kwargs):\n    \"\"\"Write neuron to file.\"\"\"\n    if isinstance(neuron, core.NeuronList):\n        for n in config.tqdm(neuron, desc='Writing',\n                             leave=False,\n                             disable=config.pbar_hide):\n            self.write_neurons(n, overwrite=overwrite,\n                               annotations=annotations, **kwargs)\n        return\n\n    if isinstance(neuron, core.TreeNeuron):\n        self.write_treeneuron(neuron, serialized=serialized, raw=raw,\n                              overwrite=overwrite, **kwargs)\n    elif isinstance(neuron, core.MeshNeuron):\n        self.write_meshneuron(neuron, serialized=serialized, raw=raw,\n                              overwrite=overwrite, **kwargs)\n    elif isinstance(neuron, core.Dotprops):\n        self.write_dotprops(neuron, serialized=serialized, raw=raw,\n                            overwrite=overwrite, **kwargs)\n    else:\n        raise TypeError(f'Unable to write object of type \"{type(neuron)}\"'\n                        'to HDF5 file.')\n\n    # Write annotations\n    if not isinstance(annotations, type(None)):\n        _ = self.write_annotations(neuron, annotations,\n                                   overwrite=overwrite,\n                                   **kwargs)\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.H5WriterV1.write_treeneuron","title":"<code>write_treeneuron</code>","text":"<p>Write TreeNeuron to file.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def write_treeneuron(self, neuron,\n                     serialized=True, raw=False,\n                     overwrite=True, **kwargs):\n    \"\"\"Write TreeNeuron to file.\"\"\"\n    assert isinstance(neuron, core.TreeNeuron)\n\n    # Get the group for this neuron -&gt; this also write BaseInfo\n    neuron_grp = self.get_neuron_group(neuron)\n\n    # See if this neuron already has a skeleton\n    if 'skeleton' in neuron_grp:\n        if not overwrite:\n            raise ValueError('File already contains a skeleton for neuron '\n                             f'{neuron.id}')\n        del neuron_grp['skeleton']\n\n    sk_grp = neuron_grp.require_group('skeleton')\n\n    if serialized:\n        sk_grp.create_dataset('.serialized_navis',\n                              data=np.void(pickle.dumps(neuron)))\n\n    if raw:\n        # Write info\n        units = neuron_nm_units(neuron)\n        if units:\n            sk_grp.attrs['units_nm'] = units\n\n        if neuron.has_soma:\n            sk_grp.attrs['soma'] = neuron.soma\n\n        # Write node table\n        self.write_dataframe(neuron.nodes,\n                             group=sk_grp,\n                             exclude=['type'],\n                             overwrite=overwrite)\n</code></pre>"},{"location":"reference/navis/io/hdf_io/#navis.io.hdf_io.neuron_nm_units","title":"<code>navis.io.hdf_io.neuron_nm_units</code>","text":"<p>Return neuron's units in nanometers.</p> <p>Returns <code>None</code> if units are dimensionless.</p> Source code in <code>navis/io/hdf_io.py</code> <pre><code>def neuron_nm_units(neuron):\n    \"\"\"Return neuron's units in nanometers.\n\n    Returns `None` if units are dimensionless.\n\n    \"\"\"\n    units = getattr(neuron, 'units')\n\n    if isinstance(units, type(None)):\n        return\n\n    if not isinstance(units, (pint.Quantity, pint.Unit)):\n        return None\n\n    # Casting to nm throws an error if dimensionless\n    try:\n        return units.to('nm').magnitude\n    except pint.DimensionalityError:\n        return None\n    except BaseException:\n        raise\n</code></pre>"},{"location":"reference/navis/io/mesh_io/","title":"mesh_io","text":""},{"location":"reference/navis/io/mesh_io/#navis.io.mesh_io.MeshReader","title":"<code>navis.io.mesh_io.MeshReader</code>","text":"Source code in <code>navis/io/mesh_io.py</code> <pre><code>class MeshReader(base.BaseReader):\n    def __init__(\n        self,\n        output: str,\n        fmt: str = DEFAULT_FMT,\n        attrs: Optional[Dict[str, Any]] = None,\n        errors: str = \"raise\",\n    ):\n        super().__init__(\n            fmt=fmt,\n            attrs=attrs,\n            file_ext=MESH_LOAD_EXT,\n            name_fallback=\"MESH\",\n            read_binary=True,\n            errors=errors,\n        )\n        self.output = output\n\n    def format_output(self, x):\n        # This function replaces the BaseReader.format_output()\n        # This is to avoid trying to convert multiple (image, header) to NeuronList\n        if self.output == \"trimesh\":\n            return x\n        elif x:\n            return core.NeuronList(x)\n        else:\n            return core.NeuronList([])\n\n    @base.handle_errors\n    def read_buffer(\n        self, f, attrs: Optional[Dict[str, Any]] = None\n    ) -&gt; Union[tm.Trimesh, \"core.Volume\", \"core.MeshNeuron\"]:\n        \"\"\"Read buffer into mesh.\n\n        Parameters\n        ----------\n        f :         IO\n                    Readable buffer (must be bytes).\n        attrs :     dict | None\n                    Arbitrary attributes to include in the neurons.\n\n        Returns\n        -------\n        Trimesh | MeshNeuron | Volume\n\n        \"\"\"\n        if isinstance(f, HTTPResponse):\n            f = io.StringIO(f.content)\n\n        if isinstance(f, bytes):\n            f = io.BytesIO(f)\n\n        # We need to tell trimesh what file type we are reading\n        if \"file\" not in attrs:\n            raise KeyError(\n                f'Unable to parse file type. \"file\" not in attributes: {attrs}'\n            )\n\n        file_type = attrs[\"file\"].split(\".\")[-1]\n\n        mesh = tm.load_mesh(f, file_type=file_type)\n\n        if self.output == \"trimesh\":\n            return mesh\n        elif self.output == \"volume\":\n            return core.Volume(mesh.vertices, mesh.faces, **attrs)\n\n        # Turn into a MeshNeuron\n        n = core.MeshNeuron(mesh)\n\n        # Try adding properties one-by-one. If one fails, we'll keep track of it\n        # in the `.meta` attribute\n        meta = {}\n        for k, v in attrs.items():\n            try:\n                n._register_attr(k, v)\n            except (AttributeError, ValueError, TypeError):\n                meta[k] = v\n\n        if meta:\n            n.meta = meta\n\n        return n\n</code></pre>"},{"location":"reference/navis/io/mesh_io/#navis.io.mesh_io.MeshReader.read_buffer","title":"<code>read_buffer</code>","text":"<p>Read buffer into mesh.</p> PARAMETER DESCRIPTION <code>f</code> <pre><code>    Readable buffer (must be bytes).\n</code></pre> <p> TYPE: <code>        IO</code> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the neurons.\n</code></pre> <p> TYPE: <code>    dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Trimesh | MeshNeuron | Volume</code> Source code in <code>navis/io/mesh_io.py</code> <pre><code>@base.handle_errors\ndef read_buffer(\n    self, f, attrs: Optional[Dict[str, Any]] = None\n) -&gt; Union[tm.Trimesh, \"core.Volume\", \"core.MeshNeuron\"]:\n    \"\"\"Read buffer into mesh.\n\n    Parameters\n    ----------\n    f :         IO\n                Readable buffer (must be bytes).\n    attrs :     dict | None\n                Arbitrary attributes to include in the neurons.\n\n    Returns\n    -------\n    Trimesh | MeshNeuron | Volume\n\n    \"\"\"\n    if isinstance(f, HTTPResponse):\n        f = io.StringIO(f.content)\n\n    if isinstance(f, bytes):\n        f = io.BytesIO(f)\n\n    # We need to tell trimesh what file type we are reading\n    if \"file\" not in attrs:\n        raise KeyError(\n            f'Unable to parse file type. \"file\" not in attributes: {attrs}'\n        )\n\n    file_type = attrs[\"file\"].split(\".\")[-1]\n\n    mesh = tm.load_mesh(f, file_type=file_type)\n\n    if self.output == \"trimesh\":\n        return mesh\n    elif self.output == \"volume\":\n        return core.Volume(mesh.vertices, mesh.faces, **attrs)\n\n    # Turn into a MeshNeuron\n    n = core.MeshNeuron(mesh)\n\n    # Try adding properties one-by-one. If one fails, we'll keep track of it\n    # in the `.meta` attribute\n    meta = {}\n    for k, v in attrs.items():\n        try:\n            n._register_attr(k, v)\n        except (AttributeError, ValueError, TypeError):\n            meta[k] = v\n\n    if meta:\n        n.meta = meta\n\n    return n\n</code></pre>"},{"location":"reference/navis/io/nmx_io/","title":"nmx_io","text":""},{"location":"reference/navis/io/nmx_io/#navis.io.nmx_io.NMLReader","title":"<code>navis.io.nmx_io.NMLReader</code>","text":"Source code in <code>navis/io/nmx_io.py</code> <pre><code>class NMLReader(base.BaseReader):\n    def __init__(\n        self,\n        precision: int = DEFAULT_PRECISION,\n        attrs: Optional[Dict[str, Any]] = None,\n        errors: str = \"raise\",\n    ):\n        super().__init__(\n            fmt=\"\",\n            attrs=attrs,\n            file_ext=\".nml\",\n            read_binary=False,\n            errors=errors,\n            name_fallback=\"NML\",\n        )\n\n        int_, float_ = base.parse_precision(precision)\n        self._dtypes = {\n            \"node_id\": int_,\n            \"parent_id\": int_,\n            \"label\": \"category\",\n            \"x\": float_,\n            \"y\": float_,\n            \"z\": float_,\n            \"radius\": float_,\n        }\n\n    @base.handle_errors\n    def read_buffer(\n        self, f: IO, attrs: Optional[Dict[str, Any]] = None\n    ) -&gt; \"core.TreeNeuron\":\n        \"\"\"Read .nml buffer into a TreeNeuron.\n\n        NML files are XML-encoded files containing data for a single neuron.\n\n        Parameters\n        ----------\n        f :         IO\n                    Readable buffer (must be bytes).\n        attrs :     dict | None\n                    Arbitrary attributes to include in the TreeNeuron.\n\n        Returns\n        -------\n        core.TreeNeuron\n        \"\"\"\n        return self.read_nml(f.read(), attrs=attrs)\n\n    def read_nml(\n        self, f: IO, attrs: Optional[Dict[str, Any]] = None\n    ) -&gt; \"core.TreeNeuron\":\n        \"\"\"Read .nml buffer into a TreeNeuron.\n\n        NML files are XML files containing a single neuron.\n\n        Parameters\n        ----------\n        f :         IO\n                    Readable buffer.\n        attrs :     dict | None\n                    Arbitrary attributes to include in the TreeNeuron.\n\n        Returns\n        -------\n        core.TreeNeuron\n        \"\"\"\n        if isinstance(f, bytes):\n            f = f.decode()\n\n        f = io.StringIO(f)\n        root = ET.parse(f).getroot()\n\n        # Copy the attributes dict\n        for element in root:\n            if element.tag == \"thing\":\n                nodes = pd.DataFrame.from_records([n.attrib for n in element[0]])\n                edges = pd.DataFrame.from_records([n.attrib for n in element[1]])\n                edges = edges.astype(self._dtypes[\"node_id\"])\n\n                nodes.rename({\"id\": \"node_id\"}, axis=1, inplace=True)\n                nodes = nodes.astype(\n                    {k: v for k, v in self._dtypes.items() if k in nodes.columns}\n                )\n\n        G = nx.Graph()\n        G.add_edges_from(edges.values)\n        tree = nx.bfs_tree(G, list(G.nodes)[0])\n        edges = pd.DataFrame(list(tree.edges), columns=[\"source\", \"target\"])\n        nodes[\"parent_id\"] = (\n            edges.set_index(\"target\").reindex(nodes.node_id.values).source.values\n        )\n        nodes[\"parent_id\"] = nodes.parent_id.fillna(-1).astype(self._dtypes[\"node_id\"])\n        nodes.sort_values(\"node_id\", inplace=True)\n\n        return core.TreeNeuron(\n            nodes, **(self._make_attributes({\"name\": \"NML\", \"origin\": \"nml\"}, attrs))\n        )\n</code></pre>"},{"location":"reference/navis/io/nmx_io/#navis.io.nmx_io.NMLReader.read_buffer","title":"<code>read_buffer</code>","text":"<p>Read .nml buffer into a TreeNeuron.</p> <p>NML files are XML-encoded files containing data for a single neuron.</p> PARAMETER DESCRIPTION <code>f</code> <pre><code>    Readable buffer (must be bytes).\n</code></pre> <p> TYPE: <code>        IO</code> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the TreeNeuron.\n</code></pre> <p> TYPE: <code>    dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.TreeNeuron</code> Source code in <code>navis/io/nmx_io.py</code> <pre><code>@base.handle_errors\ndef read_buffer(\n    self, f: IO, attrs: Optional[Dict[str, Any]] = None\n) -&gt; \"core.TreeNeuron\":\n    \"\"\"Read .nml buffer into a TreeNeuron.\n\n    NML files are XML-encoded files containing data for a single neuron.\n\n    Parameters\n    ----------\n    f :         IO\n                Readable buffer (must be bytes).\n    attrs :     dict | None\n                Arbitrary attributes to include in the TreeNeuron.\n\n    Returns\n    -------\n    core.TreeNeuron\n    \"\"\"\n    return self.read_nml(f.read(), attrs=attrs)\n</code></pre>"},{"location":"reference/navis/io/nmx_io/#navis.io.nmx_io.NMLReader.read_nml","title":"<code>read_nml</code>","text":"<p>Read .nml buffer into a TreeNeuron.</p> <p>NML files are XML files containing a single neuron.</p> PARAMETER DESCRIPTION <code>f</code> <pre><code>    Readable buffer.\n</code></pre> <p> TYPE: <code>        IO</code> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the TreeNeuron.\n</code></pre> <p> TYPE: <code>    dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.TreeNeuron</code> Source code in <code>navis/io/nmx_io.py</code> <pre><code>def read_nml(\n    self, f: IO, attrs: Optional[Dict[str, Any]] = None\n) -&gt; \"core.TreeNeuron\":\n    \"\"\"Read .nml buffer into a TreeNeuron.\n\n    NML files are XML files containing a single neuron.\n\n    Parameters\n    ----------\n    f :         IO\n                Readable buffer.\n    attrs :     dict | None\n                Arbitrary attributes to include in the TreeNeuron.\n\n    Returns\n    -------\n    core.TreeNeuron\n    \"\"\"\n    if isinstance(f, bytes):\n        f = f.decode()\n\n    f = io.StringIO(f)\n    root = ET.parse(f).getroot()\n\n    # Copy the attributes dict\n    for element in root:\n        if element.tag == \"thing\":\n            nodes = pd.DataFrame.from_records([n.attrib for n in element[0]])\n            edges = pd.DataFrame.from_records([n.attrib for n in element[1]])\n            edges = edges.astype(self._dtypes[\"node_id\"])\n\n            nodes.rename({\"id\": \"node_id\"}, axis=1, inplace=True)\n            nodes = nodes.astype(\n                {k: v for k, v in self._dtypes.items() if k in nodes.columns}\n            )\n\n    G = nx.Graph()\n    G.add_edges_from(edges.values)\n    tree = nx.bfs_tree(G, list(G.nodes)[0])\n    edges = pd.DataFrame(list(tree.edges), columns=[\"source\", \"target\"])\n    nodes[\"parent_id\"] = (\n        edges.set_index(\"target\").reindex(nodes.node_id.values).source.values\n    )\n    nodes[\"parent_id\"] = nodes.parent_id.fillna(-1).astype(self._dtypes[\"node_id\"])\n    nodes.sort_values(\"node_id\", inplace=True)\n\n    return core.TreeNeuron(\n        nodes, **(self._make_attributes({\"name\": \"NML\", \"origin\": \"nml\"}, attrs))\n    )\n</code></pre>"},{"location":"reference/navis/io/nmx_io/#navis.io.nmx_io.NMXReader","title":"<code>navis.io.nmx_io.NMXReader</code>","text":"<p>This is a version of the NML file reader that reads from zipped archives.</p> Source code in <code>navis/io/nmx_io.py</code> <pre><code>class NMXReader(NMLReader):\n    \"\"\"This is a version of the NML file reader that reads from zipped archives.\"\"\"\n\n    def __init__(\n        self,\n        precision: int = DEFAULT_PRECISION,\n        attrs: Optional[Dict[str, Any]] = None,\n        errors: str = \"raise\",\n    ):\n        super().__init__(precision=precision, errors=errors, attrs=attrs)\n\n        # Overwrite some of the settings\n        self.read_binary = True\n        self.file_ext = \".nmx\"\n        self.name_fallback = \"NMX\"\n\n    @base.handle_errors\n    def read_buffer(\n        self, f: IO, attrs: Optional[Dict[str, Any]] = None\n    ) -&gt; \"core.TreeNeuron\":\n        \"\"\"Read .nmx buffer into a TreeNeuron.\n\n        NMX files are zip files containing XML-encoded .nml files containing\n        data for a single neuron.\n\n        Parameters\n        ----------\n        f :         IO\n                    Readable buffer (must be bytes).\n        attrs :     dict | None\n                    Arbitrary attributes to include in the TreeNeuron.\n\n        Returns\n        -------\n        core.TreeNeuron\n        \"\"\"\n        if not isinstance(f.read(0), bytes):\n            raise ValueError(f'Expected bytes, got \"{type(f.read(0))}\"')\n\n        zip = ZipFile(f)\n        for f in zip.filelist:\n            if f.filename.endswith(\".nml\") and \"skeleton\" in f.filename:\n                attrs[\"file\"] = f.filename\n                attrs[\"id\"] = f.filename.split(\"/\")[0]\n                return self.read_nml(zip.read(f), attrs=attrs)\n        logger.warning(\n            f'Skipped \"{f.filename.split(\"/\")[0]}.nmx\": failed to ' 'import skeleton.'\n        )\n</code></pre>"},{"location":"reference/navis/io/nmx_io/#navis.io.nmx_io.NMXReader.read_buffer","title":"<code>read_buffer</code>","text":"<p>Read .nmx buffer into a TreeNeuron.</p> <p>NMX files are zip files containing XML-encoded .nml files containing data for a single neuron.</p> PARAMETER DESCRIPTION <code>f</code> <pre><code>    Readable buffer (must be bytes).\n</code></pre> <p> TYPE: <code>        IO</code> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the TreeNeuron.\n</code></pre> <p> TYPE: <code>    dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.TreeNeuron</code> Source code in <code>navis/io/nmx_io.py</code> <pre><code>@base.handle_errors\ndef read_buffer(\n    self, f: IO, attrs: Optional[Dict[str, Any]] = None\n) -&gt; \"core.TreeNeuron\":\n    \"\"\"Read .nmx buffer into a TreeNeuron.\n\n    NMX files are zip files containing XML-encoded .nml files containing\n    data for a single neuron.\n\n    Parameters\n    ----------\n    f :         IO\n                Readable buffer (must be bytes).\n    attrs :     dict | None\n                Arbitrary attributes to include in the TreeNeuron.\n\n    Returns\n    -------\n    core.TreeNeuron\n    \"\"\"\n    if not isinstance(f.read(0), bytes):\n        raise ValueError(f'Expected bytes, got \"{type(f.read(0))}\"')\n\n    zip = ZipFile(f)\n    for f in zip.filelist:\n        if f.filename.endswith(\".nml\") and \"skeleton\" in f.filename:\n            attrs[\"file\"] = f.filename\n            attrs[\"id\"] = f.filename.split(\"/\")[0]\n            return self.read_nml(zip.read(f), attrs=attrs)\n    logger.warning(\n        f'Skipped \"{f.filename.split(\"/\")[0]}.nmx\": failed to ' 'import skeleton.'\n    )\n</code></pre>"},{"location":"reference/navis/io/nrrd_io/","title":"nrrd_io","text":""},{"location":"reference/navis/io/nrrd_io/#navis.io.nrrd_io.NrrdReader","title":"<code>navis.io.nrrd_io.NrrdReader</code>","text":"Source code in <code>navis/io/nrrd_io.py</code> <pre><code>class NrrdReader(base.ImageReader):\n    def __init__(\n        self,\n        output: Literal[\"voxels\", \"dotprops\", \"raw\"] = \"voxels\",\n        threshold: Optional[Union[int, float]] = None,\n        thin: bool = False,\n        dotprop_kwargs: Dict[str, Any] = {},\n        fmt: str = DEFAULT_FMT,\n        attrs: Optional[Dict[str, Any]] = None,\n        errors: str = \"raise\",\n    ):\n        if not fmt.endswith(\".nrrd\"):\n            raise ValueError('`fmt` must end with \".nrrd\"')\n\n        super().__init__(\n            fmt=fmt,\n            attrs=attrs,\n            file_ext=\".nrrd\",\n            name_fallback=\"NRRD\",\n            read_binary=True,\n            output=output,\n            threshold=threshold,\n            thin=thin,\n            dotprop_kwargs=dotprop_kwargs,\n            errors=errors,\n        )\n\n    def format_output(self, x):\n        # This function replaces the BaseReader.format_output()\n        # This is to avoid trying to convert multiple (image, header) to NeuronList\n        if self.output == \"raw\":\n            return [n for n in x if n]\n        elif x:\n            return core.NeuronList([n for n in x if n])\n        else:\n            return core.NeuronList([])\n\n    @base.handle_errors\n    def read_buffer(\n        self, f, attrs: Optional[Dict[str, Any]] = None\n    ) -&gt; Union[np.ndarray, \"core.Dotprops\", \"core.VoxelNeuron\"]:\n        \"\"\"Read buffer into (image, header) or a neuron.\n\n        Parameters\n        ----------\n        f :         IO\n                    Readable buffer (must be bytes).\n        attrs :     dict | None\n                    Arbitrary attributes to include in the neuron.\n\n        Returns\n        -------\n        core.Dotprops | core.VoxelNeuron | np.ndarray\n\n        \"\"\"\n        if isinstance(f, HTTPResponse):\n            f = io.StringIO(f.content)\n\n        if isinstance(f, bytes):\n            f = io.BytesIO(f)\n\n        header = nrrd.read_header(f)\n        data = nrrd.read_data(header, f)\n\n        if self.output == \"raw\":\n            return data, header\n\n        # Try parsing units - this is modelled after the nrrd files you get from\n        # Virtual Fly Brain (VFB)\n        units = None\n        space_units = None\n        voxdim = np.array([1, 1, 1])\n        if \"space directions\" in header:\n            sd = np.asarray(header[\"space directions\"])\n            if sd.ndim == 2:\n                voxdim = np.diag(sd)[:3]\n        if \"space units\" in header:\n            space_units = header[\"space units\"]\n            if len(space_units) == 3:\n                units = [f\"{m} {u}\" for m, u in zip(voxdim, space_units)]\n        else:\n            units = voxdim\n\n        return self.convert_image(data, attrs, header, voxdim, units, space_units)\n</code></pre>"},{"location":"reference/navis/io/nrrd_io/#navis.io.nrrd_io.NrrdReader.read_buffer","title":"<code>read_buffer</code>","text":"<p>Read buffer into (image, header) or a neuron.</p> PARAMETER DESCRIPTION <code>f</code> <pre><code>    Readable buffer (must be bytes).\n</code></pre> <p> TYPE: <code>        IO</code> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the neuron.\n</code></pre> <p> TYPE: <code>    dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.Dotprops | core.VoxelNeuron | np.ndarray</code> Source code in <code>navis/io/nrrd_io.py</code> <pre><code>@base.handle_errors\ndef read_buffer(\n    self, f, attrs: Optional[Dict[str, Any]] = None\n) -&gt; Union[np.ndarray, \"core.Dotprops\", \"core.VoxelNeuron\"]:\n    \"\"\"Read buffer into (image, header) or a neuron.\n\n    Parameters\n    ----------\n    f :         IO\n                Readable buffer (must be bytes).\n    attrs :     dict | None\n                Arbitrary attributes to include in the neuron.\n\n    Returns\n    -------\n    core.Dotprops | core.VoxelNeuron | np.ndarray\n\n    \"\"\"\n    if isinstance(f, HTTPResponse):\n        f = io.StringIO(f.content)\n\n    if isinstance(f, bytes):\n        f = io.BytesIO(f)\n\n    header = nrrd.read_header(f)\n    data = nrrd.read_data(header, f)\n\n    if self.output == \"raw\":\n        return data, header\n\n    # Try parsing units - this is modelled after the nrrd files you get from\n    # Virtual Fly Brain (VFB)\n    units = None\n    space_units = None\n    voxdim = np.array([1, 1, 1])\n    if \"space directions\" in header:\n        sd = np.asarray(header[\"space directions\"])\n        if sd.ndim == 2:\n            voxdim = np.diag(sd)[:3]\n    if \"space units\" in header:\n        space_units = header[\"space units\"]\n        if len(space_units) == 3:\n            units = [f\"{m} {u}\" for m, u in zip(voxdim, space_units)]\n    else:\n        units = voxdim\n\n    return self.convert_image(data, attrs, header, voxdim, units, space_units)\n</code></pre>"},{"location":"reference/navis/io/precomputed_io/","title":"precomputed_io","text":""},{"location":"reference/navis/io/precomputed_io/#navis.io.precomputed_io.PrecomputedMeshReader","title":"<code>navis.io.precomputed_io.PrecomputedMeshReader</code>","text":"Source code in <code>navis/io/precomputed_io.py</code> <pre><code>class PrecomputedMeshReader(PrecomputedReader):\n    def __init__(\n        self,\n        fmt: str = DEFAULT_FMT,\n        attrs: Optional[Dict[str, Any]] = None,\n        errors: str = \"raise\",\n    ):\n        super().__init__(\n            fmt=fmt,\n            attrs=attrs,\n            file_ext=(\"\", \".ngmesh\"),\n            name_fallback=\"mesh\",\n            read_binary=True,\n            errors=errors,\n        )\n\n    @base.handle_errors\n    def read_buffer(\n        self, f: IO, attrs: Optional[Dict[str, Any]] = None\n    ) -&gt; \"core.MeshNeuron\":\n        \"\"\"Read buffer into a MeshNeuron.\n\n        Parameters\n        ----------\n        f :         IO\n                    Readable buffer - must be bytes.\n        attrs :     dict | None\n                    Arbitrary attributes to include in the MeshNeuron.\n\n        Returns\n        -------\n        core.MeshNeuron\n        \"\"\"\n        if not isinstance(f.read(0), bytes):\n            raise ValueError(f\"Expected bytes, got {type(f.read(0))}\")\n\n        num_vertices = np.frombuffer(f.read(4), np.uint32)[0]\n        vertices = np.frombuffer(f.read(int(3 * 4 * num_vertices)), np.float32).reshape(\n            -1, 3\n        )\n        faces = np.frombuffer(f.read(), np.uint32).reshape(-1, 3)\n\n        return core.MeshNeuron(\n            {\"vertices\": vertices, \"faces\": faces},\n            **(\n                self._make_attributes(\n                    {\"name\": self.name_fallback, \"origin\": \"DataFrame\"}, attrs\n                )\n            ),\n        )\n</code></pre>"},{"location":"reference/navis/io/precomputed_io/#navis.io.precomputed_io.PrecomputedMeshReader.read_buffer","title":"<code>read_buffer</code>","text":"<p>Read buffer into a MeshNeuron.</p> PARAMETER DESCRIPTION <code>f</code> <pre><code>    Readable buffer - must be bytes.\n</code></pre> <p> TYPE: <code>        IO</code> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the MeshNeuron.\n</code></pre> <p> TYPE: <code>    dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.MeshNeuron</code> Source code in <code>navis/io/precomputed_io.py</code> <pre><code>@base.handle_errors\ndef read_buffer(\n    self, f: IO, attrs: Optional[Dict[str, Any]] = None\n) -&gt; \"core.MeshNeuron\":\n    \"\"\"Read buffer into a MeshNeuron.\n\n    Parameters\n    ----------\n    f :         IO\n                Readable buffer - must be bytes.\n    attrs :     dict | None\n                Arbitrary attributes to include in the MeshNeuron.\n\n    Returns\n    -------\n    core.MeshNeuron\n    \"\"\"\n    if not isinstance(f.read(0), bytes):\n        raise ValueError(f\"Expected bytes, got {type(f.read(0))}\")\n\n    num_vertices = np.frombuffer(f.read(4), np.uint32)[0]\n    vertices = np.frombuffer(f.read(int(3 * 4 * num_vertices)), np.float32).reshape(\n        -1, 3\n    )\n    faces = np.frombuffer(f.read(), np.uint32).reshape(-1, 3)\n\n    return core.MeshNeuron(\n        {\"vertices\": vertices, \"faces\": faces},\n        **(\n            self._make_attributes(\n                {\"name\": self.name_fallback, \"origin\": \"DataFrame\"}, attrs\n            )\n        ),\n    )\n</code></pre>"},{"location":"reference/navis/io/precomputed_io/#navis.io.precomputed_io.PrecomputedReader","title":"<code>navis.io.precomputed_io.PrecomputedReader</code>","text":"Source code in <code>navis/io/precomputed_io.py</code> <pre><code>class PrecomputedReader(base.BaseReader):\n    def is_valid_file(self, file):\n        \"\"\"Return True if file should be considered for reading.\"\"\"\n        if isinstance(file, zipfile.ZipInfo):\n            file = str(file.filename)\n        elif isinstance(file, Path):\n            if not file.is_file():\n                return False\n            file = str(file.name)\n        else:\n            file = str(file)\n\n        # Drop anything with a file extension or hidden files (e.g. \".DS_store\")\n        if \".\" in file:\n            return False\n        # Ignore the info file\n        if file == \"info\":\n            return False\n        # Ignore manifests\n        if file.endswith(\":0\"):\n            return False\n        return True\n</code></pre>"},{"location":"reference/navis/io/precomputed_io/#navis.io.precomputed_io.PrecomputedReader.is_valid_file","title":"<code>is_valid_file</code>","text":"<p>Return True if file should be considered for reading.</p> Source code in <code>navis/io/precomputed_io.py</code> <pre><code>def is_valid_file(self, file):\n    \"\"\"Return True if file should be considered for reading.\"\"\"\n    if isinstance(file, zipfile.ZipInfo):\n        file = str(file.filename)\n    elif isinstance(file, Path):\n        if not file.is_file():\n            return False\n        file = str(file.name)\n    else:\n        file = str(file)\n\n    # Drop anything with a file extension or hidden files (e.g. \".DS_store\")\n    if \".\" in file:\n        return False\n    # Ignore the info file\n    if file == \"info\":\n        return False\n    # Ignore manifests\n    if file.endswith(\":0\"):\n        return False\n    return True\n</code></pre>"},{"location":"reference/navis/io/precomputed_io/#navis.io.precomputed_io.PrecomputedSkeletonReader","title":"<code>navis.io.precomputed_io.PrecomputedSkeletonReader</code>","text":"Source code in <code>navis/io/precomputed_io.py</code> <pre><code>class PrecomputedSkeletonReader(PrecomputedReader):\n    def __init__(\n        self,\n        fmt: str = DEFAULT_FMT,\n        attrs: Optional[Dict[str, Any]] = None,\n        info: Dict[str, Any] = {},\n        errors: str = \"raise\",\n    ):\n        super().__init__(\n            fmt=fmt,\n            attrs=attrs,\n            file_ext=\"\",\n            name_fallback=\"skeleton\",\n            read_binary=True,\n            errors=errors,\n        )\n        self.info = info\n\n    @base.handle_errors\n    def read_buffer(\n        self, f: IO, attrs: Optional[Dict[str, Any]] = None\n    ) -&gt; \"core.TreeNeuron\":\n        \"\"\"Read buffer into a TreeNeuron.\n\n        Parameters\n        ----------\n        f :         IO\n                    Readable buffer - must be bytes.\n        attrs :     dict | None\n                    Arbitrary attributes to include in the TreeNeuron.\n\n        Returns\n        -------\n        core.TreeNeuron\n\n        \"\"\"\n        if not isinstance(f.read(0), bytes):\n            raise ValueError(f\"Expected bytes, got {type(f.read(0))}\")\n\n        num_nodes = np.frombuffer(f.read(4), np.uint32)[0]\n        num_edges = np.frombuffer(f.read(4), np.uint32)[0]\n        nodes = np.frombuffer(f.read(int(3 * 4 * num_nodes)), np.float32).reshape(-1, 3)\n        edges = np.frombuffer(f.read(int(2 * 4 * num_edges)), np.uint32).reshape(-1, 2)\n\n        swc = self.make_swc(nodes, edges)\n\n        # Check for malformed vertex attributes (should be list of dicts)\n        if isinstance(self.info.get(\"vertex_attributes\", None), dict):\n            self.info[\"vertex_attributes\"] = [self.info[\"vertex_attributes\"]]\n\n        # Parse additional vertex attributes if specified as per the info file\n        for attr in self.info.get(\"vertex_attributes\", []):\n            dtype = np.dtype(attr[\"data_type\"])\n            n_comp = attr[\"num_components\"]\n            values = np.frombuffer(\n                f.read(int(n_comp * dtype.itemsize * num_nodes)), dtype\n            ).reshape(-1, n_comp)\n            if n_comp == 1:\n                swc[attr[\"id\"]] = values.flatten()\n            else:\n                for i in range(n_comp):\n                    swc[f\"{attr['id']}_{i}\"] = values[:, i]\n\n        return core.TreeNeuron(\n            swc,\n            **(\n                self._make_attributes(\n                    {\"name\": self.name_fallback, \"origin\": \"DataFrame\"}, attrs\n                )\n            ),\n        )\n\n    def make_swc(self, nodes: np.ndarray, edges: np.ndarray) -&gt; pd.DataFrame:\n        \"\"\"Make SWC table from nodes and edges.\n\n        Parameters\n        ----------\n        nodes :     (N, 3) array\n        edges :     (N, 2) array\n\n        Returns\n        -------\n        pandas.DataFrame\n        \"\"\"\n        swc = pd.DataFrame()\n        swc[\"node_id\"] = np.arange(len(nodes))\n        swc[\"x\"], swc[\"y\"], swc[\"z\"] = nodes[:, 0], nodes[:, 1], nodes[:, 2]\n\n        edge_dict = dict(zip(edges[:, 1], edges[:, 0]))\n        swc[\"parent_id\"] = swc.node_id.map(lambda x: edge_dict.get(x, -1)).astype(\n            np.int32\n        )\n\n        return swc\n</code></pre>"},{"location":"reference/navis/io/precomputed_io/#navis.io.precomputed_io.PrecomputedSkeletonReader.make_swc","title":"<code>make_swc</code>","text":"<p>Make SWC table from nodes and edges.</p> PARAMETER DESCRIPTION <code>nodes</code> <p> TYPE: <code>    (N, 3) array</code> </p> <code>edges</code> <p> TYPE: <code>    (N, 2) array</code> </p> RETURNS DESCRIPTION <code>pandas.DataFrame</code> Source code in <code>navis/io/precomputed_io.py</code> <pre><code>def make_swc(self, nodes: np.ndarray, edges: np.ndarray) -&gt; pd.DataFrame:\n    \"\"\"Make SWC table from nodes and edges.\n\n    Parameters\n    ----------\n    nodes :     (N, 3) array\n    edges :     (N, 2) array\n\n    Returns\n    -------\n    pandas.DataFrame\n    \"\"\"\n    swc = pd.DataFrame()\n    swc[\"node_id\"] = np.arange(len(nodes))\n    swc[\"x\"], swc[\"y\"], swc[\"z\"] = nodes[:, 0], nodes[:, 1], nodes[:, 2]\n\n    edge_dict = dict(zip(edges[:, 1], edges[:, 0]))\n    swc[\"parent_id\"] = swc.node_id.map(lambda x: edge_dict.get(x, -1)).astype(\n        np.int32\n    )\n\n    return swc\n</code></pre>"},{"location":"reference/navis/io/precomputed_io/#navis.io.precomputed_io.PrecomputedSkeletonReader.read_buffer","title":"<code>read_buffer</code>","text":"<p>Read buffer into a TreeNeuron.</p> PARAMETER DESCRIPTION <code>f</code> <pre><code>    Readable buffer - must be bytes.\n</code></pre> <p> TYPE: <code>        IO</code> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the TreeNeuron.\n</code></pre> <p> TYPE: <code>    dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.TreeNeuron</code> Source code in <code>navis/io/precomputed_io.py</code> <pre><code>@base.handle_errors\ndef read_buffer(\n    self, f: IO, attrs: Optional[Dict[str, Any]] = None\n) -&gt; \"core.TreeNeuron\":\n    \"\"\"Read buffer into a TreeNeuron.\n\n    Parameters\n    ----------\n    f :         IO\n                Readable buffer - must be bytes.\n    attrs :     dict | None\n                Arbitrary attributes to include in the TreeNeuron.\n\n    Returns\n    -------\n    core.TreeNeuron\n\n    \"\"\"\n    if not isinstance(f.read(0), bytes):\n        raise ValueError(f\"Expected bytes, got {type(f.read(0))}\")\n\n    num_nodes = np.frombuffer(f.read(4), np.uint32)[0]\n    num_edges = np.frombuffer(f.read(4), np.uint32)[0]\n    nodes = np.frombuffer(f.read(int(3 * 4 * num_nodes)), np.float32).reshape(-1, 3)\n    edges = np.frombuffer(f.read(int(2 * 4 * num_edges)), np.uint32).reshape(-1, 2)\n\n    swc = self.make_swc(nodes, edges)\n\n    # Check for malformed vertex attributes (should be list of dicts)\n    if isinstance(self.info.get(\"vertex_attributes\", None), dict):\n        self.info[\"vertex_attributes\"] = [self.info[\"vertex_attributes\"]]\n\n    # Parse additional vertex attributes if specified as per the info file\n    for attr in self.info.get(\"vertex_attributes\", []):\n        dtype = np.dtype(attr[\"data_type\"])\n        n_comp = attr[\"num_components\"]\n        values = np.frombuffer(\n            f.read(int(n_comp * dtype.itemsize * num_nodes)), dtype\n        ).reshape(-1, n_comp)\n        if n_comp == 1:\n            swc[attr[\"id\"]] = values.flatten()\n        else:\n            for i in range(n_comp):\n                swc[f\"{attr['id']}_{i}\"] = values[:, i]\n\n    return core.TreeNeuron(\n        swc,\n        **(\n            self._make_attributes(\n                {\"name\": self.name_fallback, \"origin\": \"DataFrame\"}, attrs\n            )\n        ),\n    )\n</code></pre>"},{"location":"reference/navis/io/precomputed_io/#navis.io.precomputed_io.PrecomputedWriter","title":"<code>navis.io.precomputed_io.PrecomputedWriter</code>","text":"<p>Writer class that also takes care of <code>info</code> files.</p> Source code in <code>navis/io/precomputed_io.py</code> <pre><code>class PrecomputedWriter(base.Writer):\n    \"\"\"Writer class that also takes care of `info` files.\"\"\"\n\n    def write_any(self, x, filepath, write_info=True, **kwargs):\n        \"\"\"Write any to file. Default entry point.\"\"\"\n        # First write the actual neurons\n        kwargs[\"write_info\"] = False\n        super().write_any(x, filepath=filepath, **kwargs)\n\n        # Write info file to the correct directory/zipfile\n        if write_info:\n            add_props = {}\n            if kwargs.get(\"radius\", False):\n                add_props[\"vertex_attributes\"] = [\n                    {\"id\": \"radius\", \"data_type\": \"float32\", \"num_components\": 1}\n                ]\n\n            if str(self.path).endswith(\".zip\"):\n                with ZipFile(self.path, mode=\"a\") as zf:\n                    # Context-manager will remove temporary directory and its contents\n                    with tempfile.TemporaryDirectory() as tempdir:\n                        # Write info to zip\n                        if write_info:\n                            # Generate temporary filename\n                            f = os.path.join(tempdir, \"info\")\n                            write_info_file(x, f, add_props=add_props)\n                            # Add file to zip\n                            zf.write(f, arcname=\"info\", compress_type=compression)\n            else:\n                fp = self.path\n                # Find the first existing root directory\n                while not fp.is_dir():\n                    fp = fp.parent\n\n                write_info_file(x, fp, add_props=add_props)\n</code></pre>"},{"location":"reference/navis/io/precomputed_io/#navis.io.precomputed_io.PrecomputedWriter.write_any","title":"<code>write_any</code>","text":"<p>Write any to file. Default entry point.</p> Source code in <code>navis/io/precomputed_io.py</code> <pre><code>def write_any(self, x, filepath, write_info=True, **kwargs):\n    \"\"\"Write any to file. Default entry point.\"\"\"\n    # First write the actual neurons\n    kwargs[\"write_info\"] = False\n    super().write_any(x, filepath=filepath, **kwargs)\n\n    # Write info file to the correct directory/zipfile\n    if write_info:\n        add_props = {}\n        if kwargs.get(\"radius\", False):\n            add_props[\"vertex_attributes\"] = [\n                {\"id\": \"radius\", \"data_type\": \"float32\", \"num_components\": 1}\n            ]\n\n        if str(self.path).endswith(\".zip\"):\n            with ZipFile(self.path, mode=\"a\") as zf:\n                # Context-manager will remove temporary directory and its contents\n                with tempfile.TemporaryDirectory() as tempdir:\n                    # Write info to zip\n                    if write_info:\n                        # Generate temporary filename\n                        f = os.path.join(tempdir, \"info\")\n                        write_info_file(x, f, add_props=add_props)\n                        # Add file to zip\n                        zf.write(f, arcname=\"info\", compress_type=compression)\n        else:\n            fp = self.path\n            # Find the first existing root directory\n            while not fp.is_dir():\n                fp = fp.parent\n\n            write_info_file(x, fp, add_props=add_props)\n</code></pre>"},{"location":"reference/navis/io/precomputed_io/#navis.io.precomputed_io.write_info_file","title":"<code>navis.io.precomputed_io.write_info_file</code>","text":"<p>Write neuroglancer 'info' file for given neurons.</p> PARAMETER DESCRIPTION <code>data</code> <p> TYPE: <code>        navis.NeuronList | navis.Volumes | trimesh</code> </p> <code>filepath</code> <pre><code>       Path to write the file to.\n</code></pre> <p> TYPE: <code>    str | Path</code> </p> <code>add_props</code> <pre><code>       Additional properties to write to the file.\n</code></pre> <p> TYPE: <code>   dict</code> DEFAULT: <code>{}</code> </p> Source code in <code>navis/io/precomputed_io.py</code> <pre><code>def write_info_file(data, filepath, add_props={}):\n    \"\"\"Write neuroglancer 'info' file for given neurons.\n\n    Parameters\n    ----------\n    data :         navis.NeuronList | navis.Volumes | trimesh\n    filepath :     str | Path\n                   Path to write the file to.\n    add_props :    dict\n                   Additional properties to write to the file.\n\n    \"\"\"\n    info = {}\n    if utils.is_iterable(data):\n        types = list(set([type(d) for d in data]))\n        if len(types) &gt; 1:\n            raise ValueError(\n                \"Unable to write info file for mixed data: \" f\"{data.types}\"\n            )\n        data = data[0]\n\n    if utils.is_mesh(data):\n        info[\"@type\"] = \"neuroglancer_legacy_mesh\"\n    elif isinstance(data, core.TreeNeuron):\n        info[\"@type\"] = \"neuroglancer_skeletons\"\n\n        # If we know the units add transform from \"stored model\"\n        # to \"model space\" which is supposed to be nm\n        if not data.units.dimensionless:\n            u = data.units.to(\"1 nm\").magnitude\n        else:\n            u = 1\n        tr = np.zeros((4, 3), dtype=int)\n        tr[:3, :3] = np.diag([u, u, u])\n        info[\"transform\"] = tr.T.flatten().tolist()\n\n    else:\n        raise TypeError(f'Unable to write info file for data of type \"{type(data)}\"')\n\n    info.update(add_props)\n    if not str(filepath).endswith(\"/info\"):\n        filepath = os.path.join(filepath, \"info\")\n    with open(filepath, \"w\") as f:\n        json.dump(info, f)\n</code></pre>"},{"location":"reference/navis/io/rda_io/","title":"rda_io","text":""},{"location":"reference/navis/io/rda_io/#navis.io.rda_io.dotprops_constructor","title":"<code>navis.io.rda_io.dotprops_constructor</code>","text":"<p>Convert nat dotprops to navis Dotprops.</p> Source code in <code>navis/io/rda_io.py</code> <pre><code>def dotprops_constructor(obj: Any,\n                         attrs: Mapping[Union[str, bytes], Any],\n                         ) -&gt; 'core.Dotprops':\n    \"\"\"Convert nat dotprops to navis Dotprops.\"\"\"\n    pts = np.asarray(obj.pop('points'))\n    vect = np.asarray(obj.pop('vect'))\n    alpha = np.asarray(obj.pop('alpha'))\n    k = int(attrs.get('k', 1)[0])\n    file = attrs.get('file', [None])[0]\n\n    return core.Dotprops(points=pts, k=k, alpha=alpha, vect=vect, file=file)\n</code></pre>"},{"location":"reference/navis/io/rda_io/#navis.io.rda_io.neuron_constructor","title":"<code>navis.io.rda_io.neuron_constructor</code>","text":"<p>Convert nat neuron/catmaidneuron to navis TreeNeuron.</p> Source code in <code>navis/io/rda_io.py</code> <pre><code>def neuron_constructor(obj: Any,\n                       attrs: Mapping[Union[str, bytes], Any],\n                       ) -&gt; 'core.TreeNeuron':\n    \"\"\"Convert nat neuron/catmaidneuron to navis TreeNeuron.\"\"\"\n    # Data to skip\n    DO_NOT_USE = ['nTrees', 'SegList', 'NumPoints', 'StartPoint', 'EndPoints',\n                  'BranchPoints', 'NumSegs']\n\n    # Construct neuron from just the nodes\n    n = core.TreeNeuron(obj.pop('d'))\n\n    # R uses diameter, not radius - let's fix that\n    if 'radius' in n.nodes.columns:\n        has_rad = n.nodes.radius.fillna(0) &gt; 0\n        n.nodes.loc[has_rad, 'radius'] = n.nodes.loc[has_rad, 'radius'] / 2\n\n    # If this is a CATMAID neuron, we assume it's in nanometers\n    if 'catmaidneuron' in attrs.get('class', []):\n        n.units = 'nm'\n\n    # Reuse ID\n    if 'skid' in obj:\n        skid = obj.pop('skid')\n        if utils.is_iterable(skid):\n            n.id = skid[0]\n        else:\n            n.id = skid\n\n    # Try attaching other data\n    for k, v in obj.items():\n        if k in DO_NOT_USE:\n            continue\n        try:\n            setattr(n, k, v)\n        except BaseException:\n            pass\n\n    return n\n</code></pre>"},{"location":"reference/navis/io/rda_io/#navis.io.rda_io.neuronlist_constructor","title":"<code>navis.io.rda_io.neuronlist_constructor</code>","text":"<p>Convert nat neuronlists to navis NeuronLists.</p> Source code in <code>navis/io/rda_io.py</code> <pre><code>def neuronlist_constructor(obj: Any,\n                           attrs: Mapping[Union[str, bytes], Any],\n                           ) -&gt; 'core.NeuronList':\n    \"\"\"Convert nat neuronlists to navis NeuronLists.\"\"\"\n    # Set IDs\n    neurons = []\n    for k, n in obj.items():\n        if isinstance(n, (core.BaseNeuron, core.NeuronList)):\n            n.id = k\n            neurons.append(n)\n        else:\n            logger.warning(f'Unexpected object in neuronlist: {type(n)}. '\n                           'Possible parsing error.')\n\n    # Turn into NeuronList\n    nl = core.NeuronList(neurons)\n\n    # Now parse extra attributes DataFrame\n    df = attrs.get('df', None)\n    if isinstance(df, pd.DataFrame):\n        # Make sure we have still the correct order\n        nl = nl.idx[attrs['names']]\n\n        for col in df:\n            # Skip non-string columns\n            if not isinstance(col, str):\n                continue\n\n            # Skip some columns\n            if col.lower() in ['type', 'idx']:\n                continue\n            if col.lower() in nl[0].__dict__.keys():\n                continue\n\n            for n, v in zip(nl, df[col].values):\n                # Register\n                n._register_attr(col.lower(), v)\n\n    return nl\n</code></pre>"},{"location":"reference/navis/io/rda_io/#navis.io.rda_io.volume_constructor","title":"<code>navis.io.rda_io.volume_constructor</code>","text":"<p>Convert e.g. mesh3d to navis Volume.</p> Source code in <code>navis/io/rda_io.py</code> <pre><code>def volume_constructor(obj: Any,\n                       attrs: Mapping[Union[str, bytes], Any],\n                       ) -&gt; 'core.Volume':\n    \"\"\"Convert e.g. mesh3d to navis Volume.\"\"\"\n    if 'vb' in obj and 'it' in obj:\n        verts = np.asarray(obj.pop('vb'))[:3, :].T\n        faces = np.asarray(obj.pop('it')).T - 1\n        return core.Volume(vertices=verts, faces=faces)\n    elif 'Vertices' in obj and \"Regions\" in obj:\n        verts = obj['Vertices'][['X', 'Y', 'Z']].values\n\n        # If only one region\n        if len(obj['Regions']) == 1:\n            region = list(obj['Regions'].keys())[0]\n            faces = obj['Regions'][region][['V1', 'V2', 'V3']].values - 1\n            return core.Volume(vertices=verts, faces=faces)\n        else:\n            volumes = []\n            for r in obj['Regions']:\n                faces = obj['Regions'][r][['V1', 'V2', 'V3']].values - 1\n                volumes.append(core.Volume(vertices=verts, faces=faces, name=r))\n            return volumes\n    else:\n        logger.warning('Unable to construct Volume from R object of type '\n                       f'\"{attrs[\"class\"]}\". Returning raw data')\n        return obj\n</code></pre>"},{"location":"reference/navis/io/swc_io/","title":"swc_io","text":""},{"location":"reference/navis/io/swc_io/#navis.io.swc_io.SwcReader","title":"<code>navis.io.swc_io.SwcReader</code>","text":"Source code in <code>navis/io/swc_io.py</code> <pre><code>class SwcReader(base.BaseReader):\n    def __init__(\n        self,\n        connector_labels: Optional[Dict[str, Union[str, int]]] = None,\n        soma_label: Union[str, int] = 1,\n        delimiter: str = DEFAULT_DELIMITER,\n        precision: int = DEFAULT_PRECISION,\n        read_meta: bool = False,\n        fmt: str = DEFAULT_FMT,\n        errors: str = \"raise\",\n        attrs: Optional[Dict[str, Any]] = None,\n    ):\n        if not fmt.endswith(\".swc\"):\n            raise ValueError('`fmt` must end with \".swc\"')\n\n        super().__init__(\n            fmt=fmt, attrs=attrs, file_ext=\".swc\", errors=errors, name_fallback=\"SWC\"\n        )\n        self.connector_labels = connector_labels or dict()\n        self.soma_label = soma_label\n        self.delimiter = delimiter\n        self.read_meta = read_meta\n\n        int_, float_ = base.parse_precision(precision)\n        self._dtypes = {\n            \"node_id\": int_,\n            \"parent_id\": int_,\n            \"label\": \"category\",\n            \"x\": float_,\n            \"y\": float_,\n            \"z\": float_,\n            \"radius\": float_,\n        }\n\n    @base.handle_errors\n    def read_buffer(\n        self, f: IO, attrs: Optional[Dict[str, Any]] = None\n    ) -&gt; \"core.TreeNeuron\":\n        \"\"\"Read buffer into a TreeNeuron.\n\n        Parameters\n        ----------\n        f :         IO\n                    Readable buffer (if bytes, interpreted as utf-8).\n        attrs :     dict | None\n                    Arbitrary attributes to include in the TreeNeuron.\n\n        Returns\n        -------\n        core.TreeNeuron\n        \"\"\"\n        if isinstance(f, HTTPResponse):\n            f = io.StringIO(f.data.decode())\n\n        if isinstance(f.read(0), bytes):\n            f = io.TextIOWrapper(f, encoding=\"utf-8\")\n\n        header_rows = read_header_rows(f)\n        try:\n            nodes = pd.read_csv(\n                f,\n                delimiter=self.delimiter,\n                skipinitialspace=True,\n                skiprows=len(header_rows),\n                comment=COMMENT,\n                header=None,\n                na_values=NA_VALUES,\n            )\n            if len(nodes.columns) &lt; len(NODE_COLUMNS):\n                raise ValueError(\"Not enough columns in SWC file.\")\n            elif len(nodes.columns) &gt; len(NODE_COLUMNS):\n                logger.warning(\n                    f\"Found {len(nodes.columns)} instead of the expected 7 \"\n                    \"columns in SWC file. Assuming additional columns are \"\n                    \"custom properties. You can silence this warning by setting \"\n                    \"`navis.set_loggers('ERROR')`.\"\n                )\n                nodes.columns = (\n                    list(NODE_COLUMNS) + nodes.columns[len(NODE_COLUMNS) :].tolist()\n                )\n            else:\n                nodes.columns = NODE_COLUMNS\n        except pd.errors.EmptyDataError:\n            # If file is totally empty, return an empty neuron\n            # Note that the TreeNeuron will still complain but it's a better\n            # error message\n            nodes = pd.DataFrame(columns=NODE_COLUMNS)\n\n        # Check for row with JSON-formatted meta data\n        # Expected format '# Meta: {\"id\": \"12345\"}'\n        if self.read_meta:\n            meta_row = [r for r in header_rows if r.lower().startswith(\"# meta:\")]\n            if meta_row:\n                meta_data = json.loads(meta_row[0][7:].strip())\n                attrs = base.merge_dicts(meta_data, attrs)\n\n        return self.read_dataframe(\n            nodes, base.merge_dicts({\"swc_header\": \"\\n\".join(header_rows)}, attrs)\n        )\n\n    @base.handle_errors\n    def read_dataframe(\n        self, nodes: pd.DataFrame, attrs: Optional[Dict[str, Any]] = None\n    ) -&gt; \"core.TreeNeuron\":\n        \"\"\"Convert a SWC-like DataFrame into a TreeNeuron.\n\n        Parameters\n        ----------\n        nodes :     pandas.DataFrame\n        attrs :     dict or None\n                    Arbitrary attributes to include in the TreeNeuron.\n\n        Returns\n        -------\n        core.TreeNeuron\n        \"\"\"\n        n = core.TreeNeuron(\n            sanitise_nodes(nodes.astype(self._dtypes, errors=\"ignore\", copy=False)),\n            connectors=self._extract_connectors(nodes),\n        )\n\n        if self.soma_label is not None:\n            is_soma_node = n.nodes.label.values == self.soma_label\n            if any(is_soma_node):\n                n.soma = n.nodes.node_id.values[is_soma_node][0]\n\n        attrs = self._make_attributes({\"name\": \"SWC\", \"origin\": \"DataFrame\"}, attrs)\n\n        # SWC is special - we do not want to register it\n        n.swc_header = attrs.pop(\"swc_header\", \"\")\n\n        # Try adding properties one-by-one. If one fails, we'll keep track of it\n        # in the `.meta` attribute\n        meta = {}\n        for k, v in attrs.items():\n            try:\n                n._register_attr(k, v)\n            except (AttributeError, ValueError, TypeError):\n                meta[k] = v\n\n        if meta:\n            n.meta = meta\n\n        return n\n\n    def _extract_connectors(self, nodes: pd.DataFrame) -&gt; Optional[pd.DataFrame]:\n        \"\"\"Infer outgoing/incoming connectors from node labels.\n\n        Parameters\n        ----------\n        nodes :     pd.DataFrame\n\n        Returns\n        -------\n        Optional[pd.DataFrame]\n                    With columns `[\"node_id\", \"x\", \"y\", \"z\", \"connector_id\", \"type\"]`\n        \"\"\"\n        if not self.connector_labels:\n            return None\n\n        to_concat = [\n            pd.DataFrame([], columns=[\"node_id\", \"connector_id\", \"type\", \"x\", \"y\", \"z\"])\n        ]\n        for name, val in self.connector_labels.items():\n            cn = nodes[nodes.label == val][[\"node_id\", \"x\", \"y\", \"z\"]].copy()\n            cn[\"connector_id\"] = None\n            cn[\"type\"] = name\n            to_concat.append(cn)\n\n        return pd.concat(to_concat, axis=0)\n</code></pre>"},{"location":"reference/navis/io/swc_io/#navis.io.swc_io.SwcReader.read_buffer","title":"<code>read_buffer</code>","text":"<p>Read buffer into a TreeNeuron.</p> PARAMETER DESCRIPTION <code>f</code> <pre><code>    Readable buffer (if bytes, interpreted as utf-8).\n</code></pre> <p> TYPE: <code>        IO</code> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the TreeNeuron.\n</code></pre> <p> TYPE: <code>    dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.TreeNeuron</code> Source code in <code>navis/io/swc_io.py</code> <pre><code>@base.handle_errors\ndef read_buffer(\n    self, f: IO, attrs: Optional[Dict[str, Any]] = None\n) -&gt; \"core.TreeNeuron\":\n    \"\"\"Read buffer into a TreeNeuron.\n\n    Parameters\n    ----------\n    f :         IO\n                Readable buffer (if bytes, interpreted as utf-8).\n    attrs :     dict | None\n                Arbitrary attributes to include in the TreeNeuron.\n\n    Returns\n    -------\n    core.TreeNeuron\n    \"\"\"\n    if isinstance(f, HTTPResponse):\n        f = io.StringIO(f.data.decode())\n\n    if isinstance(f.read(0), bytes):\n        f = io.TextIOWrapper(f, encoding=\"utf-8\")\n\n    header_rows = read_header_rows(f)\n    try:\n        nodes = pd.read_csv(\n            f,\n            delimiter=self.delimiter,\n            skipinitialspace=True,\n            skiprows=len(header_rows),\n            comment=COMMENT,\n            header=None,\n            na_values=NA_VALUES,\n        )\n        if len(nodes.columns) &lt; len(NODE_COLUMNS):\n            raise ValueError(\"Not enough columns in SWC file.\")\n        elif len(nodes.columns) &gt; len(NODE_COLUMNS):\n            logger.warning(\n                f\"Found {len(nodes.columns)} instead of the expected 7 \"\n                \"columns in SWC file. Assuming additional columns are \"\n                \"custom properties. You can silence this warning by setting \"\n                \"`navis.set_loggers('ERROR')`.\"\n            )\n            nodes.columns = (\n                list(NODE_COLUMNS) + nodes.columns[len(NODE_COLUMNS) :].tolist()\n            )\n        else:\n            nodes.columns = NODE_COLUMNS\n    except pd.errors.EmptyDataError:\n        # If file is totally empty, return an empty neuron\n        # Note that the TreeNeuron will still complain but it's a better\n        # error message\n        nodes = pd.DataFrame(columns=NODE_COLUMNS)\n\n    # Check for row with JSON-formatted meta data\n    # Expected format '# Meta: {\"id\": \"12345\"}'\n    if self.read_meta:\n        meta_row = [r for r in header_rows if r.lower().startswith(\"# meta:\")]\n        if meta_row:\n            meta_data = json.loads(meta_row[0][7:].strip())\n            attrs = base.merge_dicts(meta_data, attrs)\n\n    return self.read_dataframe(\n        nodes, base.merge_dicts({\"swc_header\": \"\\n\".join(header_rows)}, attrs)\n    )\n</code></pre>"},{"location":"reference/navis/io/swc_io/#navis.io.swc_io.SwcReader.read_dataframe","title":"<code>read_dataframe</code>","text":"<p>Convert a SWC-like DataFrame into a TreeNeuron.</p> PARAMETER DESCRIPTION <code>nodes</code> <p> TYPE: <code>    pandas.DataFrame</code> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the TreeNeuron.\n</code></pre> <p> TYPE: <code>    dict or None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.TreeNeuron</code> Source code in <code>navis/io/swc_io.py</code> <pre><code>@base.handle_errors\ndef read_dataframe(\n    self, nodes: pd.DataFrame, attrs: Optional[Dict[str, Any]] = None\n) -&gt; \"core.TreeNeuron\":\n    \"\"\"Convert a SWC-like DataFrame into a TreeNeuron.\n\n    Parameters\n    ----------\n    nodes :     pandas.DataFrame\n    attrs :     dict or None\n                Arbitrary attributes to include in the TreeNeuron.\n\n    Returns\n    -------\n    core.TreeNeuron\n    \"\"\"\n    n = core.TreeNeuron(\n        sanitise_nodes(nodes.astype(self._dtypes, errors=\"ignore\", copy=False)),\n        connectors=self._extract_connectors(nodes),\n    )\n\n    if self.soma_label is not None:\n        is_soma_node = n.nodes.label.values == self.soma_label\n        if any(is_soma_node):\n            n.soma = n.nodes.node_id.values[is_soma_node][0]\n\n    attrs = self._make_attributes({\"name\": \"SWC\", \"origin\": \"DataFrame\"}, attrs)\n\n    # SWC is special - we do not want to register it\n    n.swc_header = attrs.pop(\"swc_header\", \"\")\n\n    # Try adding properties one-by-one. If one fails, we'll keep track of it\n    # in the `.meta` attribute\n    meta = {}\n    for k, v in attrs.items():\n        try:\n            n._register_attr(k, v)\n        except (AttributeError, ValueError, TypeError):\n            meta[k] = v\n\n    if meta:\n        n.meta = meta\n\n    return n\n</code></pre>"},{"location":"reference/navis/io/swc_io/#navis.io.swc_io.make_swc_table","title":"<code>navis.io.swc_io.make_swc_table</code>","text":"<p>Generate a node table compliant with the SWC format.</p> <p>Follows the format specified here.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>            Dotprops will be turned from points + vectors to\n            individual segments.\n</code></pre> <p> TYPE: <code>                TreeNeuron | Dotprops</code> </p> <code>labels</code> <pre><code>            Node labels. Can be::\n\n            str : column name in node table\n            dict: must be of format {node_id: 'label', ...}.\n            bool: if True, will generate automatic labels, if False all nodes have label \"0\".\n</code></pre> <p> TYPE: <code>           str | dict | bool</code> DEFAULT: <code>None</code> </p> <code>export_connectors</code> <pre><code>            If True, will label nodes with pre- (\"7\") and\n            postsynapse (\"8\"). Because only one label can be given\n            this might drop synapses (i.e. in case of multiple\n            pre- or postsynapses on a single node)! `labels`\n            must be `True` for this to have any effect.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>return_node_map</code> <pre><code>            If True, will return a dictionary mapping the old node\n            ID to the new reindexed node IDs in the file.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>SWC table :         pandas.DataFrame</code> <code>node map :          dict</code> <p>Only if <code>return_node_map=True</code>.</p> Source code in <code>navis/io/swc_io.py</code> <pre><code>def make_swc_table(\n    x: Union[\"core.TreeNeuron\", \"core.Dotprops\"],\n    labels: Union[str, dict, bool] = None,\n    export_connectors: bool = False,\n    return_node_map: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Generate a node table compliant with the SWC format.\n\n    Follows the format specified\n    [here](http://www.neuronland.org/NLMorphologyConverter/MorphologyFormats/SWC/Spec.html).\n\n    Parameters\n    ----------\n    x :                 TreeNeuron | Dotprops\n                        Dotprops will be turned from points + vectors to\n                        individual segments.\n    labels :            str | dict | bool, optional\n                        Node labels. Can be::\n\n                        str : column name in node table\n                        dict: must be of format {node_id: 'label', ...}.\n                        bool: if True, will generate automatic labels, if False all nodes have label \"0\".\n\n    export_connectors : bool, optional\n                        If True, will label nodes with pre- (\"7\") and\n                        postsynapse (\"8\"). Because only one label can be given\n                        this might drop synapses (i.e. in case of multiple\n                        pre- or postsynapses on a single node)! `labels`\n                        must be `True` for this to have any effect.\n    return_node_map :   bool\n                        If True, will return a dictionary mapping the old node\n                        ID to the new reindexed node IDs in the file.\n\n    Returns\n    -------\n    SWC table :         pandas.DataFrame\n    node map :          dict\n                        Only if `return_node_map=True`.\n\n    \"\"\"\n    if isinstance(x, core.Dotprops):\n        x = x.to_skeleton()\n\n    # Work on a copy\n    swc = x.nodes.copy()\n\n    # Add labels\n    swc[\"label\"] = 0\n    if isinstance(labels, dict):\n        swc[\"label\"] = swc.index.map(labels)\n    elif isinstance(labels, str):\n        swc[\"label\"] = swc[labels]\n    elif labels:\n        # Add end/branch labels\n        swc.loc[swc.type == \"branch\", \"label\"] = 5\n        swc.loc[swc.type == \"end\", \"label\"] = 6\n        # Add soma label\n        if not isinstance(x.soma, type(None)):\n            soma = utils.make_iterable(x.soma)\n            swc.loc[swc.node_id.isin(soma), \"label\"] = 1\n        if export_connectors:\n            # Add synapse label\n            pre_ids = x.presynapses.node_id.values\n            post_ids = x.postsynapses.node_id.values\n            swc.loc[swc.node_id.isin(pre_ids), \"label\"] = 7\n            swc.loc[swc.node_id.isin(post_ids), \"label\"] = 8\n\n    # Sort such that the parent is always before the child\n    swc.sort_values(\"parent_id\", ascending=True, inplace=True)\n\n    # Reset index\n    swc.reset_index(drop=True, inplace=True)\n\n    # Generate mapping\n    new_ids = dict(zip(swc.node_id.values, swc.index.values + 1))\n\n    swc[\"node_id\"] = swc.node_id.map(new_ids)\n    # Lambda prevents potential issue with missing parents\n    swc[\"parent_id\"] = swc.parent_id.map(lambda x: new_ids.get(x, -1))\n\n    # Get things in order\n    swc = swc[[\"node_id\", \"label\", \"x\", \"y\", \"z\", \"radius\", \"parent_id\"]]\n\n    # Make sure radius has no `None`\n    swc[\"radius\"] = swc.radius.fillna(0)\n\n    # Adjust column titles\n    swc.columns = [\"PointNo\", \"Label\", \"X\", \"Y\", \"Z\", \"Radius\", \"Parent\"]\n\n    if return_node_map:\n        return swc, new_ids\n\n    return swc\n</code></pre>"},{"location":"reference/navis/io/swc_io/#navis.io.swc_io.read_header_rows","title":"<code>navis.io.swc_io.read_header_rows</code>","text":"Source code in <code>navis/io/swc_io.py</code> <pre><code>def read_header_rows(f: TextIO):\n    f\"\"\"Read {COMMENT}-prefixed lines from the start of a buffer,\n    then seek back to the start of the buffer.\n\n    Parameters\n    ----------\n    f : io.TextIO\n\n    Returns\n    -------\n    list : List of strings\n    \"\"\"\n    out = []\n    for line in f:\n        if not line.startswith(COMMENT):\n            break\n        out.append(line)\n\n    f.seek(0)\n    return out\n</code></pre>"},{"location":"reference/navis/io/swc_io/#navis.io.swc_io.sanitise_nodes","title":"<code>navis.io.swc_io.sanitise_nodes</code>","text":"<p>Check that nodes dataframe is non-empty and is not missing any data.</p> PARAMETER DESCRIPTION <code>nodes</code> <p> TYPE: <code>pandas.DataFrame</code> </p> RETURNS DESCRIPTION <code>pandas.DataFrame</code> Source code in <code>navis/io/swc_io.py</code> <pre><code>def sanitise_nodes(nodes: pd.DataFrame, allow_empty=True) -&gt; pd.DataFrame:\n    \"\"\"Check that nodes dataframe is non-empty and is not missing any data.\n\n    Parameters\n    ----------\n    nodes : pandas.DataFrame\n\n    Returns\n    -------\n    pandas.DataFrame\n    \"\"\"\n    if not allow_empty and nodes.empty:\n        raise ValueError(\"No data found in SWC.\")\n\n    is_na = nodes[[\"node_id\", \"parent_id\", \"x\", \"y\", \"z\"]].isna().any(axis=1)\n\n    if is_na.any():\n        # Remove nodes with missing data\n        nodes = nodes.loc[~is_na.any(axis=1)]\n\n        # Because we removed nodes, we'll have to run a more complicated root\n        # detection\n        nodes.loc[~nodes.parent_id.isin(nodes.node_id), \"parent_id\"] = -1\n\n    return nodes\n</code></pre>"},{"location":"reference/navis/io/tiff_io/","title":"tiff_io","text":""},{"location":"reference/navis/io/tiff_io/#navis.io.tiff_io.TiffReader","title":"<code>navis.io.tiff_io.TiffReader</code>","text":"Source code in <code>navis/io/tiff_io.py</code> <pre><code>class TiffReader(base.ImageReader):\n    def __init__(\n        self,\n        output: Literal[\"voxels\", \"dotprops\", \"raw\"] = \"voxels\",\n        channel: int = 0,\n        threshold: Optional[Union[int, float]] = None,\n        thin: bool = False,\n        dotprop_kwargs: Dict[str, Any] = {},\n        fmt: str = DEFAULT_FMT,\n        errors: str = \"raise\",\n        attrs: Optional[Dict[str, Any]] = None,\n    ):\n        if not fmt.endswith(\".tif\") and not fmt.endswith(\".tiff\"):\n            raise ValueError('`fmt` must end with \".tif\" or \".tiff\"')\n\n        super().__init__(\n            fmt=fmt,\n            attrs=attrs,\n            file_ext=(\".tif\", \".tiff\"),\n            name_fallback=\"TIFF\",\n            read_binary=True,\n            output=output,\n            threshold=threshold,\n            thin=thin,\n            dotprop_kwargs=dotprop_kwargs,\n            errors=errors,\n        )\n        self.channel = channel\n\n    def format_output(self, x):\n        # This function replaces the BaseReader.format_output()\n        # This is to avoid trying to convert multiple (image, header) to NeuronList\n        if self.output == \"raw\":\n            return x\n        elif x:\n            return core.NeuronList([n for n in x if n])\n        else:\n            return core.NeuronList([])\n\n    @base.handle_errors\n    def read_buffer(\n        self, f, attrs: Optional[Dict[str, Any]] = None\n    ) -&gt; Union[np.ndarray, \"core.Dotprops\", \"core.VoxelNeuron\"]:\n        \"\"\"Read buffer into (image, header) or a neuron.\n\n        Parameters\n        ----------\n        f :         IO\n                    Readable buffer (must be bytes).\n        attrs :     dict | None\n                    Arbitrary attributes to include in the neuron.\n\n        Returns\n        -------\n        core.Dotprops | core.VoxelNeuron | np.ndarray\n\n        \"\"\"\n        import tifffile\n\n        if isinstance(f, HTTPResponse):\n            f = io.StringIO(f.content)\n\n        if isinstance(f, bytes):\n            f = io.BytesIO(f)\n\n        with tifffile.TiffFile(f) as tif:\n            # The header contains some but not all the info\n            if hasattr(tif, \"imagej_metadata\") and tif.imagej_metadata is not None:\n                header = tif.imagej_metadata\n            else:\n                header = {}\n\n            # Read the x/y resolution from the first \"page\" (i.e. the first slice)\n            res = tif.pages[0].resolution\n            # Resolution to spacing\n            header[\"xy_spacing\"] = (1 / res[0], 1 / res[1])\n\n            # Get the axes; this will be something like \"ZCYX\" where:\n            # Z = slices, C = channels, Y = rows, X = columns, S = color(?), Q = empty(?)\n            axes = tif.series[0].axes\n\n            # Generate volume\n            data = tif.asarray()\n\n        if self.output == \"raw\":\n            return data, header\n\n        # Drop \"Q\" axes if they have dimenions of 1 (we're assuming these are empty)\n        while \"Q\" in axes and data.shape[axes.index(\"Q\")] == 1:\n            data = np.squeeze(data, axis=axes.index(\"Q\"))\n            axes = axes.replace(\"Q\", \"\", 1)  # Only remove the first occurrence\n        if \"C\" in axes:\n            # Extract the requested channel from the volume\n            data = data.take(self.channel, axis=axes.index(\"C\"))\n            axes = axes.replace(\"C\", \"\")\n\n        # At this point we expect 3D data\n        if data.ndim != 3:\n            raise ValueError(f'Expected 3D greyscale data, got {data.ndim} (\"{axes}\").')\n\n        # Swap axes to XYZ order\n        order = []\n        for a in (\"X\", \"Y\", \"Z\"):\n            if a not in axes:\n                logger.warning(\n                    f'Expected axes to contain \"Z\", \"Y\", and \"X\", got \"{axes}\". '\n                    \"Axes will not be automatically reordered.\"\n                )\n                order = None\n                break\n            order.append(axes.index(a))\n        if order:\n            data = np.transpose(data, order)\n\n        # Try parsing units - this is modelled after the tif files you get from ImageJ\n        units = None\n        space_units = None\n        voxdim = np.array([1, 1, 1], dtype=np.float64)\n        if \"spacing\" in header:\n            voxdim[2] = header[\"spacing\"]\n        if \"xy_spacing\" in header:\n            voxdim[:2] = header[\"xy_spacing\"]\n        if \"unit\" in header:\n            space_units = header[\"unit\"]\n            units = [f\"{m} {space_units}\" for m in voxdim]\n        else:\n            units = voxdim\n\n        return self.convert_image(data, attrs, header, voxdim, units, space_units)\n</code></pre>"},{"location":"reference/navis/io/tiff_io/#navis.io.tiff_io.TiffReader.read_buffer","title":"<code>read_buffer</code>","text":"<p>Read buffer into (image, header) or a neuron.</p> PARAMETER DESCRIPTION <code>f</code> <pre><code>    Readable buffer (must be bytes).\n</code></pre> <p> TYPE: <code>        IO</code> </p> <code>attrs</code> <pre><code>    Arbitrary attributes to include in the neuron.\n</code></pre> <p> TYPE: <code>    dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>core.Dotprops | core.VoxelNeuron | np.ndarray</code> Source code in <code>navis/io/tiff_io.py</code> <pre><code>@base.handle_errors\ndef read_buffer(\n    self, f, attrs: Optional[Dict[str, Any]] = None\n) -&gt; Union[np.ndarray, \"core.Dotprops\", \"core.VoxelNeuron\"]:\n    \"\"\"Read buffer into (image, header) or a neuron.\n\n    Parameters\n    ----------\n    f :         IO\n                Readable buffer (must be bytes).\n    attrs :     dict | None\n                Arbitrary attributes to include in the neuron.\n\n    Returns\n    -------\n    core.Dotprops | core.VoxelNeuron | np.ndarray\n\n    \"\"\"\n    import tifffile\n\n    if isinstance(f, HTTPResponse):\n        f = io.StringIO(f.content)\n\n    if isinstance(f, bytes):\n        f = io.BytesIO(f)\n\n    with tifffile.TiffFile(f) as tif:\n        # The header contains some but not all the info\n        if hasattr(tif, \"imagej_metadata\") and tif.imagej_metadata is not None:\n            header = tif.imagej_metadata\n        else:\n            header = {}\n\n        # Read the x/y resolution from the first \"page\" (i.e. the first slice)\n        res = tif.pages[0].resolution\n        # Resolution to spacing\n        header[\"xy_spacing\"] = (1 / res[0], 1 / res[1])\n\n        # Get the axes; this will be something like \"ZCYX\" where:\n        # Z = slices, C = channels, Y = rows, X = columns, S = color(?), Q = empty(?)\n        axes = tif.series[0].axes\n\n        # Generate volume\n        data = tif.asarray()\n\n    if self.output == \"raw\":\n        return data, header\n\n    # Drop \"Q\" axes if they have dimenions of 1 (we're assuming these are empty)\n    while \"Q\" in axes and data.shape[axes.index(\"Q\")] == 1:\n        data = np.squeeze(data, axis=axes.index(\"Q\"))\n        axes = axes.replace(\"Q\", \"\", 1)  # Only remove the first occurrence\n    if \"C\" in axes:\n        # Extract the requested channel from the volume\n        data = data.take(self.channel, axis=axes.index(\"C\"))\n        axes = axes.replace(\"C\", \"\")\n\n    # At this point we expect 3D data\n    if data.ndim != 3:\n        raise ValueError(f'Expected 3D greyscale data, got {data.ndim} (\"{axes}\").')\n\n    # Swap axes to XYZ order\n    order = []\n    for a in (\"X\", \"Y\", \"Z\"):\n        if a not in axes:\n            logger.warning(\n                f'Expected axes to contain \"Z\", \"Y\", and \"X\", got \"{axes}\". '\n                \"Axes will not be automatically reordered.\"\n            )\n            order = None\n            break\n        order.append(axes.index(a))\n    if order:\n        data = np.transpose(data, order)\n\n    # Try parsing units - this is modelled after the tif files you get from ImageJ\n    units = None\n    space_units = None\n    voxdim = np.array([1, 1, 1], dtype=np.float64)\n    if \"spacing\" in header:\n        voxdim[2] = header[\"spacing\"]\n    if \"xy_spacing\" in header:\n        voxdim[:2] = header[\"xy_spacing\"]\n    if \"unit\" in header:\n        space_units = header[\"unit\"]\n        units = [f\"{m} {space_units}\" for m in voxdim]\n    else:\n        units = voxdim\n\n    return self.convert_image(data, attrs, header, voxdim, units, space_units)\n</code></pre>"},{"location":"reference/navis/meshes/b3d/","title":"b3d","text":""},{"location":"reference/navis/meshes/b3d/#navis.meshes.b3d.smooth_mesh_blender","title":"<code>navis.meshes.b3d.smooth_mesh_blender</code>","text":"<p>Smooth mesh using Blender's Laplacian smoothing.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Mesh object to simplify.\n</code></pre> <p> TYPE: <code>            MeshNeuron | Volume | Trimesh</code> </p> <code>iterations</code> <pre><code>        Round of smoothing to apply.\n</code></pre> <p> TYPE: <code>   int</code> DEFAULT: <code>5</code> </p> <code>L</code> <pre><code>        Diffusion speed constant lambda. Larger = more aggressive\n        smoothing.\n</code></pre> <p> TYPE: <code>            float [0-1]</code> DEFAULT: <code>0.5</code> </p> <code>inplace</code> <pre><code>        If True, will perform simplication on `x`. If False, will\n        simplify and return a copy.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>simp</code> <p>Simplified mesh object.</p> Source code in <code>navis/meshes/b3d.py</code> <pre><code>def smooth_mesh_blender(x, iterations=5, L=0.5, inplace=False):\n    \"\"\"Smooth mesh using Blender's Laplacian smoothing.\n\n    Parameters\n    ----------\n    x :             MeshNeuron | Volume | Trimesh\n                    Mesh object to simplify.\n    iterations :    int\n                    Round of smoothing to apply.\n    L :             float [0-1]\n                    Diffusion speed constant lambda. Larger = more aggressive\n                    smoothing.\n    inplace :       bool\n                    If True, will perform simplication on `x`. If False, will\n                    simplify and return a copy.\n\n    Returns\n    -------\n    simp\n                Simplified mesh object.\n\n    \"\"\"\n    if not tm.interfaces.blender.exists:\n        raise ModuleNotFoundError('No Blender 3D unavailable (executable not found).')\n    _blender_executable = tm.interfaces.blender._blender_executable\n\n    if L &gt; 1 or L &lt; 0:\n        raise ValueError(f'`L` (lambda) must be between 0 and 1, got \"{L}\"')\n\n    if isinstance(x, core.MeshNeuron):\n        mesh = x.trimesh\n    elif isinstance(x, core.Volume):\n        mesh = tm.Trimesh(x.vertices, x.faces)\n    elif isinstance(x, tm.Trimesh):\n        mesh = x\n    else:\n        raise TypeError('Expected MeshNeuron, Volume or trimesh.Trimesh, '\n                        f'got \"{type(x)}\"')\n\n    assert isinstance(mesh, tm.Trimesh)\n\n    # Load the template\n    temp_name = 'blender_smooth.py.template'\n    if temp_name in _cache:\n        template = _cache[temp_name]\n    else:\n        with open(os.path.join(_pwd, 'templates', temp_name), 'r') as f:\n            template = f.read()\n        _cache[temp_name] = template\n\n    # Replace placeholder with actual ratio\n    script = template.replace('$ITERATIONS', str(iterations))\n    script = script.replace('$LAMBDA', str(L))\n\n    # Let trimesh's MeshScript take care of exectution and clean-up\n    with tm.interfaces.generic.MeshScript(meshes=[mesh],\n                                          script=script,\n                                          debug=False) as blend:\n        result = blend.run(_blender_executable\n                           + ' --background --python $SCRIPT')\n\n    # Blender apparently returns actively incorrect face normals\n    result.face_normals = None\n\n    if not inplace:\n        x = x.copy()\n\n    x.vertices = result.vertices\n    x.faces = result.faces\n\n    return x\n</code></pre>"},{"location":"reference/navis/meshes/mesh_utils/","title":"mesh_utils","text":""},{"location":"reference/navis/meshes/mesh_utils/#navis.meshes.mesh_utils.face_dist_sorting","title":"<code>navis.meshes.mesh_utils.face_dist_sorting</code>","text":"<p>Sort faces by distance from given point.</p> <p>This allows you to e.g. use Blender's \"build\" modifier to grow neurons from a point of origin.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Mesh to sort faces for.\n</code></pre> <p> TYPE: <code>        navis.MeshNeuron</code> </p> <code>from_</code> <pre><code>    Must be either a vertex index (single int) or an x/y/z coordinate.\n</code></pre> <p> TYPE: <code>    int | list of int</code> </p> <code>strahler_weight</code> <pre><code>    If True, will use Strahler index to grow twigs slower than\n    backbone.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> <code>inplace</code> <pre><code>    Whether to modify the input mesh or a copy thereof.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>navis.MeshNeuron</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; x = navis.example_neurons(1, kind='mesh')\n&gt;&gt;&gt; x = navis.meshes.mesh_utils.face_dist_sorting(x, from_=x.soma_pos)\n</code></pre> Source code in <code>navis/meshes/mesh_utils.py</code> <pre><code>def face_dist_sorting(x, from_, strahler_weight=False, inplace=False):\n    \"\"\"Sort faces by distance from given point.\n\n    This allows you to e.g. use Blender's \"build\" modifier to grow neurons\n    from a point of origin.\n\n    Parameters\n    ----------\n    x :         navis.MeshNeuron\n                Mesh to sort faces for.\n    from_ :     int | list of int\n                Must be either a vertex index (single int) or an x/y/z coordinate.\n    strahler_weight :   bool\n                If True, will use Strahler index to grow twigs slower than\n                backbone.\n    inplace :   bool\n                Whether to modify the input mesh or a copy thereof.\n\n    Returns\n    -------\n    navis.MeshNeuron\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; x = navis.example_neurons(1, kind='mesh')\n    &gt;&gt;&gt; x = navis.meshes.mesh_utils.face_dist_sorting(x, from_=x.soma_pos)\n\n    \"\"\"\n    # Turn vertex indices to coordinates\n    if isinstance(from_, (int, np.integer)):\n        from_ = x.vertices[from_]\n\n    # Generate the skeleton\n    # (note we don't shave to avoid issues with vertex map)\n    sk = x.skeletonize(heal=True, shave=False)\n\n    # Get the node index for our from_\n    seed = sk.snap(from_)[0]\n\n    # Get distances from\n    dists = graph.geodesic_matrix(sk, from_=seed).iloc[0]\n\n    if strahler_weight:\n        sk.reroot(seed, inplace=True)\n        _ = morpho.strahler_index(sk)\n        dists = dists / sk.nodes.strahler_index\n\n    # Get sorting by distance\n    srt = np.argsort(dists.values)\n\n    # Map sorting back onto vertices\n    verts_srt = srt[sk.vertex_map]\n\n    # For each face get the skeleton nodes it maps to\n    face_nodes = sk.vertex_map[x.faces]\n\n    # For each face get the mean distance\n    face_dist = dists.values[face_nodes].mean(axis=1)\n\n    # Sort the faces\n    faces_srt = np.array(x.faces)[np.argsort(face_dist)]\n\n    if not inplace:\n        x = x.copy()\n\n    x.faces = faces_srt\n\n    return x\n</code></pre>"},{"location":"reference/navis/meshes/mesh_utils/#navis.meshes.mesh_utils.pointlabels_to_meshes","title":"<code>navis.meshes.mesh_utils.pointlabels_to_meshes</code>","text":"<p>Generate non-overlapping meshes from a labelled point cloud.</p> <p>Briefly, the default workflow is this:</p> <ol> <li>Create a Gaussian KDE for each unique label.</li> <li>Tile the point's bounding box into voxels of size <code>res</code>.</li> <li>Calculate the KDE's point density function (PDF) to assign a label to      each voxel.</li> <li>(Optional) Denoise the matrix by a round of binary erosion + dilation      and fill holes.</li> <li>Use marching cubes to produce a mesh.</li> </ol> <p>See <code>method</code> parameter for an alternative method.</p> PARAMETER DESCRIPTION <code>points</code> <pre><code>    Point cloud.\n</code></pre> <p> TYPE: <code>   (N, 3) array</code> </p> <code>labels</code> <pre><code>    A label for each point.\n</code></pre> <p> TYPE: <code>   (N, ) array</code> </p> <code>res</code> <pre><code>    Size of the voxels. Note that this also determines the\n    gap between meshes: higher resolution = smaller, more precise gaps.\n</code></pre> <p> TYPE: <code>      int</code> </p> <code>method</code> <pre><code>    Which method to use. \"kde\" (default) uses above described\n    workflow. \"majority\" vote will simply ask, for each voxel, which\n    labels are contained within it and which is the most numerous.\n    The latter is much faster but may produce coarser meshes - in\n    particular if the number of points is low.\n</code></pre> <p> TYPE: <code>   \"kde\" | \"majority\"</code> DEFAULT: <code>'kde'</code> </p> <code>threshold</code> <pre><code>    Threshold for dropping voxels that don't appear to belong to\n    any of the original labels. This is the quantile! I.e. the\n    default value of 0.05 means that we'll be dropping the bottom 5%.\n</code></pre> <p> TYPE: <code>float[0 - 1]</code> DEFAULT: <code>0.05</code> </p> <code>drop_fluff</code> <pre><code>    Whether to drop small bits and pieces from meshes and only keep\n    the largest contiguous pieces.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>volume</code> <pre><code>    Provide a mesh to contrain the sampled voxels to inside this\n    volume.\n</code></pre> <p> TYPE: <code>   Volume | Trimesh</code> DEFAULT: <code>None</code> </p> <code>n_cores</code> <pre><code>    Number of cores to use for parallel processing. Each unique\n    label will be processed on a separate core.\n</code></pre> <p> TYPE: <code>  int</code> DEFAULT: <code>os.cpu_count() // 2</code> </p> RETURNS DESCRIPTION <code>meshes</code> <p>List of <code>navis.Volume</code>. Their names correspond to unique <code>labels</code>.</p> <p> TYPE: <code>list</code> </p> Source code in <code>navis/meshes/mesh_utils.py</code> <pre><code>def pointlabels_to_meshes(\n    points,\n    labels,\n    res,\n    method=\"kde\",\n    threshold=0.05,\n    drop_fluff=True,\n    volume=None,\n    output=\"meshes\",\n    n_cores=os.cpu_count() // 2,\n    progress=True,\n):\n    \"\"\"Generate non-overlapping meshes from a labelled point cloud.\n\n    Briefly, the default workflow is this:\n\n      1. Create a Gaussian KDE for each unique label.\n      2. Tile the point's bounding box into voxels of size `res`.\n      3. Calculate the KDE's point density function (PDF) to assign a label to\n         each voxel.\n      4. (Optional) Denoise the matrix by a round of binary erosion + dilation\n         and fill holes.\n      5. Use marching cubes to produce a mesh.\n\n    See `method` parameter for an alternative method.\n\n    Parameters\n    ----------\n    points :    (N, 3) array\n                Point cloud.\n    labels :    (N, ) array\n                A label for each point.\n    res :       int\n                Size of the voxels. Note that this also determines the\n                gap between meshes: higher resolution = smaller, more precise gaps.\n    method :    \"kde\" | \"majority\"\n                Which method to use. \"kde\" (default) uses above described\n                workflow. \"majority\" vote will simply ask, for each voxel, which\n                labels are contained within it and which is the most numerous.\n                The latter is much faster but may produce coarser meshes - in\n                particular if the number of points is low.\n    threshold : float [0-1], optional\n                Threshold for dropping voxels that don't appear to belong to\n                any of the original labels. This is the quantile! I.e. the\n                default value of 0.05 means that we'll be dropping the bottom 5%.\n    drop_fluff : bool\n                Whether to drop small bits and pieces from meshes and only keep\n                the largest contiguous pieces.\n    volume :    Volume | Trimesh, optional\n                Provide a mesh to contrain the sampled voxels to inside this\n                volume.\n    n_cores :   int\n                Number of cores to use for parallel processing. Each unique\n                label will be processed on a separate core.\n\n\n    Returns\n    -------\n    meshes  :   list\n                List of `navis.Volume`. Their names correspond to unique\n                `labels`.\n\n    \"\"\"\n    if not skimage:\n        raise ModuleNotFoundError(\n            \"Meshing requires `skimage`:\\n   pip3 install scikit-image\"\n        )\n\n    if len(points) != len(labels):\n        raise ValueError(\n            f\"Number of labels ({len(labels)}) must match number \"\n            f\"of points ({len(points)})\"\n        )\n\n    assert method in (\"kde\", \"majority\")\n    assert output in (\"meshes\", \"voxels\")\n\n    points = np.asarray(points)\n    labels = np.asarray(labels)\n    labels_unique = np.unique(labels)\n\n    if method == \"kde\":\n        # Now create voxel coordinates for the volume we want to fill:\n        # First the bounding box\n        bbox = np.vstack((points.min(axis=0), points.max(axis=0)))\n\n        # We'll pad the volume by 2x the resolution\n        padding = res * 2\n\n        xco = np.arange(bbox[0][0] - padding, bbox[1][0] + padding, res)\n        yco = np.arange(bbox[0][1] - padding, bbox[1][1] + padding, res)\n        zco = np.arange(bbox[0][2] - padding, bbox[1][2] + padding, res)\n\n        i = np.arange(0, xco.shape[0], 1)\n        j = np.arange(0, yco.shape[0], 1)\n        k = np.arange(0, zco.shape[0], 1)\n\n        ii, jj, kk = np.meshgrid(i, j, k)\n        xx, yy, zz = np.meshgrid(xco, yco, zco)\n\n        voxels = np.vstack((xx.flatten(), yy.flatten(), zz.flatten())).T\n        voxels = pd.DataFrame(voxels, columns=[\"x\", \"y\", \"z\"])\n\n        voxels[\"i\"] = ii.flatten()\n        voxels[\"j\"] = jj.flatten()\n        voxels[\"k\"] = kk.flatten()\n\n        if not isinstance(volume, type(None)):\n            in_vol = intersection.in_volume(voxels[[\"x\", \"y\", \"z\"]].values, volume)\n            print(\n                f\"Dropping {(~in_vol).sum()}/{len(voxels)} voxels outside of provided volume.\"\n            )\n            voxels = voxels.loc[in_vol].copy()\n\n        # For each label create a KDE\n        kde = {}\n        for l in tqdm(\n            labels_unique, desc=\"Generating KDEs\", disable=not progress, leave=False\n        ):\n            this_p = points[labels == l]\n\n            kde[l] = stats.gaussian_kde(this_p.T)\n\n        # For each point get the point density function for each KDE\n        combinations = [(kde[l], [voxels[[\"x\", \"y\", \"z\"]].values.T], {}) for l in kde]\n        with mp.Pool(n_cores) as pool:\n            results = list(\n                tqdm(\n                    pool.imap(_worker_wrapper, combinations, chunksize=1),\n                    leave=False,\n                    disable=not progress,\n                    total=len(combinations),\n                    desc=f\"Assigning {len(voxels):,} voxels\",\n                )\n            )\n\n        # Fill results\n        for l, r in zip(kde, results):\n            voxels[l] = r\n\n        # Drop voxels that have a PDF of less than the given threshold\n        if threshold:\n            pdf = voxels[labels_unique].max(axis=1)\n            keep = pdf &gt;= np.quantile(pdf, threshold)\n            print(\n                f\"Dropping {(~keep).sum()}/{len(voxels)} voxels with too low density.\"\n            )\n            voxels = voxels.loc[keep].copy()\n\n        # Assign label to each voxel based on the max probability\n        voxels[\"label\"] = labels_unique[np.argmax(voxels[labels_unique].values, axis=1)]\n    else:\n        # Turn points into voxels of given size\n        points_vxl = points // res\n\n        # Turn labels into an array of integers\n        labels_dict = dict(zip(labels_unique, np.arange(len(labels_unique))))\n        labels_int = np.array([labels_dict[la] for la in labels])\n\n        # Combine (N, 3) voxels and (N, ) labels into (N, 4) array\n        point_labels = np.hstack((points_vxl, labels_int.reshape(-1, 1)))\n\n        # Count unique voxel + label combinations\n        point_labels_unique, cnt = np.unique(point_labels, axis=0, return_counts=True)\n\n        # For each x/y/z coordinate (but not the label column) get a unique index\n        unique_voxel, voxel_ix = np.unique(\n            point_labels_unique[:, :3], axis=0, return_inverse=True\n        )\n\n        # Turn into DataFrame for tallying up\n        point_labels_df = pd.DataFrame()\n        point_labels_df[\"voxel_ix\"] = voxel_ix\n        point_labels_df[\"label_ix\"] = point_labels_unique[:, -1]\n        point_labels_df[\"cnt\"] = cnt\n\n        # Turn into a N_point x N_labels matrix\n        adj = point_labels_df.groupby([\"voxel_ix\", \"label_ix\"]).cnt.sum().unstack()\n\n        # Now generate the DataFrame we will use to create meshes\n        voxels = pd.DataFrame()\n        voxels[\"i\"] = unique_voxel[:, 0]\n        voxels[\"j\"] = unique_voxel[:, 1]\n        voxels[\"k\"] = unique_voxel[:, 2]\n\n        # Also re-generate x/y/z coordinates\n        voxels[\"x\"] = voxels[\"i\"] * res\n        voxels[\"y\"] = voxels[\"j\"] * res\n        voxels[\"z\"] = voxels[\"k\"] * res\n\n        # Make sure i/j/k start at zero (otherwise matrix further down might\n        # end up really huge)\n        voxels[\"i\"] -= unique_voxel[:, 0].min()\n        voxels[\"j\"] -= unique_voxel[:, 1].min()\n        voxels[\"k\"] -= unique_voxel[:, 2].min()\n\n        # For each voxel get the top label...\n        voxels[\"label\"] = labels_unique[np.nanargmax(adj, axis=1)]\n\n        # Drop voxels that have less than given points inside\n        if threshold:\n            top_count = np.nanmax(adj, axis=1)\n            keep = top_count &gt;= np.quantile(top_count, threshold)\n            print(\n                f\"Dropping {(~keep).sum()}/{len(voxels)} voxels with too low density.\"\n            )\n            voxels = voxels.loc[keep].copy()\n\n        # Some settings for the meshing\n        padding = 0\n\n    if output == 'voxels':\n        return voxels\n\n    meshes = []\n    for l in tqdm(\n        labels_unique, desc=\"Creating meshes\", disable=not progress, leave=False\n    ):\n        # Generate empty matrix\n        mat = np.zeros((voxels.i.max() + 2, voxels.j.max() + 2, voxels.k.max() + 2))\n\n        # Get voxels belonging to this label\n        this = voxels[voxels.label == l]\n\n        if this.empty:\n            logger.warning(f\"Label {l} did not produce a mesh.\")\n            continue\n\n        # Fill matrix\n        mat[this.i, this.j, this.k] = 1\n\n        # Remove binary holes\n        mat = ndimage.binary_fill_holes(mat)\n\n        # We need one round of erodes to make meshes non-overlapping\n        mat = ndimage.binary_erosion(mat)\n\n        if not np.any(mat):\n            logger.warning(f'Label {l} did not produce a mesh.')\n            continue\n\n        # Use marching cubes to create surface model\n        # (newer versions of skimage have a \"marching cubes\" function and\n        # the marching_cubes_lewiner is deprecreated)\n        marching_cubes = getattr(measure, 'marching_cubes',\n                                 getattr(measure, 'marching_cubes_lewiner', None))\n        verts, faces, normals, values = marching_cubes(mat.astype(float),\n                                                       level=0,\n                                                       allow_degenerate=False,\n                                                       step_size=1)\n\n        # Scale back to original units\n        verts *= res\n\n        # Add offset\n        offset = voxels[['x', 'y', 'z']].min(axis=0).values  # keep .values !\n        verts += offset\n\n        # Somehow we seem to have introduced an offset\n        verts -= padding\n        verts += 0.5 * res\n\n        if method == 'kde':\n            verts[:, 1] -= 0.5 * res\n\n        # Make a trimesh\n        new_mesh = tm.Trimesh(vertices=verts, faces=faces, normals=normals)\n\n        if drop_fluff:\n            # Drop small stuff (anything that makes up less than 10% of the faces)\n            cc = tm.graph.connected_components(edges=new_mesh.face_adjacency,\n                                               nodes=np.arange(len(new_mesh.faces)),\n                                               min_len=1,\n                                               engine=None)\n            if len(cc) &gt; 1:\n                min_faces = new_mesh.faces.shape[0] * 0.1\n                to_keep = [c for c in cc if (len(c) &gt;= min_faces)]\n                if to_keep:\n                    new_mesh = new_mesh.submesh([np.concatenate(to_keep)])[0]\n\n        # Need to fix normals\n        new_mesh.fix_normals()\n\n        meshes.append(core.Volume(new_mesh, name=l))\n\n    return meshes\n</code></pre>"},{"location":"reference/navis/meshes/mesh_utils/#navis.meshes.mesh_utils.points_to_mesh","title":"<code>navis.meshes.mesh_utils.points_to_mesh</code>","text":"<p>Generate mesh from point cloud.</p> <p>Briefly, the workflow is this:   1. Partition the point cloud into voxels of size <code>res</code>.   2. (Optional) Discard voxels with less than <code>threshold</code> points inside.   3. Turn voxels into a (M, N, K) matrix.   4. (Optional) Denoise the matrix by a round of binary erosion + dilation      and fill holes.   5. Use marching cubes to produce a mesh.</p> PARAMETER DESCRIPTION <code>points</code> <pre><code>    Point cloud.\n</code></pre> <p> TYPE: <code>   (N, 3) array</code> </p> <code>res</code> <pre><code>    Resolution of the voxels.\n</code></pre> <p> TYPE: <code>      int</code> </p> <code>threshold</code> <pre><code>    Use this to ignore voxels with very few points inside.\n</code></pre> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>denoise</code> <pre><code>    Whether to use binary filters to reduce noise and smoothen\n    the mesh.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>trimesh.Trimesh</code> Source code in <code>navis/meshes/mesh_utils.py</code> <pre><code>def points_to_mesh(points, res, threshold=None, denoise=True):\n    \"\"\"Generate mesh from point cloud.\n\n    Briefly, the workflow is this:\n      1. Partition the point cloud into voxels of size `res`.\n      2. (Optional) Discard voxels with less than `threshold` points inside.\n      3. Turn voxels into a (M, N, K) matrix.\n      4. (Optional) Denoise the matrix by a round of binary erosion + dilation\n         and fill holes.\n      5. Use marching cubes to produce a mesh.\n\n    Parameters\n    ----------\n    points :    (N, 3) array\n                Point cloud.\n    res :       int\n                Resolution of the voxels.\n    threshold : int, optional\n                Use this to ignore voxels with very few points inside.\n    denoise :   bool\n                Whether to use binary filters to reduce noise and smoothen\n                the mesh.\n\n\n    Returns\n    -------\n    trimesh.Trimesh\n\n    \"\"\"\n    if not skimage:\n        raise ModuleNotFoundError(\n            'Meshing requires `skimage`:\\n'\n            '  pip3 install scikit-image'\n            )\n\n    points = np.asarray(points)\n\n    if points.ndim != 2 or points.shape[1] != 3:\n        raise ValueError(f'Points must be of shape (N, 3), got {points.shape}')\n\n    # Generate counts per voxel\n    vxl, cnt = np.unique((points / res).round().astype(int),\n                         return_counts=True, axis=0)\n\n    # Turn into a DataFrame\n    voxels = pd.DataFrame(np.vstack(vxl), columns=['x', 'y', 'z'])\n    voxels['count'] = cnt\n\n    if threshold:\n        voxels = voxels[voxels['count'] &gt; 1]\n\n    # Generate empty matrix\n    mat = np.zeros((voxels.x.max() + 1,\n                    voxels.y.max() + 1,\n                    voxels.z.max() + 1))\n\n    # Fill matrix\n    mat[voxels.x, voxels.y, voxels.z] = 1\n\n    if denoise:\n        # Denoise by a round of erosion...\n        mat = ndimage.binary_erosion(mat)\n\n        # ... followed by two rounds of dilation to smoothen things out...\n        mat = ndimage.binary_dilation(mat, iterations=2)\n\n        # ... followed by a round of fill holes\n        mat = ndimage.binary_fill_holes(mat)\n\n        # And a final round of erosion to get back to the correct scale\n        mat = ndimage.binary_erosion(mat, iterations=1)\n\n    # Run the marching cube algorithm\n    # (newer versions of skimage have a \"marching cubes\" function and\n    # the marching_cubes_lewiner is deprecreated)\n    marching_cubes = getattr(measure, 'marching_cubes',\n                             getattr(measure, 'marching_cubes_lewiner', None))\n    verts, faces, normals, values = marching_cubes(mat.astype(float),\n                                                   level=0,\n                                                   gradient_direction='ascent',\n                                                   allow_degenerate=False,\n                                                   step_size=1)\n    # Turn coordinates back into original units\n    verts *= res\n\n    # Somehow we seem to have introduced an offset equal to our resolution\n    verts -= res\n\n    mesh = tm.Trimesh(vertices=verts, faces=faces, normals=normals)\n\n    # Need to fix normals\n    mesh.fix_normals()\n\n    return mesh\n</code></pre>"},{"location":"reference/navis/meshes/mesh_utils/#navis.meshes.mesh_utils.smooth_mesh_trimesh","title":"<code>navis.meshes.mesh_utils.smooth_mesh_trimesh</code>","text":"<p>Smooth mesh using Trimesh's Laplacian smoothing.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Mesh object to simplify.\n</code></pre> <p> TYPE: <code>            MeshNeuron | Volume | Trimesh</code> </p> <code>iterations</code> <pre><code>        Round of smoothing to apply.\n</code></pre> <p> TYPE: <code>   int</code> DEFAULT: <code>5</code> </p> <code>L</code> <pre><code>        Diffusion speed constant lambda. Larger = more aggressive\n        smoothing.\n</code></pre> <p> TYPE: <code>            float [0-1]</code> DEFAULT: <code>0.5</code> </p> <code>inplace</code> <pre><code>        If True, will perform simplication on `x`. If False, will\n        simplify and return a copy.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>simp</code> <p>Simplified mesh object.</p> Source code in <code>navis/meshes/mesh_utils.py</code> <pre><code>def smooth_mesh_trimesh(x, iterations=5, L=0.5, inplace=False):\n    \"\"\"Smooth mesh using Trimesh's Laplacian smoothing.\n\n    Parameters\n    ----------\n    x :             MeshNeuron | Volume | Trimesh\n                    Mesh object to simplify.\n    iterations :    int\n                    Round of smoothing to apply.\n    L :             float [0-1]\n                    Diffusion speed constant lambda. Larger = more aggressive\n                    smoothing.\n    inplace :       bool\n                    If True, will perform simplication on `x`. If False, will\n                    simplify and return a copy.\n\n    Returns\n    -------\n    simp\n                Simplified mesh object.\n\n    \"\"\"\n    if L &gt; 1 or L &lt; 0:\n        raise ValueError(f'`L` (lambda) must be between 0 and 1, got \"{L}\"')\n\n    if isinstance(x, core.MeshNeuron):\n        mesh = x.trimesh.copy()\n    elif isinstance(x, core.Volume):\n        mesh = tm.Trimesh(x.vertices, x.faces)\n    elif isinstance(x, tm.Trimesh):\n        mesh = x.copy()\n    else:\n        raise TypeError('Expected MeshNeuron, Volume or trimesh.Trimesh, '\n                        f'got \"{type(x)}\"')\n\n    assert isinstance(mesh, tm.Trimesh)\n\n    # Smooth mesh\n    # This always happens in place, hence we made a copy earlier\n    tm.smoothing.filter_laplacian(mesh, lamb=L, iterations=iterations)\n\n    if not inplace:\n        x = x.copy()\n\n    x.vertices = mesh.vertices\n    x.faces = mesh.faces\n\n    return x\n</code></pre>"},{"location":"reference/navis/meshes/o3d/","title":"o3d","text":""},{"location":"reference/navis/meshes/o3d/#navis.meshes.o3d.make_o3d_mesh","title":"<code>navis.meshes.o3d.make_o3d_mesh</code>","text":"<p>Turn mesh-like object into an open3d mesh.</p> Source code in <code>navis/meshes/o3d.py</code> <pre><code>def make_o3d_mesh(x):\n    \"\"\"Turn mesh-like object into an open3d mesh.\"\"\"\n    try:\n        import open3d\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError('Please install open3d: pip3 install open3d')\n    except BaseException:\n        raise\n\n    return open3d.geometry.TriangleMesh(\n                vertices=open3d.utility.Vector3dVector(x.vertices),\n                triangles=open3d.utility.Vector3iVector(x.faces))\n</code></pre>"},{"location":"reference/navis/meshes/o3d/#navis.meshes.o3d.smooth_mesh_open3d","title":"<code>navis.meshes.o3d.smooth_mesh_open3d</code>","text":"<p>Smooth mesh using open3d's Laplacian smoothing.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Mesh object to simplify.\n</code></pre> <p> TYPE: <code>            MeshNeuron | Volume | Trimesh</code> </p> <code>iterations</code> <pre><code>        Round of smoothing to apply.\n</code></pre> <p> TYPE: <code>   int</code> DEFAULT: <code>5</code> </p> <code>L</code> <pre><code>        Diffusion speed constant lambda. Larger = more aggressive\n        smoothing.\n</code></pre> <p> TYPE: <code>            float [0-1]</code> DEFAULT: <code>0.5</code> </p> <code>inplace</code> <pre><code>        If True, will perform simplication on `x`. If False, will\n        simplify and return a copy.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>smoothed</code> <p>Smoothed mesh object.</p> Source code in <code>navis/meshes/o3d.py</code> <pre><code>def smooth_mesh_open3d(x, iterations=5, L=0.5, inplace=False):\n    \"\"\"Smooth mesh using open3d's Laplacian smoothing.\n\n    Parameters\n    ----------\n    x :             MeshNeuron | Volume | Trimesh\n                    Mesh object to simplify.\n    iterations :    int\n                    Round of smoothing to apply.\n    L :             float [0-1]\n                    Diffusion speed constant lambda. Larger = more aggressive\n                    smoothing.\n    inplace :       bool\n                    If True, will perform simplication on `x`. If False, will\n                    simplify and return a copy.\n\n    Returns\n    -------\n    smoothed\n                    Smoothed mesh object.\n\n    \"\"\"\n    if not isinstance(x, (core.MeshNeuron, core.Volume, tm.Trimesh)):\n        raise TypeError('Expected MeshNeuron, Volume or trimesh.Trimesh, '\n                        f'got \"{type(x)}\"')\n\n    mesh_o3d = make_o3d_mesh(x)\n\n    if L &gt; 1 or L &lt; 0:\n        raise ValueError(f'`L` (lambda) must be between 0 and 1, got \"{L}\"')\n\n    result = mesh_o3d.filter_smooth_laplacian(iterations, L)\n\n    if not inplace:\n        x = x.copy()\n\n    x.vertices = np.asarray(result.vertices)\n    x.faces = np.asarray(result.triangles)\n\n    return x\n</code></pre>"},{"location":"reference/navis/meshes/operations/","title":"operations","text":""},{"location":"reference/navis/meshes/operations/#navis.meshes.operations.combine_meshes","title":"<code>navis.meshes.operations.combine_meshes</code>","text":"<p>Try combining (partially overlapping) meshes.</p> <p>This function effectively works on the vertex graph and will not produce meaningful faces.</p> Source code in <code>navis/meshes/operations.py</code> <pre><code>def combine_meshes(meshes, max_dist='auto', progress=True):\n    \"\"\"Try combining (partially overlapping) meshes.\n\n    This function effectively works on the vertex graph and will not produce\n    meaningful faces.\n    \"\"\"\n    # Sort meshes by size\n    meshes = sorted(meshes, key=lambda x: len(x.vertices), reverse=True)\n\n    comb = tm.Trimesh(meshes[0].vertices.copy(), meshes[0].faces.copy())\n    comb.remove_unreferenced_vertices()\n\n    if max_dist == 'auto':\n        max_dist = comb.edges_unique_length.mean()\n\n    for m in config.tqdm(meshes[1:], desc='Combining',\n                         disable=config.pbar_hide or not progress,\n                         leave=config.pbar_leave):\n        # Generate a new up-to-date tree\n        tree = KDTree(comb.vertices)\n\n        # Offset faces\n        vertex_offset = comb.vertices.shape[0]\n        new_faces = m.faces + vertex_offset\n\n        # Find vertices that can be merged - note that we are effectively\n        # zippig the two meshes by making sure that each vertex can only be\n        # merged once\n        dist, ix = tree.query(m.vertices, distance_upper_bound=max_dist)\n\n        merged = set()\n        # Merge closest vertices first\n        for i in np.argsort(dist):\n            # Skip if no more within-distance\n            if dist[i] &gt;= np.inf:\n                break\n            # Skip if target vertex has already been merged\n            if ix[i] in merged:\n                continue\n\n            # Remap this vertex\n            new_faces[new_faces == (i + vertex_offset)] = ix[i]\n\n            # Track that target vertex has already been seen\n            merged.add(ix[i])\n\n        # Merge vertices and faces\n        comb.vertices = np.append(comb.vertices, m.vertices, axis=0)\n        comb.faces = np.append(comb.faces, new_faces, axis=0)\n\n        # Drop unreferenced vertices (i.e. those that were remapped)\n        comb.remove_unreferenced_vertices()\n\n    return comb\n</code></pre>"},{"location":"reference/navis/models/network_models/","title":"network_models","text":""},{"location":"reference/navis/models/network_models/#navis.models.network_models.BaseNetworkModel","title":"<code>navis.models.network_models.BaseNetworkModel</code>","text":"<p>Base model for network simulations.</p> Source code in <code>navis/models/network_models.py</code> <pre><code>class BaseNetworkModel:\n    \"\"\"Base model for network simulations.\"\"\"\n\n    def __init__(self, edges: pd.DataFrame, source: str, target: str):\n        \"\"\"Initialize model.\"\"\"\n        assert isinstance(edges, pd.DataFrame), f'edges must be pandas DataFrame, got \"{type(edges)}\"'\n        assert source in edges.columns, f'edges DataFrame must contain \"{source}\" column'\n        assert target in edges.columns, f'edges DataFrame must contain \"{target}\" column'\n        self.edges = edges\n        self.source = source\n        self.target = target\n\n        if (self.edges.dtypes[source] == object) or (self.edges.dtypes[target] == object):\n            logger.warning('Looks like sources and/or targets in your edge list '\n                           'might be strings? This can massively slow down '\n                           'computations. If at all possible try to use numeric IDs.')\n\n    @property\n    def n_nodes(self) -&gt; int:\n        \"\"\"Return unique nodes in network.\"\"\"\n        return np.unique(self.edges[[self.source, self.target]].values.flatten()).shape[0]\n\n    @property\n    def has_results(self) -&gt; bool:\n        \"\"\"Check if model has results.\"\"\"\n        if isinstance(getattr(self, 'results', None), pd.DataFrame):\n            return True\n        return False\n\n    def run_parallel(self,\n                     n_cores: int = 5,\n                     iterations: int = 100,\n                     **kwargs: dict) -&gt; None:\n        \"\"\"Run model using parallel processes.\"\"\"\n        # Note that we initialize each process by making \"edges\" a global argument\n        with mp.Pool(processes=n_cores,\n                     initializer=self.initializer) as pool:\n\n            # Each process will take about the same amount of time\n            # So we want each process to take a single batch of iterations/n_cores runs\n            kwargs['iterations'] = int(iterations/n_cores)\n            calls = [{**kwargs, **{'position': i}} for i in range(int(n_cores))]\n\n            # Generate processes - note the use of chunksize 1 because we have\n            # already chunked the iterations such that each worker process\n            # runs exactly once\n            p = pool.imap_unordered(self._worker_wrapper, calls, chunksize=1)\n\n            # Wait for processes to complete\n            res = list(p)\n\n        # Combine results\n        self.results = pd.concat(res, axis=0)\n        self.iterations = iterations\n\n    def _worker_wrapper(self, kwargs: dict):\n        return self.run(**kwargs)\n\n    def initializer(self):\n        pass\n\n    def run(self):\n        pass\n</code></pre>"},{"location":"reference/navis/models/network_models/#navis.models.network_models.BaseNetworkModel.has_results","title":"<code>has_results: bool</code>  <code>property</code>","text":"<p>Check if model has results.</p>"},{"location":"reference/navis/models/network_models/#navis.models.network_models.BaseNetworkModel.n_nodes","title":"<code>n_nodes: int</code>  <code>property</code>","text":"<p>Return unique nodes in network.</p>"},{"location":"reference/navis/models/network_models/#navis.models.network_models.BaseNetworkModel.__init__","title":"<code>__init__</code>","text":"<p>Initialize model.</p> Source code in <code>navis/models/network_models.py</code> <pre><code>def __init__(self, edges: pd.DataFrame, source: str, target: str):\n    \"\"\"Initialize model.\"\"\"\n    assert isinstance(edges, pd.DataFrame), f'edges must be pandas DataFrame, got \"{type(edges)}\"'\n    assert source in edges.columns, f'edges DataFrame must contain \"{source}\" column'\n    assert target in edges.columns, f'edges DataFrame must contain \"{target}\" column'\n    self.edges = edges\n    self.source = source\n    self.target = target\n\n    if (self.edges.dtypes[source] == object) or (self.edges.dtypes[target] == object):\n        logger.warning('Looks like sources and/or targets in your edge list '\n                       'might be strings? This can massively slow down '\n                       'computations. If at all possible try to use numeric IDs.')\n</code></pre>"},{"location":"reference/navis/models/network_models/#navis.models.network_models.BaseNetworkModel.run_parallel","title":"<code>run_parallel</code>","text":"<p>Run model using parallel processes.</p> Source code in <code>navis/models/network_models.py</code> <pre><code>def run_parallel(self,\n                 n_cores: int = 5,\n                 iterations: int = 100,\n                 **kwargs: dict) -&gt; None:\n    \"\"\"Run model using parallel processes.\"\"\"\n    # Note that we initialize each process by making \"edges\" a global argument\n    with mp.Pool(processes=n_cores,\n                 initializer=self.initializer) as pool:\n\n        # Each process will take about the same amount of time\n        # So we want each process to take a single batch of iterations/n_cores runs\n        kwargs['iterations'] = int(iterations/n_cores)\n        calls = [{**kwargs, **{'position': i}} for i in range(int(n_cores))]\n\n        # Generate processes - note the use of chunksize 1 because we have\n        # already chunked the iterations such that each worker process\n        # runs exactly once\n        p = pool.imap_unordered(self._worker_wrapper, calls, chunksize=1)\n\n        # Wait for processes to complete\n        res = list(p)\n\n    # Combine results\n    self.results = pd.concat(res, axis=0)\n    self.iterations = iterations\n</code></pre>"},{"location":"reference/navis/models/network_models/#navis.models.network_models.random_activation_function","title":"<code>navis.models.network_models.random_activation_function</code>","text":"<p>Sample random activation for edges given a weight to probability function.</p> PARAMETER DESCRIPTION <code>w</code> <pre><code>(N, 1) array containing the edge weights.\n</code></pre> <p> TYPE: <code>    np.ndarray</code> </p> <code>func</code> <pre><code>A function returning activation probabilities given a set of weights.\n</code></pre> <p> TYPE: <code> callable</code> </p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>True or False values for each edge.</p> Source code in <code>navis/models/network_models.py</code> <pre><code>def random_activation_function(\n    w: np.ndarray,\n    func: Callable,\n) -&gt; np.ndarray:\n    \"\"\"Sample random activation for edges given a weight to probability function.\n\n    Parameters\n    ----------\n    w :     np.ndarray\n            (N, 1) array containing the edge weights.\n    func :  callable\n            A function returning activation probabilities given a set of weights.\n\n    Returns\n    -------\n    np.ndarray\n            True or False values for each edge.\n\n    \"\"\"\n    # Generate a random number between 0 and 1 for each connection\n    r = np.random.rand(w.shape[0])\n\n    # Normalize weights to probabilities\n    w_norm = func(w)\n\n    # Test active\n    act = w_norm &gt;= r\n\n    return act\n</code></pre>"},{"location":"reference/navis/morpho/ivscc/","title":"ivscc","text":""},{"location":"reference/navis/morpho/ivscc/#navis.morpho.ivscc.ApicalDendriteFeatures","title":"<code>navis.morpho.ivscc.ApicalDendriteFeatures</code>","text":"<p>Extract features from a apical dendrite.</p> Source code in <code>navis/morpho/ivscc.py</code> <pre><code>class ApicalDendriteFeatures(CompartmentFeatures):\n    \"\"\"Extract features from a apical dendrite.\"\"\"\n\n    def __init__(self, neuron: \"core.TreeNeuron\", verbose=False):\n        super().__init__(neuron, \"apical_dendrite\", verbose=verbose)\n\n    def extract_features(self):\n        # Extract basic features via the parent class\n        super().extract_features()\n\n        return self.features\n</code></pre>"},{"location":"reference/navis/morpho/ivscc/#navis.morpho.ivscc.AxonFeatures","title":"<code>navis.morpho.ivscc.AxonFeatures</code>","text":"<p>Extract features from an axon.</p> Source code in <code>navis/morpho/ivscc.py</code> <pre><code>class AxonFeatures(CompartmentFeatures):\n    \"\"\"Extract features from an axon.\"\"\"\n\n    def __init__(self, neuron: \"core.TreeNeuron\", verbose=False):\n        super().__init__(neuron, \"axon\", verbose=verbose)\n\n    def extract_features(self):\n        # Extract basic features via the parent class\n        super().extract_features()\n\n        # Now deal witha axon-specific features:\n\n        if self.soma is not None:\n            # Distance between axon root and soma surface\n            # Note: we're catering for potentially multiple roots here\n            axon_root_pos = self.neuron.nodes.loc[\n                self.neuron.nodes.type == \"root\", [\"x\", \"y\", \"z\"]\n            ].values\n\n            # Closest dist between an axon root and the soma\n            dist = np.linalg.norm(axon_root_pos - self.soma_pos, axis=1).min()\n\n            # Subtract soma radius from the distance\n            dist -= self.soma_radius\n\n            self.record_feature(\"exit_distance\", dist)\n\n            # Axon theta: The relative radial position of the point where the neurite from which\n            # the axon derives exits the soma.\n\n            # Get the node where the axon exits the soma\n            exit_node = self.neuron.nodes.loc[self.neuron.nodes.type == \"root\"]\n\n            # Get theta\n            theta = np.arctan2(\n                exit_node.y.values - self.soma_pos[1],\n                exit_node.x.values - self.soma_pos[0],\n            )[0]\n            self.record_feature(\"exit_theta\", theta)\n\n        return self.features\n</code></pre>"},{"location":"reference/navis/morpho/ivscc/#navis.morpho.ivscc.BasalDendriteFeatures","title":"<code>navis.morpho.ivscc.BasalDendriteFeatures</code>","text":"<p>Extract features from a basal dendrite.</p> Source code in <code>navis/morpho/ivscc.py</code> <pre><code>class BasalDendriteFeatures(CompartmentFeatures):\n    \"\"\"Extract features from a basal dendrite.\"\"\"\n\n    def __init__(self, neuron: \"core.TreeNeuron\", verbose=False):\n        super().__init__(neuron, \"basal_dendrite\", verbose=verbose)\n\n    def extract_features(self):\n        # Extract basic features via the parent class\n        super().extract_features()\n\n        # Now deal with basal dendrite-specific features\n        if self.soma is not None:\n            # Number of stems sprouting from the soma\n            # (i.e. number of nodes with a parent that is the soma)\n            self.record_feature(\n                \"calculate_number_of_stems\",\n                (self.neuron.nodes.parent_id == self.soma).sum(),\n            )\n\n        return self.features\n</code></pre>"},{"location":"reference/navis/morpho/ivscc/#navis.morpho.ivscc.BasicFeatures","title":"<code>navis.morpho.ivscc.BasicFeatures</code>","text":"<p>Base class for features.</p> Source code in <code>navis/morpho/ivscc.py</code> <pre><code>class BasicFeatures(Features):\n    \"\"\"Base class for features.\"\"\"\n\n    def extract_features(self):\n        \"\"\"Extract basic features.\"\"\"\n        self.record_feature(\n            \"extent_y\", self.neuron.nodes.y.max() - self.neuron.nodes.y.min()\n        )\n        self.record_feature(\n            \"extent_x\", self.neuron.nodes.x.max() - self.neuron.nodes.x.min()\n        )\n        self.record_feature(\n            \"max_branch_order\", (self.neuron.nodes.type == \"branch\").sum() + 1\n        )\n        self.record_feature(\"num_nodes\", len(self.neuron.nodes))\n        self.record_feature(\"total_length\", self.neuron.cable_length)\n\n        if self.soma is None:\n            if self.verbose:\n                logger.warning(\n                    f\"{self.neuron.id} has no `.soma` attribute, skipping soma-related features.\"\n                )\n            return\n\n        # x/y bias from soma\n        # Note: this is absolute for x and relative for y\n        self.record_feature(\n            \"bias_x\",\n            abs(\n                (self.neuron.nodes.x.max() - self.soma_pos[0])\n                - (self.soma_pos[0] - self.neuron.nodes.x.min())\n            ),\n        )\n        self.record_feature(\n            \"bias_y\",\n            (self.neuron.nodes.y.max() - self.soma_pos[1])\n            - (self.soma_pos[1] - self.neuron.nodes.y.min()),\n        )\n\n        # Distances from soma\n        self.record_feature(\n            \"max_euclidean_distance\",\n            (\n                (self.neuron.nodes[[\"x\", \"y\", \"z\"]] - self.soma_pos)\n                .pow(2)\n                .sum(axis=1)\n                .pow(0.5)\n                .sum()\n                .max()\n            ),\n        )\n        self.record_feature(\n            \"max_path_length\",\n            self.leaf_dists.loc[\n                self.leaf_dists.index.isin(self.neuron.nodes.node_id)\n            ].values.max(),\n        )\n\n        # Tortuosity\n        self.record_feature(\"mean_contraction\", tortuosity(self.neuron))\n\n        # Branching (number of linear segments between branch)\n        self.record_feature(\"num_branches\", len(self.neuron.small_segments))\n\n        return self.features\n</code></pre>"},{"location":"reference/navis/morpho/ivscc/#navis.morpho.ivscc.BasicFeatures.extract_features","title":"<code>extract_features</code>","text":"<p>Extract basic features.</p> Source code in <code>navis/morpho/ivscc.py</code> <pre><code>def extract_features(self):\n    \"\"\"Extract basic features.\"\"\"\n    self.record_feature(\n        \"extent_y\", self.neuron.nodes.y.max() - self.neuron.nodes.y.min()\n    )\n    self.record_feature(\n        \"extent_x\", self.neuron.nodes.x.max() - self.neuron.nodes.x.min()\n    )\n    self.record_feature(\n        \"max_branch_order\", (self.neuron.nodes.type == \"branch\").sum() + 1\n    )\n    self.record_feature(\"num_nodes\", len(self.neuron.nodes))\n    self.record_feature(\"total_length\", self.neuron.cable_length)\n\n    if self.soma is None:\n        if self.verbose:\n            logger.warning(\n                f\"{self.neuron.id} has no `.soma` attribute, skipping soma-related features.\"\n            )\n        return\n\n    # x/y bias from soma\n    # Note: this is absolute for x and relative for y\n    self.record_feature(\n        \"bias_x\",\n        abs(\n            (self.neuron.nodes.x.max() - self.soma_pos[0])\n            - (self.soma_pos[0] - self.neuron.nodes.x.min())\n        ),\n    )\n    self.record_feature(\n        \"bias_y\",\n        (self.neuron.nodes.y.max() - self.soma_pos[1])\n        - (self.soma_pos[1] - self.neuron.nodes.y.min()),\n    )\n\n    # Distances from soma\n    self.record_feature(\n        \"max_euclidean_distance\",\n        (\n            (self.neuron.nodes[[\"x\", \"y\", \"z\"]] - self.soma_pos)\n            .pow(2)\n            .sum(axis=1)\n            .pow(0.5)\n            .sum()\n            .max()\n        ),\n    )\n    self.record_feature(\n        \"max_path_length\",\n        self.leaf_dists.loc[\n            self.leaf_dists.index.isin(self.neuron.nodes.node_id)\n        ].values.max(),\n    )\n\n    # Tortuosity\n    self.record_feature(\"mean_contraction\", tortuosity(self.neuron))\n\n    # Branching (number of linear segments between branch)\n    self.record_feature(\"num_branches\", len(self.neuron.small_segments))\n\n    return self.features\n</code></pre>"},{"location":"reference/navis/morpho/ivscc/#navis.morpho.ivscc.CompartmentFeatures","title":"<code>navis.morpho.ivscc.CompartmentFeatures</code>","text":"<p>Base class for compartment-specific features.</p> Source code in <code>navis/morpho/ivscc.py</code> <pre><code>class CompartmentFeatures(BasicFeatures):\n    \"\"\"Base class for compartment-specific features.\"\"\"\n\n    def __init__(self, neuron: \"core.TreeNeuron\", compartment, verbose=False):\n        if \"label\" not in neuron.nodes.columns:\n            raise ValueError(\n                f\"No 'label' column found in node table for neuron {neuron.id}\"\n            )\n\n        if (\n            compartment not in neuron.nodes.label.values\n            and comp_to_label.get(compartment, compartment)\n            not in neuron.nodes.label.values\n        ):\n            raise CompartmentNotFoundError(\n                f\"No {compartment} ({comp_to_label.get(compartment, compartment)}) compartments found in neuron {neuron.id}\"\n            )\n\n        # Initialize the parent class\n        super().__init__(neuron, label=compartment, verbose=verbose)\n\n        # Now subset the neuron to this compartment\n        self.neuron = subset_neuron(\n            self.neuron,\n            (\n                self.neuron.nodes.label.isin(\n                    (compartment, comp_to_label[compartment])\n                ).values\n            ),\n        )\n</code></pre>"},{"location":"reference/navis/morpho/ivscc/#navis.morpho.ivscc.CompartmentNotFoundError","title":"<code>navis.morpho.ivscc.CompartmentNotFoundError</code>","text":"<p>An exception raised when a compartment is not found.</p> Source code in <code>navis/morpho/ivscc.py</code> <pre><code>class CompartmentNotFoundError(Exception):\n    \"\"\"An exception raised when a compartment is not found.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/navis/morpho/ivscc/#navis.morpho.ivscc.Features","title":"<code>navis.morpho.ivscc.Features</code>","text":"Source code in <code>navis/morpho/ivscc.py</code> <pre><code>class Features(ABC):\n    def __init__(self, neuron: \"core.TreeNeuron\", label=None, verbose=False):\n        self.neuron = neuron\n        self.verbose = verbose\n\n        if label is None:\n            self.label = \"\"\n        elif not label.endswith(\"_\"):\n            self.label = f\"{label}_\"\n        else:\n            self.label = label\n\n        # Make sure the neuron is rooted to the soma (if present)\n        self.soma = self.neuron.soma\n        if self.soma is not None:\n            self.soma_pos = self.neuron.soma_pos[0]\n            self.soma_radius = self.neuron.nodes.set_index(\"node_id\").loc[\n                self.soma, \"radius\"\n            ]\n\n            if self.neuron.soma not in self.neuron.root:\n                self.neuron = self.neuron.reroot(self.neuron.soma)\n\n            # Calculate geodesic distances from leafs to all other nodes (directed)\n            self.leaf_dists = graph.geodesic_matrix(\n                self.neuron, self.neuron.leafs.node_id.values, directed=True\n            )\n            # Replace infinities with -1\n            self.leaf_dists[self.leaf_dists == float(\"inf\")] = -1\n\n        self.features = {}\n\n    def record_feature(self, name, value):\n        \"\"\"Record a feature.\"\"\"\n        self.features[f\"{self.label}{name}\"] = value\n\n    @abstractmethod\n    def extract_features(self):\n        \"\"\"Extract features.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/navis/morpho/ivscc/#navis.morpho.ivscc.Features.extract_features","title":"<code>extract_features</code>  <code>abstractmethod</code>","text":"<p>Extract features.</p> Source code in <code>navis/morpho/ivscc.py</code> <pre><code>@abstractmethod\ndef extract_features(self):\n    \"\"\"Extract features.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/navis/morpho/ivscc/#navis.morpho.ivscc.Features.record_feature","title":"<code>record_feature</code>","text":"<p>Record a feature.</p> Source code in <code>navis/morpho/ivscc.py</code> <pre><code>def record_feature(self, name, value):\n    \"\"\"Record a feature.\"\"\"\n    self.features[f\"{self.label}{name}\"] = value\n</code></pre>"},{"location":"reference/navis/morpho/ivscc/#navis.morpho.ivscc.OverlapFeatures","title":"<code>navis.morpho.ivscc.OverlapFeatures</code>","text":"<p>Features that compare two compartments (e.g. overlap).</p> Source code in <code>navis/morpho/ivscc.py</code> <pre><code>class OverlapFeatures(Features):\n    \"\"\"Features that compare two compartments (e.g. overlap).\"\"\"\n\n    # Compartments to compare\n    compartments = (\"axon\", \"basal_dendrite\", \"apical_dendrite\")\n\n    def extract_features(self):\n        # Iterate over compartments\n        for c1 in self.compartments:\n            if c1 in self.neuron.nodes.label.values:\n                c1_nodes = self.neuron.nodes[self.neuron.nodes.label == c1]\n            elif comp_to_label.get(c1, c1) in self.neuron.nodes.label.values:\n                c1_nodes = self.neuron.nodes[\n                    self.neuron.nodes.label == comp_to_label[c1]\n                ]\n            else:\n                continue\n            for c2 in self.compartments:\n                if c1 == c2:\n                    continue\n                if c2 in self.neuron.nodes.label.values:\n                    c2_nodes = self.neuron.nodes[self.neuron.nodes.label == c2]\n                elif comp_to_label.get(c2, c2) in self.neuron.nodes.label.values:\n                    c2_nodes = self.neuron.nodes[\n                        self.neuron.nodes.label == comp_to_label[c2]\n                    ]\n                else:\n                    continue\n\n                # Calculate % of nodes of a given compartment type above/overlapping/below the\n                # full y-extent of another compartment type\n                self.features[f\"{c1}_frac_above_{c2}\"] = (\n                    c1_nodes.y &gt; c2_nodes.y.max()\n                ).sum() / len(c1_nodes)\n                self.features[f\"{c1}_frac_intersect_{c2}\"] = (\n                    (c1_nodes.y &gt;= c2_nodes.y.min()) &amp; (c1_nodes.y &lt;= c2_nodes.y.max())\n                ).sum() / len(c1_nodes)\n                self.features[f\"{c1}_frac_below_{c2}\"] = (\n                    c1_nodes.y &lt; c2_nodes.y.min()\n                ).sum() / len(c1_nodes)\n\n                # Calculate earth mover's distance (EMD) between the two compartments\n                if f\"{c2}_emd_with_{c1}\" not in self.features:\n                    self.features[f\"{c1}_emd_with_{c2}\"] = wasserstein_distance(\n                        c1_nodes.y, c2_nodes.y\n                    )\n\n        return self.features\n</code></pre>"},{"location":"reference/navis/morpho/mmetrics/","title":"mmetrics","text":""},{"location":"reference/navis/morpho/mmetrics/#navis.morpho.mmetrics.parent_dist","title":"<code>navis.morpho.mmetrics.parent_dist</code>","text":"<p>Get child-&gt;parent distances for skeleton nodes.</p> PARAMETER DESCRIPTION <code>x</code> <p> TYPE: <code>        TreeNeuron | node table</code> </p> <code>root_dist</code> <pre><code>    `parent_dist` for the root's row. Set to `None`, to leave\n    at `NaN` or e.g. to `0` to set to 0.\n</code></pre> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>Array with distances in same order and size as node table.</p> Source code in <code>navis/morpho/mmetrics.py</code> <pre><code>def parent_dist(\n    x: Union[\"core.TreeNeuron\", pd.DataFrame], root_dist: Optional[int] = None\n) -&gt; None:\n    \"\"\"Get child-&gt;parent distances for skeleton nodes.\n\n    Parameters\n    ----------\n    x :         TreeNeuron | node table\n    root_dist : int | None\n                `parent_dist` for the root's row. Set to `None`, to leave\n                at `NaN` or e.g. to `0` to set to 0.\n\n    Returns\n    -------\n    np.ndarray\n                Array with distances in same order and size as node table.\n\n    \"\"\"\n    if isinstance(x, core.TreeNeuron):\n        nodes = x.nodes\n    elif isinstance(x, pd.DataFrame):\n        nodes = x\n    else:\n        raise TypeError(f'Need TreeNeuron or DataFrame, got \"{type(x)}\"')\n\n    if not utils.fastcore:\n        # Extract node coordinates\n        tn_coords = nodes[[\"x\", \"y\", \"z\"]].values\n\n        # Get parent coordinates\n        parent_coords = (\n            nodes.set_index(\"node_id\")\n            .reindex(nodes.parent_id.values)[[\"x\", \"y\", \"z\"]]\n            .values\n        )\n\n        # Calculate distances between nodes and their parents\n        w = np.sqrt(np.sum((tn_coords - parent_coords) ** 2, axis=1))\n\n        # Replace root dist (nan by default)\n        w[np.isnan(w)] = root_dist\n    else:\n        w = utils.fastcore.dag.parent_dist(\n            x.nodes.node_id.values,\n            x.nodes.parent_id.values,\n            x.nodes[[\"x\", \"y\", \"z\"]].values,\n            root_dist=root_dist,\n        )\n\n    return w\n</code></pre>"},{"location":"reference/navis/morpho/persistence/","title":"persistence","text":""},{"location":"reference/navis/morpho/persistence/#navis.morpho.persistence.persistence_diagram","title":"<code>navis.morpho.persistence.persistence_diagram</code>","text":"<p>Plot a persistence diagram.</p> PARAMETER DESCRIPTION <code>pers</code> <pre><code>    Persistent points from [`navis.persistence_points`][].\n</code></pre> <p> TYPE: <code>     pd.DataFrame</code> </p> <code>ax</code> <pre><code>    Ax to plot on.\n</code></pre> <p> TYPE: <code>       matplotlib ax</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <pre><code>    Keyword arguments are passed to `LineCollection`.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ax</code> <p> TYPE: <code>matplotlib ax</code> </p> Source code in <code>navis/morpho/persistence.py</code> <pre><code>def persistence_diagram(pers, ax=None, **kwargs):\n    \"\"\"Plot a persistence diagram.\n\n    Parameters\n    ----------\n    pers :      pd.DataFrame\n                Persistent points from [`navis.persistence_points`][].\n    ax :        matplotlib ax, optional\n                Ax to plot on.\n    **kwargs\n                Keyword arguments are passed to `LineCollection`.\n\n    Returns\n    -------\n    ax :        matplotlib ax\n\n    \"\"\"\n    if not isinstance(pers, pd.DataFrame):\n        raise TypeError(f'Expected DataFrame, got \"{type(pers)}\"')\n\n    if not ax:\n        fig, ax = plt.subplots()\n\n    segs = []\n    for i, (b, d) in enumerate(zip(pers.birth.values, pers.death.values)):\n        segs.append([[b, i], [d, i]])\n    lc = LineCollection(segs, **kwargs)\n    ax.add_collection(lc)\n\n    ax.set_xlim(-5, pers.death.max())\n    ax.set_ylim(-5, pers.shape[0])\n\n    ax.set_ylabel('segments')\n    ax.set_xlabel('time')\n\n    return ax\n</code></pre>"},{"location":"reference/navis/morpho/persistence/#navis.morpho.persistence.persistence_vector_plot","title":"<code>navis.morpho.persistence.persistence_vector_plot</code>","text":"<p>Plot persistence vectors.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Neuron(s) to calculate persistence points for. For MeshNeurons,\n    we will use the skeleton produced by/associated with its\n    `.skeleton` property.\n</code></pre> <p> TYPE: <code>        TreeNeuron | MeshNeuron | NeuronList</code> </p> RETURNS DESCRIPTION <code>ax</code> Source code in <code>navis/morpho/persistence.py</code> <pre><code>def persistence_vector_plot(x, normalize=True, ax=None,\n                            persistence_kwargs={}, vector_kwargs={}):\n    \"\"\"Plot persistence vectors.\n\n    Parameters\n    ----------\n    x :         TreeNeuron | MeshNeuron | NeuronList\n                Neuron(s) to calculate persistence points for. For MeshNeurons,\n                we will use the skeleton produced by/associated with its\n                `.skeleton` property.\n\n    Returns\n    -------\n    ax\n\n    \"\"\"\n    if not isinstance(x, core.NeuronList):\n        x = core.NeuronList(x)\n\n    # Get persistence points for each skeleton\n    pers = persistence_points(x, **persistence_kwargs)\n\n    # Get the vectors\n    vectors, samples = persistence_vectors(pers, **vector_kwargs)\n\n    # Normalizing the vectors will produce more useful distances\n    if normalize:\n        vectors = vectors / vectors.max(axis=1).reshape(-1, 1)\n    else:\n        vectors = vectors / vectors.max()\n\n    if not ax:\n        fig, ax = plt.subplots()\n\n    for n, v in zip(x, vectors):\n        ax.plot(samples, v, label=n.label)\n\n    return ax\n</code></pre>"},{"location":"reference/navis/morpho/subset/","title":"subset","text":""},{"location":"reference/navis/morpho/subset/#navis.morpho.subset.submesh","title":"<code>navis.morpho.subset.submesh</code>","text":"<p>Re-imlementation of trimesh.submesh that is faster for our use case.</p> <p>Notably we:  - ignore normals (possibly needed) and visuals (definitely not needed)  - allow only one set of faces to be passed  - return vertices and faces instead of a new mesh  - make as few copies as possible  - allow passing vertex indices instead of faces</p> <p>This function is 5-10x faster than trimesh.submesh for our use case. Note that the speed of this function was never the bottleneck though, it's about the memory footprint. See https://github.com/navis-org/navis/issues/154.</p> PARAMETER DESCRIPTION <code>mesh</code> <pre><code>        Mesh to submesh.\n</code></pre> <p> TYPE: <code>         trimesh.Trimesh</code> </p> <code>faces_index</code> <pre><code>        Indices of faces to keep.\n</code></pre> <p> TYPE: <code>  array-like</code> DEFAULT: <code>None</code> </p> <code>vertex_index</code> <pre><code>        Indices of vertices to keep.\n</code></pre> <p> TYPE: <code> array-like</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>vertices</code> <p>Vertices of submesh.</p> <p> TYPE: <code>np.ndarray</code> </p> <code>faces</code> <p>Faces of submesh.</p> <p> TYPE: <code>np.ndarray</code> </p> Source code in <code>navis/morpho/subset.py</code> <pre><code>def submesh(mesh, *, faces_index=None, vertex_index=None):\n    \"\"\"Re-imlementation of trimesh.submesh that is faster for our use case.\n\n    Notably we:\n     - ignore normals (possibly needed) and visuals (definitely not needed)\n     - allow only one set of faces to be passed\n     - return vertices and faces instead of a new mesh\n     - make as few copies as possible\n     - allow passing vertex indices instead of faces\n\n    This function is 5-10x faster than trimesh.submesh for our use case.\n    Note that the speed of this function was never the bottleneck though,\n    it's about the memory footprint.\n    See https://github.com/navis-org/navis/issues/154.\n\n    Parameters\n    ----------\n    mesh :          trimesh.Trimesh\n                    Mesh to submesh.\n    faces_index :   array-like\n                    Indices of faces to keep.\n    vertex_index :  array-like\n                    Indices of vertices to keep.\n\n    Returns\n    -------\n    vertices :  np.ndarray\n                Vertices of submesh.\n    faces :     np.ndarray\n                Faces of submesh.\n\n    \"\"\"\n    if faces_index is None and vertex_index is None:\n        raise ValueError(\"Either `faces_index` or `vertex_index` must be provided.\")\n    elif faces_index is not None and vertex_index is not None:\n        raise ValueError(\"Only one of `faces_index` or `vertex_index` can be provided.\")\n\n    # First check if we can return either an empty mesh or the original mesh right away\n    if faces_index is not None:\n        if len(faces_index) == 0:\n            return np.array([]), np.array([])\n        elif len(faces_index) == len(mesh.faces):\n            if len(np.unique(faces_index)) == len(mesh.faces):\n                return mesh.vertices.copy(), mesh.faces.copy()\n    else:\n        if len(vertex_index) == 0:\n            return np.array([]), np.array([])\n        elif len(vertex_index) == len(mesh.vertices):\n            if len(np.unique(vertex_index)) == len(mesh.vertices):\n                return mesh.vertices.copy(), mesh.faces.copy()\n\n    # Use a view of the original data\n    original_faces = mesh.faces.view(np.ndarray)\n    original_vertices = mesh.vertices.view(np.ndarray)\n\n    # If we're starting with vertices, find faces that contain at least one of our vertices\n    # This way we will also make sure to drop unreferenced vertices\n    if vertex_index is not None:\n        faces_index = np.arange(len(original_faces))[\n            np.isin(original_faces, vertex_index).all(axis=1)\n        ]\n\n    # Get unique vertices in the to-be-kept faces\n    faces = original_faces[faces_index]\n    unique = np.unique(faces.reshape(-1))\n\n    # Generate a mask for the vertices\n    # (using int32 here since we're unlikey to have more than 2B vertices)\n    mask = np.arange(len(original_vertices), dtype=np.int32)\n\n    # Remap the vertices to the new indices\n    mask[unique] = np.arange(len(unique))\n\n    # Grab the vertices in the order they are referenced\n    vertices = original_vertices[unique].copy()\n\n    # Remap the faces to the new vertex indices\n    # (making a copy to allow `mask` to be garbage collected)\n    faces = mask[faces].copy()\n\n    return vertices, faces\n</code></pre>"},{"location":"reference/navis/nbl/ablast_funcs/","title":"ablast_funcs","text":""},{"location":"reference/navis/nbl/ablast_funcs/#navis.nbl.ablast_funcs.Blaster","title":"<code>navis.nbl.ablast_funcs.Blaster</code>","text":"<p>Base class for blasting.</p> Source code in <code>navis/nbl/base.py</code> <pre><code>class Blaster(ABC):\n    \"\"\"Base class for blasting.\"\"\"\n\n    def __init__(self, dtype=np.float64, progress=True):\n        \"\"\"Initialize class.\"\"\"\n        self.dtype = dtype\n        self.progress = progress\n        self.desc = \"Blasting\"\n        self.self_hits = []\n        self.neurons = []\n        self.ids = []\n\n    def __len__(self):\n        return len(self.neurons)\n\n    @abstractmethod\n    def append(self, neurons) -&gt; NestedIndices:\n        \"\"\"Append neurons.\"\"\"\n        pass\n\n    @abstractmethod\n    def calc_self_hit(self, neurons):\n        \"\"\"Non-normalized value for self hit.\"\"\"\n        pass\n\n    @abstractmethod\n    def single_query_target(self, q_idx, t_idx, scores='forward'):\n        \"\"\"Query single target against single target.\"\"\"\n        pass\n\n    @property\n    def dtype(self):\n        \"\"\"Data type used for scores.\"\"\"\n        return self._dtype\n\n    @dtype.setter\n    def dtype(self, dtype):\n        try:\n            self._dtype = np.dtype(dtype)\n        except TypeError:\n            try:\n                self._dtype = FLOAT_DTYPES[dtype]\n            except KeyError:\n                raise ValueError(\n                    f'Unknown precision/dtype {dtype}. Expected on of the following: 16, 32 or 64 (default)'\n                )\n\n    def pair_query_target(self, pairs, scores='forward'):\n        \"\"\"BLAST multiple pairs.\n\n        Parameters\n        ----------\n        pairs :             tuples\n                            Tuples of (query_ix, target_ix) to query.\n        scores :            \"forward\" | \"mean\" | \"min\" | \"max\" | \"both\"\n                            Which scores to return.\n\n        \"\"\"\n        # See `multi_query_target` for explanation on progress bars\n        scr = []\n        for p in config.tqdm_classic(pairs,\n                             desc=f'{self.desc} pairs',\n                             leave=False,\n                             position=getattr(self,\n                                              'pbar_position',\n                                              None),\n                             disable=not self.progress):\n            scr.append(self.single_query_target(p[0], p[1], scores=scores))\n\n        return scr\n\n    def multi_query_target(self, q_idx, t_idx, scores='forward'):\n        \"\"\"BLAST multiple queries against multiple targets.\n\n        Parameters\n        ----------\n        q_idx,t_idx :       iterable\n                            Iterable of query/target neuron indices to BLAST.\n        scores :            \"forward\" | \"mean\" | \"min\" | \"max\" | \"both\"\n                            Which scores to return.\n\n        \"\"\"\n        shape = (len(q_idx), len(t_idx)) if scores != 'both' else (len(q_idx), len(t_idx), 2)\n        res = np.empty(shape, dtype=self.dtype)\n        for i, q in enumerate(config.tqdm(q_idx,\n                                          desc=self.desc,\n                                          leave=False,\n                                          position=getattr(self,\n                                                           'pbar_position',\n                                                           None),\n                                          disable=not self.progress)):\n            for k, t in enumerate(t_idx):\n                res[i, k] = self.single_query_target(q, t, scores=scores)\n\n        # Generate results\n        if res.ndim == 2:\n            res = pd.DataFrame(res)\n            res.columns = [self.ids[t] for t in t_idx]\n            res.index = [self.ids[q] for q in q_idx]\n            res.index.name = 'query'\n            res.columns.name = 'target'\n        else:\n            # For scores='both' we will create a DataFrame with multi-index\n            ix = pd.MultiIndex.from_product([[self.ids[q] for q in q_idx],\n                                             ['forward', 'reverse']],\n                                            names=[\"query\", \"score\"])\n            res = pd.DataFrame(np.hstack((res[:, :, 0],\n                                          res[:, :, 1])).reshape(len(q_idx) * 2,\n                                                                 len(t_idx)),\n                               index=ix,\n                               columns=[self.ids[t] for t in t_idx])\n            res.columns.name = 'target'\n\n        return res\n\n    def all_by_all(self, scores='forward'):\n        \"\"\"BLAST all-by-all neurons.\"\"\"\n        res = self.multi_query_target(range(len(self.neurons)),\n                                      range(len(self.neurons)),\n                                      scores='forward')\n\n        # For all-by-all BLAST we can get the mean score by\n        # transposing the scores\n        if scores == 'mean':\n            res = (res + res.T) / 2\n        elif scores == 'min':\n            res.loc[:, :] = np.dstack((res, res.T)).min(axis=2)\n        elif scores == 'max':\n            res.loc[:, :] = np.dstack((res, res.T)).max(axis=2)\n        elif scores == 'both':\n            ix = pd.MultiIndex.from_product([res.index, ['forward', 'reverse']],\n                                            names=[\"query\", \"score\"])\n            res = pd.DataFrame(np.hstack((res[:, :, 0],\n                                          res[:, :, 1])).reshape(res.shape[0] * 2,\n                                                                 res.shape[1]),\n                               index=ix,\n                               columns=res.columns)\n\n        return res\n\n    def __len__(self):\n        return len(self.neurons)\n</code></pre>"},{"location":"reference/navis/nbl/ablast_funcs/#navis.nbl.ablast_funcs.Blaster.dtype","title":"<code>dtype</code>  <code>property</code> <code>writable</code>","text":"<p>Data type used for scores.</p>"},{"location":"reference/navis/nbl/ablast_funcs/#navis.nbl.ablast_funcs.Blaster.__init__","title":"<code>__init__</code>","text":"<p>Initialize class.</p> Source code in <code>navis/nbl/base.py</code> <pre><code>def __init__(self, dtype=np.float64, progress=True):\n    \"\"\"Initialize class.\"\"\"\n    self.dtype = dtype\n    self.progress = progress\n    self.desc = \"Blasting\"\n    self.self_hits = []\n    self.neurons = []\n    self.ids = []\n</code></pre>"},{"location":"reference/navis/nbl/ablast_funcs/#navis.nbl.ablast_funcs.Blaster.all_by_all","title":"<code>all_by_all</code>","text":"<p>BLAST all-by-all neurons.</p> Source code in <code>navis/nbl/base.py</code> <pre><code>def all_by_all(self, scores='forward'):\n    \"\"\"BLAST all-by-all neurons.\"\"\"\n    res = self.multi_query_target(range(len(self.neurons)),\n                                  range(len(self.neurons)),\n                                  scores='forward')\n\n    # For all-by-all BLAST we can get the mean score by\n    # transposing the scores\n    if scores == 'mean':\n        res = (res + res.T) / 2\n    elif scores == 'min':\n        res.loc[:, :] = np.dstack((res, res.T)).min(axis=2)\n    elif scores == 'max':\n        res.loc[:, :] = np.dstack((res, res.T)).max(axis=2)\n    elif scores == 'both':\n        ix = pd.MultiIndex.from_product([res.index, ['forward', 'reverse']],\n                                        names=[\"query\", \"score\"])\n        res = pd.DataFrame(np.hstack((res[:, :, 0],\n                                      res[:, :, 1])).reshape(res.shape[0] * 2,\n                                                             res.shape[1]),\n                           index=ix,\n                           columns=res.columns)\n\n    return res\n</code></pre>"},{"location":"reference/navis/nbl/ablast_funcs/#navis.nbl.ablast_funcs.Blaster.append","title":"<code>append</code>  <code>abstractmethod</code>","text":"<p>Append neurons.</p> Source code in <code>navis/nbl/base.py</code> <pre><code>@abstractmethod\ndef append(self, neurons) -&gt; NestedIndices:\n    \"\"\"Append neurons.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/navis/nbl/ablast_funcs/#navis.nbl.ablast_funcs.Blaster.calc_self_hit","title":"<code>calc_self_hit</code>  <code>abstractmethod</code>","text":"<p>Non-normalized value for self hit.</p> Source code in <code>navis/nbl/base.py</code> <pre><code>@abstractmethod\ndef calc_self_hit(self, neurons):\n    \"\"\"Non-normalized value for self hit.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/navis/nbl/ablast_funcs/#navis.nbl.ablast_funcs.Blaster.multi_query_target","title":"<code>multi_query_target</code>","text":"<p>BLAST multiple queries against multiple targets.</p> PARAMETER DESCRIPTION <code>q_idx</code> <pre><code>            Iterable of query/target neuron indices to BLAST.\n</code></pre> <p> </p> <code>scores</code> <pre><code>            Which scores to return.\n</code></pre> <p> TYPE: <code>           \"forward\" | \"mean\" | \"min\" | \"max\" | \"both\"</code> DEFAULT: <code>'forward'</code> </p> Source code in <code>navis/nbl/base.py</code> <pre><code>def multi_query_target(self, q_idx, t_idx, scores='forward'):\n    \"\"\"BLAST multiple queries against multiple targets.\n\n    Parameters\n    ----------\n    q_idx,t_idx :       iterable\n                        Iterable of query/target neuron indices to BLAST.\n    scores :            \"forward\" | \"mean\" | \"min\" | \"max\" | \"both\"\n                        Which scores to return.\n\n    \"\"\"\n    shape = (len(q_idx), len(t_idx)) if scores != 'both' else (len(q_idx), len(t_idx), 2)\n    res = np.empty(shape, dtype=self.dtype)\n    for i, q in enumerate(config.tqdm(q_idx,\n                                      desc=self.desc,\n                                      leave=False,\n                                      position=getattr(self,\n                                                       'pbar_position',\n                                                       None),\n                                      disable=not self.progress)):\n        for k, t in enumerate(t_idx):\n            res[i, k] = self.single_query_target(q, t, scores=scores)\n\n    # Generate results\n    if res.ndim == 2:\n        res = pd.DataFrame(res)\n        res.columns = [self.ids[t] for t in t_idx]\n        res.index = [self.ids[q] for q in q_idx]\n        res.index.name = 'query'\n        res.columns.name = 'target'\n    else:\n        # For scores='both' we will create a DataFrame with multi-index\n        ix = pd.MultiIndex.from_product([[self.ids[q] for q in q_idx],\n                                         ['forward', 'reverse']],\n                                        names=[\"query\", \"score\"])\n        res = pd.DataFrame(np.hstack((res[:, :, 0],\n                                      res[:, :, 1])).reshape(len(q_idx) * 2,\n                                                             len(t_idx)),\n                           index=ix,\n                           columns=[self.ids[t] for t in t_idx])\n        res.columns.name = 'target'\n\n    return res\n</code></pre>"},{"location":"reference/navis/nbl/ablast_funcs/#navis.nbl.ablast_funcs.Blaster.pair_query_target","title":"<code>pair_query_target</code>","text":"<p>BLAST multiple pairs.</p> PARAMETER DESCRIPTION <code>pairs</code> <pre><code>            Tuples of (query_ix, target_ix) to query.\n</code></pre> <p> TYPE: <code>            tuples</code> </p> <code>scores</code> <pre><code>            Which scores to return.\n</code></pre> <p> TYPE: <code>           \"forward\" | \"mean\" | \"min\" | \"max\" | \"both\"</code> DEFAULT: <code>'forward'</code> </p> Source code in <code>navis/nbl/base.py</code> <pre><code>def pair_query_target(self, pairs, scores='forward'):\n    \"\"\"BLAST multiple pairs.\n\n    Parameters\n    ----------\n    pairs :             tuples\n                        Tuples of (query_ix, target_ix) to query.\n    scores :            \"forward\" | \"mean\" | \"min\" | \"max\" | \"both\"\n                        Which scores to return.\n\n    \"\"\"\n    # See `multi_query_target` for explanation on progress bars\n    scr = []\n    for p in config.tqdm_classic(pairs,\n                         desc=f'{self.desc} pairs',\n                         leave=False,\n                         position=getattr(self,\n                                          'pbar_position',\n                                          None),\n                         disable=not self.progress):\n        scr.append(self.single_query_target(p[0], p[1], scores=scores))\n\n    return scr\n</code></pre>"},{"location":"reference/navis/nbl/ablast_funcs/#navis.nbl.ablast_funcs.Blaster.single_query_target","title":"<code>single_query_target</code>  <code>abstractmethod</code>","text":"<p>Query single target against single target.</p> Source code in <code>navis/nbl/base.py</code> <pre><code>@abstractmethod\ndef single_query_target(self, q_idx, t_idx, scores='forward'):\n    \"\"\"Query single target against single target.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/navis/nbl/ablast_funcs/#navis.nbl.ablast_funcs.Lookup2d","title":"<code>navis.nbl.ablast_funcs.Lookup2d</code>","text":"<p>Convenience class inheriting from LookupNd for the common 2D float case. Provides IO with pandas DataFrames.</p> Source code in <code>navis/nbl/smat.py</code> <pre><code>class Lookup2d(LookupNd):\n    \"\"\"Convenience class inheriting from LookupNd for the common 2D float case.\n    Provides IO with pandas DataFrames.\n    \"\"\"\n\n    def __init__(self, axis0: Digitizer, axis1: Digitizer, cells: np.ndarray):\n        \"\"\"2D lookup table for convert NBLAST matches to scores.\n\n        Commonly read from a `pandas.DataFrame`\n        or trained on data using a `LookupDistDotBuilder`.\n\n        Parameters\n        ----------\n        axis0 : Digitizer\n            How to convert continuous values into an index for the first axis.\n        axis1 : Digitizer\n            How to convert continuous values into an index for the second axis.\n        cells : np.ndarray\n            Values to look up in the table.\n        \"\"\"\n        super().__init__([axis0, axis1], cells)\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Convert the lookup table into a `pandas.DataFrame`.\n\n        From there, it can be shared, saved, and so on.\n\n        The index and column labels describe the intervals represented by that axis.\n\n        Returns\n        -------\n        pd.DataFrame\n        \"\"\"\n        return pd.DataFrame(\n            self.cells,\n            self.axes[0].to_strings(),\n            self.axes[1].to_strings(),\n        )\n\n    @classmethod\n    def from_dataframe(cls, df: pd.DataFrame):\n        \"\"\"Parse score matrix from a dataframe with string index and column labels.\n\n        Expects the index and column labels to specify an interval\n        like `f\"[{{lower}},{{upper}})\"`.\n        Will replace the lowermost and uppermost bound with -inf and inf\n        if they are not already.\n        \"\"\"\n        return cls(\n            Digitizer.from_strings(df.index),\n            Digitizer.from_strings(df.columns),\n            df.to_numpy(),\n        )\n</code></pre>"},{"location":"reference/navis/nbl/ablast_funcs/#navis.nbl.ablast_funcs.Lookup2d.__init__","title":"<code>__init__</code>","text":"<p>2D lookup table for convert NBLAST matches to scores.</p> <p>Commonly read from a <code>pandas.DataFrame</code> or trained on data using a <code>LookupDistDotBuilder</code>.</p> PARAMETER DESCRIPTION <code>axis0</code> <p>How to convert continuous values into an index for the first axis.</p> <p> TYPE: <code>Digitizer</code> </p> <code>axis1</code> <p>How to convert continuous values into an index for the second axis.</p> <p> TYPE: <code>Digitizer</code> </p> <code>cells</code> <p>Values to look up in the table.</p> <p> TYPE: <code>np.ndarray</code> </p> Source code in <code>navis/nbl/smat.py</code> <pre><code>def __init__(self, axis0: Digitizer, axis1: Digitizer, cells: np.ndarray):\n    \"\"\"2D lookup table for convert NBLAST matches to scores.\n\n    Commonly read from a `pandas.DataFrame`\n    or trained on data using a `LookupDistDotBuilder`.\n\n    Parameters\n    ----------\n    axis0 : Digitizer\n        How to convert continuous values into an index for the first axis.\n    axis1 : Digitizer\n        How to convert continuous values into an index for the second axis.\n    cells : np.ndarray\n        Values to look up in the table.\n    \"\"\"\n    super().__init__([axis0, axis1], cells)\n</code></pre>"},{"location":"reference/navis/nbl/ablast_funcs/#navis.nbl.ablast_funcs.Lookup2d.from_dataframe","title":"<code>from_dataframe</code>  <code>classmethod</code>","text":"<p>Parse score matrix from a dataframe with string index and column labels.</p> <p>Expects the index and column labels to specify an interval like <code>f\"[{{lower}},{{upper}})\"</code>. Will replace the lowermost and uppermost bound with -inf and inf if they are not already.</p> Source code in <code>navis/nbl/smat.py</code> <pre><code>@classmethod\ndef from_dataframe(cls, df: pd.DataFrame):\n    \"\"\"Parse score matrix from a dataframe with string index and column labels.\n\n    Expects the index and column labels to specify an interval\n    like `f\"[{{lower}},{{upper}})\"`.\n    Will replace the lowermost and uppermost bound with -inf and inf\n    if they are not already.\n    \"\"\"\n    return cls(\n        Digitizer.from_strings(df.index),\n        Digitizer.from_strings(df.columns),\n        df.to_numpy(),\n    )\n</code></pre>"},{"location":"reference/navis/nbl/ablast_funcs/#navis.nbl.ablast_funcs.Lookup2d.to_dataframe","title":"<code>to_dataframe</code>","text":"<p>Convert the lookup table into a <code>pandas.DataFrame</code>.</p> <p>From there, it can be shared, saved, and so on.</p> <p>The index and column labels describe the intervals represented by that axis.</p> RETURNS DESCRIPTION <code>pd.DataFrame</code> Source code in <code>navis/nbl/smat.py</code> <pre><code>def to_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Convert the lookup table into a `pandas.DataFrame`.\n\n    From there, it can be shared, saved, and so on.\n\n    The index and column labels describe the intervals represented by that axis.\n\n    Returns\n    -------\n    pd.DataFrame\n    \"\"\"\n    return pd.DataFrame(\n        self.cells,\n        self.axes[0].to_strings(),\n        self.axes[1].to_strings(),\n    )\n</code></pre>"},{"location":"reference/navis/nbl/ablast_funcs/#navis.nbl.ablast_funcs.NBlasterAlign","title":"<code>navis.nbl.ablast_funcs.NBlasterAlign</code>","text":"<p>Implements a version of NBLAST were neurons are first aligned.</p> <p>Please note that some properties are computed on initialization and changing parameters (e.g. <code>use_alpha</code>) at a later stage will mess things up!</p> PARAMETER DESCRIPTION <code>use_alpha</code> <pre><code>        Whether or not to use alpha values for the scoring.\n        If True, the dotproduct of nearest neighbor vectors will\n        be scaled by `sqrt(alpha1 * alpha2)`.\n</code></pre> <p> TYPE: <code>    bool</code> DEFAULT: <code>False</code> </p> <code>normalized</code> <pre><code>        If True, will normalize scores by the best possible score\n        (i.e. self-self) of the query neuron.\n</code></pre> <p> TYPE: <code>   bool</code> DEFAULT: <code>True</code> </p> <code>smat</code> <pre><code>        How to convert the point match pairs into an NBLAST score,\n        usually by a lookup table:\n         - if 'auto' (default), will use the \"official\" NBLAST scoring\n           matrices based on FCWB data. Same behaviour as in R's\n           nat.nblast implementation.\n         - if `smat='v1'` it uses the analytic formulation of the\n           NBLAST scoring from Kohl et. al (2013). You can adjust parameter\n           `sigma_scaling` (default to 10) using `smat_kwargs`.\n         - DataFrames will be used to build a `Lookup2d`\n         - if `Callable` given, it passes distance and dot products as\n           first and second argument respectively\n         - if `smat=None` the scores will be generated as the\n           product of the distances and the dotproduct of the vectors\n           of nearest-neighbor pairs\n</code></pre> <p> TYPE: <code>         navis.nbl.smat.Lookup2d | pd.DataFrame | str | Callable</code> DEFAULT: <code>'auto'</code> </p> <code>smat_kwargs</code> <pre><code>        functions. For example: `smat_kwargs[\"sigma_scoring\"] = 10`.\n</code></pre> <p> DEFAULT: <code>dict()</code> </p> <code>limit_dist</code> <pre><code>        Sets the max distance for the nearest neighbor search\n        (`distance_upper_bound`). Typically this should be the\n        highest distance considered by the scoring function. If\n        \"auto\", will extract that value from the first axis of the\n        scoring matrix.\n</code></pre> <p> TYPE: <code>   float | \"auto\" | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <pre><code>        If True, will show a progress bar.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> Source code in <code>navis/nbl/ablast_funcs.py</code> <pre><code>class NBlasterAlign(Blaster):\n    \"\"\"Implements a version of NBLAST were neurons are first aligned.\n\n    Please note that some properties are computed on initialization and\n    changing parameters (e.g. `use_alpha`) at a later stage will mess things\n    up!\n\n    Parameters\n    ----------\n    use_alpha :     bool\n                    Whether or not to use alpha values for the scoring.\n                    If True, the dotproduct of nearest neighbor vectors will\n                    be scaled by `sqrt(alpha1 * alpha2)`.\n    normalized :    bool\n                    If True, will normalize scores by the best possible score\n                    (i.e. self-self) of the query neuron.\n    smat :          navis.nbl.smat.Lookup2d | pd.DataFrame | str | Callable\n                    How to convert the point match pairs into an NBLAST score,\n                    usually by a lookup table:\n                     - if 'auto' (default), will use the \"official\" NBLAST scoring\n                       matrices based on FCWB data. Same behaviour as in R's\n                       nat.nblast implementation.\n                     - if `smat='v1'` it uses the analytic formulation of the\n                       NBLAST scoring from Kohl et. al (2013). You can adjust parameter\n                       `sigma_scaling` (default to 10) using `smat_kwargs`.\n                     - DataFrames will be used to build a `Lookup2d`\n                     - if `Callable` given, it passes distance and dot products as\n                       first and second argument respectively\n                     - if `smat=None` the scores will be generated as the\n                       product of the distances and the dotproduct of the vectors\n                       of nearest-neighbor pairs\n    smat_kwargs:    Dictionary with additional parameters passed to scoring\n                    functions. For example: `smat_kwargs[\"sigma_scoring\"] = 10`.\n    limit_dist :    float | \"auto\" | None\n                    Sets the max distance for the nearest neighbor search\n                    (`distance_upper_bound`). Typically this should be the\n                    highest distance considered by the scoring function. If\n                    \"auto\", will extract that value from the first axis of the\n                    scoring matrix.\n    progress :      bool\n                    If True, will show a progress bar.\n\n    \"\"\"\n\n    def __init__(self,\n                 align_func, two_way_align=True, sample_align=None,\n                 use_alpha=False, normalized=True, smat='auto',\n                 limit_dist=None, approx_nn=False, dtype=np.float64,\n                 progress=True,\n                 smat_kwargs=dict(),\n                 align_kwargs=dict(),\n                 dotprop_kwargs=dict(),\n                 ):\n        \"\"\"Initialize class.\"\"\"\n        super().__init__(progress=progress, dtype=dtype)\n        self.align_func = align_func\n        self.two_way_align = two_way_align\n        self.sample_align = sample_align\n        self.use_alpha = use_alpha\n        self.normalized = normalized\n        self.approx_nn = approx_nn\n        self.dotprop_kwargs = dotprop_kwargs\n        self.align_kwargs = align_kwargs\n        self.desc = \"NBlasting\"\n        self.self_hits = {}\n        self.dotprops = {}\n        self.neurons = []\n\n        if smat is None:\n            self.score_fn = operator.mul\n        elif smat == 'auto':\n            from ..nbl.smat import smat_fcwb\n            self.score_fn = smat_fcwb(self.use_alpha)\n        elif smat == 'v1':\n            self.score_fn = partial(\n                _nblast_v1_scoring, sigma_scoring = smat_kwargs.get('sigma_scoring', 10)\n            )\n        elif isinstance(smat, pd.DataFrame):\n            self.score_fn = Lookup2d.from_dataframe(smat)\n        else:\n            self.score_fn = smat\n\n        if limit_dist == \"auto\":\n            try:\n                if self.score_fn.axes[0].boundaries[-1] != np.inf:\n                    self.distance_upper_bound = self.score_fn.axes[0].boundaries[-1]\n                else:\n                    # If the right boundary is open (i.e. infinity), we will use\n                    # the second highest boundary plus a 5% offset\n                    self.distance_upper_bound = self.score_fn.axes[0].boundaries[-2] * 1.05\n            except AttributeError:\n                logger.warning(\"Could not infer distance upper bound from scoring function\")\n                self.distance_upper_bound = None\n        else:\n            self.distance_upper_bound = limit_dist\n\n    def append(self, neuron: core.BaseNeuron) -&gt; NestedIndices:\n        \"\"\"Append neurons.\n\n        Returns the numerical index appended neurons.\n        If neurons is a (possibly nested) sequence of neurons,\n        return a (possibly nested) list of indices.\n\n        Note that `self_hit` is ignored (and hence calculated from scratch)\n        when `neurons` is a nested list of dotprops.\n        \"\"\"\n        if isinstance(neuron, core.BaseNeuron):\n            return self._append_neuron(neuron)\n\n        try:\n            return [self.append(n) for n in neuron]\n        except TypeError:  # i.e. not iterable\n            raise ValueError(f\"Expected Neuron or iterable thereof; got {type(neuron)}\")\n\n    def _append_neuron(self, neuron: core.BaseNeuron) -&gt; int:\n        next_id = len(self)\n        self.neurons.append(neuron)\n        self.ids.append(neuron.id)\n        return next_id\n\n    def get_dotprop(self, ix):\n        if ix not in self.dotprops:\n            if not isinstance(self.neurons[ix], core.Dotprops):\n                self.dotprops[ix] = core.make_dotprops(self.neurons[ix],\n                                                    **self.dotprop_kwargs)\n            else:\n                self.dotprops[ix] = self.neurons[ix]\n        return self.dotprops[ix]\n\n    def get_self_hit(self, ix):\n        if ix not in self.self_hits:\n            self.self_hits[ix] = self.calc_self_hit(self.get_dotprop(ix))\n        return self.self_hits[ix]\n\n    def calc_self_hit(self, dotprops):\n        \"\"\"Non-normalized value for self hit.\"\"\"\n        if not self.use_alpha:\n            return len(dotprops.points) * self.score_fn(0, 1.0)\n        else:\n            dists = np.repeat(0, len(dotprops.points))\n            alpha = dotprops.alpha * dotprops.alpha\n            dots = np.repeat(1, len(dotprops.points)) * np.sqrt(alpha)\n            return self.score_fn(dists, dots).sum()\n\n    def score_single(self, q_dp, t_dp, q_idx):\n        \"\"\"Calculate score for single query/target dotprop pair.\"\"\"\n        # Run nearest-neighbor search for query against target\n        data = q_dp.dist_dots(t_dp,\n                              alpha=self.use_alpha,\n                              # eps=0.1 means we accept 10% inaccuracy\n                              eps=.1 if self.approx_nn else 0,\n                              distance_upper_bound=self.distance_upper_bound)\n\n        if self.use_alpha:\n            dists, dots, alpha = data\n            dots *= np.sqrt(alpha)\n        else:\n            dists, dots = data\n\n        scr = self.score_fn(dists, dots).sum()\n\n        # Normalize against best hit\n        if self.normalized:\n            scr /= self.get_self_hit(q_idx)\n\n        return scr\n\n    def single_query_target(self, q_idx: int, t_idx: int, scores='forward',\n                            allow_rev_align=True):\n        \"\"\"Query single query against single target.\"\"\"\n        # Take a short-cut if this is a self-self comparison\n        if q_idx == t_idx:\n            if self.normalized:\n                return 1\n            return self.get_self_hit(q_idx)\n\n        # Align the query to the target\n        q_xf = self.align_func(self.neurons[q_idx],\n                               target=self.neurons[t_idx],\n                               sample=self.sample_align,\n                               progress=False,\n                               **self.align_kwargs)[0][0]\n\n        # The query must always be made into new dotprops because it has been\n        # moved around\n        q_dp = core.make_dotprops(q_xf, **self.dotprop_kwargs)\n\n        # The target dotprop has to be compute only once\n        t_dp = self.get_dotprop(t_idx)\n\n        scr = self.score_single(q_dp, t_dp, q_idx)\n\n        # For the mean score we also have to produce the reverse score\n        if scores in ('mean', 'min', 'max', 'both'):\n            reverse = self.score_single(t_dp, q_dp, t_idx)\n            if scores == 'mean':\n                scr = (scr + reverse) / 2\n            elif scores == 'min':\n                scr = min(scr, reverse)\n            elif scores == 'max':\n                scr = max(scr, reverse)\n            elif scores == 'both':\n                # If both scores are requested\n                scr = [scr, reverse]\n\n        if self.two_way_align and allow_rev_align:\n            rev = self.single_query_target(t_idx, q_idx, scores=scores,\n                                           allow_rev_align=False)\n            scr = (scr + rev) / 2\n\n        return scr\n</code></pre>"},{"location":"reference/navis/nbl/ablast_funcs/#navis.nbl.ablast_funcs.NBlasterAlign.__init__","title":"<code>__init__</code>","text":"<p>Initialize class.</p> Source code in <code>navis/nbl/ablast_funcs.py</code> <pre><code>def __init__(self,\n             align_func, two_way_align=True, sample_align=None,\n             use_alpha=False, normalized=True, smat='auto',\n             limit_dist=None, approx_nn=False, dtype=np.float64,\n             progress=True,\n             smat_kwargs=dict(),\n             align_kwargs=dict(),\n             dotprop_kwargs=dict(),\n             ):\n    \"\"\"Initialize class.\"\"\"\n    super().__init__(progress=progress, dtype=dtype)\n    self.align_func = align_func\n    self.two_way_align = two_way_align\n    self.sample_align = sample_align\n    self.use_alpha = use_alpha\n    self.normalized = normalized\n    self.approx_nn = approx_nn\n    self.dotprop_kwargs = dotprop_kwargs\n    self.align_kwargs = align_kwargs\n    self.desc = \"NBlasting\"\n    self.self_hits = {}\n    self.dotprops = {}\n    self.neurons = []\n\n    if smat is None:\n        self.score_fn = operator.mul\n    elif smat == 'auto':\n        from ..nbl.smat import smat_fcwb\n        self.score_fn = smat_fcwb(self.use_alpha)\n    elif smat == 'v1':\n        self.score_fn = partial(\n            _nblast_v1_scoring, sigma_scoring = smat_kwargs.get('sigma_scoring', 10)\n        )\n    elif isinstance(smat, pd.DataFrame):\n        self.score_fn = Lookup2d.from_dataframe(smat)\n    else:\n        self.score_fn = smat\n\n    if limit_dist == \"auto\":\n        try:\n            if self.score_fn.axes[0].boundaries[-1] != np.inf:\n                self.distance_upper_bound = self.score_fn.axes[0].boundaries[-1]\n            else:\n                # If the right boundary is open (i.e. infinity), we will use\n                # the second highest boundary plus a 5% offset\n                self.distance_upper_bound = self.score_fn.axes[0].boundaries[-2] * 1.05\n        except AttributeError:\n            logger.warning(\"Could not infer distance upper bound from scoring function\")\n            self.distance_upper_bound = None\n    else:\n        self.distance_upper_bound = limit_dist\n</code></pre>"},{"location":"reference/navis/nbl/ablast_funcs/#navis.nbl.ablast_funcs.NBlasterAlign.append","title":"<code>append</code>","text":"<p>Append neurons.</p> <p>Returns the numerical index appended neurons. If neurons is a (possibly nested) sequence of neurons, return a (possibly nested) list of indices.</p> <p>Note that <code>self_hit</code> is ignored (and hence calculated from scratch) when <code>neurons</code> is a nested list of dotprops.</p> Source code in <code>navis/nbl/ablast_funcs.py</code> <pre><code>def append(self, neuron: core.BaseNeuron) -&gt; NestedIndices:\n    \"\"\"Append neurons.\n\n    Returns the numerical index appended neurons.\n    If neurons is a (possibly nested) sequence of neurons,\n    return a (possibly nested) list of indices.\n\n    Note that `self_hit` is ignored (and hence calculated from scratch)\n    when `neurons` is a nested list of dotprops.\n    \"\"\"\n    if isinstance(neuron, core.BaseNeuron):\n        return self._append_neuron(neuron)\n\n    try:\n        return [self.append(n) for n in neuron]\n    except TypeError:  # i.e. not iterable\n        raise ValueError(f\"Expected Neuron or iterable thereof; got {type(neuron)}\")\n</code></pre>"},{"location":"reference/navis/nbl/ablast_funcs/#navis.nbl.ablast_funcs.NBlasterAlign.calc_self_hit","title":"<code>calc_self_hit</code>","text":"<p>Non-normalized value for self hit.</p> Source code in <code>navis/nbl/ablast_funcs.py</code> <pre><code>def calc_self_hit(self, dotprops):\n    \"\"\"Non-normalized value for self hit.\"\"\"\n    if not self.use_alpha:\n        return len(dotprops.points) * self.score_fn(0, 1.0)\n    else:\n        dists = np.repeat(0, len(dotprops.points))\n        alpha = dotprops.alpha * dotprops.alpha\n        dots = np.repeat(1, len(dotprops.points)) * np.sqrt(alpha)\n        return self.score_fn(dists, dots).sum()\n</code></pre>"},{"location":"reference/navis/nbl/ablast_funcs/#navis.nbl.ablast_funcs.NBlasterAlign.score_single","title":"<code>score_single</code>","text":"<p>Calculate score for single query/target dotprop pair.</p> Source code in <code>navis/nbl/ablast_funcs.py</code> <pre><code>def score_single(self, q_dp, t_dp, q_idx):\n    \"\"\"Calculate score for single query/target dotprop pair.\"\"\"\n    # Run nearest-neighbor search for query against target\n    data = q_dp.dist_dots(t_dp,\n                          alpha=self.use_alpha,\n                          # eps=0.1 means we accept 10% inaccuracy\n                          eps=.1 if self.approx_nn else 0,\n                          distance_upper_bound=self.distance_upper_bound)\n\n    if self.use_alpha:\n        dists, dots, alpha = data\n        dots *= np.sqrt(alpha)\n    else:\n        dists, dots = data\n\n    scr = self.score_fn(dists, dots).sum()\n\n    # Normalize against best hit\n    if self.normalized:\n        scr /= self.get_self_hit(q_idx)\n\n    return scr\n</code></pre>"},{"location":"reference/navis/nbl/ablast_funcs/#navis.nbl.ablast_funcs.NBlasterAlign.single_query_target","title":"<code>single_query_target</code>","text":"<p>Query single query against single target.</p> Source code in <code>navis/nbl/ablast_funcs.py</code> <pre><code>def single_query_target(self, q_idx: int, t_idx: int, scores='forward',\n                        allow_rev_align=True):\n    \"\"\"Query single query against single target.\"\"\"\n    # Take a short-cut if this is a self-self comparison\n    if q_idx == t_idx:\n        if self.normalized:\n            return 1\n        return self.get_self_hit(q_idx)\n\n    # Align the query to the target\n    q_xf = self.align_func(self.neurons[q_idx],\n                           target=self.neurons[t_idx],\n                           sample=self.sample_align,\n                           progress=False,\n                           **self.align_kwargs)[0][0]\n\n    # The query must always be made into new dotprops because it has been\n    # moved around\n    q_dp = core.make_dotprops(q_xf, **self.dotprop_kwargs)\n\n    # The target dotprop has to be compute only once\n    t_dp = self.get_dotprop(t_idx)\n\n    scr = self.score_single(q_dp, t_dp, q_idx)\n\n    # For the mean score we also have to produce the reverse score\n    if scores in ('mean', 'min', 'max', 'both'):\n        reverse = self.score_single(t_dp, q_dp, t_idx)\n        if scores == 'mean':\n            scr = (scr + reverse) / 2\n        elif scores == 'min':\n            scr = min(scr, reverse)\n        elif scores == 'max':\n            scr = max(scr, reverse)\n        elif scores == 'both':\n            # If both scores are requested\n            scr = [scr, reverse]\n\n    if self.two_way_align and allow_rev_align:\n        rev = self.single_query_target(t_idx, q_idx, scores=scores,\n                                       allow_rev_align=False)\n        scr = (scr + rev) / 2\n\n    return scr\n</code></pre>"},{"location":"reference/navis/nbl/ablast_funcs/#navis.nbl.ablast_funcs.find_optimal_partition","title":"<code>navis.nbl.ablast_funcs.find_optimal_partition</code>","text":"<p>Find an optimal partition for given NBLAST query.</p> PARAMETER DESCRIPTION <code>N_cores</code> <pre><code>    Number of available cores.\n</code></pre> <p> TYPE: <code>  int</code> </p> <code>q</code> <pre><code>    Query and targets, respectively.\n</code></pre> <p> </p> RETURNS DESCRIPTION <code>(n_rows, n_cols)</code> Source code in <code>navis/nbl/nblast_funcs.py</code> <pre><code>def find_optimal_partition(N_cores, q, t):\n    \"\"\"Find an optimal partition for given NBLAST query.\n\n    Parameters\n    ----------\n    N_cores :   int\n                Number of available cores.\n    q,t :       NeuronList of Dotprops\n                Query and targets, respectively.\n\n    Returns\n    -------\n    n_rows, n_cols\n\n    \"\"\"\n    neurons_per_query = []\n    for n_rows in range(1, N_cores + 1):\n        # Skip splits we can't make\n        if N_cores % n_rows:\n            continue\n        if n_rows &gt; len(q):\n            continue\n\n        n_cols = min(int(N_cores / n_rows), len(t))\n\n        n_queries = len(q) / n_rows\n        n_targets = len(t) / n_cols\n\n        neurons_per_query.append([n_rows, n_cols, n_queries + n_targets])\n\n    # Find the optimal partition\n    neurons_per_query = np.array(neurons_per_query)\n    n_rows, n_cols = neurons_per_query[np.argmin(neurons_per_query[:, 2]), :2]\n\n    return int(n_rows), int(n_cols)\n</code></pre>"},{"location":"reference/navis/nbl/ablast_funcs/#navis.nbl.ablast_funcs.nblast_preflight","title":"<code>navis.nbl.ablast_funcs.nblast_preflight</code>","text":"<p>Run preflight checks for NBLAST.</p> Source code in <code>navis/nbl/nblast_funcs.py</code> <pre><code>def nblast_preflight(query, target, n_cores, batch_size=None,\n                     req_unique_ids=False, req_dotprops=True, req_points=True,\n                     req_microns=True):\n    \"\"\"Run preflight checks for NBLAST.\"\"\"\n    if req_dotprops:\n        if query.types != (Dotprops, ):\n            raise TypeError(f'`query` must be Dotprop(s), got \"{query.types}\". '\n                            'Use `navis.make_dotprops` to convert neurons.')\n\n        if target.types != (Dotprops, ):\n            raise TypeError(f'`target` must be Dotprop(s), got \"{target.types}\". '\n                            'Use `navis.make_dotprops` to convert neurons.')\n\n        if req_points:\n            no_points = query.n_points == 0\n            if any(no_points):\n                raise ValueError('Some query dotprops appear to have no points: '\n                                 f'{query.id[no_points]}')\n            no_points = target.n_points == 0\n            if any(no_points):\n                raise ValueError('Some target dotprops appear to have no points: '\n                                 f'{target.id[no_points]}')\n\n    if req_unique_ids:\n        # At the moment, neurons need to have a unique ID for things to work\n        if query.is_degenerated:\n            raise ValueError('Queries have non-unique IDs.')\n        if target.is_degenerated:\n            raise ValueError('Targets have non-unique IDs.')\n\n    # Check if query or targets are in microns\n    # Note this test can return `None` if it can't be determined\n    if req_microns:\n        if check_microns(query) is False:\n            logger.warning('NBLAST is optimized for data in microns and it looks '\n                           'like your queries are not in microns.')\n        if check_microns(target) is False:\n            logger.warning('NBLAST is optimized for data in microns and it looks '\n                           'like your targets are not in microns.')\n\n    if not isinstance(n_cores, numbers.Number) or n_cores &lt; 1:\n        raise TypeError('`n_cores` must be an integer &gt; 0')\n\n    n_cores = int(n_cores)\n    if n_cores &gt; 1 and n_cores % 2:\n        logger.warning('NBLAST is most efficient if `n_cores` is an even number')\n    elif n_cores &lt; 1:\n        raise ValueError('`n_cores` must not be smaller than 1')\n    elif n_cores &gt; os.cpu_count():\n        logger.warning('`n_cores` should not larger than the number of '\n                       'available cores')\n\n    if batch_size is not None:\n        if batch_size &lt;= 0:\n            raise ValueError('`batch_size` must be &gt;= 1 or `None`.')\n</code></pre>"},{"location":"reference/navis/nbl/ablast_funcs/#navis.nbl.ablast_funcs.set_omp_flag","title":"<code>navis.nbl.ablast_funcs.set_omp_flag</code>","text":"<p>Set OMP_NUM_THREADS flag to given value.</p> <p>This is to avoid pykdtree causing multiple layers of concurrency which will over-subcribe and slow down NBLAST on multi-core systems.</p> <p>Use as context manager!</p> Source code in <code>navis/nbl/nblast_funcs.py</code> <pre><code>def set_omp_flag(limits=1):\n    \"\"\"Set OMP_NUM_THREADS flag to given value.\n\n    This is to avoid pykdtree causing multiple layers of concurrency which\n    will over-subcribe and slow down NBLAST on multi-core systems.\n\n    Use as context manager!\n    \"\"\"\n    class OMPSetter:\n        def __init__(self, num_threads):\n            assert isinstance(num_threads, (int, type(None)))\n            self.num_threads = num_threads\n\n        def __enter__(self):\n            if self.num_threads is None:\n                return\n            # Track old value (if there is one)\n            self.old_value = os.environ.get('OMP_NUM_THREADS', None)\n            # Set flag\n            os.environ['OMP_NUM_THREADS'] = str(self.num_threads)\n\n        def __exit__(self, *args, **kwargs):\n            if self.num_threads is None:\n                return\n\n            # Reset flag\n            if self.old_value:\n                os.environ['OMP_NUM_THREADS'] = str(self.old_value)\n            else:\n                os.environ.pop('OMP_NUM_THREADS', None)\n\n    return OMPSetter(limits)\n</code></pre>"},{"location":"reference/navis/nbl/nblast_funcs/","title":"nblast_funcs","text":""},{"location":"reference/navis/nbl/nblast_funcs/#navis.nbl.nblast_funcs.NBlaster","title":"<code>navis.nbl.nblast_funcs.NBlaster</code>","text":"<p>Implements version 2 of the NBLAST algorithm.</p> <p>Please note that some properties are computed on initialization and changing parameters (e.g. <code>use_alpha</code>) at a later stage will mess things up!</p> PARAMETER DESCRIPTION <code>use_alpha</code> <pre><code>        Whether or not to use alpha values for the scoring.\n        If True, the dotproduct of nearest neighbor vectors will\n        be scaled by `sqrt(alpha1 * alpha2)`.\n</code></pre> <p> TYPE: <code>    bool</code> DEFAULT: <code>False</code> </p> <code>normalized</code> <pre><code>        If True, will normalize scores by the best possible score\n        (i.e. self-self) of the query neuron.\n</code></pre> <p> TYPE: <code>   bool</code> DEFAULT: <code>True</code> </p> <code>smat</code> <pre><code>        How to convert the point match pairs into an NBLAST score,\n        usually by a lookup table:\n         - if 'auto' (default), will use the \"official\" NBLAST scoring\n           matrices based on FCWB data. Same behaviour as in R's\n           nat.nblast implementation.\n         - if `smat='v1'` it uses the analytic formulation of the\n           NBLAST scoring from Kohl et. al (2013). You can adjust parameter\n           `sigma_scaling` (default to 10) using `smat_kwargs`.\n         - DataFrames will be used to build a `Lookup2d`\n         - if `Callable` given, it passes distance and dot products as\n           first and second argument respectively\n         - if `smat=None` the scores will be generated as the\n           product of the distances and the dotproduct of the vectors\n           of nearest-neighbor pairs\n</code></pre> <p> TYPE: <code>         navis.nbl.smat.Lookup2d | pd.DataFrame | str | Callable</code> DEFAULT: <code>'auto'</code> </p> <code>smat_kwargs</code> <pre><code>        functions. For example: `smat_kwargs[\"sigma_scoring\"] = 10`.\n</code></pre> <p> DEFAULT: <code>dict()</code> </p> <code>limit_dist</code> <pre><code>        Sets the max distance for the nearest neighbor search\n        (`distance_upper_bound`). Typically this should be the\n        highest distance considered by the scoring function. If\n        \"auto\", will extract that value from the first axis of the\n        scoring matrix.\n</code></pre> <p> TYPE: <code>   float | \"auto\" | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <pre><code>        If True, will show a progress bar.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> Source code in <code>navis/nbl/nblast_funcs.py</code> <pre><code>class NBlaster(Blaster):\n    \"\"\"Implements version 2 of the NBLAST algorithm.\n\n    Please note that some properties are computed on initialization and\n    changing parameters (e.g. `use_alpha`) at a later stage will mess things\n    up!\n\n    Parameters\n    ----------\n    use_alpha :     bool\n                    Whether or not to use alpha values for the scoring.\n                    If True, the dotproduct of nearest neighbor vectors will\n                    be scaled by `sqrt(alpha1 * alpha2)`.\n    normalized :    bool\n                    If True, will normalize scores by the best possible score\n                    (i.e. self-self) of the query neuron.\n    smat :          navis.nbl.smat.Lookup2d | pd.DataFrame | str | Callable\n                    How to convert the point match pairs into an NBLAST score,\n                    usually by a lookup table:\n                     - if 'auto' (default), will use the \"official\" NBLAST scoring\n                       matrices based on FCWB data. Same behaviour as in R's\n                       nat.nblast implementation.\n                     - if `smat='v1'` it uses the analytic formulation of the\n                       NBLAST scoring from Kohl et. al (2013). You can adjust parameter\n                       `sigma_scaling` (default to 10) using `smat_kwargs`.\n                     - DataFrames will be used to build a `Lookup2d`\n                     - if `Callable` given, it passes distance and dot products as\n                       first and second argument respectively\n                     - if `smat=None` the scores will be generated as the\n                       product of the distances and the dotproduct of the vectors\n                       of nearest-neighbor pairs\n    smat_kwargs:    Dictionary with additional parameters passed to scoring\n                    functions. For example: `smat_kwargs[\"sigma_scoring\"] = 10`.\n    limit_dist :    float | \"auto\" | None\n                    Sets the max distance for the nearest neighbor search\n                    (`distance_upper_bound`). Typically this should be the\n                    highest distance considered by the scoring function. If\n                    \"auto\", will extract that value from the first axis of the\n                    scoring matrix.\n    progress :      bool\n                    If True, will show a progress bar.\n\n    \"\"\"\n\n    def __init__(self, use_alpha=False, normalized=True, smat='auto',\n                 limit_dist=None, approx_nn=False, dtype=np.float64,\n                 progress=True, smat_kwargs=dict()):\n        \"\"\"Initialize class.\"\"\"\n        super().__init__(progress=progress, dtype=dtype)\n        self.use_alpha = use_alpha\n        self.normalized = normalized\n        self.approx_nn = approx_nn\n        self.desc = \"NBlasting\"\n\n        if smat is None:\n            self.score_fn = operator.mul\n        elif isinstance(smat, pd.DataFrame):\n            self.score_fn = Lookup2d.from_dataframe(smat)\n        elif isinstance(smat, str) and smat == 'auto':\n            self.score_fn = smat_fcwb(self.use_alpha)\n        elif isinstance(smat, str) and smat == 'v1':\n            self.score_fn = partial(\n                _nblast_v1_scoring, sigma_scoring = smat_kwargs.get('sigma_scoring', 10)\n            )\n        else:\n            self.score_fn = smat\n\n        if limit_dist == \"auto\":\n            try:\n                if self.score_fn.axes[0].boundaries[-1] != np.inf:\n                    self.distance_upper_bound = self.score_fn.axes[0].boundaries[-1]\n                else:\n                    # If the right boundary is open (i.e. infinity), we will use\n                    # the second highest boundary plus a 5% offset\n                    self.distance_upper_bound = self.score_fn.axes[0].boundaries[-2] * 1.05\n            except AttributeError:\n                logger.warning(\"Could not infer distance upper bound from scoring function\")\n                self.distance_upper_bound = None\n        else:\n            self.distance_upper_bound = limit_dist\n\n    def append(self, dotprops: Dotprops, self_hit: Optional[float] = None) -&gt; NestedIndices:\n        \"\"\"Append dotprops.\n\n        Returns the numerical index appended dotprops.\n        If dotprops is a (possibly nested) sequence of dotprops,\n        return a (possibly nested) list of indices.\n\n        Note that `self_hit` is ignored (and hence calculated from scratch)\n        when `dotprops` is a nested list of dotprops.\n        \"\"\"\n        if isinstance(dotprops, Dotprops):\n            return self._append_dotprops(dotprops, self_hit=self_hit)\n\n        try:\n            return [self.append(n) for n in dotprops]\n        except TypeError:  # i.e. not iterable\n            raise ValueError(f\"Expected Dotprops or iterable thereof; got {type(dotprops)}\")\n\n    def _append_dotprops(self, dotprops: Dotprops, self_hit: Optional[float] = None) -&gt; int:\n        next_id = len(self)\n        self.neurons.append(dotprops)\n        self.ids.append(dotprops.id)\n        # Calculate score for self hit\n        if not self_hit:\n            self.self_hits.append(self.calc_self_hit(dotprops))\n        else:\n            self.self_hits.append(self_hit)\n        return next_id\n\n    def calc_self_hit(self, dotprops):\n        \"\"\"Non-normalized value for self hit.\"\"\"\n        if not self.use_alpha:\n            return len(dotprops.points) * self.score_fn(0, 1.0)\n        else:\n            dists = np.repeat(0, len(dotprops.points))\n            alpha = dotprops.alpha * dotprops.alpha\n            dots = np.repeat(1, len(dotprops.points)) * np.sqrt(alpha)\n            return self.score_fn(dists, dots).sum()\n\n    def single_query_target(self, q_idx: int, t_idx: int, scores='forward'):\n        \"\"\"Query single target against single target.\"\"\"\n        # Take a short-cut if this is a self-self comparison\n        if q_idx == t_idx:\n            if self.normalized:\n                return 1\n            return self.self_hits[q_idx]\n\n        # Run nearest-neighbor search for query against target\n        data = self.neurons[q_idx].dist_dots(self.neurons[t_idx],\n                                             alpha=self.use_alpha,\n                                             # eps=0.1 means we accept 10% inaccuracy\n                                             eps=.1 if self.approx_nn else 0,\n                                             distance_upper_bound=self.distance_upper_bound)\n        if self.use_alpha:\n            dists, dots, alpha = data\n            dots *= np.sqrt(alpha)\n        else:\n            dists, dots = data\n\n        scr = self.score_fn(dists, dots).sum()\n\n        # Normalize against best hit\n        if self.normalized:\n            scr /= self.self_hits[q_idx]\n\n        # For the mean score we also have to produce the reverse score\n        if scores in ('mean', 'min', 'max', 'both'):\n            reverse = self.single_query_target(t_idx, q_idx, scores='forward')\n            if scores == 'mean':\n                scr = (scr + reverse) / 2\n            elif scores == 'min':\n                scr = min(scr, reverse)\n            elif scores == 'max':\n                scr = max(scr, reverse)\n            elif scores == 'both':\n                # If both scores are requested\n                scr = [scr, reverse]\n\n        return scr\n</code></pre>"},{"location":"reference/navis/nbl/nblast_funcs/#navis.nbl.nblast_funcs.NBlaster.__init__","title":"<code>__init__</code>","text":"<p>Initialize class.</p> Source code in <code>navis/nbl/nblast_funcs.py</code> <pre><code>def __init__(self, use_alpha=False, normalized=True, smat='auto',\n             limit_dist=None, approx_nn=False, dtype=np.float64,\n             progress=True, smat_kwargs=dict()):\n    \"\"\"Initialize class.\"\"\"\n    super().__init__(progress=progress, dtype=dtype)\n    self.use_alpha = use_alpha\n    self.normalized = normalized\n    self.approx_nn = approx_nn\n    self.desc = \"NBlasting\"\n\n    if smat is None:\n        self.score_fn = operator.mul\n    elif isinstance(smat, pd.DataFrame):\n        self.score_fn = Lookup2d.from_dataframe(smat)\n    elif isinstance(smat, str) and smat == 'auto':\n        self.score_fn = smat_fcwb(self.use_alpha)\n    elif isinstance(smat, str) and smat == 'v1':\n        self.score_fn = partial(\n            _nblast_v1_scoring, sigma_scoring = smat_kwargs.get('sigma_scoring', 10)\n        )\n    else:\n        self.score_fn = smat\n\n    if limit_dist == \"auto\":\n        try:\n            if self.score_fn.axes[0].boundaries[-1] != np.inf:\n                self.distance_upper_bound = self.score_fn.axes[0].boundaries[-1]\n            else:\n                # If the right boundary is open (i.e. infinity), we will use\n                # the second highest boundary plus a 5% offset\n                self.distance_upper_bound = self.score_fn.axes[0].boundaries[-2] * 1.05\n        except AttributeError:\n            logger.warning(\"Could not infer distance upper bound from scoring function\")\n            self.distance_upper_bound = None\n    else:\n        self.distance_upper_bound = limit_dist\n</code></pre>"},{"location":"reference/navis/nbl/nblast_funcs/#navis.nbl.nblast_funcs.NBlaster.append","title":"<code>append</code>","text":"<p>Append dotprops.</p> <p>Returns the numerical index appended dotprops. If dotprops is a (possibly nested) sequence of dotprops, return a (possibly nested) list of indices.</p> <p>Note that <code>self_hit</code> is ignored (and hence calculated from scratch) when <code>dotprops</code> is a nested list of dotprops.</p> Source code in <code>navis/nbl/nblast_funcs.py</code> <pre><code>def append(self, dotprops: Dotprops, self_hit: Optional[float] = None) -&gt; NestedIndices:\n    \"\"\"Append dotprops.\n\n    Returns the numerical index appended dotprops.\n    If dotprops is a (possibly nested) sequence of dotprops,\n    return a (possibly nested) list of indices.\n\n    Note that `self_hit` is ignored (and hence calculated from scratch)\n    when `dotprops` is a nested list of dotprops.\n    \"\"\"\n    if isinstance(dotprops, Dotprops):\n        return self._append_dotprops(dotprops, self_hit=self_hit)\n\n    try:\n        return [self.append(n) for n in dotprops]\n    except TypeError:  # i.e. not iterable\n        raise ValueError(f\"Expected Dotprops or iterable thereof; got {type(dotprops)}\")\n</code></pre>"},{"location":"reference/navis/nbl/nblast_funcs/#navis.nbl.nblast_funcs.NBlaster.calc_self_hit","title":"<code>calc_self_hit</code>","text":"<p>Non-normalized value for self hit.</p> Source code in <code>navis/nbl/nblast_funcs.py</code> <pre><code>def calc_self_hit(self, dotprops):\n    \"\"\"Non-normalized value for self hit.\"\"\"\n    if not self.use_alpha:\n        return len(dotprops.points) * self.score_fn(0, 1.0)\n    else:\n        dists = np.repeat(0, len(dotprops.points))\n        alpha = dotprops.alpha * dotprops.alpha\n        dots = np.repeat(1, len(dotprops.points)) * np.sqrt(alpha)\n        return self.score_fn(dists, dots).sum()\n</code></pre>"},{"location":"reference/navis/nbl/nblast_funcs/#navis.nbl.nblast_funcs.NBlaster.single_query_target","title":"<code>single_query_target</code>","text":"<p>Query single target against single target.</p> Source code in <code>navis/nbl/nblast_funcs.py</code> <pre><code>def single_query_target(self, q_idx: int, t_idx: int, scores='forward'):\n    \"\"\"Query single target against single target.\"\"\"\n    # Take a short-cut if this is a self-self comparison\n    if q_idx == t_idx:\n        if self.normalized:\n            return 1\n        return self.self_hits[q_idx]\n\n    # Run nearest-neighbor search for query against target\n    data = self.neurons[q_idx].dist_dots(self.neurons[t_idx],\n                                         alpha=self.use_alpha,\n                                         # eps=0.1 means we accept 10% inaccuracy\n                                         eps=.1 if self.approx_nn else 0,\n                                         distance_upper_bound=self.distance_upper_bound)\n    if self.use_alpha:\n        dists, dots, alpha = data\n        dots *= np.sqrt(alpha)\n    else:\n        dists, dots = data\n\n    scr = self.score_fn(dists, dots).sum()\n\n    # Normalize against best hit\n    if self.normalized:\n        scr /= self.self_hits[q_idx]\n\n    # For the mean score we also have to produce the reverse score\n    if scores in ('mean', 'min', 'max', 'both'):\n        reverse = self.single_query_target(t_idx, q_idx, scores='forward')\n        if scores == 'mean':\n            scr = (scr + reverse) / 2\n        elif scores == 'min':\n            scr = min(scr, reverse)\n        elif scores == 'max':\n            scr = max(scr, reverse)\n        elif scores == 'both':\n            # If both scores are requested\n            scr = [scr, reverse]\n\n    return scr\n</code></pre>"},{"location":"reference/navis/nbl/nblast_funcs/#navis.nbl.nblast_funcs.align_dtypes","title":"<code>navis.nbl.nblast_funcs.align_dtypes</code>","text":"<p>Align data types of dotprops.</p> PARAMETER DESCRIPTION <code>*x</code> <p> TYPE: <code>       Dotprops | NeuronLists thereof</code> DEFAULT: <code>()</code> </p> <code>downcast</code> <pre><code>    If True, will downcast all points to the lowest precision\n    dtype.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>False</code> </p> <code>inplace</code> <pre><code>    If True, will modify the original neuron objects. If False, will\n    make a copy before changing dtypes.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>*x</code> <p>Input data with aligned dtypes.</p> Source code in <code>navis/nbl/nblast_funcs.py</code> <pre><code>def align_dtypes(*x, downcast=False, inplace=True):\n    \"\"\"Align data types of dotprops.\n\n    Parameters\n    ----------\n    *x :        Dotprops | NeuronLists thereof\n    downcast :  bool\n                If True, will downcast all points to the lowest precision\n                dtype.\n    inplace :   bool\n                If True, will modify the original neuron objects. If False, will\n                make a copy before changing dtypes.\n\n    Returns\n    -------\n    *x\n                Input data with aligned dtypes.\n\n    \"\"\"\n    dtypes = get_dtypes(x)\n\n    if len(dtypes) == 1:\n        return x\n\n    if not inplace:\n        for i in range(x):\n            x[i] = x[i].copy()\n\n    if downcast:\n        target = lowest_type(*x)\n    else:\n        target = np.result_type(*dtypes)\n\n    for n in x:\n        if isinstance(n, NeuronList):\n            for i in range(len(n)):\n                n[i].points = n[i].points.astype(target, copy=False)\n        elif isinstance(n, Dotprops):\n            n.points = n.points.astype(target, copy=False)\n        else:\n            raise TypeError(f'Unable to process \"{type(n)}\"')\n\n    return x\n</code></pre>"},{"location":"reference/navis/nbl/nblast_funcs/#navis.nbl.nblast_funcs.check_microns","title":"<code>navis.nbl.nblast_funcs.check_microns</code>","text":"<p>Check if neuron data is in microns.</p> <p>Returns either [True, None (=unclear), False]</p> Source code in <code>navis/nbl/nblast_funcs.py</code> <pre><code>def check_microns(x):\n    \"\"\"Check if neuron data is in microns.\n\n    Returns either [True, None (=unclear), False]\n    \"\"\"\n    microns = [config.ureg.Unit('microns'),\n               config.ureg.Unit('um'),\n               config.ureg.Unit('micrometer'),\n               config.ureg.Unit('dimensionless')]\n    if not isinstance(x, NeuronList):\n        x = NeuronList(x)\n\n    # For very large NeuronLists converting the unit string to pint units is\n    # the time consuming step. Here we will first reduce to unique units:\n    unit_str = []\n    for n in x:\n        if isinstance(n._unit_str, str):\n            unit_str.append(n._unit_str)\n        else:\n            unit_str += list(n._unit_str)\n    unit_str = np.unique(unit_str)\n\n    any_not_microns = False\n    all_units = True\n    for u in unit_str:\n        # If not a unit (i.e. `None`)\n        if not u:\n            all_units = False\n            continue\n\n        # Convert to proper unit\n        u = config.ureg(u).to_compact().units\n\n        if u not in microns:\n            any_not_microns = True\n\n    if any_not_microns:\n        return False\n    elif all_units:\n        return True\n    return None\n</code></pre>"},{"location":"reference/navis/nbl/nblast_funcs/#navis.nbl.nblast_funcs.check_pykdtree_flag","title":"<code>navis.nbl.nblast_funcs.check_pykdtree_flag</code>","text":"<p>Check if pykdtree is used and if the OMP_NUM_THREADS flag is set.</p> <p>The issue is that on Linux pykdtree uses threads by default which causes issues when we're also using multiple cores (= multiple layers of concurrency).</p> Source code in <code>navis/nbl/nblast_funcs.py</code> <pre><code>def check_pykdtree_flag():\n    \"\"\"Check if pykdtree is used and if the OMP_NUM_THREADS flag is set.\n\n    The issue is that on Linux pykdtree uses threads by default which causes\n    issues when we're also using multiple cores (= multiple layers of concurrency).\n    \"\"\"\n    # This is only relevant for Linux (unless someone compiled pykdtree\n    # themselves using a compiler that supports openmp)\n    from sys import platform\n    if platform not in (\"linux\", \"linux2\"):\n        return\n\n    # See if pykdtree is present\n    try:\n        import pykdtree\n    except ModuleNotFoundError:\n        # If not present, just return\n        return\n\n    import os\n    if os.environ.get('OMP_NUM_THREADS', None) != \"1\":\n        msg = ('`OMP_NUM_THREADS` environment variable not set to 1. This may '\n               'result in multiple layers of concurrency which in turn will '\n               'slow down NBLAST when using multiple cores. '\n               'See also https://github.com/navis-org/navis/issues/49')\n        logger.warning(msg)\n</code></pre>"},{"location":"reference/navis/nbl/nblast_funcs/#navis.nbl.nblast_funcs.demote_types","title":"<code>navis.nbl.nblast_funcs.demote_types</code>","text":"<p>Determine the lower of two dtypes.</p> Source code in <code>navis/nbl/nblast_funcs.py</code> <pre><code>def demote_types(a, b):\n    \"\"\"Determine the lower of two dtypes.\"\"\"\n    if isinstance(a, np.ndarray):\n        a = a.dtype\n    if isinstance(b, np.ndarray):\n        b = b.dtype\n\n    # No change is same\n    if a == b:\n        return a\n\n    # First, get the \"higher\" type\n    higher = np.promote_types(a, b)\n    # Now get the one that's not higher\n    if a != higher:\n        return a\n    return b\n</code></pre>"},{"location":"reference/navis/nbl/nblast_funcs/#navis.nbl.nblast_funcs.eval_limit_dist","title":"<code>navis.nbl.nblast_funcs.eval_limit_dist</code>","text":"<p>Evaluate <code>limit_dist</code> parameter.</p> Source code in <code>navis/nbl/nblast_funcs.py</code> <pre><code>def eval_limit_dist(x):\n    \"\"\"Evaluate `limit_dist` parameter.\"\"\"\n    if x == 'auto':\n        return\n    if isinstance(x, type(None)):\n        return\n    if isinstance(x, numbers.Number):\n        return\n\n    raise ValueError(f'`limit_dist` must be None, \"auto\" or float, got {x}' )\n</code></pre>"},{"location":"reference/navis/nbl/nblast_funcs/#navis.nbl.nblast_funcs.find_batch_partition","title":"<code>navis.nbl.nblast_funcs.find_batch_partition</code>","text":"<p>Find partitions such that each batch takes about <code>T</code> seconds.</p> PARAMETER DESCRIPTION <code>q</code> <pre><code>    Query and targets, respectively.\n</code></pre> <p> </p> <code>T</code> <pre><code>    Time (in seconds) to aim for.\n</code></pre> <p> TYPE: <code>        int</code> DEFAULT: <code>10</code> </p> <code>n_cores</code> <pre><code>    Number of cores that will be used. If provided, will try to\n    make sure that (n_rows * n_cols) is a multiple of n_cores by\n    increasing the number of rows (thereby decreasing the time\n    per batch).\n</code></pre> <p> TYPE: <code>  int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>(n_rows, n_cols)</code> Source code in <code>navis/nbl/nblast_funcs.py</code> <pre><code>def find_batch_partition(q, t, T=10, n_cores=None):\n    \"\"\"Find partitions such that each batch takes about `T` seconds.\n\n    Parameters\n    ----------\n    q,t :       NeuronList of Dotprops\n                Query and targets, respectively.\n    T :         int\n                Time (in seconds) to aim for.\n    n_cores :   int, optional\n                Number of cores that will be used. If provided, will try to\n                make sure that (n_rows * n_cols) is a multiple of n_cores by\n                increasing the number of rows (thereby decreasing the time\n                per batch).\n\n    Returns\n    -------\n    n_rows, n_cols\n\n    \"\"\"\n    # Test a single query\n    time_per_query = test_single_query_time(q, t)\n\n    # Number of queries per job such that each job runs in `T` second\n    queries_per_batch = T / time_per_query\n\n    # Number of neurons per batch\n    neurons_per_batch  = max(1, int(np.sqrt(queries_per_batch)))\n\n    n_rows = max(1, len(q) // neurons_per_batch)\n    n_cols = max(1, len(t) // neurons_per_batch)\n\n    if n_cores and ((n_rows * n_cols) &gt; n_cores):\n        while (n_rows * n_cols) % n_cores:\n            n_rows += 1\n\n    return n_rows, n_cols\n</code></pre>"},{"location":"reference/navis/nbl/nblast_funcs/#navis.nbl.nblast_funcs.force_dotprops","title":"<code>navis.nbl.nblast_funcs.force_dotprops</code>","text":"<p>Force data into Dotprops.</p> Source code in <code>navis/nbl/nblast_funcs.py</code> <pre><code>def force_dotprops(x, k, resample, progress=False):\n    \"\"\"Force data into Dotprops.\"\"\"\n    if isinstance(x, (NeuronList, list)):\n        dp = [force_dotprops(n, k, resample) for n in config.tqdm(x,\n                                                                  desc='Dotprops',\n                                                                  disable=not progress,\n                                                                  leave=False)]\n        return NeuronList(dp)\n\n    # Try converting non-Dotprops\n    if not isinstance(x, Dotprops):\n        return make_dotprops(x, k=k, resample=resample)\n\n    # Return Dotprops\n    return x\n</code></pre>"},{"location":"reference/navis/nbl/nblast_funcs/#navis.nbl.nblast_funcs.get_dtypes","title":"<code>navis.nbl.nblast_funcs.get_dtypes</code>","text":"<p>Collect data types of dotprops points.</p> Source code in <code>navis/nbl/nblast_funcs.py</code> <pre><code>def get_dtypes(*x):\n    \"\"\"Collect data types of dotprops points.\"\"\"\n    dtypes = set()\n    for n in x:\n        if isinstance(n, NeuronList):\n            dtypes = dtypes | get_dtypes(n)\n        elif isinstance(n, Dotprops):\n            dtypes.add(n.points.dtype)\n        else:\n            raise TypeError(f'Unable to process \"{type(n)}\"')\n    return dtypes\n</code></pre>"},{"location":"reference/navis/nbl/nblast_funcs/#navis.nbl.nblast_funcs.lowest_type","title":"<code>navis.nbl.nblast_funcs.lowest_type</code>","text":"<p>Find the lowest data type.</p> Source code in <code>navis/nbl/nblast_funcs.py</code> <pre><code>def lowest_type(*x):\n    \"\"\"Find the lowest data type.\"\"\"\n    dtypes = get_dtypes(x)\n\n    if len(dtypes) == 1:\n        return dtypes[0]\n\n    lowest = dtypes[0]\n    for dt in dtypes[1:]:\n        lowest = demote_types(lowest, dt)\n\n    return lowest\n</code></pre>"},{"location":"reference/navis/nbl/nblast_funcs/#navis.nbl.nblast_funcs.sim_to_dist","title":"<code>navis.nbl.nblast_funcs.sim_to_dist</code>","text":"<p>Convert similarity scores to distances.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>Similarity score matrix to invert.\n</code></pre> <p> TYPE: <code>    (M, M) np.ndarray | pandas.DataFrame</code> </p> RETURNS DESCRIPTION <code>distances</code> Source code in <code>navis/nbl/nblast_funcs.py</code> <pre><code>def sim_to_dist(x):\n    \"\"\"Convert similarity scores to distances.\n\n    Parameters\n    ----------\n    x :     (M, M) np.ndarray | pandas.DataFrame\n            Similarity score matrix to invert.\n\n    Returns\n    -------\n    distances\n\n    \"\"\"\n    if not isinstance(x, (np.ndarray, pd.DataFrame)):\n        raise TypeError(f'Expected numpy array or pandas DataFrame, got \"{type(x)}\"')\n\n    if isinstance(x, pd.DataFrame):\n        mx = x.values.max()\n    else:\n        mx = x.max()\n\n    return (x - mx) * -1\n</code></pre>"},{"location":"reference/navis/nbl/nblast_funcs/#navis.nbl.nblast_funcs.smat_fcwb","title":"<code>navis.nbl.nblast_funcs.smat_fcwb</code>","text":"Source code in <code>navis/nbl/smat.py</code> <pre><code>def smat_fcwb(alpha=False):\n    # deepcopied so that mutations do not propagate to cache\n    return deepcopy(_smat_fcwb(alpha))\n</code></pre>"},{"location":"reference/navis/nbl/nblast_funcs/#navis.nbl.nblast_funcs.test_single_query_time","title":"<code>navis.nbl.nblast_funcs.test_single_query_time</code>","text":"<p>Test average time of a single NBLAST query.</p> Source code in <code>navis/nbl/nblast_funcs.py</code> <pre><code>def test_single_query_time(q, t, it=100):\n    \"\"\"Test average time of a single NBLAST query.\"\"\"\n    # Get a median-sized query and target\n    q_ix = np.argsort(q.n_points)[len(q)//2]\n    t_ix = np.argsort(t.n_points)[len(t)//2]\n\n    # Run a quick single query benchmark\n    timings = []\n    for i in range(it):  # Run N tests\n        s = time.time()\n        _ = t[t_ix].dist_dots(q[q_ix])  # Dist dot (ignore scoring / normalizing)\n        timings.append(time.time() - s)\n    return np.mean(timings)  # seconds per medium sized query\n</code></pre>"},{"location":"reference/navis/nbl/smat/","title":"smat","text":""},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.Digitizer","title":"<code>navis.nbl.smat.Digitizer</code>","text":"<p>Class converting continuous values into discrete indices.</p> PARAMETER DESCRIPTION <code>boundaries</code> <p>N boundaries specifying N-1 bins. Must be monotonically increasing.</p> <p> TYPE: <code>Sequence[float]</code> </p> <code>clip</code> <p>Whether to set the bottom and top boundaries to -infinity and infinity respectively, effectively clipping incoming values: by default (True, True). False means \"add a new bin for out-of-range values\".</p> <p> TYPE: <code>Tuple[bool, bool]</code> DEFAULT: <code>(True, True)</code> </p> <code>right</code> <p>Whether bins should include their right (rather than left) boundary, by default False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>navis/nbl/smat.py</code> <pre><code>class Digitizer(LookupAxis[float]):\n    \"\"\"Class converting continuous values into discrete indices.\n\n    Parameters\n    ----------\n    boundaries : Sequence[float]\n        N boundaries specifying N-1 bins.\n        Must be monotonically increasing.\n    clip : Tuple[bool, bool], optional\n        Whether to set the bottom and top boundaries to -infinity and\n        infinity respectively, effectively clipping incoming values: by\n        default (True, True).\n        False means \"add a new bin for out-of-range values\".\n    right : bool, optional\n        Whether bins should include their right (rather than left) boundary,\n        by default False.\n    \"\"\"\n\n    def __init__(\n        self,\n        boundaries: Sequence[float],\n        clip: Tuple[bool, bool] = (True, True),\n        right=False,\n    ):\n        self.right = right\n\n        boundaries = list(boundaries)\n        self._min = -math.inf\n        if clip[0]:\n            self._min = boundaries[0]\n            boundaries[0] = -math.inf\n        elif boundaries[0] != -math.inf:\n            self._min = -math.inf\n            boundaries.insert(0, -math.inf)\n\n        self._max = math.inf\n        if clip[1]:\n            self._max = boundaries[-1]\n            boundaries[-1] = math.inf\n        elif boundaries[-1] != math.inf:\n            boundaries.append(math.inf)\n\n        if not is_monotonically_increasing(boundaries):\n            raise ValueError(\n                \"Boundaries are not monotonically increasing: \" f\"{boundaries}\"\n            )\n\n        self.boundaries = np.asarray(boundaries)\n\n    def __len__(self):\n        return len(self.boundaries) - 1\n\n    def __call__(self, value: float):\n        # searchsorted is marginally faster than digitize as it skips monotonicity checks\n        return (\n            np.searchsorted(\n                self.boundaries, value, side=\"left\" if self.right else \"right\"\n            )\n            - 1\n        )\n\n    def to_strings(self, round=None) -&gt; List[str]:\n        \"\"\"Turn boundaries into list of labels.\n\n        Parameters\n        ----------\n        round :     int, optional\n                    Use to round bounds to the Nth decimal.\n        \"\"\"\n        if self.right:\n            lb = \"(\"\n            rb = \"]\"\n        else:\n            lb = \"[\"\n            rb = \")\"\n\n        b = self.boundaries.copy()\n        b[0] = self._min\n        b[-1] = self._max\n\n        if round:\n            b = [np.round(x, round) for x in b]\n\n        return [f\"{lb}{lower},{upper}{rb}\" for lower, upper in zip(b[:-1], b[1:])]\n\n    @classmethod\n    def from_strings(cls, interval_strs: Sequence[str]):\n        \"\"\"Set digitizer boundaries based on a sequence of interval expressions.\n\n        e.g. `[\"(0, 1]\", \"(1, 5]\", \"(5, 10]\"]`\n\n        The lowermost and uppermost boundaries are converted to -infinity and\n        infinity respectively.\n\n        Parameters\n        ----------\n        bound_strs : Sequence[str]\n            Strings representing intervals, which must abut and have open/closed\n            boundaries specified by brackets.\n\n        Returns\n        -------\n        Digitizer\n        \"\"\"\n        bounds: List[float] = []\n        last_upper = None\n        last_right = None\n        for item in interval_strs:\n            (lower, upper), right = parse_boundary(item)\n            bounds.append(float(lower))\n\n            if last_right is not None:\n                if right != last_right:\n                    raise ValueError(\"Inconsistent half-open interval\")\n            else:\n                last_right = right\n\n            if last_upper is not None:\n                if lower != last_upper:\n                    raise ValueError(\"Half-open intervals do not abut\")\n\n            last_upper = upper\n\n        bounds.append(float(last_upper))\n        return cls(bounds, right=last_right)\n\n    @classmethod\n    def from_linear(cls, lower: float, upper: float, nbins: int, right=False):\n        \"\"\"Choose digitizer boundaries spaced linearly between two values.\n\n        Input values will be clipped to fit within the given interval.\n\n        Parameters\n        ----------\n        lower : float\n            Lowest value\n        upper : float\n            Highest value\n        nbins : int\n            Number of bins\n        right : bool, optional\n            Whether bins should include their right (rather than left) boundary,\n            by default False\n\n        Returns\n        -------\n        Digitizer\n        \"\"\"\n        arr = np.linspace(lower, upper, nbins + 1, endpoint=True)\n        return cls(arr, right=right)\n\n    @classmethod\n    def from_geom(\n        cls, lowest_upper: float, highest_lower: float, nbins: int, right=False\n    ):\n        \"\"\"Choose digitizer boundaries in a geometric sequence.\n\n        Additional bins will be added above and below the given values.\n\n        Parameters\n        ----------\n        lowest_upper : float\n            Upper bound of the lowest bin. The lower bound of the lowest bin is\n            often 0, which cannot be represented in a nontrivial geometric\n            sequence.\n        highest_lower : float\n            Lower bound of the highest bin.\n        nbins : int\n            Number of bins\n        right : bool, optional\n            Whether bins should include their right (rather than left) boundary,\n            by default False\n\n        Returns\n        -------\n        Digitizer\n        \"\"\"\n        arr = np.geomspace(lowest_upper, highest_lower, nbins - 1, True)\n        return cls(arr, clip=(False, False), right=right)\n\n    @classmethod\n    def from_data(\n        cls, data: Sequence[float], nbins: int, right=False, method=\"quantile\"\n    ):\n        \"\"\"Choose digitizer boundaries to evenly partition the given values.\n\n        Parameters\n        ----------\n        data : Sequence[float]\n            Data which should be partitioned by the resulting digitizer.\n        nbins : int\n            Number of bins\n        right : bool, optional\n            Whether bins should include their right (rather than left) boundary,\n            by default False\n        method : \"quantile\" | \"linear\" | \"geometric\"\n            Method to use for partitioning the data space:\n             - 'quantile' (default) will partition the data such that each bin\n               contains the same number of data points. This is usually the\n               method of choice because it is robust against outlier and because\n               we are guaranteed to not have empty bin.\n             - 'linear' will partition the data into evenly spaced bins.\n             - 'geometric' will produce a log scale partition. This will not work\n               if data has negative values.\n\n        Returns\n        -------\n        Digitizer\n        \"\"\"\n        assert method in (\"quantile\", \"linear\", \"geometric\")\n\n        if method == \"quantile\":\n            arr = np.quantile(data, np.linspace(0, 1, nbins + 1, True))\n        elif method == \"linear\":\n            arr = np.linspace(min(data), max(data), nbins + 1, True)\n        elif method == \"geometric\":\n            if min(data) &lt;= 0:\n                raise ValueError(\n                    \"Data must not have values &lt;= 0 for creating \"\n                    \"geometric (logarithmic) bins.\"\n                )\n            arr = np.geomspace(min(data), max(data), nbins + 1, True)\n        return cls(arr, right=right)\n\n    def __eq__(self, other: object) -&gt; bool:\n        if not isinstance(other, Digitizer):\n            return NotImplemented\n        return self.right == other.right and np.allclose(\n            self.boundaries, other.boundaries\n        )\n</code></pre>"},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.Digitizer.from_data","title":"<code>from_data</code>  <code>classmethod</code>","text":"<p>Choose digitizer boundaries to evenly partition the given values.</p> PARAMETER DESCRIPTION <code>data</code> <p>Data which should be partitioned by the resulting digitizer.</p> <p> TYPE: <code>Sequence[float]</code> </p> <code>nbins</code> <p>Number of bins</p> <p> TYPE: <code>int</code> </p> <code>right</code> <p>Whether bins should include their right (rather than left) boundary, by default False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>method</code> <p>Method to use for partitioning the data space:  - 'quantile' (default) will partition the data such that each bin    contains the same number of data points. This is usually the    method of choice because it is robust against outlier and because    we are guaranteed to not have empty bin.  - 'linear' will partition the data into evenly spaced bins.  - 'geometric' will produce a log scale partition. This will not work    if data has negative values.</p> <p> TYPE: <code>'quantile' | 'linear' | 'geometric'</code> DEFAULT: <code>'quantile'</code> </p> RETURNS DESCRIPTION <code>Digitizer</code> Source code in <code>navis/nbl/smat.py</code> <pre><code>@classmethod\ndef from_data(\n    cls, data: Sequence[float], nbins: int, right=False, method=\"quantile\"\n):\n    \"\"\"Choose digitizer boundaries to evenly partition the given values.\n\n    Parameters\n    ----------\n    data : Sequence[float]\n        Data which should be partitioned by the resulting digitizer.\n    nbins : int\n        Number of bins\n    right : bool, optional\n        Whether bins should include their right (rather than left) boundary,\n        by default False\n    method : \"quantile\" | \"linear\" | \"geometric\"\n        Method to use for partitioning the data space:\n         - 'quantile' (default) will partition the data such that each bin\n           contains the same number of data points. This is usually the\n           method of choice because it is robust against outlier and because\n           we are guaranteed to not have empty bin.\n         - 'linear' will partition the data into evenly spaced bins.\n         - 'geometric' will produce a log scale partition. This will not work\n           if data has negative values.\n\n    Returns\n    -------\n    Digitizer\n    \"\"\"\n    assert method in (\"quantile\", \"linear\", \"geometric\")\n\n    if method == \"quantile\":\n        arr = np.quantile(data, np.linspace(0, 1, nbins + 1, True))\n    elif method == \"linear\":\n        arr = np.linspace(min(data), max(data), nbins + 1, True)\n    elif method == \"geometric\":\n        if min(data) &lt;= 0:\n            raise ValueError(\n                \"Data must not have values &lt;= 0 for creating \"\n                \"geometric (logarithmic) bins.\"\n            )\n        arr = np.geomspace(min(data), max(data), nbins + 1, True)\n    return cls(arr, right=right)\n</code></pre>"},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.Digitizer.from_geom","title":"<code>from_geom</code>  <code>classmethod</code>","text":"<p>Choose digitizer boundaries in a geometric sequence.</p> <p>Additional bins will be added above and below the given values.</p> PARAMETER DESCRIPTION <code>lowest_upper</code> <p>Upper bound of the lowest bin. The lower bound of the lowest bin is often 0, which cannot be represented in a nontrivial geometric sequence.</p> <p> TYPE: <code>float</code> </p> <code>highest_lower</code> <p>Lower bound of the highest bin.</p> <p> TYPE: <code>float</code> </p> <code>nbins</code> <p>Number of bins</p> <p> TYPE: <code>int</code> </p> <code>right</code> <p>Whether bins should include their right (rather than left) boundary, by default False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Digitizer</code> Source code in <code>navis/nbl/smat.py</code> <pre><code>@classmethod\ndef from_geom(\n    cls, lowest_upper: float, highest_lower: float, nbins: int, right=False\n):\n    \"\"\"Choose digitizer boundaries in a geometric sequence.\n\n    Additional bins will be added above and below the given values.\n\n    Parameters\n    ----------\n    lowest_upper : float\n        Upper bound of the lowest bin. The lower bound of the lowest bin is\n        often 0, which cannot be represented in a nontrivial geometric\n        sequence.\n    highest_lower : float\n        Lower bound of the highest bin.\n    nbins : int\n        Number of bins\n    right : bool, optional\n        Whether bins should include their right (rather than left) boundary,\n        by default False\n\n    Returns\n    -------\n    Digitizer\n    \"\"\"\n    arr = np.geomspace(lowest_upper, highest_lower, nbins - 1, True)\n    return cls(arr, clip=(False, False), right=right)\n</code></pre>"},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.Digitizer.from_linear","title":"<code>from_linear</code>  <code>classmethod</code>","text":"<p>Choose digitizer boundaries spaced linearly between two values.</p> <p>Input values will be clipped to fit within the given interval.</p> PARAMETER DESCRIPTION <code>lower</code> <p>Lowest value</p> <p> TYPE: <code>float</code> </p> <code>upper</code> <p>Highest value</p> <p> TYPE: <code>float</code> </p> <code>nbins</code> <p>Number of bins</p> <p> TYPE: <code>int</code> </p> <code>right</code> <p>Whether bins should include their right (rather than left) boundary, by default False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Digitizer</code> Source code in <code>navis/nbl/smat.py</code> <pre><code>@classmethod\ndef from_linear(cls, lower: float, upper: float, nbins: int, right=False):\n    \"\"\"Choose digitizer boundaries spaced linearly between two values.\n\n    Input values will be clipped to fit within the given interval.\n\n    Parameters\n    ----------\n    lower : float\n        Lowest value\n    upper : float\n        Highest value\n    nbins : int\n        Number of bins\n    right : bool, optional\n        Whether bins should include their right (rather than left) boundary,\n        by default False\n\n    Returns\n    -------\n    Digitizer\n    \"\"\"\n    arr = np.linspace(lower, upper, nbins + 1, endpoint=True)\n    return cls(arr, right=right)\n</code></pre>"},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.Digitizer.from_strings","title":"<code>from_strings</code>  <code>classmethod</code>","text":"<p>Set digitizer boundaries based on a sequence of interval expressions.</p> <p>e.g. <code>[\"(0, 1]\", \"(1, 5]\", \"(5, 10]\"]</code></p> <p>The lowermost and uppermost boundaries are converted to -infinity and infinity respectively.</p> PARAMETER DESCRIPTION <code>bound_strs</code> <p>Strings representing intervals, which must abut and have open/closed boundaries specified by brackets.</p> <p> TYPE: <code>Sequence[str]</code> </p> RETURNS DESCRIPTION <code>Digitizer</code> Source code in <code>navis/nbl/smat.py</code> <pre><code>@classmethod\ndef from_strings(cls, interval_strs: Sequence[str]):\n    \"\"\"Set digitizer boundaries based on a sequence of interval expressions.\n\n    e.g. `[\"(0, 1]\", \"(1, 5]\", \"(5, 10]\"]`\n\n    The lowermost and uppermost boundaries are converted to -infinity and\n    infinity respectively.\n\n    Parameters\n    ----------\n    bound_strs : Sequence[str]\n        Strings representing intervals, which must abut and have open/closed\n        boundaries specified by brackets.\n\n    Returns\n    -------\n    Digitizer\n    \"\"\"\n    bounds: List[float] = []\n    last_upper = None\n    last_right = None\n    for item in interval_strs:\n        (lower, upper), right = parse_boundary(item)\n        bounds.append(float(lower))\n\n        if last_right is not None:\n            if right != last_right:\n                raise ValueError(\"Inconsistent half-open interval\")\n        else:\n            last_right = right\n\n        if last_upper is not None:\n            if lower != last_upper:\n                raise ValueError(\"Half-open intervals do not abut\")\n\n        last_upper = upper\n\n    bounds.append(float(last_upper))\n    return cls(bounds, right=last_right)\n</code></pre>"},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.Digitizer.to_strings","title":"<code>to_strings</code>","text":"<p>Turn boundaries into list of labels.</p> PARAMETER DESCRIPTION <code>round</code> <pre><code>    Use to round bounds to the Nth decimal.\n</code></pre> <p> TYPE: <code>    int</code> DEFAULT: <code>None</code> </p> Source code in <code>navis/nbl/smat.py</code> <pre><code>def to_strings(self, round=None) -&gt; List[str]:\n    \"\"\"Turn boundaries into list of labels.\n\n    Parameters\n    ----------\n    round :     int, optional\n                Use to round bounds to the Nth decimal.\n    \"\"\"\n    if self.right:\n        lb = \"(\"\n        rb = \"]\"\n    else:\n        lb = \"[\"\n        rb = \")\"\n\n    b = self.boundaries.copy()\n    b[0] = self._min\n    b[-1] = self._max\n\n    if round:\n        b = [np.round(x, round) for x in b]\n\n    return [f\"{lb}{lower},{upper}{rb}\" for lower, upper in zip(b[:-1], b[1:])]\n</code></pre>"},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.LookupAxis","title":"<code>navis.nbl.smat.LookupAxis</code>","text":"<p>Class converting some data into a linear index.</p> Source code in <code>navis/nbl/smat.py</code> <pre><code>class LookupAxis(ABC, Generic[T]):\n    \"\"\"Class converting some data into a linear index.\"\"\"\n\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"Number of bins represented by this instance.\"\"\"\n        pass\n\n    @abstractmethod\n    def __call__(self, value: Union[T, Sequence[T]]) -&gt; Union[int, Sequence[int]]:\n        \"\"\"Convert some data into a linear index.\n\n        Parameters\n        ----------\n        value : Union[T, Sequence[T]]\n            Value to convert into an index\n\n        Returns\n        -------\n        Union[int, Sequence[int]]\n            If a scalar was given, return a scalar; otherwise, a numpy array of ints.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.LookupDistDotBuilder","title":"<code>navis.nbl.smat.LookupDistDotBuilder</code>","text":"<p>Class for building a 2-dimensional score lookup for NBLAST.</p> <p>The scores are</p> <ol> <li>The distances between best-matching points</li> <li>The dot products of direction vectors around those points,     optionally scaled by the colinearity <code>alpha</code>.</li> </ol> PARAMETER DESCRIPTION <code>dotprops</code> <pre><code>            An indexable sequence of all neurons which will be\n            used as the training set, as Dotprops objects.\n</code></pre> <p> TYPE: <code>         dict or list of Dotprops</code> </p> <code>matching_lists</code> <pre><code>            List of neurons, as indices into `dotprops`, which\n            should be considered matches.\n</code></pre> <p> TYPE: <code>   list of lists of indices into dotprops</code> </p> <code>nonmatching_list</code> <pre><code>            List of neurons, as indices into `dotprops`,\n            which should not be considered matches.\n            If not given, all `dotprops` will be used\n            (on the assumption that matches are a small subset\n            of possible pairs).\n</code></pre> <p> TYPE: <code> list of indices into dotprops</code> DEFAULT: <code>None</code> </p> <code>use_alpha</code> <pre><code>            If true, multiply the dot product by the geometric\n            mean of the matched points' alpha values\n            (i.e. `sqrt(alpha1 * alpha2)`).\n</code></pre> <p> TYPE: <code>        bool</code> DEFAULT: <code>False</code> </p> <code>draw_strat</code> <pre><code>        Strategy for randomly drawing non-matching pairs.\n        \"batched\" should be the right choice in most scenarios.\n        \"greedy\" can be better if your pool of neurons is very\n        small.\n</code></pre> <p> TYPE: <code>   \"batched\" | \"greedy\"</code> DEFAULT: <code>'batched'</code> </p> <code>seed</code> <pre><code>            Non-matching pairs are drawn at random using this\n            seed, by default {DEFAULT_SEED}.\n</code></pre> <p> TYPE: <code>             int</code> DEFAULT: <code>DEFAULT_SEED</code> </p> Source code in <code>navis/nbl/smat.py</code> <pre><code>class LookupDistDotBuilder(LookupNdBuilder):\n    \"\"\"Class for building a 2-dimensional score lookup for NBLAST.\n\n    The scores are\n\n    1. The distances between best-matching points\n    2. The dot products of direction vectors around those points,\n        optionally scaled by the colinearity `alpha`.\n\n    Parameters\n    ----------\n    dotprops :          dict or list of Dotprops\n                        An indexable sequence of all neurons which will be\n                        used as the training set, as Dotprops objects.\n    matching_lists :    list of lists of indices into dotprops\n                        List of neurons, as indices into `dotprops`, which\n                        should be considered matches.\n    nonmatching_list :  list of indices into dotprops, optional\n                        List of neurons, as indices into `dotprops`,\n                        which should not be considered matches.\n                        If not given, all `dotprops` will be used\n                        (on the assumption that matches are a small subset\n                        of possible pairs).\n    use_alpha :         bool, optional\n                        If true, multiply the dot product by the geometric\n                        mean of the matched points' alpha values\n                        (i.e. `sqrt(alpha1 * alpha2)`).\n    draw_strat :    \"batched\" | \"greedy\"\n                    Strategy for randomly drawing non-matching pairs.\n                    \"batched\" should be the right choice in most scenarios.\n                    \"greedy\" can be better if your pool of neurons is very\n                    small.\n    seed :              int, optional\n                        Non-matching pairs are drawn at random using this\n                        seed, by default {DEFAULT_SEED}.\n    \"\"\"\n\n    def __init__(\n        self,\n        dotprops: Union[List[\"core.Dotprops\"], Mapping[NeuronKey, \"core.Dotprops\"]],\n        matching_lists: List[List[NeuronKey]],\n        nonmatching_list: Optional[List[NeuronKey]] = None,\n        use_alpha: bool = False,\n        draw_strat: str = \"batched\",\n        seed: int = DEFAULT_SEED,\n    ):\n        match_fn = dist_dot_alpha if use_alpha else dist_dot\n        super().__init__(\n            dotprops,\n            matching_lists,\n            match_fn,\n            nonmatching_list,\n            draw_strat=draw_strat,\n            seed=seed,\n        )\n        self._ndim = 2\n\n    def build(self, threads=None) -&gt; Lookup2d:\n        (dig0, dig1), cells = self._build(threads)\n        return Lookup2d(dig0, dig1, cells)\n</code></pre>"},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.LookupNd","title":"<code>navis.nbl.smat.LookupNd</code>","text":"Source code in <code>navis/nbl/smat.py</code> <pre><code>class LookupNd:\n    def __init__(self, axes: List[LookupAxis], cells: np.ndarray):\n        if [len(b) for b in axes] != list(cells.shape):\n            raise ValueError(\"boundaries and cells have inconsistent bin counts\")\n        self.axes = axes\n        self.cells = cells\n\n    def __call__(self, *args):\n        if len(args) != len(self.axes):\n            raise TypeError(\n                f\"Lookup takes {len(self.axes)} arguments but {len(args)} were given\"\n            )\n\n        idxs = tuple(d(arg) for d, arg in zip(self.axes, args))\n        out = self.cells[idxs]\n        return out\n</code></pre>"},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.LookupNdBuilder","title":"<code>navis.nbl.smat.LookupNdBuilder</code>","text":"<p>Class for building an N-dimensional score lookup (for e.g. NBLAST).</p> <p>Once instantiated, the axes of the lookup table must be defined. Call <code>.with_digitizers()</code> to manually define them, or <code>.with_bin_counts()</code> to learn them from the matched-pair data.</p> <p>Then call <code>.build()</code> to build the lookup table.</p> PARAMETER DESCRIPTION <code>neurons</code> <pre><code>        An indexable, consistently-ordered sequence of all\n        objects (typically neurons) which will be used as the\n        training set. Importantly: each object must have a\n        `len()`!\n</code></pre> <p> TYPE: <code>      dict or list of objects (e.g. Dotprops)</code> </p> <code>matching_sets</code> <pre><code>        Lists of neurons, as indices into `neurons`, which\n        should be considered matches:\n\n            [[0, 1, 2, 4], [5, 6], [9, 10, 11]]\n</code></pre> <p> TYPE: <code>list of lists of index into `neurons`</code> </p> <code>match_fn</code> <pre><code>        Function taking 2 arguments, both instances of type\n        `neurons`, and returning a list of 1D\n        `numpy.ndarray`s of floats. The length of the list\n        must be the same as the length of `boundaries`.\n        The length of the `array`s must be the same as the\n        number of points in the first argument. This function\n        returns values describing the quality of point matches\n        from a query to a target neuron.\n</code></pre> <p> TYPE: <code>     Callable[[object, object], List[np.ndarray[float]]]</code> </p> <code>nonmatching</code> <pre><code>        List of objects, as indices into `neurons`, which\n        should be be considered NON-matches. If not given,\n        all `neurons` will be used (on the assumption that\n        matches are a small subset of possible pairs).\n</code></pre> <p> TYPE: <code>  list of index into `neurons`</code> </p> <code>draw_strat</code> <pre><code>        Strategy for randomly drawing non-matching pairs. Only\n        relevant if `nonmatching` is not provided.\n        \"batched\" should be the right choice in most scenarios.\n        \"greedy\" can be better if your pool of neurons is very\n        small.\n</code></pre> <p> TYPE: <code>   \"batched\" | \"greedy\"</code> DEFAULT: <code>'batched'</code> </p> <code>seed</code> <pre><code>        Non-matching pairs are drawn at random using this seed,\n        by default {DEFAULT_SEED}.\n</code></pre> <p> TYPE: <code>         int</code> DEFAULT: <code>DEFAULT_SEED</code> </p> Source code in <code>navis/nbl/smat.py</code> <pre><code>class LookupNdBuilder:\n    \"\"\"Class for building an N-dimensional score lookup (for e.g. NBLAST).\n\n    Once instantiated, the axes of the lookup table must be defined.\n    Call `.with_digitizers()` to manually define them, or\n    `.with_bin_counts()` to learn them from the matched-pair data.\n\n    Then call `.build()` to build the lookup table.\n\n    Parameters\n    ----------\n    neurons :       dict or list of objects (e.g. Dotprops)\n                    An indexable, consistently-ordered sequence of all\n                    objects (typically neurons) which will be used as the\n                    training set. Importantly: each object must have a\n                    `len()`!\n    matching_sets : list of lists of index into `neurons`\n                    Lists of neurons, as indices into `neurons`, which\n                    should be considered matches:\n\n                        [[0, 1, 2, 4], [5, 6], [9, 10, 11]]\n\n    match_fn :      Callable[[object, object], List[np.ndarray[float]]]\n                    Function taking 2 arguments, both instances of type\n                    `neurons`, and returning a list of 1D\n                    `numpy.ndarray`s of floats. The length of the list\n                    must be the same as the length of `boundaries`.\n                    The length of the `array`s must be the same as the\n                    number of points in the first argument. This function\n                    returns values describing the quality of point matches\n                    from a query to a target neuron.\n    nonmatching :   list of index into `neurons`, optional\n                    List of objects, as indices into `neurons`, which\n                    should be be considered NON-matches. If not given,\n                    all `neurons` will be used (on the assumption that\n                    matches are a small subset of possible pairs).\n    draw_strat :    \"batched\" | \"greedy\"\n                    Strategy for randomly drawing non-matching pairs. Only\n                    relevant if `nonmatching` is not provided.\n                    \"batched\" should be the right choice in most scenarios.\n                    \"greedy\" can be better if your pool of neurons is very\n                    small.\n    seed :          int, optional\n                    Non-matching pairs are drawn at random using this seed,\n                    by default {DEFAULT_SEED}.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        neurons: Union[List[T], Mapping[NeuronKey, T]],\n        matching_lists: List[List[NeuronKey]],\n        match_fn: Callable[[T, T], List[np.ndarray]],\n        nonmatching_list: Optional[List[NeuronKey]] = None,\n        draw_strat: str = \"batched\",\n        seed: int = DEFAULT_SEED,\n    ) -&gt; None:\n        self.objects = neurons\n        self.matching_lists = matching_lists\n        self._nonmatching_list = nonmatching_list\n        self.match_fn = match_fn\n        self.nonmatching_draw = draw_strat\n\n        self.digitizers: Optional[List[Digitizer]] = None\n        self.bin_counts: Optional[List[int]] = None\n\n        self.seed = seed\n        self._ndim: Optional[int] = None\n\n    @property\n    def ndim(self) -&gt; int:\n        if self._ndim is None:\n            idx1, idx2 = self._object_keys()[:2]\n            self._ndim = len(self._query(idx1, idx2))\n        return self._ndim\n\n    def with_digitizers(self, digitizers: List[Digitizer]):\n        \"\"\"Specify the axes of the output lookup table directly.\n\n        Parameters\n        ----------\n        digitizers : List[Digitizer]\n\n        Returns\n        -------\n        self\n            For chaining convenience.\n        \"\"\"\n        if len(digitizers) != self.ndim:\n            raise ValueError(\n                f\"Match function returns {self.ndim} values \"\n                f\"but provided {len(digitizers)} digitizers\"\n            )\n\n        self.digitizers = digitizers\n        self.bin_counts = None\n        return self\n\n    def with_bin_counts(self, bin_counts: List[int], method=\"quantile\"):\n        \"\"\"Specify the number of bins on each axis of the output lookup table.\n\n        The bin boundaries will be determined by evenly partitioning the data\n        from the matched pairs into quantiles, in each dimension.\n\n        Parameters\n        ----------\n        bin_counts : List[int]\n        method :     'quantile' | 'geometric' | 'linear'\n                     Method used to tile the data space.\n\n        Returns\n        -------\n        self\n            For chaining convenience.\n        \"\"\"\n        if len(bin_counts) != self.ndim:\n            raise ValueError(\n                f\"Match function returns {self.ndim} values \"\n                f\"but provided {len(bin_counts)} bin counts\"\n            )\n\n        self.bin_counts = bin_counts\n        self.digitizers = None\n        self.bin_method = method\n        return self\n\n    def _object_keys(self) -&gt; Sequence[NeuronKey]:\n        \"\"\"Get all indices into objects instance member.\"\"\"\n        try:\n            return self.objects.keys()\n        except AttributeError:\n            return range(len(self.objects))\n\n    @property\n    def nonmatching(self) -&gt; List[NeuronKey]:\n        \"\"\"Indices of nonmatching set of neurons.\"\"\"\n        if self._nonmatching_list is None:\n            return list(self._object_keys())\n        return self._nonmatching_list\n\n    def _yield_matching_pairs(self) -&gt; Iterator[Tuple[NeuronKey, NeuronKey]]:\n        \"\"\"Yield all index pairs within all matching pairs.\"\"\"\n        for ms in self.matching_lists:\n            yield from yield_not_same(permutations(ms, 2))\n\n    def _yield_nonmatching_pairs(self) -&gt; Iterator[Tuple[NeuronKey, NeuronKey]]:\n        \"\"\"Yield all index pairs within all non-matching pairs.\"\"\"\n        if self._nonmatching_list is None:\n            raise ValueError(\"Must provide non-matching pairs explicitly.\")\n        yield from yield_not_same(permutations(self._nonmatching_list, 2))\n\n    def _yield_nonmatching_pairs_greedy(\n        self, rng=None\n    ) -&gt; Iterator[Tuple[NeuronKey, NeuronKey]]:\n        \"\"\"Yield all index pairs within nonmatching list.\"\"\"\n        return yield_not_same(permutations(self.nonmatching, 2))\n\n    def _yield_nonmatching_pairs_batched(self) -&gt; Iterator[Tuple[NeuronKey, NeuronKey]]:\n        \"\"\"Yield all index pairs within nonmatching list.\n\n        This function tries to generate truely random draws of all possible\n        non-matching pairs without actually having to generate all pairs.\n        Instead, we generate new randomly permutated pairs in batches from which\n        we then remove previously seen pairs.\n\n        This works reasonable well as long as we only need a small subset\n        of all possible non-matches. Otherwise this becomes inefficient.\n\n        \"\"\"\n        nonmatching = np.array(self.nonmatching)\n\n        seen = []\n        rng = np.random.default_rng(self.seed)\n\n        # Generate random pairs\n        pairs = np.vstack(\n            (rng.permutation(nonmatching), rng.permutation(nonmatching))\n        ).T\n        pairs = pairs[pairs[:, 0] != pairs[:, 1]]  # drop self hits\n        seen = set([tuple(p) for p in pairs])  # track already seen pairs\n        i = 0\n        while True:\n            # If exhausted, generate a new batch of random permutation\n            if i &gt;= len(pairs):\n                pairs = np.vstack(\n                    (rng.permutation(nonmatching), rng.permutation(nonmatching))\n                ).T\n                pairs = pairs[pairs[:, 0] != pairs[:, 1]]  # drop self hits\n                pairs = set([tuple(p) for p in pairs])\n                pairs = pairs - seen\n                seen = seen | pairs\n                pairs = list(pairs)\n                i = 0\n\n            # Pick a pair\n            ix1, ix2 = pairs[i]\n            i += 1\n\n            yield (ix1, ix2)\n\n    def _empty_counts(self) -&gt; np.ndarray:\n        \"\"\"Create an empty array in which to store counts; shape determined by digitizer sizes.\"\"\"\n        shape = [len(b) for b in self.digitizers]\n        return np.zeros(shape, int)\n\n    def _query(self, q_idx, t_idx) -&gt; List[np.ndarray]:\n        \"\"\"Get the results of applying the match function to objects specified by indices.\"\"\"\n        return self.match_fn(self.objects[q_idx], self.objects[t_idx])\n\n    def _query_many(self, idx_pairs, threads=None) -&gt; Iterator[List[np.ndarray]]:\n        \"\"\"Yield results from querying many pairs of neuron indices.\"\"\"\n        if threads is None or (threads == 0 and cpu_count == 1):\n            for q_idx, t_idx in idx_pairs:\n                yield self._query(q_idx, t_idx)\n            return\n\n        threads = threads or cpu_count\n        idx_pairs = np.asarray(idx_pairs)\n        chunks = chunksize(len(idx_pairs), threads)\n\n        with ProcessPoolExecutor(threads) as exe:\n            yield from exe.map(\n                self.match_fn,\n                [self.objects[ix] for ix in idx_pairs[:, 0]],\n                [self.objects[ix] for ix in idx_pairs[:, 1]],\n                chunksize=chunks,\n            )\n\n    def _query_to_idxs(self, q_idx, t_idx, counts=None):\n        \"\"\"Produce a digitized counts array from a given query-target pair.\"\"\"\n        return self._count_results(self._query(q_idx, t_idx), counts)\n\n    def _count_results(self, results: List[np.ndarray], counts=None):\n        \"\"\"Convert raw match function ouput into a digitized counts array.\n\n        Requires digitizers.\n        \"\"\"\n        # Digitize\n        idxs = [dig(r) for dig, r in zip(self.digitizers, results)]\n\n        # Make a stack\n        stack = np.vstack(idxs).T\n\n        # Create empty matrix if necessary\n        if counts is None:\n            counts = self._empty_counts()\n\n        # Get counts per cell -&gt; this is the actual bottleneck of this function\n        cells, cnt = np.unique(stack, axis=0, return_counts=True)\n\n        # Fill matrix\n        counts[tuple(cells[:, i] for i in range(cells.shape[1]))] += cnt\n\n        return counts\n\n    def _counts_array(\n        self,\n        idx_pairs,\n        threads=None,\n        progress=True,\n        desc=None,\n    ):\n        \"\"\"Convert index pairs into a digitized counts array.\n\n        Requires digitizers.\n        \"\"\"\n        counts = self._empty_counts()\n        if threads is None or (threads == 0 and cpu_count == 1):\n            for q_idx, t_idx in config.tqdm(\n                idx_pairs, leave=False, desc=desc, disable=not progress\n            ):\n                counts = self._query_to_idxs(q_idx, t_idx, counts)\n            return counts\n\n        threads = threads or cpu_count\n        idx_pairs = np.asarray(idx_pairs, dtype=int)\n        chunks = chunksize(len(idx_pairs), threads)\n\n        # because digitizing is not necessarily free,\n        # keep this parallelisation separate to that in _query_many\n        with ProcessPoolExecutor(threads) as exe:\n            # This is the progress bar\n            with config.tqdm(\n                desc=desc, total=len(idx_pairs), leave=False, disable=not progress\n            ) as pbar:\n                for distdots in exe.map(\n                    self.match_fn,\n                    [self.objects[ix] for ix in idx_pairs[:, 0]],\n                    [self.objects[ix] for ix in idx_pairs[:, 1]],\n                    chunksize=chunks,\n                ):\n                    counts = self._count_results(distdots, counts)\n                    pbar.update(1)\n\n        return counts\n\n    def _pick_nonmatching_pairs(self, n_matching_qual_vals, progress=True):\n        \"\"\"Using the seeded RNG, pick which non-matching pairs to use.\"\"\"\n        # pre-calculating which pairs we're going to use,\n        # rather than drawing them as we need them,\n        # means that we can parallelise the later step more effectively.\n        # Slowdowns here are practically meaningless\n        # because of how long distdot calculation will take\n        nonmatching_pairs = []\n        n_nonmatching_qual_vals = 0\n        if self.nonmatching_draw == \"batched\":\n            # This is a generator that tries to generate random pairs in\n            # batches to avoid having to calculate all possible pairs\n            gen = self._yield_nonmatching_pairs_batched()\n            with config.tqdm(\n                desc=\"Drawing non-matching pairs\",\n                total=n_matching_qual_vals,\n                leave=False,\n                disable=not progress,\n            ) as pbar:\n                # Draw non-matching pairs until we have enough data\n                for nonmatching_pair in gen:\n                    nonmatching_pairs.append(nonmatching_pair)\n                    new_vals = len(self.objects[nonmatching_pair[0]])\n                    n_nonmatching_qual_vals += new_vals\n\n                    pbar.update(new_vals)\n\n                    if n_nonmatching_qual_vals &gt;= n_matching_qual_vals:\n                        break\n        elif self.nonmatching_draw == \"greedy\":\n            # Generate all possible non-matching pairs\n            possible_pairs = len(self.nonmatching) ** 2 - len(self.nonmatching)\n            all_nonmatching_pairs = [\n                p\n                for p in config.tqdm(\n                    self._yield_nonmatching_pairs_greedy(),\n                    total=possible_pairs,\n                    desc=\"Generating non-matching pairs\",\n                )\n            ]\n            # Randomly pick non-matching pairs until we have enough data\n            rng = np.random.default_rng(self.seed)\n            with config.tqdm(\n                desc=\"Drawing non-matching pairs\",\n                total=n_matching_qual_vals,\n                leave=False,\n                disable=not progress,\n            ) as pbar:\n                while n_nonmatching_qual_vals &lt; n_matching_qual_vals:\n                    idx = rng.integers(0, len(all_nonmatching_pairs))\n                    nonmatching_pair = all_nonmatching_pairs.pop(idx)\n                    nonmatching_pairs.append(nonmatching_pair)\n\n                    new_vals = len(self.objects[nonmatching_pair[0]])\n                    n_nonmatching_qual_vals += new_vals\n\n                    pbar.update(new_vals)\n        else:\n            raise ValueError(\n                \"Unknown strategy for non-matching pair draw:\"\n                f\"{self.nonmatching_draw}\"\n            )\n\n        return nonmatching_pairs\n\n    def _get_pairs(self):\n        matching_pairs = list(set(self._yield_matching_pairs()))\n\n        # If no explicit non-matches provided, pick them from the entire pool\n        if self._nonmatching_list is None:\n            # need to know the eventual distdot count\n            # so we know how many non-matching pairs to draw\n            q_idx_count = Counter(p[0] for p in matching_pairs)\n            n_matching_qual_vals = sum(\n                len(self.objects[q_idx]) * n_reps\n                for q_idx, n_reps in q_idx_count.items()\n            )\n\n            nonmatching_pairs = self._pick_nonmatching_pairs(n_matching_qual_vals)\n        else:\n            nonmatching_pairs = list(set(self._yield_nonmatching_pairs()))\n\n        return matching_pairs, nonmatching_pairs\n\n    def _build(self, threads, progress=True) -&gt; Tuple[List[Digitizer], np.ndarray]:\n        # Asking for more threads than available CPUs seems to crash on Github\n        # actions\n        if threads and threads &gt;= cpu_count:\n            threads = cpu_count\n\n        if self.digitizers is None and self.bin_counts is None:\n            raise ValueError(\n                \"Builder needs either digitizers or bin_counts - \" \"see with_* methods.\"\n            )\n\n        self.matching_pairs, self.nonmatching_pairs = self._get_pairs()\n\n        logger.info(\"Comparing matching pairs\")\n        if self.digitizers:\n            self.match_counts_ = self._counts_array(\n                self.matching_pairs,\n                threads=threads,\n                progress=progress,\n                desc=\"Comparing matching pairs\",\n            )\n        else:\n            match_results = concat_results(\n                self._query_many(self.matching_pairs, threads),\n                progress=progress,\n                desc=\"Comparing matching pairs\",\n                total=len(self.matching_pairs),\n            )\n            self.match_results_ = match_results\n            self.digitizers = []\n            for i, (data, nbins) in enumerate(zip(match_results, self.bin_counts)):\n                if not isinstance(nbins, Digitizer):\n                    try:\n                        self.digitizers.append(\n                            Digitizer.from_data(data, nbins, method=self.bin_method)\n                        )\n                    except BaseException as e:\n                        logger.error(f\"Error creating digitizers for axes {i + 1}\")\n                        raise e\n                else:\n                    self.digitizers.append(nbins)\n\n            logger.info(\"Counting results (this may take a while)\")\n            self.match_counts_ = self._count_results(match_results)\n\n        logger.info(\"Comparing non-matching pairs\")\n        self.nonmatch_counts_ = self._counts_array(\n            self.nonmatching_pairs,\n            threads=threads,\n            progress=progress,\n            desc=\"Comparing non-matching pairs\",\n        )\n\n        # Account for there being different total numbers of datapoints for\n        # matches and nonmatches\n        self.matching_factor_ = self.nonmatch_counts_.sum() / self.match_counts_.sum()\n        if np.any(self.match_counts_ + self.nonmatch_counts_ == 0):\n            logger.warning(\"Some lookup cells have no data in them\")\n\n        self.cells_ = np.log2(\n            (self.match_counts_ * self.matching_factor_ + epsilon)\n            / (self.nonmatch_counts_ + epsilon)\n        )\n\n        return self.digitizers, self.cells_\n\n    def build(self, threads=None) -&gt; LookupNd:\n        \"\"\"Build the score matrix.\n\n        All non-identical neuron pairs within all matching sets are selected,\n        and the scoring function is evaluated for those pairs.\n        Then, the minimum number of non-matching pairs are randomly drawn\n        so that at least as many data points can be calculated for non-matching\n        pairs.\n\n        In each bin of the score matrix, the log2 odds ratio of a score\n        in that bin belonging to a match vs. non-match is calculated.\n\n        Parameters\n        ----------\n        threads :   int, optional\n                    If None, act in serial.\n                    If 0, use cpu_count - 1.\n                    Otherwise, use the given value.\n                    Will be clipped at number of available cores - 1.\n                    Note that with the currently implementation a large number\n                    of threads might (and somewhat counterintuitively) actually\n                    be slower than running building the scoring function in serial.\n\n        Returns\n        -------\n        LookupNd\n        \"\"\"\n        dig, cells = self._build(threads)\n        return LookupNd(dig, cells)\n</code></pre>"},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.LookupNdBuilder.nonmatching","title":"<code>nonmatching: List[NeuronKey]</code>  <code>property</code>","text":"<p>Indices of nonmatching set of neurons.</p>"},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.LookupNdBuilder.build","title":"<code>build</code>","text":"<p>Build the score matrix.</p> <p>All non-identical neuron pairs within all matching sets are selected, and the scoring function is evaluated for those pairs. Then, the minimum number of non-matching pairs are randomly drawn so that at least as many data points can be calculated for non-matching pairs.</p> <p>In each bin of the score matrix, the log2 odds ratio of a score in that bin belonging to a match vs. non-match is calculated.</p> PARAMETER DESCRIPTION <code>threads</code> <pre><code>    If None, act in serial.\n    If 0, use cpu_count - 1.\n    Otherwise, use the given value.\n    Will be clipped at number of available cores - 1.\n    Note that with the currently implementation a large number\n    of threads might (and somewhat counterintuitively) actually\n    be slower than running building the scoring function in serial.\n</code></pre> <p> TYPE: <code>  int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LookupNd</code> Source code in <code>navis/nbl/smat.py</code> <pre><code>def build(self, threads=None) -&gt; LookupNd:\n    \"\"\"Build the score matrix.\n\n    All non-identical neuron pairs within all matching sets are selected,\n    and the scoring function is evaluated for those pairs.\n    Then, the minimum number of non-matching pairs are randomly drawn\n    so that at least as many data points can be calculated for non-matching\n    pairs.\n\n    In each bin of the score matrix, the log2 odds ratio of a score\n    in that bin belonging to a match vs. non-match is calculated.\n\n    Parameters\n    ----------\n    threads :   int, optional\n                If None, act in serial.\n                If 0, use cpu_count - 1.\n                Otherwise, use the given value.\n                Will be clipped at number of available cores - 1.\n                Note that with the currently implementation a large number\n                of threads might (and somewhat counterintuitively) actually\n                be slower than running building the scoring function in serial.\n\n    Returns\n    -------\n    LookupNd\n    \"\"\"\n    dig, cells = self._build(threads)\n    return LookupNd(dig, cells)\n</code></pre>"},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.LookupNdBuilder.with_bin_counts","title":"<code>with_bin_counts</code>","text":"<p>Specify the number of bins on each axis of the output lookup table.</p> <p>The bin boundaries will be determined by evenly partitioning the data from the matched pairs into quantiles, in each dimension.</p> PARAMETER DESCRIPTION <code>bin_counts</code> <p> TYPE: <code>List[int]</code> </p> <code>method</code> <pre><code>     Method used to tile the data space.\n</code></pre> <p> TYPE: <code>    'quantile' | 'geometric' | 'linear'</code> DEFAULT: <code>'quantile'</code> </p> RETURNS DESCRIPTION <code>self</code> <p>For chaining convenience.</p> Source code in <code>navis/nbl/smat.py</code> <pre><code>def with_bin_counts(self, bin_counts: List[int], method=\"quantile\"):\n    \"\"\"Specify the number of bins on each axis of the output lookup table.\n\n    The bin boundaries will be determined by evenly partitioning the data\n    from the matched pairs into quantiles, in each dimension.\n\n    Parameters\n    ----------\n    bin_counts : List[int]\n    method :     'quantile' | 'geometric' | 'linear'\n                 Method used to tile the data space.\n\n    Returns\n    -------\n    self\n        For chaining convenience.\n    \"\"\"\n    if len(bin_counts) != self.ndim:\n        raise ValueError(\n            f\"Match function returns {self.ndim} values \"\n            f\"but provided {len(bin_counts)} bin counts\"\n        )\n\n    self.bin_counts = bin_counts\n    self.digitizers = None\n    self.bin_method = method\n    return self\n</code></pre>"},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.LookupNdBuilder.with_digitizers","title":"<code>with_digitizers</code>","text":"<p>Specify the axes of the output lookup table directly.</p> PARAMETER DESCRIPTION <code>digitizers</code> <p> TYPE: <code>List[Digitizer]</code> </p> RETURNS DESCRIPTION <code>self</code> <p>For chaining convenience.</p> Source code in <code>navis/nbl/smat.py</code> <pre><code>def with_digitizers(self, digitizers: List[Digitizer]):\n    \"\"\"Specify the axes of the output lookup table directly.\n\n    Parameters\n    ----------\n    digitizers : List[Digitizer]\n\n    Returns\n    -------\n    self\n        For chaining convenience.\n    \"\"\"\n    if len(digitizers) != self.ndim:\n        raise ValueError(\n            f\"Match function returns {self.ndim} values \"\n            f\"but provided {len(digitizers)} digitizers\"\n        )\n\n    self.digitizers = digitizers\n    self.bin_counts = None\n    return self\n</code></pre>"},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.SimpleLookup","title":"<code>navis.nbl.smat.SimpleLookup</code>","text":"<p>Look up in a list of items and return their index.</p> PARAMETER DESCRIPTION <code>items</code> <p>The item's position in the list is the index which will be returned.</p> <p> TYPE: <code>List[Hashable]</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>items are non-unique.</p> Source code in <code>navis/nbl/smat.py</code> <pre><code>class SimpleLookup(LookupAxis[Hashable]):\n    \"\"\"Look up in a list of items and return their index.\n\n    Parameters\n    ----------\n    items : List[Hashable]\n        The item's position in the list is the index which will be returned.\n\n    Raises\n    ------\n    ValueError\n        items are non-unique.\n    \"\"\"\n\n    def __init__(self, items: List[Hashable]):\n        self.items = {item: idx for idx, item in enumerate(items)}\n        if len(self.items) != len(items):\n            raise ValueError(\"Items are not unique\")\n\n    def __len__(self) -&gt; int:\n        return len(self.items)\n\n    def __call__(\n        self, value: Union[Hashable, Sequence[Hashable]]\n    ) -&gt; Union[int, Sequence[int]]:\n        if np.isscalar(value):\n            return self.items[value]\n        else:\n            return np.array([self.items[v] for v in value], int)\n</code></pre>"},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.check_score_fn","title":"<code>navis.nbl.smat.check_score_fn</code>","text":"<p>Checks functionally that the callable can be used as a score function.</p> PARAMETER DESCRIPTION <code>nargs</code> <p>How many positional arguments the score function should have.</p> <p> TYPE: <code>optional int</code> DEFAULT: <code>2</code> </p> <code>scalar</code> <p>Check that the function can be used on <code>nargs</code> scalars.</p> <p> TYPE: <code>optional bool</code> DEFAULT: <code>True</code> </p> <code>array</code> <p>Check that the function can be used on <code>nargs</code> 1D <code>numpy.ndarray</code>s.</p> <p> TYPE: <code>optional bool</code> DEFAULT: <code>True</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the score function is not appropriate.</p> Source code in <code>navis/nbl/smat.py</code> <pre><code>def check_score_fn(fn: Callable, nargs=2, scalar=True, array=True):\n    \"\"\"Checks functionally that the callable can be used as a score function.\n\n    Parameters\n    ----------\n    nargs : optional int, default 2\n        How many positional arguments the score function should have.\n    scalar : optional bool, default True\n        Check that the function can be used on `nargs` scalars.\n    array : optional bool, default True\n        Check that the function can be used on `nargs` 1D `numpy.ndarray`s.\n\n    Raises\n    ------\n    ValueError\n        If the score function is not appropriate.\n    \"\"\"\n    if scalar:\n        scalars = [0.5] * nargs\n        if not isinstance(fn(*scalars), float):\n            raise ValueError(\"smat does not take 2 floats and return a float\")\n\n    if array:\n        test_arr = np.array([0.5] * 3)\n        arrs = [test_arr] * nargs\n        try:\n            out = fn(*arrs)\n        except Exception as e:\n            raise ValueError(f\"Failed to use smat with numpy arrays: {e}\")\n\n        if out.shape != test_arr.shape:\n            raise ValueError(\n                f\"smat produced inconsistent shape: input {test_arr.shape}; output {out.shape}\"\n            )\n</code></pre>"},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.chunksize","title":"<code>navis.nbl.smat.chunksize</code>","text":"Source code in <code>navis/nbl/smat.py</code> <pre><code>def chunksize(it_len, cpu_count, min_chunk=50):\n    return max(min_chunk, int(it_len / (cpu_count * 4)))\n</code></pre>"},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.concat_results","title":"<code>navis.nbl.smat.concat_results</code>","text":"<p>Helper function to concatenate batches of e.g. [(dist, dots), (dist, dots)] into single (dist, dot) arrays.</p> Source code in <code>navis/nbl/smat.py</code> <pre><code>def concat_results(\n    results: Iterable[List[np.ndarray]],\n    total: Optional[int] = None,\n    desc: str = \"Querying\",\n    progress: bool = True,\n) -&gt; List[np.ndarray]:\n    \"\"\"Helper function to concatenate batches of e.g. [(dist, dots), (dist, dots)]\n    into single (dist, dot) arrays.\n    \"\"\"\n    intermediate = defaultdict(list)\n    with config.tqdm(desc=desc, total=total, leave=False, disable=not progress) as pbar:\n        for result_lst in results:\n            for idx, array in enumerate(result_lst):\n                intermediate[idx].append(array)\n            pbar.update(1)\n\n    return [np.concatenate(arrs) for arrs in intermediate.values()]\n</code></pre>"},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.dist_dot","title":"<code>navis.nbl.smat.dist_dot</code>","text":"Source code in <code>navis/nbl/smat.py</code> <pre><code>def dist_dot(q: \"core.Dotprops\", t: \"core.Dotprops\"):\n    return list(q.dist_dots(t))\n</code></pre>"},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.dist_dot_alpha","title":"<code>navis.nbl.smat.dist_dot_alpha</code>","text":"Source code in <code>navis/nbl/smat.py</code> <pre><code>def dist_dot_alpha(q: \"core.Dotprops\", t: \"core.Dotprops\"):\n    dist, dot, alpha = q.dist_dots(t, alpha=True)\n    return [dist, dot * np.sqrt(alpha)]\n</code></pre>"},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.is_monotonically_increasing","title":"<code>navis.nbl.smat.is_monotonically_increasing</code>","text":"Source code in <code>navis/nbl/smat.py</code> <pre><code>def is_monotonically_increasing(lst):\n    for prev_idx, item in enumerate(lst[1:]):\n        if item &lt;= lst[prev_idx]:\n            return False\n    return True\n</code></pre>"},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.parse_boundary","title":"<code>navis.nbl.smat.parse_boundary</code>","text":"Source code in <code>navis/nbl/smat.py</code> <pre><code>def parse_boundary(item: str):\n    explicit_interval = item[0] + item[-1]\n    if explicit_interval == \"[)\":\n        right = False\n    elif explicit_interval == \"(]\":\n        right = True\n    else:\n        raise ValueError(\n            f\"Enclosing characters '{explicit_interval}' do not match a half-open interval\"\n        )\n    return tuple(float(i) for i in item[1:-1].split(\",\")), right\n</code></pre>"},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.parse_score_fn","title":"<code>navis.nbl.smat.parse_score_fn</code>","text":"Source code in <code>navis/nbl/smat.py</code> <pre><code>def parse_score_fn(smat, alpha=False):\n    f\"\"\"Interpret `smat` as a score function.\n    Primarily for backwards compatibility.\n    {SCORE_FN_DESCR}\n    Parameters\n    ----------\n    smat : None | \"auto\" | str | os.PathLike | pandas.DataFrame | Callable[[float, float], float]\n        If `None`, use `operator.mul`.\n        If `\"auto\"`, use `navis.nbl.smat.smat_fcwb(alpha)`.\n        If a dataframe, use `navis.nbl.smat.Lookup2d.from_dataframe(smat)`.\n        If another string or path-like, load from CSV in a dataframe and uses as above.\n        Also checks the signature of the callable.\n        Raises an error, probably a ValueError, if it can't be interpreted.\n    alpha : optional bool, default False\n        If `smat` is `\"auto\"`, choose whether to use the FCWB matrices\n        with or without alpha.\n    Returns\n    -------\n    Callable\n    Raises\n    ------\n    ValueError\n        If score function cannot be interpreted.\n    \"\"\"\n    if smat is None:\n        smat = operator.mul\n    elif smat == \"auto\":\n        smat = smat_fcwb(alpha)\n\n    if isinstance(smat, (str, os.PathLike)):\n        smat = pd.read_csv(smat, index_col=0)\n\n    if isinstance(smat, pd.DataFrame):\n        smat = Lookup2d.from_dataframe(smat)\n\n    if not callable(smat):\n        raise ValueError(\n            \"smat should be a callable, a path, a pandas.DataFrame, or 'auto'\"\n        )\n\n    check_score_fn(smat)\n\n    return smat\n</code></pre>"},{"location":"reference/navis/nbl/smat/#navis.nbl.smat.yield_not_same","title":"<code>navis.nbl.smat.yield_not_same</code>","text":"Source code in <code>navis/nbl/smat.py</code> <pre><code>def yield_not_same(pairs: Iterable[Tuple[Any, Any]]) -&gt; Iterator[Tuple[Any, Any]]:\n    for a, b in pairs:\n        if a != b:\n            yield a, b\n</code></pre>"},{"location":"reference/navis/nbl/synblast_funcs/","title":"synblast_funcs","text":""},{"location":"reference/navis/nbl/synblast_funcs/#navis.nbl.synblast_funcs.SynBlaster","title":"<code>navis.nbl.synblast_funcs.SynBlaster</code>","text":"<p>Implements a synapsed-based NBLAST algorithm.</p> <p>Please note that some properties are computed on initialization and changing parameters at a later stage will mess things up!</p> TODOs <ul> <li>implement <code>use_alpha</code> as average synapse density (i.e. emphasize areas   where a neuron has lots of synapses)</li> </ul> PARAMETER DESCRIPTION <code>normalized</code> <pre><code>        If True, will normalize scores by the best possible score\n        (i.e. self-self) of the query neuron.\n</code></pre> <p> TYPE: <code>   bool</code> DEFAULT: <code>True</code> </p> <code>by_type</code> <pre><code>        If True will only compare synapses with the same value in\n        the \"type\" column.\n</code></pre> <p> TYPE: <code>      bool</code> DEFAULT: <code>True</code> </p> <code>smat</code> <pre><code>        How to convert the point match pairs into an NBLAST score,\n        usually by a lookup table.\n        If 'auto' (default), will use scoring matrices\n        from FCWB. Same behaviour as in R's nat.nblast\n        implementation.\n        Dataframes will be used to build a `Lookup2d`.\n        If `smat=None` the scores will be\n        generated as the product of the distances and the dotproduct\n        of the vectors of nearest-neighbor pairs.\n</code></pre> <p> TYPE: <code>         navis.nbl.smat.Lookup2d | pd.DataFrame | str</code> DEFAULT: <code>'auto'</code> </p> <code>progress</code> <pre><code>        If True, will show a progress bar.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> Source code in <code>navis/nbl/synblast_funcs.py</code> <pre><code>class SynBlaster(Blaster):\n    \"\"\"Implements a synapsed-based NBLAST algorithm.\n\n    Please note that some properties are computed on initialization and\n    changing parameters at a later stage will mess things up!\n\n    TODOs\n    -----\n    - implement `use_alpha` as average synapse density (i.e. emphasize areas\n      where a neuron has lots of synapses)\n\n    Parameters\n    ----------\n    normalized :    bool\n                    If True, will normalize scores by the best possible score\n                    (i.e. self-self) of the query neuron.\n    by_type :       bool\n                    If True will only compare synapses with the same value in\n                    the \"type\" column.\n    smat :          navis.nbl.smat.Lookup2d | pd.DataFrame | str\n                    How to convert the point match pairs into an NBLAST score,\n                    usually by a lookup table.\n                    If 'auto' (default), will use scoring matrices\n                    from FCWB. Same behaviour as in R's nat.nblast\n                    implementation.\n                    Dataframes will be used to build a `Lookup2d`.\n                    If `smat=None` the scores will be\n                    generated as the product of the distances and the dotproduct\n                    of the vectors of nearest-neighbor pairs.\n    progress :      bool\n                    If True, will show a progress bar.\n\n    \"\"\"\n\n    def __init__(self, normalized=True, by_type=True,\n                 smat='auto', progress=True):\n        \"\"\"Initialize class.\"\"\"\n        super().__init__(progress=progress)\n        self.normalized = normalized\n        self.by_type = by_type\n\n        if smat is None:\n            self.score_fn = operator.mul\n        elif smat == 'auto':\n            self.score_fn = smat_fcwb()\n        elif isinstance(smat, pd.DataFrame):\n            self.score_fn = Lookup2d.from_dataframe(smat)\n        else:\n            self.score_fn = smat\n\n        self.ids = []\n\n    def append(self, neuron, id=None, self_hit=None) -&gt; NestedIndices:\n        \"\"\"Append neurons/connector tables, returning numerical indices of added objects.\n\n        Note that `self_hit` is ignored and hence calculated from scratch\n        when `neuron` is a (nested) list of neurons.\n        \"\"\"\n        if isinstance(neuron, pd.DataFrame):\n            return self._append_connectors(neuron, id, self_hit=self_hit)\n\n        if isinstance(neuron, BaseNeuron):\n            if not neuron.has_connectors:\n                raise ValueError('Neuron must have synapses')\n            return self._append_connectors(neuron.connectors, neuron.id, self_hit=self_hit)\n\n        try:\n            return [self.append(n) for n in neuron]\n        except TypeError:\n            raise ValueError(\n                \"Expected a dataframe, or a Neuron or sequence thereof; got \"\n                f\"{type(neuron)}\"\n            )\n\n    def _append_connectors(self, connectors: pd.DataFrame, id, self_hit=None) -&gt; int:\n        if id is None:\n            raise ValueError(\"Explicit non-None id required for appending connectors\")\n\n        next_idx = len(self)\n        self.ids.append(id)\n        self.neurons.append({})\n        if not self.by_type:\n            data = connectors[['x', 'y', 'z']].values\n            # Generate the KDTree\n            self.neurons[-1]['all'] = data # KDTree(data)\n        else:\n            if 'type' not in connectors.columns:\n                raise ValueError('Connector tables must have a \"type\" column '\n                                 'if `by_type=True`')\n            for ty in connectors['type'].unique():\n                data = connectors.loc[connectors['type'] == ty, ['x', 'y', 'z']].values\n                # Generate the KDTree\n                self.neurons[-1][ty] = data # KDTree(data)\n\n        # Calculate score for self hit if required\n        if not self_hit:\n            self_hit = self.calc_self_hit(connectors)\n        self.self_hits.append(self_hit)\n\n        return next_idx\n\n    def _build_trees(self):\n        \"\"\"Build KDTree for all neurons.\"\"\"\n        for i, n in enumerate(self.neurons):\n            for ty, data in n.items():\n                n[ty] = KDTree(data)\n\n    def calc_self_hit(self, cn):\n        \"\"\"Non-normalized value for self hit.\"\"\"\n        return cn.shape[0] * self.score_fn(0, 1)\n\n    def single_query_target(self, q_idx: int, t_idx: int, scores='forward'):\n        \"\"\"Query single target against single target.\"\"\"\n        # Take a short-cut if this is a self-self comparison\n        if q_idx == t_idx:\n            if self.normalized:\n                return 1\n            return self.self_hits[q_idx]\n\n        # Run nearest-neighbor search for query against target\n        t_trees = self.neurons[t_idx]\n        q_trees = self.neurons[q_idx]\n\n        dists = []\n        # Go over all connector types (e.g. \"pre\" and \"post\") present in the\n        # query neuron. If `by_type=False`, there will be a single tree simply\n        # called \"all\"\n        for ty, qt in q_trees.items():\n            # If target does not have this type of connectors\n            if ty not in t_trees:\n                # Note that this infinite distance will simply get the worst\n                # score possible in the scoring function\n                dists = np.append(dists, [self.score_fn.max_dist] * qt.data.shape[0])\n            else:\n                # Note: we're building the trees lazily here once we actually need them.\n                # The main reason is that pykdtree is not picklable and hence\n                # we can't pass it to the worker processes.\n                if not isinstance(t_trees[ty], KDTree):\n                    t_trees[ty] = KDTree(t_trees[ty])\n\n                tt = t_trees[ty]\n                if isinstance(qt, KDTree):\n                    data = qt.data\n                else:\n                    data = qt\n\n                # pykdtree tracks data as flat array\n                if data.ndim == 1:\n                    data = data.reshape((qt.n, qt.ndim))\n                dists = np.append(dists, tt.query(data)[0])\n\n        # We use the same scoring function as for normal NBLAST but ignore the\n        # vector dotproduct component\n        scr = self.score_fn(dists, 1).sum()\n\n        # Normalize against best possible hit (self hit)\n        if self.normalized:\n            scr /= self.self_hits[q_idx]\n\n        # For the mean score we also have to produce the reverse score\n        if scores in ('mean', 'min', 'max'):\n            reverse = self.single_query_target(t_idx, q_idx, scores='forward')\n            if scores == 'mean':\n                scr = (scr + reverse) / 2\n            elif scores == 'min':\n                scr = min(scr, reverse)\n            elif scores == 'max':\n                scr = max(scr, reverse)\n\n        return scr\n</code></pre>"},{"location":"reference/navis/nbl/synblast_funcs/#navis.nbl.synblast_funcs.SynBlaster.__init__","title":"<code>__init__</code>","text":"<p>Initialize class.</p> Source code in <code>navis/nbl/synblast_funcs.py</code> <pre><code>def __init__(self, normalized=True, by_type=True,\n             smat='auto', progress=True):\n    \"\"\"Initialize class.\"\"\"\n    super().__init__(progress=progress)\n    self.normalized = normalized\n    self.by_type = by_type\n\n    if smat is None:\n        self.score_fn = operator.mul\n    elif smat == 'auto':\n        self.score_fn = smat_fcwb()\n    elif isinstance(smat, pd.DataFrame):\n        self.score_fn = Lookup2d.from_dataframe(smat)\n    else:\n        self.score_fn = smat\n\n    self.ids = []\n</code></pre>"},{"location":"reference/navis/nbl/synblast_funcs/#navis.nbl.synblast_funcs.SynBlaster.append","title":"<code>append</code>","text":"<p>Append neurons/connector tables, returning numerical indices of added objects.</p> <p>Note that <code>self_hit</code> is ignored and hence calculated from scratch when <code>neuron</code> is a (nested) list of neurons.</p> Source code in <code>navis/nbl/synblast_funcs.py</code> <pre><code>def append(self, neuron, id=None, self_hit=None) -&gt; NestedIndices:\n    \"\"\"Append neurons/connector tables, returning numerical indices of added objects.\n\n    Note that `self_hit` is ignored and hence calculated from scratch\n    when `neuron` is a (nested) list of neurons.\n    \"\"\"\n    if isinstance(neuron, pd.DataFrame):\n        return self._append_connectors(neuron, id, self_hit=self_hit)\n\n    if isinstance(neuron, BaseNeuron):\n        if not neuron.has_connectors:\n            raise ValueError('Neuron must have synapses')\n        return self._append_connectors(neuron.connectors, neuron.id, self_hit=self_hit)\n\n    try:\n        return [self.append(n) for n in neuron]\n    except TypeError:\n        raise ValueError(\n            \"Expected a dataframe, or a Neuron or sequence thereof; got \"\n            f\"{type(neuron)}\"\n        )\n</code></pre>"},{"location":"reference/navis/nbl/synblast_funcs/#navis.nbl.synblast_funcs.SynBlaster.calc_self_hit","title":"<code>calc_self_hit</code>","text":"<p>Non-normalized value for self hit.</p> Source code in <code>navis/nbl/synblast_funcs.py</code> <pre><code>def calc_self_hit(self, cn):\n    \"\"\"Non-normalized value for self hit.\"\"\"\n    return cn.shape[0] * self.score_fn(0, 1)\n</code></pre>"},{"location":"reference/navis/nbl/synblast_funcs/#navis.nbl.synblast_funcs.SynBlaster.single_query_target","title":"<code>single_query_target</code>","text":"<p>Query single target against single target.</p> Source code in <code>navis/nbl/synblast_funcs.py</code> <pre><code>def single_query_target(self, q_idx: int, t_idx: int, scores='forward'):\n    \"\"\"Query single target against single target.\"\"\"\n    # Take a short-cut if this is a self-self comparison\n    if q_idx == t_idx:\n        if self.normalized:\n            return 1\n        return self.self_hits[q_idx]\n\n    # Run nearest-neighbor search for query against target\n    t_trees = self.neurons[t_idx]\n    q_trees = self.neurons[q_idx]\n\n    dists = []\n    # Go over all connector types (e.g. \"pre\" and \"post\") present in the\n    # query neuron. If `by_type=False`, there will be a single tree simply\n    # called \"all\"\n    for ty, qt in q_trees.items():\n        # If target does not have this type of connectors\n        if ty not in t_trees:\n            # Note that this infinite distance will simply get the worst\n            # score possible in the scoring function\n            dists = np.append(dists, [self.score_fn.max_dist] * qt.data.shape[0])\n        else:\n            # Note: we're building the trees lazily here once we actually need them.\n            # The main reason is that pykdtree is not picklable and hence\n            # we can't pass it to the worker processes.\n            if not isinstance(t_trees[ty], KDTree):\n                t_trees[ty] = KDTree(t_trees[ty])\n\n            tt = t_trees[ty]\n            if isinstance(qt, KDTree):\n                data = qt.data\n            else:\n                data = qt\n\n            # pykdtree tracks data as flat array\n            if data.ndim == 1:\n                data = data.reshape((qt.n, qt.ndim))\n            dists = np.append(dists, tt.query(data)[0])\n\n    # We use the same scoring function as for normal NBLAST but ignore the\n    # vector dotproduct component\n    scr = self.score_fn(dists, 1).sum()\n\n    # Normalize against best possible hit (self hit)\n    if self.normalized:\n        scr /= self.self_hits[q_idx]\n\n    # For the mean score we also have to produce the reverse score\n    if scores in ('mean', 'min', 'max'):\n        reverse = self.single_query_target(t_idx, q_idx, scores='forward')\n        if scores == 'mean':\n            scr = (scr + reverse) / 2\n        elif scores == 'min':\n            scr = min(scr, reverse)\n        elif scores == 'max':\n            scr = max(scr, reverse)\n\n    return scr\n</code></pre>"},{"location":"reference/navis/nbl/synblast_funcs/#navis.nbl.synblast_funcs.find_batch_partition","title":"<code>navis.nbl.synblast_funcs.find_batch_partition</code>","text":"<p>Find partitions such that each batch takes about <code>T</code> seconds.</p> Source code in <code>navis/nbl/synblast_funcs.py</code> <pre><code>def find_batch_partition(q, t, T=10):\n    \"\"\"Find partitions such that each batch takes about `T` seconds.\"\"\"\n    # Get a median-sized query and target\n    q_ix = np.argsort(q.n_connectors)[len(q)//2]\n    t_ix = np.argsort(t.n_connectors)[len(t)//2]\n\n    # Generate the KDTree\n    tree = KDTree(q[q_ix].connectors[['x', 'y', 'z']].values)\n\n    # Run a quick single query benchmark\n    timings = []\n    for i in range(10):  # Run 10 tests\n        s = time.time()\n        _ = tree.query(t[t_ix].connectors[['x', 'y', 'z']].values)  #  ignoring scoring / normalizing\n        timings.append(time.time() - s)\n    time_per_query = min(timings)  # seconds per medium sized query\n\n    # Number of queries per job such that each job runs in `T` second\n    queries_per_batch = T / time_per_query\n\n    # Number of neurons per batch\n    neurons_per_batch  = max(1, int(np.sqrt(queries_per_batch)))\n\n    n_rows = len(q) // neurons_per_batch\n    n_cols = len(t) // neurons_per_batch\n\n    return max(1, n_rows), max(1, n_cols)\n</code></pre>"},{"location":"reference/navis/nbl/utils/","title":"utils","text":""},{"location":"reference/navis/nbl/utils/#navis.nbl.utils.make_linkage","title":"<code>navis.nbl.utils.make_linkage</code>","text":"<p>Make linkage from input. If input looks like linkage it is passed through.</p> Source code in <code>navis/nbl/utils.py</code> <pre><code>def make_linkage(x, method='single', optimal_ordering=False):\n    \"\"\"Make linkage from input. If input looks like linkage it is passed through.\"\"\"\n    if isinstance(x, pd.DataFrame):\n        # Make sure it is symmetric\n        if x.shape[0] != x.shape[1]:\n            raise ValueError(f'Scores must be symmetric, got shape {x.shape}')\n        # A cheap check for whether these are mean scores\n        if any(x.values[0].round(5) != x.values[:, 0].round(5)):\n            logger.warning(f'Symmetrizing scores because they do not look like mean scores!')\n            x = (x + x.values.T) / 2\n\n        dists = squareform(1 - x.values, checks=False)\n        Z = sch.linkage(dists, method=method, optimal_ordering=optimal_ordering)\n    elif isinstance(x, np.ndarray):\n        Z = x\n    else:\n        raise TypeError(f'Expected scores) (DataFrame) or linkage (array), got {type(x)}')\n\n    return Z\n</code></pre>"},{"location":"reference/navis/nbl/utils/#navis.nbl.utils.most","title":"<code>navis.nbl.utils.most</code>","text":"<p>Check if most (as opposed to all) entries are True.</p> Source code in <code>navis/nbl/utils.py</code> <pre><code>def most(x, f=.9):\n    \"\"\"Check if most (as opposed to all) entries are True.\"\"\"\n    if x.sum() &gt;= (x.shape[0] * f):\n        return True\n    return False\n</code></pre>"},{"location":"reference/navis/nbl/utils/#navis.nbl.utils.nblast_prime","title":"<code>navis.nbl.utils.nblast_prime</code>","text":"<p>Generate a smoothed version of the NBLAST scores.</p> <p>In brief:  1. Run PCA on the NBLAST scores and extract the first N components.  2. From that calulate a new similarity matrix.</p> <p>Requires scikit-learn.</p> PARAMETER DESCRIPTION <code>scores</code> <pre><code>    The all-by-all NBLAST scores.\n</code></pre> <p> TYPE: <code>   pandas.DataFrame</code> </p> <code>n_dim</code> <pre><code>    The number of dimensions to use. If float (0 &lt; n_dim &lt; 1) will\n    use `scores.shape[0] * n_dim`.\n</code></pre> <p> TYPE: <code>    float | int</code> DEFAULT: <code>0.2</code> </p> <code>metric</code> <pre><code>    Which distance metric to use. Directly passed through to the\n    `scipy.spatial.distance.pdist` function.\n</code></pre> <p> TYPE: <code>   str</code> DEFAULT: <code>'euclidean'</code> </p> RETURNS DESCRIPTION <code>scores_new</code> Source code in <code>navis/nbl/utils.py</code> <pre><code>def nblast_prime(scores, n_dim=.2, metric='euclidean'):\n    \"\"\"Generate a smoothed version of the NBLAST scores.\n\n    In brief:\n     1. Run PCA on the NBLAST scores and extract the first N components.\n     2. From that calulate a new similarity matrix.\n\n    Requires scikit-learn.\n\n    Parameters\n    ----------\n    scores :    pandas.DataFrame\n                The all-by-all NBLAST scores.\n    n_dim :     float | int\n                The number of dimensions to use. If float (0 &lt; n_dim &lt; 1) will\n                use `scores.shape[0] * n_dim`.\n    metric :    str\n                Which distance metric to use. Directly passed through to the\n                `scipy.spatial.distance.pdist` function.\n\n    Returns\n    -------\n    scores_new\n\n    \"\"\"\n    try:\n        from sklearn.decomposition import PCA\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            'Please install scikit-learn to use `nblast_prime`:\\n'\n            '  pip3 install scikit-learn -U'\n            )\n\n    if not isinstance(scores, pd.DataFrame):\n        raise TypeError(f'`scores` must be pandas DataFrame, got \"{type(scores)}\"')\n\n    if (scores.shape[0] != scores.shape[1]) or ~np.all(scores.columns == scores.index):\n        logger.warning('NBLAST matrix is not symmetric - are you sure this is '\n                       'an all-by-all matrix?')\n\n    if n_dim &lt; 1:\n        n_dim = int(scores.shape[1] * n_dim)\n\n    pca = PCA(n_components=n_dim)\n    X_new = pca.fit_transform(scores.values)\n\n    dist = pdist(X_new, metric=metric)\n\n    return pd.DataFrame(1 - squareform(dist), index=scores.index, columns=scores.columns)\n</code></pre>"},{"location":"reference/navis/plotting/colors/","title":"colors","text":""},{"location":"reference/navis/plotting/colors/#navis.plotting.colors.add_alpha","title":"<code>navis.plotting.colors.add_alpha</code>","text":"<p>Add/adjust alpha for color.</p> Source code in <code>navis/plotting/colors.py</code> <pre><code>def add_alpha(c, alpha):\n    \"\"\"Add/adjust alpha for color.\"\"\"\n    return (c[0], c[1], c[2], alpha)\n</code></pre>"},{"location":"reference/navis/plotting/colors/#navis.plotting.colors.color_to_int","title":"<code>navis.plotting.colors.color_to_int</code>","text":"<p>Convert color to int-packed color.</p> <p>See also StackOverflow: https://stackoverflow.com/questions/209513/convert-hex-string-to-int-in-python?rq=1</p> PARAMETER DESCRIPTION <code>color</code> <pre><code>    A single color either as str (name or hex) or as RGB(A). RGB\n    tuple must be in range 0-255! Alpha channel is ignored. Integers\n    are just passed-through.\n</code></pre> <p> TYPE: <code>    str | tuple</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from navis.plotting.colors import color_to_int\n&gt;&gt;&gt; color_to_int('r')\n16711680\n&gt;&gt;&gt; color_to_int((255, 0, 0))\n16711680\n&gt;&gt;&gt; color_to_int((0, 255, 0))\n65280\n</code></pre> Source code in <code>navis/plotting/colors.py</code> <pre><code>def color_to_int(color: AnyColor) -&gt; int:\n    \"\"\"Convert color to int-packed color.\n\n    See also StackOverflow:\n    https://stackoverflow.com/questions/209513/convert-hex-string-to-int-in-python?rq=1\n\n    Parameters\n    ----------\n    color :     str | tuple\n                A single color either as str (name or hex) or as RGB(A). RGB\n                tuple must be in range 0-255! Alpha channel is ignored. Integers\n                are just passed-through.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from navis.plotting.colors import color_to_int\n    &gt;&gt;&gt; color_to_int('r')\n    16711680\n    &gt;&gt;&gt; color_to_int((255, 0, 0))\n    16711680\n    &gt;&gt;&gt; color_to_int((0, 255, 0))\n    65280\n\n    \"\"\"\n    if isinstance(color, int):\n        return color\n    elif isinstance(color, str):\n        color = np.array(mcl.to_rgb(color)) * 255\n    else:\n        color = np.asarray(color)\n\n    r, g, b = color.astype(int)[:3]\n\n    return int('%02x%02x%02x' % (r, g, b), 16)\n</code></pre>"},{"location":"reference/navis/plotting/colors/#navis.plotting.colors.generate_colors","title":"<code>navis.plotting.colors.generate_colors</code>","text":"<p>Divide colorspace into N evenly distributed colors.</p> RETURNS DESCRIPTION <code>colormap</code> <p>[(r, g, b), (r, g, b), ...]</p> <p> TYPE: <code>list</code> </p> Source code in <code>navis/plotting/colors.py</code> <pre><code>def generate_colors(N: int,\n                    palette: str = 'hls',\n                    color_range: Union[Literal[1],\n                                       Literal[255]] = 1\n                    ) -&gt; List[Tuple[float, float, float]]:\n    \"\"\"Divide colorspace into N evenly distributed colors.\n\n    Returns\n    -------\n    colormap :  list\n                [(r, g, b), (r, g, b), ...]\n\n    \"\"\"\n    if N == 1:\n        return [eval_color(config.default_color, color_range)]\n    elif N == 0:\n        return []\n\n    if not isinstance(palette, str):\n        palette = 'hls'\n\n    colormap = sns.color_palette(palette, N)\n\n    if color_range == 255:\n        colormap = [(int(c[0] * 255), int(c[1] * 255), int(c[2] * 255)) for c in colormap]\n\n    return colormap\n</code></pre>"},{"location":"reference/navis/plotting/colors/#navis.plotting.colors.hex_to_rgb","title":"<code>navis.plotting.colors.hex_to_rgb</code>","text":"<p>Convert hex to rgb.</p> Source code in <code>navis/plotting/colors.py</code> <pre><code>def hex_to_rgb(value: str) -&gt; Tuple[int, int, int]:\n    \"\"\"Convert hex to rgb.\"\"\"\n    value = value.lstrip('#')\n    lv = len(value)\n    return tuple(int(value[i:i + lv // 3], 16) for i in range(0, lv, lv // 3))  # type: ignore\n</code></pre>"},{"location":"reference/navis/plotting/colors/#navis.plotting.colors.map_colors","title":"<code>navis.plotting.colors.map_colors</code>","text":"<p>Map color(s) onto list of objects.</p> PARAMETER DESCRIPTION <code>colors</code> <pre><code>        Color(s) to map onto `objects`. Can be::\n\n          str: e.g. \"blue\", \"k\" or \"y\"\n          tuple: (0, 0, 1), (0, 0, 0) or (0, 1, 1)\n          list-like of the above: [(0, 0, 1), 'r', 'k', ...]\n          dict mapping objects to colors: {object1: 'r',\n                                           object2: (1, 1, 1)}\n\n        If list-like or dict do not cover all `objects`, will\n        fall back to `navis.config.default_color`. If `None`,\n        will generate evenly spread out colors.\n</code></pre> <p> TYPE: <code>       None | str | tuple | list-like | dict | None</code> </p> <code>objects</code> <pre><code>        Object(s) to map color onto.\n</code></pre> <p> TYPE: <code>      list-like</code> </p> <code>color_range</code> <p> TYPE: <code>  int</code> DEFAULT: <code>255</code> </p> RETURNS DESCRIPTION <code>list of tuples</code> <p>Will match length of <code>objects</code>.</p> Source code in <code>navis/plotting/colors.py</code> <pre><code>def map_colors(colors: Optional[Union[str,\n                                      Tuple[float, float, float],\n                                      Dict[Any, str],\n                                      Dict[Any, Tuple[float, float, float]],\n                                      List[Union[str,\n                                                 Tuple[float, float, float]]\n                                           ]\n                                      ]\n                                ],\n               objects: Sequence[Any],\n               color_range: Union[Literal[1], Literal[255]] = 255\n               ) -&gt; List[Tuple[float, float, float]]:\n    \"\"\"Map color(s) onto list of objects.\n\n    Parameters\n    ----------\n    colors :        None | str | tuple | list-like | dict | None\n                    Color(s) to map onto `objects`. Can be::\n\n                      str: e.g. \"blue\", \"k\" or \"y\"\n                      tuple: (0, 0, 1), (0, 0, 0) or (0, 1, 1)\n                      list-like of the above: [(0, 0, 1), 'r', 'k', ...]\n                      dict mapping objects to colors: {object1: 'r',\n                                                       object2: (1, 1, 1)}\n\n                    If list-like or dict do not cover all `objects`, will\n                    fall back to `navis.config.default_color`. If `None`,\n                    will generate evenly spread out colors.\n\n    objects :       list-like\n                    Object(s) to map color onto.\n    color_range :   int, optional\n\n    Returns\n    -------\n    list of tuples\n                    Will match length of `objects`.\n\n    \"\"\"\n    if not utils.is_iterable(objects):\n        objects = [objects]\n\n    # If no colors, generate random colors\n    if isinstance(colors, type(None)):\n        if len(objects) == 1:\n            return [eval_color(config.default_color, color_range)]\n        return generate_colors(len(objects),\n                               color_range=color_range)\n\n    # Bring colors in the right space\n    colors = eval_color(colors, color_range=color_range)\n\n    # Match them to objects\n    if isinstance(colors, dict):\n        # If dict, try mapping to objects\n        if set(objects) - set(colors.keys()):\n            logger.warning('Objects w/o colors - falling back to default.')\n        return [colors.get(o, config.default_color) for o in objects]\n    elif isinstance(colors, tuple):\n        # If single color map to each object\n        return [colors] * len(objects)\n    elif isinstance(colors, list):\n        # If list of correct length, map onto objets\n        if len(colors) != len(objects):\n            logger.warning('N colours does not match N objects.')\n        miss = len(objects) - len(colors) if len(objects) &gt; len(colors) else 0\n        return colors[: len(objects)] + [config.default_color] * miss\n    else:\n        raise TypeError(f'Unable to interpret colors of type \"{type(colors)}\"')\n</code></pre>"},{"location":"reference/navis/plotting/colors/#navis.plotting.colors.prepare_colormap","title":"<code>navis.plotting.colors.prepare_colormap</code>","text":"<p>Map color(s) to neuron/dotprop colorlists.</p> Source code in <code>navis/plotting/colors.py</code> <pre><code>def prepare_colormap(colors,\n                     neurons: Optional['core.NeuronObject'] = None,\n                     volumes: Optional[List] = None,\n                     alpha: Optional[float] = None,\n                     color_by: Optional[List[Any]] = None,\n                     palette: Optional[str] = None,\n                     color_range: Union[Literal[1],\n                                        Literal[255]] = 255):\n    \"\"\"Map color(s) to neuron/dotprop colorlists.\"\"\"\n    # Prepare dummies in case either no neuron data, no dotprops or no volumes\n    if isinstance(neurons, type(None)):\n        neurons = core.NeuronList([])\n    elif not isinstance(neurons, core.NeuronList):\n        neurons = core.NeuronList((neurons))\n\n    if isinstance(volumes, type(None)):\n        volumes = np.array([])\n\n    if not isinstance(volumes, np.ndarray):\n        volumes = np.array(volumes)\n\n    # Only neurons absolutely REQUIRE a color\n    # (Volumes are second class citiziens here)\n    colors_required = neurons.shape[0]\n\n    if not colors_required and not len(volumes):\n        # If no neurons to plot, just return None\n        # This happens when there is only a scatter plot\n        return [None], [None]\n\n    # If labels are provided override all existing colors\n    if not isinstance(color_by, type(None)):\n        if isinstance(color_by, str):\n            color_by = getattr(neurons, color_by)\n\n        color_by = utils.make_iterable(color_by)\n        if len(color_by) != len(neurons):\n            raise ValueError('Must provide a label for all neurons: got '\n                             f'{len(color_by)} groups for {len(neurons)} neurons')\n\n        # Turn e.g. labels into colors\n        cmap = {g: c for g, c in zip(np.unique(color_by),\n                                     generate_colors(len(np.unique(color_by)),\n                                                     palette=palette,\n                                                     color_range=color_range))}\n\n        colors = []\n        for cb in color_by:\n            if utils.is_iterable(cb):\n                colors.append(np.array([cmap[g] for g in cb]))\n            else:\n                colors += [cmap[cb]]\n\n        colors += [getattr(v, 'color', (1, 1, 1)) for v in volumes]\n\n    # If no colors, generate random colors\n    if isinstance(colors, type(None)):\n        colors = []\n        colors += generate_colors(colors_required,\n                                  palette=palette,\n                                  color_range=color_range)\n        colors += [getattr(v, 'color', (1, 1, 1)) for v in volumes]\n\n    # We need to parse once here to convert named colours to rgb\n    colors = eval_color(colors, color_range=color_range)\n\n    # If dictionary, map colors to neuron IDs\n    neuron_cmap = []\n    volumes_cmap = []\n    dc = config.default_color\n    if isinstance(colors, dict):\n        # Try finding color first by neuron, then uuid and finally by name\n        neuron_cmap = []\n        for n in neurons:\n            this_c = dc\n            for k in [n, n.id, n.name]:\n                if k in colors:\n                    this_c = colors[k]\n                    break\n            neuron_cmap.append(this_c)\n\n        # Try finding color first by volume, then uuid and finally by name\n        # If no color found, fall back to color property\n        volumes_cmap = []\n        for v in volumes:\n            this_c = getattr(v, 'color', (.95, .95, .95, .1))\n            for k in [v, v.id, getattr(v, 'name', None)]:\n                if k and k in colors:\n                    this_c = colors[k]\n                    break\n            volumes_cmap.append(this_c)\n    elif isinstance(colors, mcl.Colormap):\n        # Generate colors for neurons and dotprops\n        neuron_cmap = [colors(i / len(neurons)) for i in range(len(neurons))]\n\n        # Colormaps are not applied to volumes\n        volumes_cmap = [getattr(v, 'color', (.95, .95, .95, .1)) for v in volumes]\n    # If list of colors\n    elif isinstance(colors, (list, tuple, np.ndarray)):\n        if isinstance(colors, np.ndarray):\n            # If this is an array of a single color convert to rgba tuple\n            if colors.ndim == 1 and colors.shape[0] in (3, 4):\n                colors = colors.tolist()\n            # If this is an array of multiple colors convert to list of rgba arrays\n            elif colors.ndim == 2:\n                colors = [c for c in colors]\n\n        # If color is a single color, convert to list of colors, one for each neuron\n        if all([isinstance(elem, numbers.Number) for elem in colors]):\n            # Generate at least one color\n            colors = [colors] * max(colors_required, 1)\n\n        if len(colors) &lt; colors_required:\n            raise ValueError(f'Need colors for {colors_required} neurons, '\n                             f'got {len(colors)}')\n        elif len(colors) &gt; colors_required:\n            logger.debug(f'More colors than required: got {len(colors)}, '\n                         f'needed {colors_required}')\n\n        if len(neurons):\n            neuron_cmap = [colors.pop(0) for i in range(neurons.shape[0])]\n\n        if len(volumes):\n            # Volume have their own color property as fallback\n            volumes_cmap = []\n            for v in volumes:\n                if colors:\n                    volumes_cmap.append(colors.pop(0))\n                else:\n                    volumes_cmap.append(getattr(v, 'color', (.8, .8, .8, .2)))\n    else:\n        raise TypeError(f'Unable to parse colors of type \"{type(colors)}\"')\n\n    # If alpha is given, we will override all values\n    if not isinstance(alpha, type(None)):\n        if isinstance(alpha, numbers.Number):\n            neuron_cmap = [add_alpha(c, alpha) for c in neuron_cmap]\n        elif isinstance(alpha, (list, tuple, np.ndarray)):\n            if len(alpha) != len(neurons):\n                raise ValueError(f'Need alpha for {len(neurons)} neurons, '\n                                 f'got {len(alpha)}')\n            neuron_cmap = [add_alpha(c, a) for c, a in zip(neuron_cmap, alpha)]\n        else:\n            raise TypeError(f'Unable to parse alpha of type \"{type(alpha)}\"')\n\n        # Only apply to volumes if there aren't any neurons\n        if not neuron_cmap:\n            if not isinstance(alpha, numbers.Number):\n                raise ValueError('Must provide single alpha value for volumes.')\n            volumes_cmap = [add_alpha(c, alpha) for c in volumes_cmap]\n\n    # Make sure colour range checks out\n    neuron_cmap = [eval_color(c, color_range=color_range)\n                   for c in neuron_cmap]\n    volumes_cmap = [eval_color(c, color_range=color_range)\n                    for c in volumes_cmap]\n\n    logger.debug('Neuron colormap: ' + str(neuron_cmap))\n    logger.debug('Volumes colormap: ' + str(volumes_cmap))\n\n    return neuron_cmap, volumes_cmap\n</code></pre>"},{"location":"reference/navis/plotting/colors/#navis.plotting.colors.prepare_connector_cmap","title":"<code>navis.plotting.colors.prepare_connector_cmap</code>","text":"<p>Look for \"label\" or \"type\" column in connector tables and generates a color for every unique type. See <code>navis.set_default_connector_colors</code>.</p> RETURNS DESCRIPTION <code>dict</code> <p>Maps type to color. Will be empty if no types.</p> Source code in <code>navis/plotting/colors.py</code> <pre><code>def prepare_connector_cmap(x) -&gt; Dict[str, Tuple[float, float, float]]:\n    \"\"\"Look for \"label\" or \"type\" column in connector tables and generates\n    a color for every unique type. See `navis.set_default_connector_colors`.\n\n    Returns\n    -------\n    dict\n            Maps type to color. Will be empty if no types.\n\n    \"\"\"\n    if isinstance(x, (core.NeuronList, core.TreeNeuron)):\n        connectors = getattr(x, 'connectors', None)\n\n        if not isinstance(connectors, pd.DataFrame) or connectors.empty:\n            unique: List[str] = []\n        elif 'type' in connectors:\n            unique = connectors.type.unique()\n        elif 'label' in connectors:\n            unique = connectors.label.unique()\n        elif 'relation' in connectors:\n            unique = connectors.relation.unique()\n        else:\n            unique = []\n    else:\n        unique = list(set(x))\n\n    colors = config.default_connector_colors\n    if isinstance(colors, (list, np.ndarray)):\n        if len(unique) &gt; len(colors):\n            raise ValueError('Must define more default connector colors. See'\n                             'navis.set_default_connector_colors')\n\n        return {t: config.default_connector_colors[i] for i, t in enumerate(unique)}\n    elif isinstance(colors, dict):\n        miss = [l for l in unique if l not in colors]\n        if miss:\n            raise ValueError(f'Connector labels/types {\",\".join(miss)} are not'\n                             ' defined in default connector colors. '\n                             'See navis.set_default_connector_colors')\n        return colors\n    else:\n        raise TypeError('config.default_color must be dict or iterable, '\n                        f'not {type(config.default_color)}')\n</code></pre>"},{"location":"reference/navis/plotting/colors/#navis.plotting.colors.set_alpha","title":"<code>navis.plotting.colors.set_alpha</code>","text":"<p>Set alpha channel for given color.</p> <p>Will add alpha channel if not present.</p> PARAMETER DESCRIPTION <code>color</code> <pre><code>Single RGB or RGBA color or array of colors.\n</code></pre> <p> TYPE: <code>(array - like, shape(..., 3) or (..., 4))</code> </p> <code>alpha</code> <pre><code>Alpha value to set, in range [0, 1].\n</code></pre> <p> TYPE: <code>float</code> </p> Source code in <code>navis/plotting/colors.py</code> <pre><code>def set_alpha(color: Union[np.ndarray, list, tuple], alpha: float):\n    \"\"\"Set alpha channel for given color.\n\n    Will add alpha channel if not present.\n\n    Parameters\n    ----------\n    color : array-like, shape (..., 3) or (..., 4)\n            Single RGB or RGBA color or array of colors.\n    alpha : float\n            Alpha value to set, in range [0, 1].\n\n    \"\"\"\n    if isinstance(color, np.ndarray):\n        color = color.copy()\n        if color.ndim == 2:\n            if color.shape[1] == 3:\n                alpha_channel = np.full((color.shape[0], 1), alpha)\n                color = np.hstack((color, alpha_channel))\n            elif color.shape[1] == 4:\n                color[:, 3] = alpha\n            else:\n                raise ValueError(\"Color array must have shape (..., 3) or (..., 4).\")\n        elif color.ndim == 1:\n            if color.shape[0] == 3:\n                color = np.append(color, alpha)\n            elif color.shape[0] == 4:\n                color[3] = alpha\n            else:\n                raise ValueError(\"Color array must have shape (..., 3) or (..., 4).\")\n        else:\n            raise ValueError(\"Color array must have shape (..., 3) or (..., 4).\")\n    elif isinstance(color, list):\n        color = color.copy()\n        if len(color) == 3:\n            color.append(alpha)\n        elif len(color) == 4:\n            color[3] = alpha\n        else:\n            raise ValueError(\"Color list must have length 3 or 4.\")\n    elif isinstance(color, tuple):\n        if len(color) == 3:\n            color = list(color) + [alpha]\n        elif len(color) == 4:\n            color = list(color)\n            color[3] = alpha\n        else:\n            raise ValueError(\"Color tuple must have length 3 or 4.\")\n        color = tuple(color)\n    else:\n        raise TypeError(\"Color must be a numpy array, list, or tuple.\")\n\n    return color\n</code></pre>"},{"location":"reference/navis/plotting/colors/#navis.plotting.colors.vertex_colors","title":"<code>navis.plotting.colors.vertex_colors</code>","text":"<p>Generate a color and/or alpha values for each node/face/point of a neuron.</p> PARAMETER DESCRIPTION <code>neurons</code> <pre><code>    Neurons to generate colors for.\n</code></pre> <p> TYPE: <code>  NeuronList | Neuron | pandas.DataFrame</code> </p> <code>by</code> <pre><code>    Must provide a vector for each node/face of a neuron or map to\n    a column in node table. Data can be numerical or categorical.\n</code></pre> <p> TYPE: <code>       str | iterable | list of iterables</code> </p> <code>palette</code> <pre><code>    Name of a matplotlib or seaborn color palette, list of colors\n    or (for caterogical) data a dict mapping colors to values. If\n    data is numerical must be a matplotlib palette.\n</code></pre> <p> TYPE: <code>  str | list of colors | dict</code> </p> <code>alpha</code> <pre><code>    Sets the alpha value for all colors.\n</code></pre> <p> TYPE: <code>    float [0-1]</code> DEFAULT: <code>1</code> </p> <code>use_alpha</code> <pre><code>    If True will also use the alpha channel. Applies only if data\n    is numerical.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>vmin</code> <pre><code>    Min/Max values for normalizing numerical data.\n</code></pre> <p> DEFAULT: <code>None</code> </p> <code>na</code> <pre><code>    Determine what to do if `by` is missing for a given neuron or\n    a node:\n     - \"raise\" will raise ValueError\n     - color (str, rgb tuple) will be used to fill missing values\n</code></pre> <p> TYPE: <code>       \"raise\" | color</code> DEFAULT: <code>'raise'</code> </p> <code>norm_global</code> <pre><code>    If True and no vmin/vmax is provided, will normalize across\n    all `neurons`. If False, will normalize neurons individually.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>List of (N, 4) arrays</code> <p>One list per neuron. Each array contains a color for each of the N faces/nodes.</p> Source code in <code>navis/plotting/colors.py</code> <pre><code>def vertex_colors(neurons, by, palette, alpha=1, use_alpha=False, vmin=None, vmax=None,\n                  na='raise', norm_global=True, color_range=255):\n    \"\"\"Generate a color and/or alpha values for each node/face/point of a neuron.\n\n    Parameters\n    ----------\n    neurons :   NeuronList | Neuron | pandas.DataFrame\n                Neurons to generate colors for.\n    by :        str | iterable | list of iterables\n                Must provide a vector for each node/face of a neuron or map to\n                a column in node table. Data can be numerical or categorical.\n    palette :   str | list of colors | dict\n                Name of a matplotlib or seaborn color palette, list of colors\n                or (for caterogical) data a dict mapping colors to values. If\n                data is numerical must be a matplotlib palette.\n    alpha :     float [0-1]\n                Sets the alpha value for all colors.\n    use_alpha : bool\n                If True will also use the alpha channel. Applies only if data\n                is numerical.\n    vmin|vmax : float, optional\n                Min/Max values for normalizing numerical data.\n    na :        \"raise\" | color\n                Determine what to do if `by` is missing for a given neuron or\n                a node:\n                 - \"raise\" will raise ValueError\n                 - color (str, rgb tuple) will be used to fill missing values\n    norm_global : bool\n                If True and no vmin/vmax is provided, will normalize across\n                all `neurons`. If False, will normalize neurons individually.\n\n    Returns\n    -------\n    List of (N, 4) arrays\n                One list per neuron. Each array contains a color for each of the\n                N faces/nodes.\n\n    \"\"\"\n    if not isinstance(neurons, core.NeuronList):\n        neurons = core.NeuronList(neurons)\n\n    if not isinstance(palette, (str, dict)) and not utils.is_iterable(palette):\n        raise TypeError('Expected palette as name (str), list of '\n                        f'colors or dictionary, got \"{type(palette)}\"')\n\n    # If by points to column collect values\n    if isinstance(by, str):\n        # For convenience we will compute this if required\n        if by == 'strahler_index':\n            for n in neurons:\n                if isinstance(n, core.TreeNeuron):\n                    if 'strahler_index' not in n.nodes:\n                        _ = morpho.strahler_index(n)\n                elif isinstance(n, core.MeshNeuron):\n                    if not hasattr(n, 'strahler_index'):\n                        _ = morpho.strahler_index(n)\n        values = []\n        for n in neurons:\n            if isinstance(n, core.TreeNeuron):\n                # If column exists add to values\n                if by in n.nodes.columns:\n                    values.append(n.nodes[by].values)\n                elif na == 'raise':\n                    raise ValueError(f'Column \"{by}\" does not exists in neuron {n.id}')\n                # If column does not exists, add a bunch of NaNs - we will worry\n                # about it later\n                else:\n                    values.append(np.repeat(np.nan, n.nodes.shape[0]))\n            elif isinstance(n, core.MeshNeuron):\n                if hasattr(n, by):\n                    values.append(getattr(n, by))\n                elif na == 'raise':\n                    raise ValueError(f'{n.id} does not have a \"{by}\" property')\n                # If column does not exists, add a bunch of NaNs - we will worry\n                # about it later\n                else:\n                    values.append(np.repeat(np.nan, n.vertices.shape[0]))\n            else:\n                raise TypeError('`color_by=str` currently not supported for '\n                                f'{type(n)}')\n    # If by already contains the actual values\n    else:\n        # Make sure values are list of lists (in case we started with a single\n        # neuron)\n        if len(neurons) == 1 and len(by) != len(neurons):\n            values = [by]\n        else:\n            values = by\n\n    # At this point we expect to have values for each neuron\n    if len(values) != len(neurons):\n        raise ValueError(f'Got {len(values)} values for {len(neurons)} neurons.')\n\n    # We also expect to have a value for every single node/vertex\n    for n, v in zip(neurons, values):\n        if isinstance(n, core.TreeNeuron):\n            if len(v) != n.n_nodes:\n                raise ValueError(f'Got {len(v)} for {neurons.n_nodes} nodes '\n                                 f'for neuron {n.id}')\n        elif isinstance(n, core.MeshNeuron):\n            if len(v) != n.n_faces and len(v) != n.n_vertices:\n                raise ValueError(f'Got {len(v)} for {neurons.n_faces} faces '\n                                 f'and {neurons.n_vertices} vertices for '\n                                 f'neuron {n.id}')\n        else:\n            raise TypeError(f'Unable to map colors for neurons of type {type(n)}')\n\n    # Now check for NaNs\n    has_nan = False\n    for v in values:\n        if any(pd.isnull(v)):\n            has_nan = True\n            break\n\n    if has_nan:\n        if na == 'raise':\n            raise ValueError('Values contain NaNs.')\n        else:\n            # Make sure na is a valid color\n            try:\n                na = mcl.to_rgba(na, alpha=alpha)\n            except ValueError:\n                raise ValueError('`na` must be either \"raise\" or a valid color '\n                                 f'to replace NA values. Unable to convert {na}'\n                                 ' to a color.')\n\n    # First check if data is numerical or categorical\n    is_num = [utils.is_numeric(a, bool_numeric=False, try_convert=False) for a in values]\n    # If numerical and we weren't given a categorical palette\n    if all(is_num) and not isinstance(palette, dict):\n        # Get min/max values\n        if not vmin:\n            vmin = [np.nanmin(v) for v in values]\n\n            if norm_global:\n                vmin = np.repeat(np.min(vmin), len(values))\n        else:\n            vmin = np.repeat(vmin, len(values))\n\n        if not vmax:\n            vmax = [np.nanmax(v) for v in values]\n\n            if norm_global:\n                vmax = np.repeat(np.max(vmax), len(values))\n        else:\n            vmax = np.repeat(vmax, len(values))\n\n        if any(vmin == vmax):\n            raise ValueError('Unable to normalize values: at least some min '\n                             f'and max values in \"{by}\" are the same. Use '\n                             '`vmin` and `vmax` parameters to manually set '\n                             'range for normalization.')\n\n        # Normalize values\n        values = [(np.asarray(v) - mn) / (mx - mn) for v, mn, mx in zip(values, vmin, vmax)]\n\n        # Get the colormap\n        if not isinstance(palette, str):\n            raise TypeError('Expected name of matplotlib colormap for numerical'\n                            f' data, got {type(palette)}')\n        cmap = plt.get_cmap(palette)\n        colors = []\n        for v in values:\n            c = np.zeros((len(v), 4))\n            if any(pd.isnull(v)):\n                c[pd.isnull(v), :] = na\n            c[~pd.isnull(v), :] = cmap(v[~pd.isnull(v)], alpha=alpha)\n\n            if color_range == 255:\n                c[:, :3] = (c[:, :3] * 255).astype(int)\n\n            # Add alpha - note that we slightly clip the value to prevent\n            # any color from being entirely invisible\n            if use_alpha:\n                c[:, 3] = np.clip(v + 0.05, a_max=1, a_min=0)\n\n            colors.append(c)\n    # We don't want to deal with mixed data\n    # elif any(is_num):\n    #     raise ValueError('Data appears to be mixed numeric and non-numeric.')\n    else:\n        # Find unique values\n        unique_v = np.unique([v for l in values for v in np.unique(l)])\n\n        if isinstance(palette, str):\n            palette = sns.color_palette(palette, len(unique_v))\n\n        if not isinstance(palette, dict):\n            if len(palette) != len(unique_v):\n                raise ValueError(f'Got {len(palette)} colors for '\n                                 f'{len(unique_v)} unique values.')\n            palette = dict(zip(unique_v, palette))\n\n        # Check if dict palette contains all possible values\n        miss = [v for v in unique_v if v not in palette]\n        if any(miss):\n            raise ValueError('Value(s) missing from palette dictionary: '\n                             ', '.join(miss))\n\n        # Make sure colors are what we need\n        palette = {v: mcl.to_rgba(c, alpha=alpha) for v, c in palette.items()}\n\n        # Alpha values doesn't exactly make sense for categorical data but\n        # who am I to judge? We will simply use the alphanumerical order.\n        if use_alpha:\n            alpha_map = {v: (i + 1)/(len(palette) + 1) for i, v in enumerate(palette.keys())}\n\n        colors = []\n        for v in values:\n            c = [palette.get(x, na) for x in v]\n            c = np.array(c)\n\n            if color_range == 255:\n                c[:, :3] = (c[:, :3] * 255).astype(int)\n\n            if use_alpha:\n                c[:, 3] = [alpha_map.get(x, 0) for x in v]\n\n            colors.append(c)\n\n    return colors\n</code></pre>"},{"location":"reference/navis/plotting/dd/","title":"dd","text":""},{"location":"reference/navis/plotting/dd/#navis.plotting.dd.Matplotlib2dSettings","title":"<code>navis.plotting.dd.Matplotlib2dSettings</code>  <code>dataclass</code>","text":"<p>Additional plotting parameters for Matplotlib 2d backend.</p> Source code in <code>navis/plotting/settings.py</code> <pre><code>@dataclass\nclass Matplotlib2dSettings(BasePlottingSettings):\n    \"\"\"Additional plotting parameters for Matplotlib 2d backend.\"\"\"\n\n    _name = \"matplotlib backend\"\n\n    method: Literal[\"2d\", \"3d\", \"3d_complex\"] = \"2d\"\n    group_neurons: bool = False\n    autoscale: bool = True\n    orthogonal: bool = True\n    scalebar: Union[int, float, str, pint.Quantity] = False\n    volume_outlines: bool = False\n    volume_outlines_alpha: float = 0.001\n    rasterize: bool = False\n    view: Tuple[str, str] = (\"x\", \"y\")\n    figsize: Optional[Tuple[float, float]] = None\n    ax: Optional[mpl.axes.Axes] = None\n    mesh_shade: bool = False\n    non_view_axes3d: Literal[\"hide\", \"show\", \"fade\"] = \"hide\"\n    cn_zorder: Optional[int] = None\n\n    depth_coloring: bool = False\n    depth_scale: bool = True\n</code></pre>"},{"location":"reference/navis/plotting/dd/#navis.plotting.dd.color_to_cmap","title":"<code>navis.plotting.dd.color_to_cmap</code>","text":"<p>Convert single color to color palette.</p> Source code in <code>navis/plotting/dd.py</code> <pre><code>def color_to_cmap(color):\n    \"\"\"Convert single color to color palette.\"\"\"\n    color = mcl.to_rgb(color)\n\n    colors = [[color[0], color[1], color[2], 0], [color[0], color[1], color[2], 1]]\n\n    return mcl.LinearSegmentedColormap.from_list(\"Palette\", colors, N=256)\n</code></pre>"},{"location":"reference/navis/plotting/dd/#navis.plotting.dd.proj_points","title":"<code>navis.plotting.dd.proj_points</code>","text":"<p>Project points using a projection matrix.</p> <p>This was previously done using the analagous function mpl_toolkits.mplot3d.proj3d.proj_points but that is deprecated.</p> Source code in <code>navis/plotting/dd.py</code> <pre><code>def proj_points(points, M):\n    \"\"\"Project points using a projection matrix.\n\n    This was previously done using the analagous function\n    mpl_toolkits.mplot3d.proj3d.proj_points but that is deprecated.\n    \"\"\"\n    xs, ys, zs = zip(*points)\n    vec = np.array([xs, ys, zs, np.ones_like(xs)])\n\n    vecw = np.dot(M, vec)\n    w = vecw[3]\n    # clip here..\n    txs, tys, tzs = vecw[0] / w, vecw[1] / w, vecw[2] / w\n\n    return np.column_stack((txs, tys, tzs))\n</code></pre>"},{"location":"reference/navis/plotting/dd/#navis.plotting.dd.segments_to_coords","title":"<code>navis.plotting.dd.segments_to_coords</code>","text":"<p>Turn lists of node IDs into coordinates.</p> <p>Will use navis-fastcore if available.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Must contain the nodes\n</code></pre> <p> TYPE: <code>            TreeNeuron</code> </p> <code>node_colors</code> <pre><code>        A color for each node in `x.nodes`. If provided, will\n        also return a list of colors sorted to match coordinates.\n</code></pre> <p> TYPE: <code>  numpy.ndarray</code> DEFAULT: <code>None</code> </p> <code>modifier</code> <pre><code>        Use e.g. to modify/invert x/y/z axes.\n</code></pre> <p> TYPE: <code>     ints</code> DEFAULT: <code>(1, 1, 1)</code> </p> RETURNS DESCRIPTION <code>coords</code> <p>[(x, y, z), (x, y, z), ... ]</p> <p> TYPE: <code>list of tuples</code> </p> <code>colors</code> <p>If <code>node_colors</code> provided will return a copy of it sorted to match <code>coords</code>.</p> <p> TYPE: <code>list of colors</code> </p> Source code in <code>navis/plotting/plot_utils.py</code> <pre><code>def segments_to_coords(\n    x: core.TreeNeuron,\n    modifier: Optional[Tuple[float, float, float]] = (1, 1, 1),\n    node_colors: Optional[np.ndarray] = None,\n) -&gt; List[np.ndarray]:\n    \"\"\"Turn lists of node IDs into coordinates.\n\n    Will use navis-fastcore if available.\n\n    Parameters\n    ----------\n    x :             TreeNeuron\n                    Must contain the nodes\n    node_colors :   numpy.ndarray, optional\n                    A color for each node in `x.nodes`. If provided, will\n                    also return a list of colors sorted to match coordinates.\n    modifier :      ints, optional\n                    Use e.g. to modify/invert x/y/z axes.\n\n    Returns\n    -------\n    coords :        list of tuples\n                    [(x, y, z), (x, y, z), ... ]\n    colors :        list of colors\n                    If `node_colors` provided will return a copy of it sorted\n                    to match `coords`.\n\n    \"\"\"\n    colors = None\n\n    if utils.fastcore is not None:\n        if node_colors is None:\n            coords = utils.fastcore.segment_coords(\n                x.nodes.node_id.values,\n                x.nodes.parent_id.values,\n                x.nodes[[\"x\", \"y\", \"z\"]].values,\n            )\n        else:\n            coords, colors = utils.fastcore.segment_coords(\n                x.nodes.node_id.values,\n                x.nodes.parent_id.values,\n                x.nodes[[\"x\", \"y\", \"z\"]].values,\n                node_colors=node_colors,\n            )\n    else:\n        # Using a dictionary here is orders of manitude faster than .loc[]!\n        locs: Dict[int, Tuple[float, float, float]]\n        # Oddly, this is also the fastest way to generate the dictionary\n        nodes = x.nodes\n        locs = {\n            i: (x, y, z)\n            for i, x, y, z in zip(\n                nodes.node_id.values, nodes.x.values, nodes.y.values, nodes.z.values\n            )\n        }  # type: ignore\n        # locs = {r.node_id: (r.x, r.y, r.z) for r in x.nodes.itertuples()}  # type: ignore\n        coords = [np.array([locs[tn] for tn in s]) for s in x.segments]\n\n        if not isinstance(node_colors, type(None)):\n            ilocs = dict(zip(x.nodes.node_id.values, np.arange(x.nodes.shape[0])))\n            colors = [node_colors[[ilocs[tn] for tn in s]] for s in x.segments]\n\n    modifier = np.asarray(modifier)\n    if (modifier != 1).any():\n        for seg in coords:\n            np.multiply(seg, modifier, out=seg)\n\n    if colors is not None:\n        return coords, colors\n\n    return coords\n</code></pre>"},{"location":"reference/navis/plotting/dd/#navis.plotting.dd.tn_pairs_to_coords","title":"<code>navis.plotting.dd.tn_pairs_to_coords</code>","text":"<p>Return pairs of child-&gt;parent node coordinates.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Must contain the nodes.\n</code></pre> <p> TYPE: <code>        TreeNeuron</code> </p> <code>modifier</code> <pre><code>    Use to modify/invert x/y/z axes.\n</code></pre> <p> TYPE: <code> ints</code> DEFAULT: <code>(1, 1, 1)</code> </p> RETURNS DESCRIPTION <code>coords</code> <p><code>[[[x1, y1, z1], [x2, y2, z2]], [[x3, y3, y4], [x4, y4, z4]]]</code></p> <p> TYPE: <code>np.array</code> </p> Source code in <code>navis/plotting/plot_utils.py</code> <pre><code>def tn_pairs_to_coords(\n    x: core.TreeNeuron, modifier: Optional[Tuple[float, float, float]] = (1, 1, 1)\n) -&gt; np.ndarray:\n    \"\"\"Return pairs of child-&gt;parent node coordinates.\n\n    Parameters\n    ----------\n    x :         TreeNeuron\n                Must contain the nodes.\n    modifier :  ints, optional\n                Use to modify/invert x/y/z axes.\n\n    Returns\n    -------\n    coords :    np.array\n                `[[[x1, y1, z1], [x2, y2, z2]], [[x3, y3, y4], [x4, y4, z4]]]`\n\n    \"\"\"\n    if not isinstance(modifier, np.ndarray):\n        modifier = np.array(modifier)\n\n    nodes = x.nodes[x.nodes.parent_id &gt;= 0]\n\n    tn_co = nodes.loc[:, [\"x\", \"y\", \"z\"]].values\n    parent_co = (\n        x.nodes.set_index(\"node_id\").loc[nodes.parent_id.values, [\"x\", \"y\", \"z\"]].values\n    )\n\n    coords = np.append(tn_co, parent_co, axis=1)\n\n    if any(modifier != 1):\n        coords *= modifier\n\n    return coords.reshape((coords.shape[0], 2, 3))\n</code></pre>"},{"location":"reference/navis/plotting/dd/#navis.plotting.dd.update_axes3d_bounds","title":"<code>navis.plotting.dd.update_axes3d_bounds</code>","text":"<p>Update axis bounds and remove default points (0,0,0) and (1,1,1).</p> Source code in <code>navis/plotting/dd.py</code> <pre><code>def update_axes3d_bounds(ax):\n    \"\"\"Update axis bounds and remove default points (0,0,0) and (1,1,1).\"\"\"\n    # Collect data points present in the figure\n    points = []\n    for c in ax.collections:\n        if isinstance(c, Line3DCollection):\n            for s in c._segments3d:\n                points.append(s)\n        elif isinstance(c, Poly3DCollection):\n            points.append(c._vec[:3, :].T)\n        elif isinstance(c, (Path3DCollection, Patch3DCollection)):\n            points.append(np.array(c._offsets3d).T)\n\n    if not len(points):\n        return\n\n    points = np.vstack(points)\n\n    # If this is the first set of points, we need to overwrite the defaults\n    # That should happen automatically but for some reason doesn't for 3d axes\n    if not getattr(ax, \"had_data\", False):\n        mn = points.min(axis=0)\n        mx = points.max(axis=0)\n        new_xybounds = np.array([[mn[0], mn[1]], [mx[0], mx[1]]])\n        new_zzbounds = np.array([[mn[2], mn[2]], [mx[2], mx[2]]])\n        ax.xy_dataLim.set_points(new_xybounds)\n        ax.zz_dataLim.set_points(new_zzbounds)\n        ax.xy_viewLim.set_points(new_xybounds)\n        ax.zz_viewLim.set_points(new_zzbounds)\n        ax.had_data = True\n    else:\n        ax.auto_scale_xyz(\n            points[:, 0].tolist(),\n            points[:, 1].tolist(),\n            points[:, 2].tolist(),\n            had_data=True,\n        )\n</code></pre>"},{"location":"reference/navis/plotting/ddd/","title":"ddd","text":""},{"location":"reference/navis/plotting/ddd/#navis.plotting.ddd.K3dSettings","title":"<code>navis.plotting.ddd.K3dSettings</code>  <code>dataclass</code>","text":"<p>Additional plotting parameters for K3d backend.</p> Source code in <code>navis/plotting/settings.py</code> <pre><code>@dataclass\nclass K3dSettings(BasePlottingSettings):\n    \"\"\"Additional plotting parameters for K3d backend.\"\"\"\n\n    _name = \"k3d backend\"\n\n    height: int = 600\n    inline: bool = True\n    legend_group: Optional[str] = None\n    plot: Optional[\"k3d.Plot\"] = None\n</code></pre>"},{"location":"reference/navis/plotting/ddd/#navis.plotting.ddd.OctarineSettings","title":"<code>navis.plotting.ddd.OctarineSettings</code>  <code>dataclass</code>","text":"<p>Additional plotting parameters for Octarine backend.</p> Source code in <code>navis/plotting/settings.py</code> <pre><code>@dataclass\nclass OctarineSettings(BasePlottingSettings):\n    \"\"\"Additional plotting parameters for Octarine backend.\"\"\"\n\n    _name = \"octarine backend\"\n\n    clear: bool = False\n    center: bool = True\n    viewer: Optional[Union[\"octarine.Viewer\", Literal[\"new\"]]] = None\n    random_ids: bool = False\n    camera: Literal[\"ortho\", \"perspective\"] = \"ortho\"\n    control: Literal[\"trackball\", \"panzoom\", \"fly\", \"orbit\"] = \"trackball\"\n    show: bool = True\n    size: Optional[Tuple[int, int]] = None\n    offscreen: bool = False\n    spacing: Optional[Tuple[float, float, float]] = None\n\n    # These are viewer-specific settings that we must not pass to the plotting\n    # function\n    _viewer_settings: tuple[str] = (\n        \"clear\",\n        \"center\",\n        \"viewer\",\n        \"camera\",\n        \"control\",\n        \"show\",\n        \"size\",\n        \"offscreen\",\n        \"scatter_kws\",\n        \"spacing\"\n    )\n</code></pre>"},{"location":"reference/navis/plotting/ddd/#navis.plotting.ddd.PlotlySettings","title":"<code>navis.plotting.ddd.PlotlySettings</code>  <code>dataclass</code>","text":"<p>Additional plotting parameters for Plotly backend.</p> Source code in <code>navis/plotting/settings.py</code> <pre><code>@dataclass\nclass PlotlySettings(BasePlottingSettings):\n    \"\"\"Additional plotting parameters for Plotly backend.\"\"\"\n\n    _name = \"plotly backend\"\n\n    fig: Optional[Union[\"plotly.Figure\", dict]] = None\n    inline: bool = True\n    title: Optional[str] = None\n    fig_autosize: bool = True\n    hover_name: Optional[str] = False\n    hover_id: bool = False\n    legend: bool = True\n    legend_orientation: Literal[\"h\", \"v\"] = \"v\"\n    legend_group: Optional[str] = None\n    volume_legend: bool = False\n    width: Optional[int] = None\n    height: Optional[int] = 600\n    linewidth: Optional[float] = None  # for plotly, linewidth 1 is too thin, we default to 3 in graph_objs.py\n    linestyle: str = \"-\"\n</code></pre>"},{"location":"reference/navis/plotting/ddd/#navis.plotting.ddd.VispySettings","title":"<code>navis.plotting.ddd.VispySettings</code>  <code>dataclass</code>","text":"<p>Additional plotting parameters for Vispy backend.</p> Source code in <code>navis/plotting/settings.py</code> <pre><code>@dataclass\nclass VispySettings(BasePlottingSettings):\n    \"\"\"Additional plotting parameters for Vispy backend.\"\"\"\n\n    _name = \"vispy backend\"\n\n    clear: bool = False\n    center: bool = True\n    combine: bool = False\n    title: Optional[str] = None\n    viewer: Optional[\"navis.Viewer\"] = None\n    shininess: float = 0\n    shading: str = \"smooth\"\n    size: Optional[Tuple[int, int]] = (800, 600)\n    show: bool = True\n    name: Optional[str] = None\n</code></pre>"},{"location":"reference/navis/plotting/ddd/#navis.plotting.ddd.plot3d_k3d","title":"<code>navis.plotting.ddd.plot3d_k3d</code>","text":"<p>Plot3d() helper function to generate k3d 3D plots. This is just to improve readability and structure of the code.</p> Source code in <code>navis/plotting/ddd.py</code> <pre><code>def plot3d_k3d(x, **kwargs):\n    \"\"\"\n    Plot3d() helper function to generate k3d 3D plots. This is just to\n    improve readability and structure of the code.\n    \"\"\"\n    # Lazy import because k3d is not (yet) a hard dependency\n    try:\n        import k3d\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"navis.plot3d() with `k3d` backend requires the k3d library \"\n            \"to be installed:\\n  pip3 install k3d -U\"\n        )\n\n    from .k3d.k3d_objects import neuron2k3d, volume2k3d, scatter2k3d\n\n    settings = K3dSettings().update_settings(**kwargs)\n\n    # Parse objects to plot\n    (neurons, volumes, points, visual) = utils.parse_objects(x)\n\n    neuron_cmap, volumes_cmap = prepare_colormap(\n        settings.color,\n        neurons=neurons,\n        volumes=volumes,\n        palette=settings.palette,\n        color_by=None,\n        alpha=settings.alpha,\n        color_range=255,\n    )\n\n    data = []\n    if neurons:\n        data += neuron2k3d(neurons, neuron_cmap, settings)\n    if volumes:\n        data += volume2k3d(volumes, volumes_cmap, settings)\n    if points:\n        data += scatter2k3d(points, **settings.scatter_kws)\n\n    # If not provided generate a plot\n    if not settings.plot:\n        plot = k3d.plot(height=settings.height)\n        plot.camera_rotate_speed = 5\n        plot.camera_zoom_speed = 2\n        plot.camera_pan_speed = 1\n        plot.grid_visible = False\n\n    # Add data\n    for trace in data:\n        plot += trace\n\n    if settings.inline and utils.is_jupyter():\n        plot.display()\n    else:\n        logger.info(\"Use the `.display()` method to show the plot.\")\n        return plot\n</code></pre>"},{"location":"reference/navis/plotting/ddd/#navis.plotting.ddd.plot3d_octarine","title":"<code>navis.plotting.ddd.plot3d_octarine</code>","text":"<p>Plot3d() helper function to generate octarine 3D plots.</p> <p>This is just to improve readability. Its only purpose is to find the existing viewer or generate a new one.</p> Source code in <code>navis/plotting/ddd.py</code> <pre><code>def plot3d_octarine(x, **kwargs):\n    \"\"\"Plot3d() helper function to generate octarine 3D plots.\n\n    This is just to improve readability. Its only purpose is to find the\n    existing viewer or generate a new one.\n\n    \"\"\"\n    # Lazy import because octarine is not a hard dependency\n    try:\n        import octarine as oc\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"navis.plot3d() with the `octarine` backend requires the `octarine3d` library \"\n            \"to be installed:\\n  pip3 install octarine3 octarine-navis-plugin -U\"\n        )\n\n    if not hasattr(oc.Viewer, \"add_neurons\"):\n        raise ModuleNotFoundError(\n            \"Looks like the navis plugin for octarine is not installed. \"\n            \"Please install it via pip:\\n  pip install octarine-navis-plugin\"\n        )\n\n    settings = OctarineSettings().update_settings(**kwargs)\n\n    # Parse objects to plot\n    (neurons, volumes, points, visuals) = utils.parse_objects(x)\n\n    # Check if any existing viewer has already been closed\n    if isinstance(getattr(config, \"primary_viewer\", None), oc.Viewer):\n        try:\n            getattr(config, \"primary_viewer\").canvas.__repr__()\n        except RuntimeError:\n            config.primary_viewer = None\n\n    if settings.viewer in (None, \"new\"):\n        # If it does not exists yet, initialize a canvas object and make global\n        if (\n            not isinstance(getattr(config, \"primary_viewer\", None), oc.Viewer)\n            or settings.viewer == \"new\"\n        ):\n            viewer = config.primary_viewer = oc.Viewer(\n                size=settings.size,\n                camera=settings.camera,\n                control=settings.control,\n                show=False,\n                offscreen=settings.offscreen or os.environ.get(\"NAVIS_HEADLESS\", False),\n            )\n        else:\n            viewer = getattr(config, \"primary_viewer\", None)\n    else:\n        viewer = settings.pop(\"viewer\", getattr(config, \"primary_viewer\"))\n\n    # Make sure viewer is visible\n    if settings.show:\n        viewer.show()\n\n    # We need to pop clear/clear3d to prevent clearing again later\n    if settings.clear:\n        settings.clear = False  # clear only once\n        viewer.clear()\n\n    # Add object (the viewer currently takes care of producing the visuals)\n    if neurons:\n        # We need to pop viewer-specific settings to prevent errors in plotting functions\n        neuron_settings = settings.to_dict()\n        for key in settings._viewer_settings:\n            neuron_settings.pop(key, None)\n        viewer.add_neurons(neurons, center=settings.get(\"center\", True), **neuron_settings)\n    if volumes:\n        for v in volumes:\n            viewer.add_mesh(\n                v,\n                name=getattr(v, \"name\", None),\n                color=getattr(v, \"color\", (0.95, 0.95, 0.95, 0.1)),\n                alpha=getattr(v, \"alpha\", None),\n                center=settings.center,\n            )\n    if points:\n        for p in points:\n            viewer.add_points(p, center=settings.center, **settings.scatter_kws)\n\n    return viewer\n</code></pre>"},{"location":"reference/navis/plotting/ddd/#navis.plotting.ddd.plot3d_plotly","title":"<code>navis.plotting.ddd.plot3d_plotly</code>","text":"<p>Plot3d() helper function to generate plotly 3D plots. This is just to improve readability and structure of the code.</p> Source code in <code>navis/plotting/ddd.py</code> <pre><code>def plot3d_plotly(x, **kwargs):\n    \"\"\"\n    Plot3d() helper function to generate plotly 3D plots. This is just to\n    improve readability and structure of the code.\n    \"\"\"\n    # Lazy import because plotly is not a hard dependency\n    try:\n        import plotly.graph_objs as go\n        from .plotly.graph_objs import (\n            neuron2plotly,\n            volume2plotly,\n            scatter2plotly,\n            layout2plotly,\n        )\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"navis.plot3d() with the `plotly` backend requires the `plotly` library \"\n            \"to be installed:\\n  pip3 install plotly -U\"\n        )\n\n    settings = PlotlySettings().update_settings(**kwargs)\n\n    # Parse objects to plot\n    (neurons, volumes, points, visual) = utils.parse_objects(x)\n\n    neuron_cmap, volumes_cmap = prepare_colormap(\n        settings.color,\n        neurons=neurons,\n        volumes=volumes,\n        palette=settings.palette,\n        color_by=None,\n        alpha=settings.alpha,\n        color_range=255,\n    )\n\n    data = []\n    if neurons:\n        data += neuron2plotly(neurons, neuron_cmap, settings)\n    if volumes:\n        data += volume2plotly(volumes, volumes_cmap, settings)\n    if points:\n        data += scatter2plotly(points, **settings.scatter_kws)\n\n    layout = layout2plotly(**settings.to_dict())\n\n    # If not provided generate a figure dictionary\n    fig = settings.fig if settings.fig else go.Figure(layout=layout)\n    if not isinstance(fig, (dict, go.Figure)):\n        raise TypeError(\n            \"`fig` must be plotly.graph_objects.Figure or dict, got \" f\"{type(fig)}\"\n        )\n\n    # Add data\n    for trace in data:\n        fig.add_trace(trace)\n\n    if settings.inline and utils.is_jupyter():\n        fig.show()\n    else:\n        logger.info(\"Use the `.show()` method to plot the figure.\")\n        return fig\n</code></pre>"},{"location":"reference/navis/plotting/ddd/#navis.plotting.ddd.plot3d_vispy","title":"<code>navis.plotting.ddd.plot3d_vispy</code>","text":"<p>Plot3d() helper function to generate vispy 3D plots.</p> <p>This is just to improve readability. Its only purpose is to find the existing viewer or generate a new one.</p> Source code in <code>navis/plotting/ddd.py</code> <pre><code>def plot3d_vispy(x, **kwargs):\n    \"\"\"Plot3d() helper function to generate vispy 3D plots.\n\n    This is just to improve readability. Its only purpose is to find the\n    existing viewer or generate a new one.\n\n    \"\"\"\n    from .vispy.viewer import Viewer\n\n    # If this likely the first invoke, warn the user that vispy is deprecated\n    if not hasattr(config, \"primary_viewer\"):\n        warnings.warn(\n            (\n                \"The `vispy` backend is depcrecated and will be removed in a future version of navis. \"\n                \"We recommend to use the `octarine` backend instead. If that is for some reason not possible, \"\n                \"please let us know via the issue tracker at https://github.com/navis-org/navis/issues asap.\"\n            ),\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    settings = VispySettings().update_settings(**kwargs)\n\n    # Parse objects to plot\n    (neurons, volumes, points, visuals) = utils.parse_objects(x)\n\n    if settings.viewer in (None, \"new\"):\n        # If does not exists yet, initialise a canvas object and make global\n        if (\n            not isinstance(getattr(config, \"primary_viewer\", None), Viewer)\n            or settings.viewer == \"new\"\n        ):\n            viewer = config.primary_viewer = Viewer(size=settings.size)\n        else:\n            viewer = getattr(config, \"primary_viewer\", None)\n    else:\n        viewer = settings.viewer\n\n    # Make sure viewer is visible\n    if settings.show:\n        viewer.show()\n\n    # We need to pop clear/clear3d to prevent clearing again later\n    if settings.clear:\n        settings.clear = False\n        viewer.clear()\n\n    # Add objects (the viewer currently takes care of producing the visuals)\n    if neurons:\n        viewer.add(neurons, **settings.to_dict())\n    if volumes:\n        viewer.add(volumes, **settings.to_dict())\n    if points:\n        viewer.add(points, scatter_kws=settings.scatter_kws)\n\n    return viewer\n</code></pre>"},{"location":"reference/navis/plotting/plot_utils/","title":"plot_utils","text":""},{"location":"reference/navis/plotting/plot_utils/#navis.plotting.plot_utils.fibonacci_sphere","title":"<code>navis.plotting.plot_utils.fibonacci_sphere</code>","text":"<p>Generate points on a sphere.</p> Source code in <code>navis/plotting/plot_utils.py</code> <pre><code>def fibonacci_sphere(samples: int = 1, randomize: bool = True) -&gt; list:\n    \"\"\"Generate points on a sphere.\"\"\"\n    rnd = 1.0\n    if randomize:\n        rnd = random.random() * samples\n\n    points = []\n    offset = 2.0 / samples\n    increment = math.pi * (3.0 - math.sqrt(5.0))\n\n    for i in range(samples):\n        y = ((i * offset) - 1) + (offset / 2)\n        r = math.sqrt(1 - pow(y, 2))\n\n        phi = ((i + rnd) % samples) * increment\n\n        x = math.cos(phi) * r\n        z = math.sin(phi) * r\n\n        points.append([x, y, z])\n\n    return np.array(points)\n</code></pre>"},{"location":"reference/navis/plotting/plot_utils/#navis.plotting.plot_utils.make_tube","title":"<code>navis.plotting.plot_utils.make_tube</code>","text":"<p>Generate tube mesh (vertices + faces) from lines.</p> <p>This code was modified from the vispy library.</p> PARAMETER DESCRIPTION <code>segments</code> <pre><code>        List of lists of x/y/z coordinates.\n</code></pre> <p> TYPE: <code>     list</code> </p> <code>radii</code> <pre><code>        Either a single radius used for all nodes or list of lists of\n        floats with the same shape as `segments`.\n</code></pre> <p> TYPE: <code>        float | list of floats</code> DEFAULT: <code>1.0</code> </p> <code>tube_points</code> <pre><code>        Number of points making up the circle of the cross-section\n        of the tube.\n</code></pre> <p> TYPE: <code>  int</code> DEFAULT: <code>8</code> </p> <code>use_normals</code> <pre><code>        If True will rotate tube along it's curvature.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>vertices</code> <p> TYPE: <code>np.ndarray</code> </p> <code>faces</code> <p> TYPE: <code>np.ndarray</code> </p> Source code in <code>navis/plotting/plot_utils.py</code> <pre><code>def make_tube(segments, radii=1.0, tube_points=8, use_normals=True):\n    \"\"\"Generate tube mesh (vertices + faces) from lines.\n\n    This code was modified from the vispy library.\n\n    Parameters\n    ----------\n    segments :      list\n                    List of lists of x/y/z coordinates.\n    radii :         float | list of floats\n                    Either a single radius used for all nodes or list of lists of\n                    floats with the same shape as `segments`.\n    tube_points :   int\n                    Number of points making up the circle of the cross-section\n                    of the tube.\n    use_normals :   bool\n                    If True will rotate tube along it's curvature.\n\n    Returns\n    -------\n    vertices :      np.ndarray\n    faces :         np.ndarray\n\n    \"\"\"\n    vertices = np.empty((0, 3), dtype=np.float64)\n    indices = np.empty((0, 3), dtype=np.uint32)\n\n    if not isinstance(radii, Iterable):\n        radii = [[radii] * len(points) for points in segments]\n\n    for points, radius in zip(segments, radii):\n        # Need to make sure points are floats\n        points = np.array(points).astype(float)\n\n        # Skip single points\n        if len(points) &lt; 2:\n            continue\n\n        if use_normals:\n            _, normals, binormals = _frenet_frames(points)\n        else:\n            _ = normals = binormals = np.ones((len(points), 3))\n\n        n_segments = len(points) - 1\n\n        if not isinstance(radius, Iterable):\n            radius = [radius] * len(points)\n\n        radius = np.array(radius)\n\n        # Vertices for each point on the circle\n        verts = np.repeat(points, tube_points, axis=0)\n\n        v = np.arange(tube_points, dtype=np.float64) / tube_points * 2 * np.pi\n\n        all_cx = (\n            radius\n            * -1.0\n            * np.tile(np.cos(v), points.shape[0]).reshape(\n                (tube_points, points.shape[0]), order=\"F\"\n            )\n        ).T\n        cx_norm = (all_cx[:, :, np.newaxis] * normals[:, np.newaxis, :]).reshape(\n            verts.shape\n        )\n\n        all_cy = (\n            radius\n            * np.tile(np.sin(v), points.shape[0]).reshape(\n                (tube_points, points.shape[0]), order=\"F\"\n            )\n        ).T\n        cy_norm = (all_cy[:, :, np.newaxis] * binormals[:, np.newaxis, :]).reshape(\n            verts.shape\n        )\n\n        verts = verts + cx_norm + cy_norm\n\n        # Generate indices for the first segment\n        ix = np.arange(0, tube_points)\n\n        # Repeat indices n_segments-times\n        ix = np.tile(ix, n_segments)\n\n        # Offset indices by number segments and tube points\n        offsets = np.repeat((np.arange(0, n_segments)) * tube_points, tube_points)\n        ix += offsets\n\n        # Turn indices into faces\n        ix_a = ix\n        ix_b = ix + tube_points\n\n        ix_c = ix_b.reshape((n_segments, tube_points))\n        ix_c = np.append(ix_c[:, 1:], ix_c[:, [0]], axis=1)\n        ix_c = ix_c.ravel()\n\n        ix_d = ix_a.reshape((n_segments, tube_points))\n        ix_d = np.append(ix_d[:, 1:], ix_d[:, [0]], axis=1)\n        ix_d = ix_d.ravel()\n\n        faces1 = np.concatenate((ix_a, ix_b, ix_d), axis=0).reshape(\n            (n_segments * tube_points, 3), order=\"F\"\n        )\n        faces2 = np.concatenate((ix_b, ix_c, ix_d), axis=0).reshape(\n            (n_segments * tube_points, 3), order=\"F\"\n        )\n\n        faces = np.append(faces1, faces2, axis=0)\n\n        # Offset faces against already existing vertices\n        faces += vertices.shape[0]\n\n        # Add vertices and faces to total collection\n        vertices = np.append(vertices, verts, axis=0)\n        indices = np.append(indices, faces, axis=0)\n\n    return vertices, indices\n</code></pre>"},{"location":"reference/navis/plotting/plot_utils/#navis.plotting.plot_utils.rotate","title":"<code>navis.plotting.plot_utils.rotate</code>","text":"<p>Generate a 4x4 rotation matrix for rotation about a vector.</p> <p>Modified from <code>vispy.utils.transforms</code>.</p> PARAMETER DESCRIPTION <code>angle</code> <pre><code>    The angle of rotation, in degrees.\n</code></pre> <p> TYPE: <code>    float</code> </p> <code>axis</code> <pre><code>    The x, y, z coordinates of the axis direction vector.\n</code></pre> <p> TYPE: <code>     ndarray</code> </p> RETURNS DESCRIPTION <code>M</code> <p>Transformation matrix describing the rotation.</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>navis/plotting/plot_utils.py</code> <pre><code>def rotate(angle, axis, dtype=None):\n    \"\"\"Generate a 4x4 rotation matrix for rotation about a vector.\n\n    Modified from `vispy.utils.transforms`.\n\n    Parameters\n    ----------\n    angle :     float\n                The angle of rotation, in degrees.\n    axis :      ndarray\n                The x, y, z coordinates of the axis direction vector.\n\n    Returns\n    -------\n    M :     ndarray\n            Transformation matrix describing the rotation.\n\n    \"\"\"\n    angle = np.radians(angle)\n    assert len(axis) == 3\n    x, y, z = axis / np.linalg.norm(axis)\n    c, s = math.cos(angle), math.sin(angle)\n    cx, cy, cz = (1 - c) * x, (1 - c) * y, (1 - c) * z\n    M = np.array(\n        [\n            [cx * x + c, cy * x - z * s, cz * x + y * s, 0.0],\n            [cx * y + z * s, cy * y + c, cz * y - x * s, 0.0],\n            [cx * z - y * s, cy * z + x * s, cz * z + c, 0.0],\n            [0.0, 0.0, 0.0, 1.0],\n        ],\n        dtype,\n    ).T\n    return M\n</code></pre>"},{"location":"reference/navis/plotting/settings/","title":"settings","text":""},{"location":"reference/navis/plotting/settings/#navis.plotting.settings.BasePlottingSettings","title":"<code>navis.plotting.settings.BasePlottingSettings</code>  <code>dataclass</code>","text":"<p>Plotting parameters common to all functions/backends.</p> Source code in <code>navis/plotting/settings.py</code> <pre><code>@dataclass\nclass BasePlottingSettings(Settings):\n    \"\"\"Plotting parameters common to all functions/backends.\"\"\"\n\n    _name = \"BasePlottingSettings\"\n\n    # For TreeNeurons\n    soma: bool = True\n    radius: bool = False  # True | False | \"auto\"\n    linewidth: float = 1\n    linestyle: str = \"-\"\n\n    # For Dotprops\n    dps_scale_vec: float = \"auto\"\n\n    # All neurons\n    connectors: bool = False\n    connectors_only: bool = False\n    cn_size: Optional[float] = None\n    cn_alpha: Optional[float] = None\n    cn_layout: dict = field(default_factory=dict)\n    cn_colors: dict = field(default_factory=dict)\n    cn_mesh_colors: bool = False\n    color: Optional[\n        Union[\n            str,\n            Tuple[float, float, float],\n            List[Union[str, Tuple[float, float, float]]],\n            dict,\n        ]\n    ] = None\n    color_by: Optional[Union[str, np.ndarray, List[np.ndarray]]] = None\n    shade_by: Optional[Union[str, np.ndarray, List[np.ndarray]]] = None\n    palette: Optional[Union[str, np.ndarray]] = None\n    alpha: Optional[float] = None\n    vmin: Optional[float] = None\n    vmax: Optional[float] = None\n    smin: Optional[float] = None\n    smax: Optional[float] = None\n    norm_global: bool = True\n\n    # Other\n    scatter_kws: dict = field(default_factory=dict)\n\n    _synonyms: List[Tuple] = field(\n        default_factory=lambda: [\n            (\"linestyle\", \"ls\"),\n            (\"linewidth\", \"lw\"),\n            (\"color\", \"colors\", \"c\"),\n        ]\n    )\n</code></pre>"},{"location":"reference/navis/plotting/settings/#navis.plotting.settings.Settings","title":"<code>navis.plotting.settings.Settings</code>  <code>dataclass</code>","text":"<p>Class that works a bit like a dictionary but can validate keys and has some extra features.</p> Source code in <code>navis/plotting/settings.py</code> <pre><code>@dataclass\nclass Settings:\n    \"\"\"Class that works a bit like a dictionary but can validate keys and has some extra features.\"\"\"\n\n    # We can define synonyms for arguments, so that they can be used interchangeably\n    _synonyms: List[Tuple] = field(default_factory=list)\n    _name = \"Settings\"\n\n    def __setattr__(self, key, value, check_valid=False):\n        if check_valid and key not in self.properties:\n            raise AttributeError(\n                f\"The '{key}' argument is invalid for {self._name}. Valid arguments are: {', '.join(self.properties)}\"\n            )\n        self.__dict__[key] = value\n\n    def __contains__(self, key):\n        return key in self.properties\n\n    @property\n    def properties(self):\n        return tuple(\n            [\n                p\n                for p in dir(self)\n                if not p.startswith(\"_\")\n                and (p != \"properties\")   # we need this to avoid infinite recursion\n                and not callable(getattr(self, p, None))\n            ]\n        )\n\n    def update_settings(self, **kwargs):\n        # Deal with synonyms\n        for syn in self._synonyms:\n            present = [s for s in syn if s in kwargs]\n            if len(present) &gt; 1:\n                raise ValueError(f\"Multiple synonyms for the same argument: {present}\")\n\n            for s in syn[1:]:\n                if s in kwargs:\n                    kwargs[syn[0]] = kwargs.pop(s)\n\n        for k, v in kwargs.items():\n            self.__setattr__(k, v, check_valid=VALIDATE_SETTINGS)\n\n        # For method chaining\n        return self\n\n    def to_dict(self):\n        return {k: v for k, v in self.__dict__.items() if not k.startswith(\"_\")}\n\n    def get(self, key, default=None):\n        value = self.__dict__.get(key, default)\n        if value is None:\n            value = default\n        return value\n\n    def pop(self, key, default=None):\n        return self.__dict__.pop(key, default)\n</code></pre>"},{"location":"reference/navis/plotting/k3d/k3d_objects/","title":"k3d_objects","text":""},{"location":"reference/navis/plotting/k3d/k3d_objects/#navis.plotting.k3d.k3d_objects.dotprops2k3d","title":"<code>navis.plotting.k3d.k3d_objects.dotprops2k3d</code>","text":"<p>Convert Dotprops to plotly graph object.</p> Source code in <code>navis/plotting/k3d/k3d_objects.py</code> <pre><code>def dotprops2k3d(x, legendgroup, showlegend, label, color, settings):\n    \"\"\"Convert Dotprops to plotly graph object.\"\"\"\n    tn = x.to_skeleton(scale_vec=settings.dps_scale_vec)\n\n    return skeleton2k3d(tn, legendgroup, showlegend, label, color, settings=settings)\n</code></pre>"},{"location":"reference/navis/plotting/k3d/k3d_objects/#navis.plotting.k3d.k3d_objects.mesh2k3d","title":"<code>navis.plotting.k3d.k3d_objects.mesh2k3d</code>","text":"<p>Convert MeshNeuron to k3d object.</p> Source code in <code>navis/plotting/k3d/k3d_objects.py</code> <pre><code>def mesh2k3d(neuron, legendgroup, showlegend, label, color, settings):\n    \"\"\"Convert MeshNeuron to k3d object.\"\"\"\n    # Skip empty neurons\n    if neuron.n_vertices == 0:\n        return []\n\n    opacity = 1\n    if isinstance(color, np.ndarray) and color.ndim == 2:\n        if len(color) == len(neuron.vertices):\n            color = [color_to_int(c) for c in color]\n            color_kwargs = dict(colors=color)\n        else:\n            raise ValueError(\"Colors must match number of vertices for K3D meshes.\")\n    else:\n        c = color_to_int(color)\n        color_kwargs = dict(color=c)\n\n        if len(color) == 4:\n            opacity = color[3]\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        trace_data = [\n            k3d.mesh(\n                vertices=neuron.vertices.astype(\"float32\"),\n                indices=neuron.faces.astype(\"uint32\"),\n                name=label,\n                flat_shading=False,\n                opacity=opacity,\n                **color_kwargs,\n            )\n        ]\n\n    return trace_data\n</code></pre>"},{"location":"reference/navis/plotting/k3d/k3d_objects/#navis.plotting.k3d.k3d_objects.neuron2k3d","title":"<code>navis.plotting.k3d.k3d_objects.neuron2k3d</code>","text":"<p>Convert neurons to k3d objects.</p> Source code in <code>navis/plotting/k3d/k3d_objects.py</code> <pre><code>def neuron2k3d(x, colormap, settings):\n    \"\"\"Convert neurons to k3d objects.\"\"\"\n    if isinstance(x, core.BaseNeuron):\n        x = core.NeuronList(x)\n    elif not isinstance(x, core.NeuronList):\n        raise TypeError('Unable to process data of type \"{}\"'.format(type(x)))\n\n    # palette = kwargs.get(\"palette\", None)\n    # color_by = kwargs.get(\"color_by\", None)\n    # shade_by = kwargs.get(\"shade_by\", None)\n    # lg = kwargs.pop(\"legend_group\", None)\n\n    if settings.color_by is not None:\n        if not settings.palette:\n            raise ValueError(\n                'Must provide `palette` (e.g. \"viridis\") argument '\n                \"if using `color_by`\"\n            )\n\n        colormap = vertex_colors(\n            x,\n            by=settings.color_by,\n            alpha=settings.alpha,\n            use_alpha=False,\n            palette=settings.palette,\n            vmin=settings.vmin,\n            vmax=settings.vmax,\n            na=\"raise\",\n            color_range=255,\n        )\n\n    if not isinstance(settings.shade_by, type(None)):\n        logger.warning(\"`shade_by` does not work with the k3d backend\")\n\n    cn_lay = config.default_connector_colors.copy()\n    cn_lay.update(settings.cn_layout)\n\n    trace_data = []\n    _radius_warned = False\n    for i, neuron in enumerate(x):\n        name = str(getattr(neuron, \"name\", neuron.id))\n        color = colormap[i]\n\n        try:\n            # Try converting this neuron's ID\n            neuron_id = str(neuron.id)\n        except BaseException:\n            # If that doesn't work generate a new ID\n            neuron_id = str(str(uuid.uuid1()))\n\n        showlegend = True\n        label = neuron.label\n        if (\n            isinstance(settings.legend_group, dict)\n            and neuron.id in settings.legend_group\n        ):\n            # Check if this the first entry for this legendgroup\n            label = legendgroup = settings.legend_group[neuron.id]\n            for d in trace_data:\n                # If it is not the first entry, hide it\n                if getattr(d, \"legendgroup\", None) == legendgroup:\n                    showlegend = False\n                    break\n        elif isinstance(settings.legend_group, str):\n            legendgroup = settings.legend_group\n        else:\n            legendgroup = neuron_id\n\n        if isinstance(neuron, core.TreeNeuron) and settings.radius == \"auto\":\n            # Number of nodes with radii\n            n_radii = (neuron.nodes.get(\"radius\", pd.Series([])).fillna(0) &gt; 0).sum()\n            # If less than 30% of nodes have a radius, we will fall back to lines\n            if n_radii / neuron.nodes.shape[0] &lt; 0.3:\n                settings.radius = False\n\n        if isinstance(neuron, core.TreeNeuron) and settings.radius:\n            # Warn once if more than 5% of nodes have missing radii\n            if not _radius_warned:\n                if (\n                    (neuron.nodes.radius.fillna(0).values &lt;= 0).sum() / neuron.n_nodes\n                ) &gt; 0.05:\n                    logger.warning(\n                        \"Some skeleton nodes have radius &lt;= 0. This may lead to \"\n                        \"rendering artifacts. Set `radius=False` to plot skeletons \"\n                        \"as single-width lines instead.\"\n                    )\n                    _radius_warned = True\n\n            _neuron = conversion.tree2meshneuron(\n                neuron,\n                warn_missing_radii=False,\n                radius_scale_factor=settings.get(\"linewidth\", 1),\n            )\n            _neuron.connectors = neuron.connectors\n            neuron = _neuron\n\n            # See if we need to map colors to vertices\n            if isinstance(color, np.ndarray) and color.ndim == 2:\n                color = color[neuron.vertex_map]\n\n        if not settings.connectors_only:\n            if isinstance(neuron, core.TreeNeuron):\n                trace_data += skeleton2k3d(\n                    neuron,\n                    label=label,\n                    legendgroup=legendgroup,\n                    showlegend=showlegend,\n                    color=color,\n                    settings=settings,\n                )\n            elif isinstance(neuron, core.MeshNeuron):\n                trace_data += mesh2k3d(\n                    neuron,\n                    label=label,\n                    legendgroup=legendgroup,\n                    showlegend=showlegend,\n                    color=color,\n                    settings=settings,\n                )\n            elif isinstance(neuron, core.Dotprops):\n                trace_data += dotprops2k3d(\n                    neuron,\n                    label=label,\n                    legendgroup=legendgroup,\n                    showlegend=showlegend,\n                    color=color,\n                    settings=settings,\n                )\n            elif isinstance(neuron, core.VoxelNeuron):\n                trace_data += voxel2k3d(\n                    neuron,\n                    label=label,\n                    legendgroup=legendgroup,\n                    showlegend=showlegend,\n                    color=color,\n                    settings=settings,\n                )\n            else:\n                raise TypeError(f'Unable to plot neurons of type \"{type(neuron)}\"')\n\n        # Add connectors\n        if (settings.connectors or settings.connectors_only) and neuron.has_connectors:\n            if isinstance(settings.connectors, (list, np.ndarray, tuple)):\n                connectors = neuron.connectors[\n                    neuron.connectors.type.isin(settings.connectors)\n                ]\n            elif settings.connectors in (\"pre\", \"presynapses\"):\n                connectors = neuron.presynapses\n            elif settings.connectors in (\"post\", \"postsynapses\"):\n                connectors = neuron.postsynapses\n            elif isinstance(settings.connectors, str):\n                connectors = neuron.connectors[\n                    neuron.connectors.type == settings.connectors\n                ]\n            else:\n                connectors = neuron.connectors\n\n            for j, this_cn in connectors.groupby(\"type\"):\n                if isinstance(settings.cn_colors, dict):\n                    c = settings.cn_colors.get(\n                        j, cn_lay.get(j, {\"color\": (10, 10, 10)})[\"color\"]\n                    )\n                elif settings.cn_colors == \"neuron\":\n                    c = color\n                elif settings.cn_colors:\n                    c = settings.cn_colors\n                else:\n                    c = cn_lay.get(j, {\"color\": (10, 10, 10)})[\"color\"]\n\n                c = color_to_int(eval_color(c, color_range=255))\n\n                cn_label = f'{cn_lay.get(j, {\"name\": \"connector\"})[\"name\"]} of {name}'\n\n                if cn_lay[\"display\"] == \"circles\" or not isinstance(\n                    neuron, core.TreeNeuron\n                ):\n                    with warnings.catch_warnings():\n                        warnings.simplefilter(\"ignore\")\n                        trace_data.append(\n                            k3d.points(\n                                positions=this_cn[[\"x\", \"y\", \"z\"]].values,\n                                name=cn_label,\n                                shader=\"flat\",\n                                point_size=settings.cn_size\n                                if settings.cn_size\n                                else cn_lay[\"size\"] * 50,\n                                color=c,\n                                opacity=settings.get('cn_alpha', 1),\n                            )\n                        )\n                elif cn_lay[\"display\"] == \"lines\":\n                    # Find associated treenodes\n                    co1 = this_cn[[\"x\", \"y\", \"z\"]].values\n                    co2 = (\n                        neuron.nodes.set_index(\"node_id\")\n                        .loc[this_cn.node_id.values, [\"x\", \"y\", \"z\"]]\n                        .values\n                    )\n\n                    coords = np.array(\n                        [\n                            co\n                            for seg in zip(\n                                co1, co1, co2, co2, [[np.nan] * 3] * len(co1)\n                            )\n                            for co in seg\n                        ]\n                    )\n\n                    with warnings.catch_warnings():\n                        warnings.simplefilter(\"ignore\")\n                        trace_data.append(\n                            k3d.line(\n                                coords,\n                                color=c,\n                                name=cn_label,\n                                width=settings.linewidth,\n                                shader=\"thick\",\n                            )\n                        )\n                else:\n                    raise ValueError(\n                        f'Unknown display type for connectors \"{cn_lay[\"display\"]}\"'\n                    )\n\n    return trace_data\n</code></pre>"},{"location":"reference/navis/plotting/k3d/k3d_objects/#navis.plotting.k3d.k3d_objects.scatter2k3d","title":"<code>navis.plotting.k3d.k3d_objects.scatter2k3d</code>","text":"<p>Convert DataFrame with x,y,z columns to plotly scatter plot.</p> Source code in <code>navis/plotting/k3d/k3d_objects.py</code> <pre><code>def scatter2k3d(x, **kwargs):\n    \"\"\"Convert DataFrame with x,y,z columns to plotly scatter plot.\"\"\"\n    c = eval_color(\n        kwargs.get(\"color\", kwargs.get(\"c\", (100, 100, 100))), color_range=255\n    )\n    c = color_to_int(c)\n    s = kwargs.get(\"size\", kwargs.get(\"s\", 1))\n    name = kwargs.get(\"name\", None)\n\n    trace_data = []\n    for scatter in x:\n        if isinstance(scatter, pd.DataFrame):\n            if not all([c in scatter.columns for c in [\"x\", \"y\", \"z\"]]):\n                raise ValueError(\"DataFrame must have x, y and z columns\")\n            scatter = [[\"x\", \"y\", \"z\"]].values\n\n        if not isinstance(scatter, np.ndarray):\n            scatter = np.array(scatter)\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            trace_data.append(\n                k3d.points(\n                    positions=scatter, name=name, color=c, point_size=s, shader=\"dot\"\n                )\n            )\n    return trace_data\n</code></pre>"},{"location":"reference/navis/plotting/k3d/k3d_objects/#navis.plotting.k3d.k3d_objects.skeleton2k3d","title":"<code>navis.plotting.k3d.k3d_objects.skeleton2k3d</code>","text":"<p>Convert skeleton (i.e. TreeNeuron) to plotly line plot.</p> Source code in <code>navis/plotting/k3d/k3d_objects.py</code> <pre><code>def skeleton2k3d(neuron, legendgroup, showlegend, label, color, settings):\n    \"\"\"Convert skeleton (i.e. TreeNeuron) to plotly line plot.\"\"\"\n    if neuron.nodes.empty:\n        logger.warning(f\"Skipping TreeNeuron w/o nodes: {neuron.label}\")\n        return []\n    elif neuron.nodes.shape[0] == 1:\n        logger.warning(f\"Skipping single-node skeleton: {neuron.label}\")\n        return []\n\n    coords = segments_to_coords(neuron)\n\n    # We have to add (None, None, None) to the end of each segment to\n    # make that line discontinuous. For reasons I don't quite understand\n    # we have to also duplicate the first and the last coordinate in each segment\n    # (possibly a bug)\n    coords = np.concatenate(\n        [co for seg in coords for co in [seg[:1], seg, seg[-1:], [[None] * 3]]], axis=0\n    )\n\n    color_kwargs = {}\n    if isinstance(color, np.ndarray) and color.ndim == 2:\n        # Change colors to rgb integers\n        c = [color_to_int(c) for c in color]\n\n        # Next we have to make colors match the segments in `coords`\n        c = np.asarray(c)\n        ix = dict(zip(neuron.nodes.node_id.values, np.arange(neuron.n_nodes)))\n        # Construct sequence node IDs just like we did in `coords`\n        # (note that we insert a \"-1\" for breaks between segments)\n        seg_ids = [\n            co for seg in neuron.segments for co in [seg[:1], seg, seg[-1:], [-1]]\n        ]\n        seg_ids = np.concatenate(seg_ids, axis=0)\n        # Translate to node indices\n        seg_ix = [ix.get(n, 0) for n in seg_ids]\n\n        # Now map this to vertex colors\n        seg_colors = [c[i] for i in seg_ix]\n\n        color_kwargs[\"colors\"] = seg_colors\n    else:\n        color_kwargs[\"color\"] = c = color_to_int(color)\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        trace_data = [\n            k3d.line(\n                coords,\n                width=settings.linewidth,\n                shader=\"thick\",\n                name=label,\n                **color_kwargs,\n            )\n        ]\n\n    # Add soma(s):\n    soma = utils.make_iterable(neuron.soma)\n    if settings.soma:\n        # If soma detection is messed up we might end up producing\n        # hundreds of soma which will freeze the session\n        if len(soma) &gt;= 10:\n            logger.warning(\n                f\"Neuron {neuron.id} appears to have {len(soma)} \"\n                \"somas. That does not look right - will ignore \"\n                \"them for plotting.\"\n            )\n        else:\n            for s in soma:\n                # Skip `None` somas\n                if isinstance(s, type(None)):\n                    continue\n\n                # If we have colors for every vertex, we need to find the\n                # color that corresponds to this root (or it's parent to be\n                # precise)\n                if isinstance(c, (list, np.ndarray)):\n                    s_ix = np.where(neuron.nodes.node_id == s)[0][0]\n                    soma_color = int(c[s_ix])\n                else:\n                    soma_color = int(c)\n\n                n = neuron.nodes.set_index(\"node_id\").loc[s]\n                r = (\n                    getattr(n, neuron.soma_radius)\n                    if isinstance(neuron.soma_radius, str)\n                    else neuron.soma_radius\n                )\n\n                # It's possible that the radius column is either missing or just\n                # contains NaNs. In that case we will skip this soma.\n                if pd.isnull(r):\n                    logger.warning(\n                        f\"Skipping soma {s} of neuron {neuron.id} \"\n                        \"because it appears to have no radius.\"\n                    )\n                    continue\n\n                sp = tm.primitives.Sphere(radius=r, subdivisions=2)\n\n                with warnings.catch_warnings():\n                    warnings.simplefilter(\"ignore\")\n                    trace_data.append(\n                        k3d.mesh(\n                            vertices=sp.vertices\n                            + n[[\"x\", \"y\", \"z\"]].values.astype(\"float32\"),\n                            indices=sp.faces.astype(\"uint32\"),\n                            color=soma_color,\n                            flat_shading=False,\n                            name=f\"soma of {label}\",\n                        )\n                    )\n\n    return trace_data\n</code></pre>"},{"location":"reference/navis/plotting/k3d/k3d_objects/#navis.plotting.k3d.k3d_objects.volume2k3d","title":"<code>navis.plotting.k3d.k3d_objects.volume2k3d</code>","text":"<p>Convert Volumes to plotly objects.</p> Source code in <code>navis/plotting/k3d/k3d_objects.py</code> <pre><code>def volume2k3d(x, colormap, settings):\n    \"\"\"Convert Volumes to plotly objects.\"\"\"\n    trace_data = []\n    for i, v in enumerate(x):\n        # Skip empty data\n        if isinstance(v.vertices, np.ndarray):\n            if not v.vertices.any():\n                continue\n        elif not v.vertices:\n            continue\n\n        name = getattr(v, \"name\", None)\n\n        c = colormap[i]\n        if len(c) == 4:\n            opacity = c[3]\n        else:\n            opacity = 0.5\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            trace_data = [\n                k3d.mesh(\n                    vertices=v.vertices.astype(\"float32\"),\n                    indices=v.faces.astype(\"uint32\"),\n                    name=name,\n                    color=color_to_int(c[:3]),\n                    flat_shading=False,\n                    opacity=opacity,\n                )\n            ]\n\n    return trace_data\n</code></pre>"},{"location":"reference/navis/plotting/k3d/k3d_objects/#navis.plotting.k3d.k3d_objects.voxel2k3d","title":"<code>navis.plotting.k3d.k3d_objects.voxel2k3d</code>","text":"<p>Convert VoxelNeuron to k3d object.</p> Source code in <code>navis/plotting/k3d/k3d_objects.py</code> <pre><code>def voxel2k3d(neuron, legendgroup, showlegend, label, color, settings):\n    \"\"\"Convert VoxelNeuron to k3d object.\"\"\"\n    # Skip empty neurons\n    if min(neuron.shape) == 0:\n        return []\n\n    img = neuron.grid\n    if img.dtype not in (np.float32, np.float64):\n        img = img.astype(np.float32)\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        trace_data = [\n            k3d.volume(\n                img.T,\n                bounds=neuron.bbox.flatten(),\n                interpolation=False,\n            )\n        ]\n\n    return trace_data\n</code></pre>"},{"location":"reference/navis/plotting/plotly/graph_objs/","title":"graph_objs","text":""},{"location":"reference/navis/plotting/plotly/graph_objs/#navis.plotting.plotly.graph_objs.dotprops2plotly","title":"<code>navis.plotting.plotly.graph_objs.dotprops2plotly</code>","text":"<p>Convert Dotprops to plotly graph object.</p> Source code in <code>navis/plotting/plotly/graph_objs.py</code> <pre><code>def dotprops2plotly(x, legendgroup, showlegend, label, color, settings):\n    \"\"\"Convert Dotprops to plotly graph object.\"\"\"\n    return skeleton2plotly(\n        x.to_skeleton(scale_vec=settings.dps_scale_vec),\n        legendgroup,\n        showlegend,\n        label,\n        color,\n        settings,\n    )\n</code></pre>"},{"location":"reference/navis/plotting/plotly/graph_objs/#navis.plotting.plotly.graph_objs.layout2plotly","title":"<code>navis.plotting.plotly.graph_objs.layout2plotly</code>","text":"<p>Generate layout for plotly figures.</p> Source code in <code>navis/plotting/plotly/graph_objs.py</code> <pre><code>def layout2plotly(**kwargs):\n    \"\"\"Generate layout for plotly figures.\"\"\"\n    layout = dict(\n        width=kwargs.get(\"width\", None),  # these override autosize\n        height=kwargs.get(\"height\", 600),  # these override autosize\n        autosize=kwargs.get(\"fig_autosize\", True),\n        title=kwargs.get(\"pl_title\", None),\n        showlegend=kwargs.get(\"legend\", True),\n        legend_orientation=kwargs.get(\"legend_orientation\", \"v\"),\n        legend_bgcolor=\"rgba(0,0,0,0)\",\n        plot_bgcolor=\"rgba(0,0,0,0)\",\n        paper_bgcolor=\"rgba(0,0,0,0)\",\n        scene=dict(\n            xaxis=dict(\n                gridcolor=\"rgb(255, 255, 255)\",\n                zerolinecolor=\"rgb(255, 255, 255)\",\n                showbackground=False,\n                showline=False,\n                showgrid=False,\n                backgroundcolor=\"rgb(240, 240, 240)\",\n            ),\n            yaxis=dict(\n                gridcolor=\"rgb(255, 255, 255)\",\n                zerolinecolor=\"rgb(255, 255, 255)\",\n                showbackground=False,\n                showline=False,\n                showgrid=False,\n                backgroundcolor=\"rgb(240, 240, 240)\",\n            ),\n            zaxis=dict(\n                gridcolor=\"rgb(255, 255, 255)\",\n                zerolinecolor=\"rgb(255, 255, 255)\",\n                showbackground=False,\n                showline=False,\n                showgrid=False,\n                backgroundcolor=\"rgb(240, 240, 240)\",\n            ),\n            camera=dict(\n                up=dict(x=0, y=0, z=1),\n                eye=dict(\n                    x=-1.7428,\n                    y=1.0707,\n                    z=0.7100,\n                ),\n            ),\n            aspectratio=dict(x=1, y=1, z=1),\n            aspectmode=\"data\",\n        ),\n    )\n\n    return layout\n</code></pre>"},{"location":"reference/navis/plotting/plotly/graph_objs/#navis.plotting.plotly.graph_objs.lines2plotly","title":"<code>navis.plotting.plotly.graph_objs.lines2plotly</code>","text":"<p>Convert DataFrame with x, y, z, x1, y1, z1 columns to line plots.</p> Source code in <code>navis/plotting/plotly/graph_objs.py</code> <pre><code>def lines2plotly(x, **kwargs):\n    \"\"\"Convert DataFrame with x, y, z, x1, y1, z1 columns to line plots.\"\"\"\n    name = kwargs.get(\"name\", None)\n    c = kwargs.get(\"color\", (10, 10, 10))\n\n    x_coords = [\n        n\n        for sublist in zip(x.x.values, x.x1.values, [None] * x.shape[0])\n        for n in sublist\n    ]\n    y_coords = [\n        n\n        for sublist in zip(x.y.values, x.y1.values, [None] * x.shape[0])\n        for n in sublist\n    ]\n    z_coords = [\n        n\n        for sublist in zip(x.z.values, x.z1.values, [None] * x.shape[0])\n        for n in sublist\n    ]\n\n    trace_data = []\n    trace_data.append(\n        go.Scatter3d(\n            x=x_coords,\n            y=y_coords,\n            z=z_coords,\n            mode=\"lines\",\n            line=dict(color=f\"rgb{str(c)}\", width=5),\n            name=name,\n            showlegend=True,\n            hoverinfo=\"none\",\n        )\n    )\n\n    return trace_data\n</code></pre>"},{"location":"reference/navis/plotting/plotly/graph_objs/#navis.plotting.plotly.graph_objs.mesh2plotly","title":"<code>navis.plotting.plotly.graph_objs.mesh2plotly</code>","text":"<p>Convert MeshNeuron to plotly object.</p> Source code in <code>navis/plotting/plotly/graph_objs.py</code> <pre><code>def mesh2plotly(neuron, legendgroup, showlegend, label, color, settings):\n    \"\"\"Convert MeshNeuron to plotly object.\"\"\"\n    # Skip empty neurons\n    if neuron.n_vertices == 0:\n        return []\n\n    if isinstance(color, np.ndarray) and color.ndim == 2:\n        if len(color) == len(neuron.vertices):\n            # For some reason single colors are 0-255 but face/vertex colors\n            # have to be 0-1\n            color_kwargs = dict(\n                vertexcolor=color / [255, 255, 255, 1][: color.shape[1]]\n            )\n        elif len(color) == len(neuron.faces):\n            color_kwargs = dict(facecolor=color / [255, 255, 255, 1][: color.shape[1]])\n        else:\n            color_kwargs = dict(color=color)\n    else:\n        try:\n            if len(color) == 3:\n                c = \"rgb{}\".format(color)\n            elif len(color) == 4:\n                c = \"rgba{}\".format(color)\n        except BaseException:\n            c = \"rgb(10,10,10)\"\n        color_kwargs = dict(color=c)\n\n    if settings.hover_name:\n        hoverinfo = \"text\"\n        hovertext = neuron.label\n    else:\n        hoverinfo = \"none\"\n        hovertext = \" \"\n\n    trace_data = [\n        go.Mesh3d(\n            x=neuron.vertices[:, 0],\n            y=neuron.vertices[:, 1],\n            z=neuron.vertices[:, 2],\n            i=neuron.faces[:, 0],\n            j=neuron.faces[:, 1],\n            k=neuron.faces[:, 2],\n            name=label,\n            legendgroup=legendgroup,\n            legendgrouptitle_text=legendgroup,\n            showlegend=showlegend,\n            hovertext=hovertext,\n            hoverinfo=hoverinfo,\n            **color_kwargs,\n        )\n    ]\n\n    return trace_data\n</code></pre>"},{"location":"reference/navis/plotting/plotly/graph_objs/#navis.plotting.plotly.graph_objs.neuron2plotly","title":"<code>navis.plotting.plotly.graph_objs.neuron2plotly</code>","text":"<p>Convert neurons to plotly objects.</p> Source code in <code>navis/plotting/plotly/graph_objs.py</code> <pre><code>def neuron2plotly(x, colormap, settings):\n    \"\"\"Convert neurons to plotly objects.\"\"\"\n    if isinstance(x, core.BaseNeuron):\n        x = core.NeuronList(x)\n    elif not isinstance(x, core.NeuronList):\n        raise TypeError('Unable to process data of type \"{}\"'.format(type(x)))\n\n    if not isinstance(settings.color_by, type(None)):\n        if not settings.palette:\n            raise ValueError(\n                'Must provide `palette` (e.g. \"viridis\") argument if using `color_by`'\n            )\n\n        colormap = vertex_colors(\n            x,\n            by=settings.color_by,\n            alpha=settings.alpha,\n            use_alpha=False,\n            palette=settings.palette,\n            vmin=settings.vmin,\n            vmax=settings.vmax,\n            na=\"raise\",\n            color_range=255,\n        )\n\n    if settings.shade_by is not None:\n        alphamap = vertex_colors(\n            x,\n            by=settings.shade_by,\n            use_alpha=True,\n            palette=\"viridis\",  # palette is irrelevant here\n            vmin=settings.smin,\n            vmax=settings.smax,\n            na=\"raise\",\n            color_range=255,\n        )\n\n        new_colormap = []\n        for c, a in zip(colormap, alphamap):\n            if not (isinstance(c, np.ndarray) and c.ndim == 2):\n                c = np.tile(c, (a.shape[0], 1))\n\n            if c.dtype not in (np.float16, np.float32, np.float64):\n                c = c.astype(np.float16)\n\n            if c.shape[1] == 4:\n                c[:, 3] = a[:, 3]\n            else:\n                c = np.insert(c, 3, a[:, 3], axis=1)\n\n            new_colormap.append(c)\n        colormap = new_colormap\n\n    cn_lay = config.default_connector_colors.copy()\n    cn_lay.update(settings.cn_layout)\n\n    trace_data = []\n    _radius_warned = False\n    for i, neuron in enumerate(x):\n        name = str(getattr(neuron, \"name\", neuron.id))\n        color = colormap[i]\n\n        try:\n            # Try converting this neuron's ID\n            neuron_id = str(neuron.id)\n        except BaseException:\n            # If that doesn't work generate a new ID\n            neuron_id = str(str(uuid.uuid1()))\n\n        showlegend = True\n        label = neuron.label\n        if (\n            isinstance(settings.legend_group, dict)\n            and neuron.id in settings.legend_group\n        ):\n            # Check if this the first entry for this legendgroup\n            legendgroup = settings.legend_group[neuron.id]\n            for d in trace_data:\n                # If it is not the first entry, hide it\n                if getattr(d, \"legendgroup\", None) == settings.legend_group:\n                    showlegend = False\n                    break\n        elif isinstance(settings.legend_group, str):\n            legendgroup = settings.legend_group\n        else:\n            legendgroup = neuron_id\n\n        if isinstance(neuron, core.TreeNeuron) and settings.radius == \"auto\":\n            # Number of nodes with radii\n            n_radii = (neuron.nodes.get(\"radius\", pd.Series([])).fillna(0) &gt; 0).sum()\n            # If less than 30% of nodes have a radius, we will fall back to lines\n            if n_radii / neuron.nodes.shape[0] &lt; 0.3:\n                settings.radius = False\n\n        if isinstance(neuron, core.TreeNeuron) and settings.radius:\n            # Warn once if more than 5% of nodes have missing radii\n            if not _radius_warned:\n                if (\n                    (neuron.nodes.radius.fillna(0).values &lt;= 0).sum() / neuron.n_nodes\n                ) &gt; 0.05:\n                    logger.warning(\n                        \"Some skeleton nodes have radius &lt;= 0. This may lead to \"\n                        \"rendering artifacts. Set `radius=False` to plot skeletons \"\n                        \"as single-width lines instead.\"\n                    )\n                    _radius_warned = True\n\n            _neuron = conversion.tree2meshneuron(\n                neuron,\n                warn_missing_radii=False,\n                radius_scale_factor=settings.get(\"linewidth\", 1),\n            )\n            _neuron.connectors = neuron.connectors\n            neuron = _neuron\n\n            # See if we need to map colors to vertices\n            if isinstance(color, np.ndarray) and color.ndim == 2:\n                color = color[neuron.vertex_map]\n\n        if not settings.connectors_only:\n            if isinstance(neuron, core.TreeNeuron):\n                trace_data += skeleton2plotly(\n                    neuron,\n                    label=label,\n                    legendgroup=legendgroup,\n                    showlegend=showlegend,\n                    color=color,\n                    settings=settings,\n                )\n            elif isinstance(neuron, core.MeshNeuron):\n                trace_data += mesh2plotly(\n                    neuron,\n                    label=label,\n                    legendgroup=legendgroup,\n                    showlegend=showlegend,\n                    color=color,\n                    settings=settings,\n                )\n            elif isinstance(neuron, core.VoxelNeuron):\n                trace_data += voxel2plotly(\n                    neuron,\n                    label=label,\n                    legendgroup=legendgroup,\n                    showlegend=showlegend,\n                    color=color,\n                    settings=settings,\n                )\n            elif isinstance(neuron, core.Dotprops):\n                trace_data += dotprops2plotly(\n                    neuron,\n                    label=label,\n                    legendgroup=legendgroup,\n                    showlegend=showlegend,\n                    color=color,\n                    settings=settings,\n                )\n            else:\n                raise TypeError(f'Unable to plot neurons of type \"{type(neuron)}\"')\n\n        # Add connectors\n        if (settings.connectors or settings.connectors_only) and neuron.has_connectors:\n            if isinstance(settings.connectors, (list, np.ndarray, tuple)):\n                connectors = neuron.connectors[\n                    neuron.connectors.type.isin(settings.connectors)\n                ]\n            elif settings.connectors in (\"pre\", \"presynapses\"):\n                connectors = neuron.presynapses\n            elif settings.connectors in (\"post\", \"postsynapses\"):\n                connectors = neuron.postsynapses\n            elif isinstance(settings.connectors, str):\n                connectors = neuron.connectors[\n                    neuron.connectors.type == settings.connectors\n                ]\n            else:\n                connectors = neuron.connectors\n\n            for j, this_cn in connectors.groupby(\"type\"):\n                if settings.cn_colors == \"neuron\" or settings.get(\n                    \"cn_mesh_colors\", False\n                ):\n                    c = color\n                elif isinstance(settings.cn_colors, dict):\n                    c = settings.cn_colors.get(\n                        j, cn_lay.get(j, {\"color\": (10, 10, 10)})[\"color\"]\n                    )\n                elif settings.cn_colors is not None:\n                    c = settings.cn_colors\n                else:\n                    c = cn_lay.get(j, {\"color\": (10, 10, 10)})[\"color\"]\n\n                c = eval_color(c, color_range=255)\n\n                if settings.get(\"cn_alpha\", None) is not None:\n                    c = set_alpha(c, settings.cn_alpha)\n\n                if cn_lay[\"display\"] == \"circles\" or isinstance(\n                    neuron, core.MeshNeuron\n                ):\n                    trace_data.append(\n                        go.Scatter3d(\n                            x=this_cn.x.values,\n                            y=this_cn.y.values,\n                            z=this_cn.z.values,\n                            mode=\"markers\",\n                            marker=dict(\n                                color=f\"rgb{c}\",\n                                opacity=settings.get(\"cn_alpha\", 1),\n                                size=(\n                                    settings.cn_size\n                                    if settings.cn_size\n                                    else cn_lay[\"size\"]\n                                ),\n                            ),\n                            name=f\"{cn_lay.get(j, {'name': 'connector'})['name']} of {name}\",\n                            showlegend=False,\n                            legendgroup=legendgroup,\n                            hoverinfo=\"none\",\n                        )\n                    )\n                elif cn_lay[\"display\"] == \"lines\":\n                    # Find associated treenodes\n                    tn = neuron.nodes.set_index(\"node_id\").loc[this_cn.node_id.values]\n                    x_coords = [\n                        n\n                        for sublist in zip(\n                            this_cn.x.values, tn.x.values, [None] * this_cn.shape[0]\n                        )\n                        for n in sublist\n                    ]\n                    y_coords = [\n                        n\n                        for sublist in zip(\n                            this_cn.y.values, tn.y.values, [None] * this_cn.shape[0]\n                        )\n                        for n in sublist\n                    ]\n                    z_coords = [\n                        n\n                        for sublist in zip(\n                            this_cn.z.values, tn.z.values, [None] * this_cn.shape[0]\n                        )\n                        for n in sublist\n                    ]\n\n                    trace_data.append(\n                        go.Scatter3d(\n                            x=x_coords,\n                            y=y_coords,\n                            z=z_coords,\n                            mode=\"lines\",\n                            line=dict(\n                                color=\"rgb%s\" % str(c),\n                                width=5,\n                                opacity=settings.get(\"cn_alpha\", 1),\n                            ),\n                            name=f\"{cn_lay.get(j, {'name': 'connector'})['name']} of {name}\",\n                            showlegend=False,\n                            legendgroup=legendgroup,\n                            hoverinfo=\"none\",\n                        )\n                    )\n                else:\n                    raise ValueError(\n                        f'Unknown display type for connectors \"{cn_lay[\"display\"]}\"'\n                    )\n\n    return trace_data\n</code></pre>"},{"location":"reference/navis/plotting/plotly/graph_objs/#navis.plotting.plotly.graph_objs.scatter2plotly","title":"<code>navis.plotting.plotly.graph_objs.scatter2plotly</code>","text":"<p>Convert DataFrame with x,y,z columns to plotly scatter plot.</p> Source code in <code>navis/plotting/plotly/graph_objs.py</code> <pre><code>def scatter2plotly(x, **scatter_kws):\n    \"\"\"Convert DataFrame with x,y,z columns to plotly scatter plot.\"\"\"\n    c = eval_color(\n        scatter_kws.get(\"color\", scatter_kws.get(\"c\", (100, 100, 100))), color_range=255\n    )\n    s = scatter_kws.get(\"size\", scatter_kws.get(\"s\", 2))\n    name = scatter_kws.get(\"name\", None)\n\n    trace_data = []\n    for scatter in x:\n        if isinstance(scatter, pd.DataFrame):\n            if not all([c in scatter.columns for c in [\"x\", \"y\", \"z\"]]):\n                raise ValueError(\"DataFrame must have x, y and z columns\")\n            scatter = scatter[[\"x\", \"y\", \"z\"]].values\n\n        if not isinstance(scatter, np.ndarray):\n            scatter = np.array(scatter)\n\n        trace_data.append(\n            go.Scatter3d(\n                x=scatter[:, 0],\n                y=scatter[:, 1],\n                z=scatter[:, 2],\n                mode=scatter_kws.get(\"mode\", \"markers\"),\n                marker=dict(\n                    color=\"rgb%s\" % str(c),\n                    size=s,\n                    opacity=scatter_kws.get(\"opacity\", 1),\n                ),\n                name=name,\n                showlegend=True,\n                hoverinfo=\"none\",\n            )\n        )\n    return trace_data\n</code></pre>"},{"location":"reference/navis/plotting/plotly/graph_objs/#navis.plotting.plotly.graph_objs.skeleton2plotly","title":"<code>navis.plotting.plotly.graph_objs.skeleton2plotly</code>","text":"<p>Convert skeleton (i.e. TreeNeuron) to plotly line plot.</p> Source code in <code>navis/plotting/plotly/graph_objs.py</code> <pre><code>def skeleton2plotly(neuron, legendgroup, showlegend, label, color, settings):\n    \"\"\"Convert skeleton (i.e. TreeNeuron) to plotly line plot.\"\"\"\n    if not hasattr(neuron, \"nodes\") or neuron.nodes.empty:\n        logger.warning(f\"Skipping TreeNeuron w/o nodes: {neuron.label}\")\n        return []\n    elif neuron.nodes.shape[0] == 1:\n        logger.warning(f\"Skipping single-node TreeNeuron: {neuron.label}\")\n        return []\n\n    coords = segments_to_coords(neuron)\n\n    # For some reason, plotly seems to ignore the alpha channel when given an RGBA color\n    # (the color still changes somewhat but the line doesn't turn transparent)\n    # Instead, we have to set the `opacity` property of the whole scatter object\n    # - we will adjust opacity further down if according to the color\n    opacity = 1\n\n    # We have to add (None, None, None) to the end of each segment to\n    # make that line discontinuous\n    coords = np.vstack([np.append(t, [[None] * 3], axis=0) for t in coords])\n\n    if isinstance(color, np.ndarray) and color.ndim == 2:\n        # Change colors to rgb/a strings\n        if color.shape[1] == 4:\n            c = [f\"rgba({c[0]},{c[1]},{c[2]},{c[3]:.3f})\" for c in color]\n        else:\n            c = [f\"rgb({c[0]},{c[1]},{c[2]})\" for c in color]\n\n        # Next we have to make colors match the segments in `coords`\n        c = np.asarray(c)\n        ix = dict(zip(neuron.nodes.node_id.values, np.arange(neuron.n_nodes)))\n        c = [\n            col\n            for s in neuron.segments\n            for col in np.append(c[[ix[n] for n in s]], \"rgb(0,0,0)\")\n        ]\n\n    else:\n        if len(color) == 4:\n            opacity = color[3]\n\n        c = f\"rgb({color[0]},{color[1]},{color[2]})\"\n\n    if settings.hover_id:\n        hoverinfo = \"text\"\n        hovertext = [str(i) for seg in neuron.segments for i in seg + [None]]\n    elif settings.hover_name:\n        hoverinfo = \"text\"\n        hovertext = neuron.label\n    else:\n        hoverinfo = \"none\"\n        hovertext = \" \"\n\n    # Options for linestyle: \"solid\", \"dot\", \"dash\", \"longdash\", \"dashdot\", or \"longdashdot\"\n    # Translate `linestyle` setting to plotly's `dash` setting\n    dash = {\"-\": \"solid\", \"--\": \"dash\", \"-.\": \"dashdot\", \":\": \"dot\"}.get(\n        settings.linestyle, settings.linestyle\n    )\n\n    trace_data = [\n        go.Scatter3d(\n            x=coords[:, 0],\n            y=coords[:, 1],\n            z=coords[:, 2],\n            opacity=opacity,\n            mode=\"lines\",\n            line=dict(color=c, width=settings.get(\"linewidth\", 3), dash=dash),\n            name=label,\n            legendgroup=legendgroup,\n            legendgrouptitle_text=legendgroup,\n            showlegend=showlegend,\n            hoverinfo=hoverinfo,\n            hovertext=hovertext,\n        )\n    ]\n\n    # Add soma(s):\n    soma = utils.make_iterable(neuron.soma)\n    if settings.soma:\n        # If soma detection is messed up we might end up producing\n        # hundreds of soma which will freeze the session\n        if len(soma) &gt;= 10:\n            logger.warning(\n                f\"Neuron {neuron.id} appears to have {len(soma)} \"\n                \"somas. That does not look right - will ignore \"\n                \"them for plotting.\"\n            )\n        else:\n            for s in soma:\n                # Skip `None` somas\n                if isinstance(s, type(None)):\n                    continue\n\n                # If we have colors for every vertex, we need to find the\n                # color that corresponds to this root (or it's parent to be\n                # precise)\n                if isinstance(c, list):\n                    s_ix = np.where(neuron.nodes.node_id == s)[0][0]\n                    soma_color = c[s_ix]\n                else:\n                    soma_color = c\n\n                n = neuron.nodes.set_index(\"node_id\").loc[s]\n                r = (\n                    getattr(n, neuron.soma_radius)\n                    if isinstance(neuron.soma_radius, str)\n                    else neuron.soma_radius\n                )\n\n                # It's possible that the radius column is either missing or just\n                # contains NaNs. In that case we will skip this soma.\n                if pd.isnull(r):\n                    logger.warning(\n                        f\"Skipping soma {s} of neuron {neuron.id} \"\n                        \"because it appears to have no radius.\"\n                    )\n                    continue\n\n                trace_data += [\n                    go.Mesh3d(\n                        x=BASE_SPHERE.vertices[:, 0] * r + n.x,\n                        y=BASE_SPHERE.vertices[:, 1] * r + n.y,\n                        z=BASE_SPHERE.vertices[:, 2] * r + n.z,\n                        i=BASE_SPHERE.faces[:, 0],\n                        j=BASE_SPHERE.faces[:, 1],\n                        k=BASE_SPHERE.faces[:, 2],\n                        legendgroup=legendgroup,\n                        showlegend=False,\n                        hoverinfo=\"name\",\n                        color=soma_color,\n                    )\n                ]\n\n    return trace_data\n</code></pre>"},{"location":"reference/navis/plotting/plotly/graph_objs/#navis.plotting.plotly.graph_objs.volume2plotly","title":"<code>navis.plotting.plotly.graph_objs.volume2plotly</code>","text":"<p>Convert Volumes to plotly objects.</p> Source code in <code>navis/plotting/plotly/graph_objs.py</code> <pre><code>def volume2plotly(x, colormap, settings):\n    \"\"\"Convert Volumes to plotly objects.\"\"\"\n    trace_data = []\n    for i, v in enumerate(x):\n        # Skip empty data\n        if isinstance(v.vertices, np.ndarray):\n            if not v.vertices.any():\n                continue\n        elif not v.vertices:\n            continue\n\n        name = getattr(v, \"name\", None)\n\n        c = colormap[i]\n        if len(c) == 3:\n            c = (c[0], c[1], c[2], 0.5)\n\n        rgba_str = f\"rgba({c[0]:.0f},{c[1]:.0f},{c[2]:.0f},{c[3]:.1f})\"\n        trace_data.append(\n            go.Mesh3d(\n                x=v.vertices[:, 0],\n                y=v.vertices[:, 1],\n                z=v.vertices[:, 2],\n                i=v.faces[:, 0],\n                j=v.faces[:, 1],\n                k=v.faces[:, 2],\n                color=rgba_str,\n                name=name,\n                showlegend=settings.volume_legend,\n                hoverinfo=\"none\",\n            )\n        )\n\n    return trace_data\n</code></pre>"},{"location":"reference/navis/plotting/plotly/graph_objs/#navis.plotting.plotly.graph_objs.voxel2plotly","title":"<code>navis.plotting.plotly.graph_objs.voxel2plotly</code>","text":"<p>Convert VoxelNeuron to plotly object.</p> <p>Turns out that plotly is horrendous for plotting voxel data (Volumes): anything more than a few thousand voxels (e.g. 40x40x40) and the html encoding and loading the plot takes ages. Unfortunately, the same happens with Isosurfaces.</p> <p>I'm adding an implementation here but until plotly gets MUCH better at this, there is really no point. For now, we will fallback to plotting the voxels as scatter plots using the top 10k voxels sorted by brightness.</p> Source code in <code>navis/plotting/plotly/graph_objs.py</code> <pre><code>def voxel2plotly(\n    neuron, legendgroup, showlegend, label, color, settings, as_scatter=True\n):\n    \"\"\"Convert VoxelNeuron to plotly object.\n\n    Turns out that plotly is horrendous for plotting voxel data (Volumes):\n    anything more than a few thousand voxels (e.g. 40x40x40) and the html\n    encoding and loading the plot takes ages. Unfortunately, the same happens\n    with Isosurfaces.\n\n    I'm adding an implementation here but until plotly gets MUCH better at this,\n    there is really no point. For now, we will fallback to plotting the\n    voxels as scatter plots using the top 10k voxels sorted by brightness.\n\n    \"\"\"\n    # Skip empty neurons\n    if min(neuron.shape) == 0:\n        return []\n\n    try:\n        if len(color) == 3:\n            c = \"rgb{}\".format(color)\n        elif len(color) == 4:\n            c = \"rgba{}\".format(color)\n    except BaseException:\n        c = \"rgb(10,10,10)\"\n\n    if settings.hover_name:\n        hoverinfo = \"text\"\n        hovertext = neuron.label\n    else:\n        hoverinfo = \"none\"\n        hovertext = \" \"\n\n    if not as_scatter:\n        # Downsample heavily\n        ds = ndimage.zoom(neuron.grid, 0.2, order=1)\n\n        # Generate X, Y, Z, coordinates for values in grid\n        X, Y, Z = np.meshgrid(\n            range(ds.shape[0]), range(ds.shape[1]), range(ds.shape[2]), indexing=\"ij\"\n        )\n\n        # Flatten and scale coordinates\n        X = X.flatten() * neuron.units_xyz[0] + neuron.offset[0]\n        Y = Y.flatten() * neuron.units_xyz[1] + neuron.offset[1]\n        Z = Z.flatten() * neuron.units_xyz[2] + neuron.offset[2]\n\n        # Flatten and normalize values\n        values = ds.flatten() / ds.max()\n\n        trace_data = [\n            go.Isosurface(\n                x=X,\n                y=Y,\n                z=Z,\n                value=values,\n                isomin=0.001,\n                isomax=1,\n                opacity=0.1,\n                surface_count=21,\n            )\n        ]\n    else:\n        voxels, values = neuron.voxels, neuron.values\n\n        # Sort by brightness\n        srt = np.argsort(values)\n\n        # Take the top 100k voxels\n        values = values[srt[-100000:]]\n        voxels = voxels[srt[-100000:]]\n\n        # Scale and offset voxels\n        voxels = voxels * neuron.units_xyz.magnitude + neuron.offset\n\n        with warnings.catch_warnings():\n            trace_data = [\n                go.Scatter3d(\n                    x=voxels[:, 0],\n                    y=voxels[:, 1],\n                    z=voxels[:, 2],\n                    mode=\"markers\",\n                    marker=dict(\n                        color=values, size=4, colorscale=\"viridis\", opacity=0.1\n                    ),\n                    name=label,\n                    legendgroup=legendgroup,\n                    legendgrouptitle_text=legendgroup,\n                    showlegend=showlegend,\n                    hovertext=hovertext,\n                    hoverinfo=hoverinfo,\n                )\n            ]\n\n    return trace_data\n</code></pre>"},{"location":"reference/navis/plotting/vispy/viewer/","title":"viewer","text":""},{"location":"reference/navis/plotting/vispy/viewer/#navis.plotting.vispy.viewer.block_all","title":"<code>navis.plotting.vispy.viewer.block_all</code>","text":"<p>Block all events on canvas and view while changes are being made.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def block_all(function):\n    \"\"\"Block all events on canvas and view while changes are being made.\"\"\"\n    @wraps(function)\n    def wrapper(*args, **kwargs):\n        viewer = args[0]\n        viewer.canvas.events.block_all()\n        viewer.view3d.events.block_all()\n        try:\n            # Execute function\n            res = function(*args, **kwargs)\n        except BaseException:\n            raise\n        finally:\n            viewer.canvas.events.unblock_all()\n            viewer.view3d.events.unblock_all()\n        # Return result\n        return res\n    return wrapper\n</code></pre>"},{"location":"reference/navis/plotting/vispy/viewer/#navis.plotting.vispy.viewer.block_canvas","title":"<code>navis.plotting.vispy.viewer.block_canvas</code>","text":"<p>Decorator to block all events on canvas while changes are being made.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def block_canvas(function):\n    \"\"\" Decorator to block all events on canvas while changes are being made.\n    \"\"\"\n    @wraps(function)\n    def wrapper(*args, **kwargs):\n        viewer = args[0]\n        viewer.canvas.events.block_all()\n        try:\n            # Execute function\n            res = function(*args, **kwargs)\n        except BaseException:\n            raise\n        finally:\n            viewer.canvas.events.unblock_all()\n        # Return result\n        return res\n    return wrapper\n</code></pre>"},{"location":"reference/navis/plotting/vispy/viewer/#navis.plotting.vispy.viewer.on_key_press","title":"<code>navis.plotting.vispy.viewer.on_key_press</code>","text":"<p>Manage keyboard shortcuts for canvas.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def on_key_press(event):\n    \"\"\"Manage keyboard shortcuts for canvas.\"\"\"\n    canvas = event.source\n    viewer = canvas._wrapper\n\n    if event.text.lower() == 'o':\n        viewer.toggle_overlay()\n    elif event.text.lower() == 'l':\n        viewer.show_legend = viewer.show_legend is False\n    elif event.text.lower() == 'd':\n        viewer.selected = []\n    elif event.text.lower() == 'q':\n        viewer._cycle_neurons(-1)\n    elif event.text.lower() == 'w':\n        viewer._cycle_neurons(1)\n    elif event.text.lower() == 'h':\n        viewer.hide_selected()\n    elif event.text.lower() == 'u':\n        viewer.unhide_neurons(check_alpha=True)\n    elif event.text.lower() == 'f':\n        viewer._toggle_fps()\n    elif event.text.lower() == 'p':\n        viewer.toggle_picking()\n    elif event.text.lower() == 'b':\n        viewer.toggle_bounds()\n    elif event.text.lower() == '1':\n        viewer.set_view('XY')\n    elif event.text.lower() == '2':\n        viewer.set_view('XZ')\n    elif event.text.lower() == '3':\n        viewer.set_view('YZ')\n</code></pre>"},{"location":"reference/navis/plotting/vispy/viewer/#navis.plotting.vispy.viewer.on_mouse_press","title":"<code>navis.plotting.vispy.viewer.on_mouse_press</code>","text":"<p>Manage picking on canvas.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def on_mouse_press(event):\n    \"\"\"Manage picking on canvas.\"\"\"\n    canvas = event.source\n    viewer = canvas._wrapper\n\n    try:\n        viewer.interactive = False\n        canvas._overlay.interactive = False\n        vis_at = viewer.visuals_at([event.pos[0] + 15,\n                                    event.pos[1] + 15])\n    finally:\n        viewer.interactive = True\n        canvas._overlay.interactive = True\n\n    logger.debug(f'Mouse press at {event.pos}: {vis_at}')\n\n    modifiers = [key.name for key in event.modifiers]\n    if event.modifiers:\n        logger.debug(f'Modifiers found: {modifiers}')\n\n    # Iterate over visuals in this canvas at cursor position\n    for v in vis_at:\n        # Skip views\n        if isinstance(v, scene.widgets.ViewBox):\n            continue\n        # If legend entry, toggle visibility\n        elif isinstance(v, scene.visuals.Text):\n            viewer.toggle_neurons(v._object_id)\n            break\n        # If control modifier, try snapping cursor\n        if 'Control' in modifiers:\n            viewer._snap_cursor(event.pos, v,\n                                open_browser='Shift' in modifiers)\n            break\n        # If shift modifier, add to/remove from current selection\n        elif (isinstance(v, scene.visuals.VisualNode)\n              and getattr(v, '_id', None)\n              and 'Shift' in modifiers):\n            if v._id not in set(viewer.selected):\n                viewer.selected = np.append(viewer.selected, v._id).astype('object')\n            else:\n                viewer.selected = viewer.selected[viewer.selected != v._id]\n            break\n</code></pre>"},{"location":"reference/navis/plotting/vispy/viewer/#navis.plotting.vispy.viewer.on_resize","title":"<code>navis.plotting.vispy.viewer.on_resize</code>","text":"<p>Keep overlay in place upon resize.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def on_resize(event):\n    \"\"\"Keep overlay in place upon resize.\"\"\"\n    viewer = event.source._wrapper\n    viewer._shortcuts.pos = (10, event.size[1])\n    viewer._picking_text.pos = (10, event.size[1] - 10)\n    viewer._fps_text.pos = (event.size[0] - 10, 10)\n</code></pre>"},{"location":"reference/navis/plotting/vispy/viewer/#navis.plotting.vispy.viewer.to_rgba","title":"<code>navis.plotting.vispy.viewer.to_rgba</code>","text":"<p>Convert color or array of colors to RGBA.</p> <p>matplotlib.colors.to_rgba can't deal with vispy color arrays.</p> Source code in <code>navis/plotting/vispy/viewer.py</code> <pre><code>def to_rgba(c, alpha=None):\n    \"\"\"Convert color or array of colors to RGBA.\n\n    matplotlib.colors.to_rgba can't deal with vispy color arrays.\n    \"\"\"\n    # Vispy color arrays (used on meshes) have an _rgba property\n    if hasattr(c, '_rgba'):\n        c = c._rgba\n\n    # Make sure we deal with an array\n    c = np.asarray(c)\n\n    if c.ndim == 2:\n        if c.shape[1] == 3:\n            c = np.insert(c,\n                          3,\n                          np.ones(c.shape[0]),\n                          axis=1)\n\n        if not isinstance(alpha, type(None)):\n            c[:, 3] = alpha\n    elif c.ndim == 1:\n        if c.shape[0] == 3:\n            c = np.insert(c, 3, 1)\n\n        if not isinstance(alpha, type(None)):\n            c[3] = alpha\n    else:\n        raise ValueError(f'Got {c.ndim} dimensional array of colors.')\n\n    return c\n</code></pre>"},{"location":"reference/navis/sampling/downsampling/","title":"downsampling","text":""},{"location":"reference/navis/sampling/downsampling/#navis.sampling.downsampling.sample_points_uniform","title":"<code>navis.sampling.downsampling.sample_points_uniform</code>","text":"<p>Draw uniform sample from point cloud.</p> <p>This functions works by iteratively removing the point with the smallest distance to its nearest neighbor until the desired number of points is reached.</p> PARAMETER DESCRIPTION <code>points</code> <pre><code>    Point cloud to sample from.\n</code></pre> <p> TYPE: <code>   (N, 3 ) array</code> </p> <code>size</code> <pre><code>    Number of samples to draw.\n</code></pre> <p> TYPE: <code>     int</code> </p> <code>output</code> <pre><code>    If \"points\", returns the sampled points. If \"indices\", returns\n    the indices of the sampled points. If \"mask\", returns a boolean\n    mask of the sampled points.\n</code></pre> <p> TYPE: <code>   \"points\" | \"indices\" | \"mask\"</code> DEFAULT: <code>'points'</code> </p> RETURNS DESCRIPTION <code>See `output` parameter.</code> Source code in <code>navis/sampling/utils.py</code> <pre><code>def sample_points_uniform(points, size, output=\"points\"):\n    \"\"\"Draw uniform sample from point cloud.\n\n    This functions works by iteratively removing the point with the smallest\n    distance to its nearest neighbor until the desired number of points is\n    reached.\n\n    Parameters\n    ----------\n    points :    (N, 3 ) array\n                Point cloud to sample from.\n    size :      int\n                Number of samples to draw.\n    output :    \"points\" | \"indices\" | \"mask\", optional\n                If \"points\", returns the sampled points. If \"indices\", returns\n                the indices of the sampled points. If \"mask\", returns a boolean\n                mask of the sampled points.\n\n    Returns\n    -------\n    See `output` parameter.\n\n    \"\"\"\n    points = np.asarray(points)\n\n    assert isinstance(points, np.ndarray) and points.ndim == 2 and points.shape[1] == 3\n    assert output in (\"points\", \"indices\", \"mask\")\n    assert (size &gt; 0) and (size &lt;= len(points))\n\n    # Start with all points in the mask\n    mask = np.ones(len(points), dtype=bool)\n\n    # Generate a tree\n    tree = KDTree(points)\n\n    p_ind = np.arange(len(points))\n\n    while mask.sum() &gt; size:\n        # Find the point with the largest distance to its nearest neighbor\n        d, ind = tree.query(points[mask], k=2, mask=~mask)\n        d, ind = d[:, 1], ind[:, 1]\n\n        # Find pairs of nodes that are close to each other\n        is_close = d == d.min()\n        pairs = np.stack((p_ind[mask][is_close], p_ind[ind][is_close]), axis=1)\n\n        # At this point we will have pairs show up multiple times - (a, b) and (b, a)\n        pairs = np.unique(np.sort(pairs, axis=1), axis=0)\n\n        # Imagine we have two candidate pairs for removal: (a, b) and (b, c)\n        # In that case we can remove (a and c) or (b) but not (a, b) or (b, c)\n        # because that might leave a hole in the point cloud\n        G = nx.Graph()\n        G.add_edges_from(pairs)\n\n        to_remove = []\n        for cc in nx.connected_components(G):\n            # If these are two nodes, it doesn't matter which one we drop\n            if len(cc) &lt;= 2:\n                to_remove.append(cc.pop())\n                continue\n            # If we have three or more nodes, we will simply remove the one\n            # with the highest degree\n            to_remove.append(sorted(cc, key=lambda x: G.degree(x))[-1])\n\n        # Number of nodes we still need to remove\n        n_remove = mask.sum() - size\n\n        if n_remove &gt;= len(to_remove):\n            mask[to_remove] = False\n        else:\n            mask[to_remove[:n_remove]] = False\n\n    if output == \"mask\":\n        return mask\n    elif output == \"indices\":\n        return p_ind[mask]\n    elif output == \"points\":\n        return points[mask].copy()\n</code></pre>"},{"location":"reference/navis/transforms/affine/","title":"affine","text":""},{"location":"reference/navis/transforms/affine/#navis.transforms.affine.BaseTransform","title":"<code>navis.transforms.affine.BaseTransform</code>","text":"<p>Abstract base class for transforms.</p> <p>If the transform is invertible, implement via neg method.</p> Source code in <code>navis/transforms/base.py</code> <pre><code>class BaseTransform(ABC):\n    \"\"\"Abstract base class for transforms.\n\n    If the transform is invertible, implement via __neg__ method.\n    \"\"\"\n\n    def append(self, other: 'BaseTransform'):\n        \"\"\"Append another transform to this one.\n\n        This is used to try to concatenate transforms of the same type into\n        a single step to speed things up (e.g. for CMTK transforms). If that's\n        not possible or not useful, must raise a `NotImplementedError`.\n        \"\"\"\n        raise NotImplementedError(f'Unable to append {type(other)} to {type(self)}')\n\n    def check_if_possible(self, on_error: str = 'raise'):\n        \"\"\"Test if running the transform is possible.\"\"\"\n        return\n\n    @abstractmethod\n    def copy(self) -&gt; 'BaseTransform':\n        \"\"\"Return copy.\"\"\"\n        pass\n\n    @abstractmethod\n    def xform(self, points: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Return copy.\n\n        Must accept a (N, 3) numpy array as first input and return the\n        transformed (N, 3) points as sole output.s\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/navis/transforms/affine/#navis.transforms.affine.BaseTransform.append","title":"<code>append</code>","text":"<p>Append another transform to this one.</p> <p>This is used to try to concatenate transforms of the same type into a single step to speed things up (e.g. for CMTK transforms). If that's not possible or not useful, must raise a <code>NotImplementedError</code>.</p> Source code in <code>navis/transforms/base.py</code> <pre><code>def append(self, other: 'BaseTransform'):\n    \"\"\"Append another transform to this one.\n\n    This is used to try to concatenate transforms of the same type into\n    a single step to speed things up (e.g. for CMTK transforms). If that's\n    not possible or not useful, must raise a `NotImplementedError`.\n    \"\"\"\n    raise NotImplementedError(f'Unable to append {type(other)} to {type(self)}')\n</code></pre>"},{"location":"reference/navis/transforms/affine/#navis.transforms.affine.BaseTransform.check_if_possible","title":"<code>check_if_possible</code>","text":"<p>Test if running the transform is possible.</p> Source code in <code>navis/transforms/base.py</code> <pre><code>def check_if_possible(self, on_error: str = 'raise'):\n    \"\"\"Test if running the transform is possible.\"\"\"\n    return\n</code></pre>"},{"location":"reference/navis/transforms/affine/#navis.transforms.affine.BaseTransform.copy","title":"<code>copy</code>  <code>abstractmethod</code>","text":"<p>Return copy.</p> Source code in <code>navis/transforms/base.py</code> <pre><code>@abstractmethod\ndef copy(self) -&gt; 'BaseTransform':\n    \"\"\"Return copy.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/navis/transforms/affine/#navis.transforms.affine.BaseTransform.xform","title":"<code>xform</code>  <code>abstractmethod</code>","text":"<p>Return copy.</p> <p>Must accept a (N, 3) numpy array as first input and return the transformed (N, 3) points as sole output.s</p> Source code in <code>navis/transforms/base.py</code> <pre><code>@abstractmethod\ndef xform(self, points: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Return copy.\n\n    Must accept a (N, 3) numpy array as first input and return the\n    transformed (N, 3) points as sole output.s\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/navis/transforms/align/","title":"align","text":""},{"location":"reference/navis/transforms/align/#navis.transforms.align.MovingLeastSquaresTransform","title":"<code>navis.transforms.align.MovingLeastSquaresTransform</code>","text":"Source code in <code>navis/transforms/moving_least_squares.py</code> <pre><code>class MovingLeastSquaresTransform(BaseTransform):\n    def __init__(\n        self,\n        landmarks_source: np.ndarray,\n        landmarks_target: np.ndarray,\n        direction: str = \"forward\",\n        batch_size: int = 100_000,\n    ) -&gt; None:\n        \"\"\"Moving Least Squares transforms of 3D spatial data.\n\n        Uses the [molesq](https://github.com/clbarnes/molesq) library, which packages the\n        [implementation](https://github.com/ceesem/catalysis/blob/master/catalysis/transform.py)\n        by Casey Schneider-Mizell of the affine algorithm published in\n        [Schaefer et al. 2006](https://dl.acm.org/doi/pdf/10.1145/1179352.1141920).\n\n        Notes\n        -----\n        At least in my hands, `TPStransforms` are significantly faster than\n        `MovingLeastSquaresTransforms`. The results are similar but not identical,\n        so make sure to use the one that works best for your use case.\n\n        Parameters\n        ----------\n        landmarks_source : np.ndarray\n            Source landmarks as x/y/z coordinates.\n        landmarks_target : np.ndarray\n            Target landmarks as x/y/z coordinates.\n        direction : str\n            'forward' (default) or 'inverse' (treat the target as the source and vice versa)\n        batch_size : int, optional\n            Batch size for transforming points. At one point during the transformation,\n            molesq generates a (N, M) distance matrix, where N is the number of landmarks\n            and M is the number of locations, which can get prohibitively expensive.\n            We avoid the issue by batching the transformation by default. Note that the\n            overhead from batching seems negligible.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from navis import transforms\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; # Generate some mock landmarks\n        &gt;&gt;&gt; src = np.array([[0, 0, 0], [10, 10, 10], [100, 100, 100], [80, 10, 30]])\n        &gt;&gt;&gt; trg = np.array([[1, 15, 5], [9, 18, 21], [80, 99, 120], [5, 10, 80]])\n        &gt;&gt;&gt; tr = transforms.MovingLeastSquaresTransform(src, trg)\n        &gt;&gt;&gt; points = np.array([[0, 0, 0], [50, 50, 50]])\n        &gt;&gt;&gt; tr.xform(points)                                        # doctest: +SKIP\n        array([[  1.        ,  15.        ,   5.        ],\n               [ 81.56361725, 155.32071504, 187.3147564 ]])\n\n        \"\"\"\n        assert direction in (\"forward\", \"inverse\")\n        self.transformer = Transformer(landmarks_source, landmarks_target)\n        self.reverse = direction == \"inverse\"\n        self.batch_size = int(batch_size)\n\n    def xform(self, points: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Transform points.\n\n        Parameters\n        ----------\n        points :    (N, 3) array\n                    Points to transform.\n\n        Returns\n        -------\n        pointsxf :  (N, 3) array\n                    Transformed points.\n\n        \"\"\"\n        if isinstance(points, pd.DataFrame):\n            if any([c not in points for c in [\"x\", \"y\", \"z\"]]):\n                raise ValueError(\"DataFrame must have x/y/z columns.\")\n            points = points[[\"x\", \"y\", \"z\"]].values\n\n        batch_size = self.batch_size if self.batch_size else len(points)\n        points_xf = []\n        for i in range(0, len(points), batch_size):\n            batch = points[i : i + batch_size]\n            points_xf.append(self.transformer.transform(batch, reverse=self.reverse))\n\n        return np.concatenate(points_xf, axis=0)\n\n    def __neg__(self) -&gt; \"MovingLeastSquaresTransform\":\n        \"\"\"Invert direction\"\"\"\n        out = self.copy()\n        out.reverse = not self.reverse\n        return out\n\n    def __eq__(self, o: object) -&gt; bool:\n        if not isinstance(o, MovingLeastSquaresTransform):\n            return False\n        for cp_this, cp_that in zip(self._control_points(), o._control_points()):\n            if not np.array_equal(cp_this, cp_that):\n                return False\n        return True\n\n    def _control_points(self):\n        cp1 = self.transformer.control_points\n        cp2 = self.transformer.deformed_control_points\n        if self.reverse:\n            cp2, cp1 = cp1, cp2\n        return cp1, cp2\n\n    def copy(self):\n        \"\"\"Make a copy\"\"\"\n        return deepcopy(self)\n</code></pre>"},{"location":"reference/navis/transforms/align/#navis.transforms.align.MovingLeastSquaresTransform.__init__","title":"<code>__init__</code>","text":"<p>Moving Least Squares transforms of 3D spatial data.</p> <p>Uses the molesq library, which packages the implementation by Casey Schneider-Mizell of the affine algorithm published in Schaefer et al. 2006.</p> Notes <p>At least in my hands, <code>TPStransforms</code> are significantly faster than <code>MovingLeastSquaresTransforms</code>. The results are similar but not identical, so make sure to use the one that works best for your use case.</p> PARAMETER DESCRIPTION <code>landmarks_source</code> <p>Source landmarks as x/y/z coordinates.</p> <p> TYPE: <code>np.ndarray</code> </p> <code>landmarks_target</code> <p>Target landmarks as x/y/z coordinates.</p> <p> TYPE: <code>np.ndarray</code> </p> <code>direction</code> <p>'forward' (default) or 'inverse' (treat the target as the source and vice versa)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'forward'</code> </p> <code>batch_size</code> <p>Batch size for transforming points. At one point during the transformation, molesq generates a (N, M) distance matrix, where N is the number of landmarks and M is the number of locations, which can get prohibitively expensive. We avoid the issue by batching the transformation by default. Note that the overhead from batching seems negligible.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100000</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from navis import transforms\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Generate some mock landmarks\n&gt;&gt;&gt; src = np.array([[0, 0, 0], [10, 10, 10], [100, 100, 100], [80, 10, 30]])\n&gt;&gt;&gt; trg = np.array([[1, 15, 5], [9, 18, 21], [80, 99, 120], [5, 10, 80]])\n&gt;&gt;&gt; tr = transforms.MovingLeastSquaresTransform(src, trg)\n&gt;&gt;&gt; points = np.array([[0, 0, 0], [50, 50, 50]])\n&gt;&gt;&gt; tr.xform(points)\narray([[  1.        ,  15.        ,   5.        ],\n       [ 81.56361725, 155.32071504, 187.3147564 ]])\n</code></pre> Source code in <code>navis/transforms/moving_least_squares.py</code> <pre><code>def __init__(\n    self,\n    landmarks_source: np.ndarray,\n    landmarks_target: np.ndarray,\n    direction: str = \"forward\",\n    batch_size: int = 100_000,\n) -&gt; None:\n    \"\"\"Moving Least Squares transforms of 3D spatial data.\n\n    Uses the [molesq](https://github.com/clbarnes/molesq) library, which packages the\n    [implementation](https://github.com/ceesem/catalysis/blob/master/catalysis/transform.py)\n    by Casey Schneider-Mizell of the affine algorithm published in\n    [Schaefer et al. 2006](https://dl.acm.org/doi/pdf/10.1145/1179352.1141920).\n\n    Notes\n    -----\n    At least in my hands, `TPStransforms` are significantly faster than\n    `MovingLeastSquaresTransforms`. The results are similar but not identical,\n    so make sure to use the one that works best for your use case.\n\n    Parameters\n    ----------\n    landmarks_source : np.ndarray\n        Source landmarks as x/y/z coordinates.\n    landmarks_target : np.ndarray\n        Target landmarks as x/y/z coordinates.\n    direction : str\n        'forward' (default) or 'inverse' (treat the target as the source and vice versa)\n    batch_size : int, optional\n        Batch size for transforming points. At one point during the transformation,\n        molesq generates a (N, M) distance matrix, where N is the number of landmarks\n        and M is the number of locations, which can get prohibitively expensive.\n        We avoid the issue by batching the transformation by default. Note that the\n        overhead from batching seems negligible.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from navis import transforms\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; # Generate some mock landmarks\n    &gt;&gt;&gt; src = np.array([[0, 0, 0], [10, 10, 10], [100, 100, 100], [80, 10, 30]])\n    &gt;&gt;&gt; trg = np.array([[1, 15, 5], [9, 18, 21], [80, 99, 120], [5, 10, 80]])\n    &gt;&gt;&gt; tr = transforms.MovingLeastSquaresTransform(src, trg)\n    &gt;&gt;&gt; points = np.array([[0, 0, 0], [50, 50, 50]])\n    &gt;&gt;&gt; tr.xform(points)                                        # doctest: +SKIP\n    array([[  1.        ,  15.        ,   5.        ],\n           [ 81.56361725, 155.32071504, 187.3147564 ]])\n\n    \"\"\"\n    assert direction in (\"forward\", \"inverse\")\n    self.transformer = Transformer(landmarks_source, landmarks_target)\n    self.reverse = direction == \"inverse\"\n    self.batch_size = int(batch_size)\n</code></pre>"},{"location":"reference/navis/transforms/align/#navis.transforms.align.MovingLeastSquaresTransform.copy","title":"<code>copy</code>","text":"<p>Make a copy</p> Source code in <code>navis/transforms/moving_least_squares.py</code> <pre><code>def copy(self):\n    \"\"\"Make a copy\"\"\"\n    return deepcopy(self)\n</code></pre>"},{"location":"reference/navis/transforms/align/#navis.transforms.align.MovingLeastSquaresTransform.xform","title":"<code>xform</code>","text":"<p>Transform points.</p> PARAMETER DESCRIPTION <code>points</code> <pre><code>    Points to transform.\n</code></pre> <p> TYPE: <code>   (N, 3) array</code> </p> RETURNS DESCRIPTION <code>pointsxf</code> <p>Transformed points.</p> <p> TYPE: <code>(N, 3) array</code> </p> Source code in <code>navis/transforms/moving_least_squares.py</code> <pre><code>def xform(self, points: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Transform points.\n\n    Parameters\n    ----------\n    points :    (N, 3) array\n                Points to transform.\n\n    Returns\n    -------\n    pointsxf :  (N, 3) array\n                Transformed points.\n\n    \"\"\"\n    if isinstance(points, pd.DataFrame):\n        if any([c not in points for c in [\"x\", \"y\", \"z\"]]):\n            raise ValueError(\"DataFrame must have x/y/z columns.\")\n        points = points[[\"x\", \"y\", \"z\"]].values\n\n    batch_size = self.batch_size if self.batch_size else len(points)\n    points_xf = []\n    for i in range(0, len(points), batch_size):\n        batch = points[i : i + batch_size]\n        points_xf.append(self.transformer.transform(batch, reverse=self.reverse))\n\n    return np.concatenate(points_xf, axis=0)\n</code></pre>"},{"location":"reference/navis/transforms/align/#navis.transforms.align.align_deform","title":"<code>navis.transforms.align.align_deform</code>","text":"<p>Align neurons using a deformable registration.</p> <p>Requires the <code>pycpd</code> library. Note that it's often beneficial to first run a rough affine alignment via <code>rigid_align</code>. Anecdotally, this works well to align backbones but tends to pull denser parts (e.g. dendrites) into a tight ball.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Neurons to align.\n</code></pre> <p> TYPE: <code>            navis.NeuronList</code> </p> <code>target</code> <pre><code>        The neuron that all neurons in `x` will be aligned to.\n        If `None`, neurons will be aligned to the first neuron in `x`!\n</code></pre> <p> TYPE: <code>       navis.Neuron | np.ndarray</code> DEFAULT: <code>None</code> </p> <code>sample</code> <pre><code>        If provided, will calculate an initial registration on only\n        the given fraction of points followed by a landmark transform\n        to transform the rest. Use this to speed things up.\n</code></pre> <p> TYPE: <code>       float [0-1]</code> DEFAULT: <code>None</code> </p> <code>progress</code> <pre><code>        Whether to show a progress bar.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> <code>**kwargs</code> <pre><code>        Additional keyword-argumens are passed through to\n        pycpd.DeformableRegistration. In brief: lower `alpha` and\n        higher `beta` typically make for more fitting deform. I have\n        gone as far as alpha=.01 and beta=10000.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>xf</code> <p>The aligned neurons.</p> <p> TYPE: <code>navis.NeuronList</code> </p> <code>regs</code> <p>The pycpd registration objects.</p> <p> TYPE: <code>list</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n1, n2 = navis.example_neurons(2, kind='skeleton')\n&gt;&gt;&gt; n1_aligned, regs = navis.align.align_deform(n1, n2, sample=.2)\n</code></pre> Source code in <code>navis/transforms/align.py</code> <pre><code>def align_deform(x, target=None, sample=None, progress=True, **kwargs):\n    \"\"\"Align neurons using a deformable registration.\n\n    Requires the `pycpd` library. Note that it's often beneficial to first\n    run a rough affine alignment via `rigid_align`. Anecdotally, this works\n    well to align backbones but tends to pull denser parts (e.g. dendrites)\n    into a tight ball.\n\n    Parameters\n    ----------\n    x :             navis.NeuronList\n                    Neurons to align.\n    target :        navis.Neuron | np.ndarray\n                    The neuron that all neurons in `x` will be aligned to.\n                    If `None`, neurons will be aligned to the first neuron in `x`!\n    sample :        float [0-1], optional\n                    If provided, will calculate an initial registration on only\n                    the given fraction of points followed by a landmark transform\n                    to transform the rest. Use this to speed things up.\n    progress :      bool\n                    Whether to show a progress bar.\n    **kwargs\n                    Additional keyword-argumens are passed through to\n                    pycpd.DeformableRegistration. In brief: lower `alpha` and\n                    higher `beta` typically make for more fitting deform. I have\n                    gone as far as alpha=.01 and beta=10000.\n\n    Returns\n    -------\n    xf :    navis.NeuronList\n            The aligned neurons.\n    regs :  list\n            The pycpd registration objects.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n1, n2 = navis.example_neurons(2, kind='skeleton')\n    &gt;&gt;&gt; n1_aligned, regs = navis.align.align_deform(n1, n2, sample=.2)\n\n    \"\"\"\n    try:\n        from pycpd import DeformableRegistration as Registration\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            '`align_deform()` requires the `pycpd` library:\\n'\n            '  pip3 install git+https://github.com/siavashk/pycpd@master -U'\n            )\n\n    if isinstance(x, core.BaseNeuron):\n        x = core.NeuronList(x)\n\n    assert isinstance(x, core.NeuronList), f\"Expected NeuronList, got {type(x)}\"\n\n    if target is None:\n        target = x[0]\n\n    target_co = _extract_coords(target)\n\n    # This wraps the registration process\n    register = _reg_subsample(Registration, sample=sample)\n\n    # pycpd's deformable registration is very sensitive to the scale of the\n    # data. We will hence normalize the neurons to be within the -1 to 1 range\n    scale_factor = 0\n    for n in x:\n        co = _extract_coords(n)\n        mx = np.abs(co).max()\n        scale_factor = mx if mx &gt; scale_factor else scale_factor\n\n    xf = x / scale_factor\n    target_co = target_co / scale_factor\n    regs = []\n    for n in config.tqdm(xf,\n                         disable=(not progress) or (len(xf) == 1),\n                         desc='Aligning'):\n        if n is target:\n            continue\n        TY, params, reg = register(X=target_co, Y=_extract_coords(n), **kwargs)\n        _set_coords(n, TY)\n        regs.append(reg)\n\n    return xf * scale_factor, regs\n</code></pre>"},{"location":"reference/navis/transforms/align/#navis.transforms.align.align_pairwise","title":"<code>navis.transforms.align.align_pairwise</code>","text":"<p>Run a pairwise alignment between given neurons.</p> <p>Requires the <code>pycpd</code> library.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>    Neurons to align to other neurons.\n</code></pre> <p> TYPE: <code>        navis.NeuronList</code> </p> <code>y</code> <pre><code>    The neurons to align to. If `None`, will run pairwise\n    alignment of `x` vs `x`.\n</code></pre> <p> TYPE: <code>        navis.NeuronList</code> DEFAULT: <code>None</code> </p> <code>method</code> <pre><code>    Which method to use for alignment. Maps to the respective\n    `navis.align_{method}` function. \"rigid+deform\" performs a\n    rigid followed by a warping alignment.\n</code></pre> <p> TYPE: <code>   \"rigid\" | \"deform\" | \"pca\" | \"rigid+deform\"</code> DEFAULT: <code>'rigid'</code> </p> <code>sample</code> <pre><code>    If provided, will calculate an initial registration on only\n    the given fraction of points followed by a landmark transform\n    to transform the rest. Use this to speed things up.\n</code></pre> <p> TYPE: <code>   float [0-1]</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <pre><code>    Keyword arguments are passed through to the respective\n    alignment function.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>Array of shape (x, y) with the pairwise-aligned neurons.</p> See Also <p><code>navis.nblast_align</code>             Runs an NBLAST where neurons are first aligned pairwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; nl = navis.example_neurons(2, kind='skeleton')\n&gt;&gt;&gt; aligned = navis.align.align_pairwise(nl, method='rigid', sample=.2)\n</code></pre> Source code in <code>navis/transforms/align.py</code> <pre><code>def align_pairwise(x, y=None, method='rigid', sample=None, progress=True, **kwargs):\n    \"\"\"Run a pairwise alignment between given neurons.\n\n    Requires the `pycpd` library.\n\n    Parameters\n    ----------\n    x :         navis.NeuronList\n                Neurons to align to other neurons.\n    y :         navis.NeuronList, optional\n                The neurons to align to. If `None`, will run pairwise\n                alignment of `x` vs `x`.\n    method :    \"rigid\" | \"deform\" | \"pca\" | \"rigid+deform\"\n                Which method to use for alignment. Maps to the respective\n                `navis.align_{method}` function. \"rigid+deform\" performs a\n                rigid followed by a warping alignment.\n    sample :    float [0-1], optional\n                If provided, will calculate an initial registration on only\n                the given fraction of points followed by a landmark transform\n                to transform the rest. Use this to speed things up.\n    **kwargs\n                Keyword arguments are passed through to the respective\n                alignment function.\n\n    Returns\n    -------\n    np.ndarray\n                Array of shape (x, y) with the pairwise-aligned neurons.\n\n    See Also\n    --------\n    [`navis.nblast_align`][]\n                Runs an NBLAST where neurons are first aligned pairwise.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; nl = navis.example_neurons(2, kind='skeleton')\n    &gt;&gt;&gt; aligned = navis.align.align_pairwise(nl, method='rigid', sample=.2)\n\n    \"\"\"\n    if y is None:\n        y = x\n\n    utils.eval_param(x, name='x', allowed_types=(core.NeuronList, ))\n    utils.eval_param(y, name='y', allowed_types=(core.NeuronList, ))\n    utils.eval_param(method, name='method',\n                     allowed_values=('rigid', 'deform', 'pca'))\n\n    func = {'rigid': align_rigid,\n            'deform': align_deform,\n            'pca': align_pca,\n            'rigid+deform': _align_rigid_deform}[method]\n\n    aligned = []\n    for n1 in config.tqdm(x,\n                          desc='Aligning',\n                          disable=not progress or len(x) == 1):\n        aligned.append([])\n        for n2 in y:\n            if n1 is n2:\n                xf = n1\n            else:\n                xf = func(n1, target=n2, sample=sample, progress=False, **kwargs)[0][0]\n            aligned[-1].append(xf)\n\n    return np.array(aligned)\n</code></pre>"},{"location":"reference/navis/transforms/align/#navis.transforms.align.align_pca","title":"<code>navis.transforms.align.align_pca</code>","text":"<p>Align neurons along their first principal components.</p> <p>This will in effect turn the neurons into a 1-dimensional line. Requires the <code>scikit-learn</code> library.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        The neurons to align.\n</code></pre> <p> TYPE: <code>            navis.NeuronList | np.ndarray</code> </p> <code>individually</code> <pre><code>        Whether to align neurons along their individual or\n        collective first principical component.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>xf</code> <p>The PCA-aligned neurons.</p> <p> TYPE: <code>navis.NeuronList</code> </p> <code>pcas</code> <p>The scikit-learn PCA object(s)</p> <p> TYPE: <code>list</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n1, n2 = navis.example_neurons(2, kind='skeleton')\n&gt;&gt;&gt; n1_aligned, pcas = navis.align.align_pca(n1, n2)\n</code></pre> Source code in <code>navis/transforms/align.py</code> <pre><code>def align_pca(x, individually=True):\n    \"\"\"Align neurons along their first principal components.\n\n    This will in effect turn the neurons into a 1-dimensional line.\n    Requires the `scikit-learn` library.\n\n    Parameters\n    ----------\n    x :             navis.NeuronList | np.ndarray\n                    The neurons to align.\n    individually :  bool\n                    Whether to align neurons along their individual or\n                    collective first principical component.\n\n    Returns\n    -------\n    xf :    navis.NeuronList\n            The PCA-aligned neurons.\n    pcas :  list\n            The scikit-learn PCA object(s)\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n1, n2 = navis.example_neurons(2, kind='skeleton')\n    &gt;&gt;&gt; n1_aligned, pcas = navis.align.align_pca(n1, n2)\n\n    \"\"\"\n    try:\n        from sklearn.decomposition import PCA\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            '`align_pca()` requires the `scikit-learn` library:\\n'\n            '  pip3 install scikit-learn -U'\n            )\n\n    if isinstance(x, core.BaseNeuron):\n        x = core.NeuronList(x)\n\n    assert isinstance(x, core.NeuronList)\n\n    pcas = []\n    if not individually:\n        # Collect coordinates\n        co = [_extract_coords(n) for n in x]\n        n_points = [len(c) for c in co]  # track how many points per neuron\n        co = np.vstack(co)\n\n        pca = PCA(n_components=1)\n        co_new = pca.fit_transform(X=co)\n\n        xf = x.copy()\n        i = 0\n        for n, le in zip(xf, n_points):\n            _set_coords(n, co_new[i: i + le])\n            i += le\n        pcas.append(pca)\n    else:\n        xf = x.copy()\n        for n in xf:\n            pca = PCA(n_components=1)\n            _set_coords(n, pca.fit_transform(X=_extract_coords(n)))\n            pcas.append(pca)\n    return xf, pcas\n</code></pre>"},{"location":"reference/navis/transforms/align/#navis.transforms.align.align_rigid","title":"<code>navis.transforms.align.align_rigid</code>","text":"<p>Align neurons using a rigid registration.</p> <p>Requires the <code>pycpd</code> library.</p> PARAMETER DESCRIPTION <code>x</code> <pre><code>        Neurons to align.\n</code></pre> <p> TYPE: <code>            navis.NeuronList</code> </p> <code>target</code> <pre><code>        The neuron that all neurons in `x` will be aligned to.\n        If `None`, neurons will be aligned to the first neuron in `x`!\n</code></pre> <p> TYPE: <code>       navis.Neuron | np.ndarray</code> DEFAULT: <code>None</code> </p> <code>scale</code> <pre><code>        If True, will also scale the neuron.\n</code></pre> <p> TYPE: <code>        bool</code> DEFAULT: <code>False</code> </p> <code>w</code> <pre><code>        `w` is used to account for outliers: higher w = more forgiving.\n        The default is w=0 which can lead to failure to converge on a\n        solution (in particular when scale=False). In that case we\n        incrementally increase `w` by a factor of 10 until we find\n        a solution. Set `verbose=True` to get detailed feedback\n        on the solution.\n</code></pre> <p> TYPE: <code>            float</code> DEFAULT: <code>0</code> </p> <code>sample</code> <pre><code>        If provided, will calculate an initial registration on only\n        the given fraction of points followed by a landmark transform\n        to transform the rest. Use this to speed things up.\n</code></pre> <p> TYPE: <code>       float [0-1]</code> DEFAULT: <code>None</code> </p> <code>progress</code> <pre><code>        Whether to show a progress bar.\n</code></pre> <p> TYPE: <code>     bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>xf</code> <p>The aligned neurons.</p> <p> TYPE: <code>navis.NeuronList</code> </p> <code>regs</code> <p>The pycpd registration objects.</p> <p> TYPE: <code>list</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import navis\n&gt;&gt;&gt; n1, n2 = navis.example_neurons(2, kind='skeleton')\n&gt;&gt;&gt; n1_aligned, regs = navis.align.align_rigid(n1, n2, sample=.2)\n</code></pre> Source code in <code>navis/transforms/align.py</code> <pre><code>def align_rigid(x, target=None, scale=False, w=0, verbose=False, sample=None, progress=True):\n    \"\"\"Align neurons using a rigid registration.\n\n    Requires the `pycpd` library.\n\n    Parameters\n    ----------\n    x :             navis.NeuronList\n                    Neurons to align.\n    target :        navis.Neuron | np.ndarray\n                    The neuron that all neurons in `x` will be aligned to.\n                    If `None`, neurons will be aligned to the first neuron in `x`!\n    scale :         bool\n                    If True, will also scale the neuron.\n    w :             float\n                    `w` is used to account for outliers: higher w = more forgiving.\n                    The default is w=0 which can lead to failure to converge on a\n                    solution (in particular when scale=False). In that case we\n                    incrementally increase `w` by a factor of 10 until we find\n                    a solution. Set `verbose=True` to get detailed feedback\n                    on the solution.\n    sample :        float [0-1], optional\n                    If provided, will calculate an initial registration on only\n                    the given fraction of points followed by a landmark transform\n                    to transform the rest. Use this to speed things up.\n    progress :      bool\n                    Whether to show a progress bar.\n\n    Returns\n    -------\n    xf :    navis.NeuronList\n            The aligned neurons.\n    regs :  list\n            The pycpd registration objects.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import navis\n    &gt;&gt;&gt; n1, n2 = navis.example_neurons(2, kind='skeleton')\n    &gt;&gt;&gt; n1_aligned, regs = navis.align.align_rigid(n1, n2, sample=.2)\n\n    \"\"\"\n    try:\n        from pycpd import RigidRegistration as Registration\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            '`align_rigid()` requires the `pycpd` library:\\n'\n            '  pip3 install git+https://github.com/siavashk/pycpd@master -U'\n            )\n\n    if isinstance(x, core.BaseNeuron):\n        x = core.NeuronList(x)\n\n    assert isinstance(x, core.NeuronList)\n\n    if target is None:\n        target = x[0]\n\n    target_co = _extract_coords(target)\n\n    # This wraps the registration process\n    register = _reg_subsample(Registration, sample=sample)\n\n    xf = x.copy()\n    regs = []\n    for n in config.tqdm(xf,\n                         disable=(not progress) or (len(xf) == 1),\n                         desc='Aligning'):\n        if n is target:\n            continue\n        # `w` is used to account for outliers -&gt; higher w = more forgiving\n        # the default is w=0 which can lead to failure to converge on a solution\n        # in particular when scale=False\n        # Our work-around here is to start at w=0 and incrementally increase w\n        # if we fail to converge\n        # Also note that pycpd ignores the `scale` in earlier versions. The\n        # version on PyPI is currently outdated. From what I understand we need\n        # the Github version.\n        converged = False\n        while w &lt;= 0.001:\n            try:\n                with warnings.catch_warnings():\n                    warnings.simplefilter(\"ignore\")\n                    TY, params, reg = register(X=target_co,\n                                               Y=_extract_coords(n),\n                                               scale=scale,\n                                               s=1,\n                                               w=w)\n                _set_coords(n, TY)\n                converged = True\n                regs.append(reg)\n                break\n            except np.linalg.LinAlgError:\n                if w == 0:\n                    w += 0.000000001\n                else:\n                    w *= 10\n\n        if verbose:\n            if not converged:\n                logger.info(f'Registration of {n.id} onto {target.id} did not converge')\n            else:\n                logger.info(f'Registration of {n.id} onto {target.id} converged for w={w}')\n\n    return xf, regs\n</code></pre>"},{"location":"reference/navis/transforms/base/","title":"base","text":""},{"location":"reference/navis/transforms/base/#navis.transforms.base.TransOptimizer","title":"<code>navis.transforms.base.TransOptimizer</code>","text":"<p>Optimizes a Transform or TransformSequence.</p> <p>The purpose of this class is to change a bunch of settings (depending on the type of transforms) before running the transformation and do a clean up after it has finished.</p> <p>Currently, the optimizations are very hands-on but in the future, this might be delegated to the individuals <code>Transform</code> classes - e.g. by implementing an <code>.optimize()</code> method which is then called by <code>TransOptimizer</code>.</p> <p>Currently, it really only manages caching for H5 transforms.</p> PARAMETER DESCRIPTION <code>tr</code> <pre><code>    The transform or sequence thereof to be optimized.\n</code></pre> <p> TYPE: <code>       Transform | TransformSequence</code> </p> <code>mode</code> <pre><code>    Mode for optimization:\n      - `None`: no optimization\n      - \"medium\": some optimization but keep upfront cost low\n      - \"aggressive\": high upfront cost but should be faster in the long run\n</code></pre> <p> TYPE: <code>     None | \"medium\" | \"aggressive\"</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from navis.transforms import h5reg\n&gt;&gt;&gt; from navis.transforms.base import TransOptimizer\n&gt;&gt;&gt; tr = h5reg.H5transform('path/to/reg.h5', direction='inverse')\n&gt;&gt;&gt; with TransOptimizer(tr, mode='aggressive'):\n&gt;&gt;&gt;     xf = tr.xform(pts)\n</code></pre> Source code in <code>navis/transforms/base.py</code> <pre><code>class TransOptimizer:\n    \"\"\"Optimizes a Transform or TransformSequence.\n\n    The purpose of this class is to change a bunch of settings (depending on the\n    type of transforms) before running the transformation and do a clean up\n    after it has finished.\n\n    Currently, the optimizations are very hands-on but in the future, this might\n    be delegated to the individuals `Transform` classes - e.g. by implementing\n    an `.optimize()` method which is then called by `TransOptimizer`.\n\n    Currently, it really only manages caching for H5 transforms.\n\n    Parameters\n    ----------\n    tr :        Transform | TransformSequence\n                The transform or sequence thereof to be optimized.\n    mode :      None | \"medium\" | \"aggressive\"\n                Mode for optimization:\n                  - `None`: no optimization\n                  - \"medium\": some optimization but keep upfront cost low\n                  - \"aggressive\": high upfront cost but should be faster in the long run\n\n    Examples\n    --------\n    &gt;&gt;&gt; from navis.transforms import h5reg\n    &gt;&gt;&gt; from navis.transforms.base import TransOptimizer\n    &gt;&gt;&gt; tr = h5reg.H5transform('path/to/reg.h5', direction='inverse') # doctest: +SKIP\n    &gt;&gt;&gt; with TransOptimizer(tr, mode='aggressive'):                   # doctest: +SKIP\n    &gt;&gt;&gt;     xf = tr.xform(pts)                                        # doctest: +SKIP\n\n    \"\"\"\n\n    def __init__(self, tr, bbox, caching: bool):\n        \"\"\"Initialize Optimizer.\"\"\"\n        assert isinstance(caching, bool)\n\n        self.caching = caching\n        self.bbox = np.asarray(bbox)\n\n        assert self.bbox.ndim == 2 and self.bbox.shape == (3, 2)\n\n        if isinstance(tr, BaseTransform):\n            self.transforms = [tr]\n        elif isinstance(tr, TransformSequence):\n            self.transforms = tr.transforms\n        else:\n            raise TypeError(f'Expected Transform/Sequence, got \"{type(tr)}\"')\n\n    def __enter__(self):\n        \"\"\"Apply optimizations.\"\"\"\n        if not self.caching:\n            return\n\n        # Check if there are any transforms we can optimize\n        if not any(['H5transform' in str(type(tr)) for tr in self.transforms]):\n            return\n\n        if not config.pbar_hide:\n            logger.info('Pre-caching deformation field(s) for transforms...')\n\n        bbox_xf = self.bbox\n        for tr in self.transforms:\n            # We are monkey patching here to avoid circular imports\n            # not pretty but works\n            if 'H5transform' in str(type(tr)):\n                # Precache values in the bounding box\n                tr.precache(bbox_xf, padding=True)\n            # To pre-cache sequential transforms we need to xform the bounding\n            # box as we move along\n            bbox_xf = tr.xform(bbox_xf.T).T\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Revert optimizations.\"\"\"\n        if not self.caching:\n            return\n\n        for tr in self.transforms:\n            # We are monkey patching here to avoid circular imports\n            # not pretty but works\n            if 'H5transform' in str(type(tr)):\n                # Clears the cache\n                tr.use_cache = False\n</code></pre>"},{"location":"reference/navis/transforms/base/#navis.transforms.base.TransOptimizer.__init__","title":"<code>__init__</code>","text":"<p>Initialize Optimizer.</p> Source code in <code>navis/transforms/base.py</code> <pre><code>def __init__(self, tr, bbox, caching: bool):\n    \"\"\"Initialize Optimizer.\"\"\"\n    assert isinstance(caching, bool)\n\n    self.caching = caching\n    self.bbox = np.asarray(bbox)\n\n    assert self.bbox.ndim == 2 and self.bbox.shape == (3, 2)\n\n    if isinstance(tr, BaseTransform):\n        self.transforms = [tr]\n    elif isinstance(tr, TransformSequence):\n        self.transforms = tr.transforms\n    else:\n        raise TypeError(f'Expected Transform/Sequence, got \"{type(tr)}\"')\n</code></pre>"},{"location":"reference/navis/transforms/base/#navis.transforms.base.TransformSequence","title":"<code>navis.transforms.base.TransformSequence</code>","text":"<p>A sequence of transforms.</p> <p>Use this to apply multiple (different types of) transforms in sequence.</p> PARAMETER DESCRIPTION <code>*transforms</code> <pre><code>        The transforms to bundle in this sequence.\n</code></pre> <p> TYPE: <code>  Transform/Sequences.</code> DEFAULT: <code>()</code> </p> <code>copy</code> <pre><code>        Whether to make a copy of the transform on initialization.\n        This is highly recommended because otherwise we might alter\n        the original as we add more transforms (e.g. for CMTK\n        transforms).\n</code></pre> <p> TYPE: <code>         bool</code> DEFAULT: <code>True</code> </p> Source code in <code>navis/transforms/base.py</code> <pre><code>class TransformSequence:\n    \"\"\"A sequence of transforms.\n\n    Use this to apply multiple (different types of) transforms in sequence.\n\n    Parameters\n    ----------\n    *transforms :   Transform/Sequences.\n                    The transforms to bundle in this sequence.\n    copy :          bool\n                    Whether to make a copy of the transform on initialization.\n                    This is highly recommended because otherwise we might alter\n                    the original as we add more transforms (e.g. for CMTK\n                    transforms).\n\n    \"\"\"\n\n    def __init__(self, *transforms, copy=True):\n        \"\"\"Initialize.\"\"\"\n        self.transforms = []\n        for tr in transforms:\n            if not isinstance(tr, (BaseTransform, TransformSequence)):\n                raise TypeError(f'Expected transform, got \"{type(tr)}\"')\n            if copy:\n                tr = tr.copy()\n            self.append(tr)\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __repr__(self):\n        return f'TransformSequence with {len(self)} transform(s)'\n\n    def __len__(self) -&gt; int:\n        \"\"\"Count number of transforms in this sequence.\"\"\"\n        return len(self.transforms)\n\n    def __neg__(self) -&gt; 'TransformSequence':\n        \"\"\"Invert transform sequence.\"\"\"\n        return TransformSequence(*[-t for t in self.transforms[::-1]])\n\n    def append(self, transform: 'BaseTransform'):\n        \"\"\"Add transform to list.\"\"\"\n        if isinstance(transform, TransformSequence):\n            # Unpack if other is sequence of transforms\n            transform = transform.transforms\n\n        for tr in utils.make_iterable(transform):\n            if not isinstance(tr, BaseTransform):\n                raise TypeError(f'Unable append \"{type(tr)}\"')\n\n            if not hasattr(transform, 'xform') or not callable(transform.xform):\n                raise TypeError('Transform does not appear to have a `xform` method')\n\n            # Try to merge with the last transform in the sequence\n            if len(self):\n                try:\n                    self.transforms[-1].append(tr)\n                except NotImplementedError:\n                    self.transforms.append(tr)\n                except BaseException:\n                    raise\n            else:\n                self.transforms.append(tr)\n\n    def xform(self, points: np.ndarray,\n              affine_fallback: bool = True,\n              **kwargs) -&gt; np.ndarray:\n        \"\"\"Perform transforms in sequence.\"\"\"\n        # First check if any of the transforms raise any issues ahead of time\n        # This can e.g. be missing binaries like CMTK's streamxform\n        for tr in self.transforms:\n            tr.check_if_possible(on_error='raise')\n\n        # Now transform points in sequence\n        # Make a copy of the points to avoid changing the originals\n        # Note dtype float64 in case our precision in case precisio must go up\n        # -&gt; e.g. when converting from nm to micron space\n        xf = np.asarray(points).astype(np.float64)\n        for tr in self.transforms:\n            # Check this transforms signature for accepted Parameters\n            params = signature(tr.xform).parameters\n\n            # We must not pass NaN value from one transform to the next\n            is_nan = np.any(np.isnan(xf), axis=1)\n\n            # Skip if all points are NaN\n            if all(is_nan):\n                continue\n\n            if 'affine_fallback' in params:\n                xf[~is_nan] = tr.xform(xf[~is_nan],\n                                       affine_fallback=affine_fallback,\n                                       **kwargs)\n            else:\n                xf[~is_nan] = tr.xform(xf[~is_nan], **kwargs)\n\n        return xf\n</code></pre>"},{"location":"reference/navis/transforms/base/#navis.transforms.base.TransformSequence.__init__","title":"<code>__init__</code>","text":"<p>Initialize.</p> Source code in <code>navis/transforms/base.py</code> <pre><code>def __init__(self, *transforms, copy=True):\n    \"\"\"Initialize.\"\"\"\n    self.transforms = []\n    for tr in transforms:\n        if not isinstance(tr, (BaseTransform, TransformSequence)):\n            raise TypeError(f'Expected transform, got \"{type(tr)}\"')\n        if copy:\n            tr = tr.copy()\n        self.append(tr)\n</code></pre>"},{"location":"reference/navis/transforms/base/#navis.transforms.base.TransformSequence.append","title":"<code>append</code>","text":"<p>Add transform to list.</p> Source code in <code>navis/transforms/base.py</code> <pre><code>def append(self, transform: 'BaseTransform'):\n    \"\"\"Add transform to list.\"\"\"\n    if isinstance(transform, TransformSequence):\n        # Unpack if other is sequence of transforms\n        transform = transform.transforms\n\n    for tr in utils.make_iterable(transform):\n        if not isinstance(tr, BaseTransform):\n            raise TypeError(f'Unable append \"{type(tr)}\"')\n\n        if not hasattr(transform, 'xform') or not callable(transform.xform):\n            raise TypeError('Transform does not appear to have a `xform` method')\n\n        # Try to merge with the last transform in the sequence\n        if len(self):\n            try:\n                self.transforms[-1].append(tr)\n            except NotImplementedError:\n                self.transforms.append(tr)\n            except BaseException:\n                raise\n        else:\n            self.transforms.append(tr)\n</code></pre>"},{"location":"reference/navis/transforms/base/#navis.transforms.base.TransformSequence.xform","title":"<code>xform</code>","text":"<p>Perform transforms in sequence.</p> Source code in <code>navis/transforms/base.py</code> <pre><code>def xform(self, points: np.ndarray,\n          affine_fallback: bool = True,\n          **kwargs) -&gt; np.ndarray:\n    \"\"\"Perform transforms in sequence.\"\"\"\n    # First check if any of the transforms raise any issues ahead of time\n    # This can e.g. be missing binaries like CMTK's streamxform\n    for tr in self.transforms:\n        tr.check_if_possible(on_error='raise')\n\n    # Now transform points in sequence\n    # Make a copy of the points to avoid changing the originals\n    # Note dtype float64 in case our precision in case precisio must go up\n    # -&gt; e.g. when converting from nm to micron space\n    xf = np.asarray(points).astype(np.float64)\n    for tr in self.transforms:\n        # Check this transforms signature for accepted Parameters\n        params = signature(tr.xform).parameters\n\n        # We must not pass NaN value from one transform to the next\n        is_nan = np.any(np.isnan(xf), axis=1)\n\n        # Skip if all points are NaN\n        if all(is_nan):\n            continue\n\n        if 'affine_fallback' in params:\n            xf[~is_nan] = tr.xform(xf[~is_nan],\n                                   affine_fallback=affine_fallback,\n                                   **kwargs)\n        else:\n            xf[~is_nan] = tr.xform(xf[~is_nan], **kwargs)\n\n    return xf\n</code></pre>"},{"location":"reference/navis/transforms/base/#navis.transforms.base.trigger_init","title":"<code>navis.transforms.base.trigger_init</code>","text":"<p>Trigger delayed initialization.</p> Source code in <code>navis/transforms/base.py</code> <pre><code>def trigger_init(func):\n    \"\"\"Trigger delayed initialization.\"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        self = args[0]\n        # Check if has already been initialized\n        if not self.initialized:\n            self.__delayed_init__()\n        return func(*args, **kwargs)\n    return wrapper\n</code></pre>"},{"location":"reference/navis/transforms/cmtk/","title":"cmtk","text":""},{"location":"reference/navis/transforms/cmtk/#navis.transforms.cmtk.cmtk_version","title":"<code>navis.transforms.cmtk.cmtk_version</code>","text":"<p>Get CMTK version.</p> Source code in <code>navis/transforms/cmtk.py</code> <pre><code>@requires_cmtk\ndef cmtk_version(as_string=False):\n    \"\"\"Get CMTK version.\"\"\"\n    p = subprocess.run([_cmtkbin / \"streamxform\", \"--version\"], capture_output=True)\n    version = p.stdout.decode(\"utf-8\").rstrip()\n\n    if as_string:\n        return version\n    else:\n        return tuple(int(v) for v in version.split(\".\"))\n</code></pre>"},{"location":"reference/navis/transforms/cmtk/#navis.transforms.cmtk.find_cmtkbin","title":"<code>navis.transforms.cmtk.find_cmtkbin</code>","text":"<p>Find directory with CMTK binaries.</p> Source code in <code>navis/transforms/cmtk.py</code> <pre><code>def find_cmtkbin(tool: str = \"streamxform\") -&gt; str:\n    \"\"\"Find directory with CMTK binaries.\"\"\"\n    for path in _search_path:\n        path = pathlib.Path(path).absolute()\n        if not path.is_dir():\n            continue\n\n        try:\n            return next(path.glob(tool)).resolve().parent\n        except StopIteration:\n            continue\n        except BaseException:\n            raise\n</code></pre>"},{"location":"reference/navis/transforms/cmtk/#navis.transforms.cmtk.parse_target_specs","title":"<code>navis.transforms.cmtk.parse_target_specs</code>","text":"<p>Parse target specs into argument that can be passed to CMTK.</p> Source code in <code>navis/transforms/cmtk.py</code> <pre><code>def parse_target_specs(target):\n    \"\"\"Parse target specs into argument that can be passed to CMTK.\"\"\"\n    # Note to self: this function should also deal with VoxelNeurons and NRRD filepaths\n    # For NRRD filepaths: we need to add an empty \"--\" before the filepath (I think)\n\n    from .templates import TemplateBrain  # avoid circular import\n\n    assert isinstance(target, (str, TemplateBrain, np.ndarray, list, tuple))\n\n    if isinstance(target, str):\n        from . import registry\n\n        target = registry.find_template(target)\n\n    if isinstance(target, TemplateBrain):\n        specs = list(target.dims) + list(target.voxdims)\n        # Note to self: need to check TemplateBrain (and flybrains) consistent definition of\n        # dims, voxdims and origin (maybe even add origin)\n\n    # At this point we expect specs to be an iterable\n    specs = np.asarray(target)\n    assert len(specs) in (\n        6,\n        9,\n    ), f\"Target specs must be of length 6 or 9, got {len(specs)}\"\n    target = \"--target-grid \"\n    target += \",\".join(\n        map(str, specs[:3].astype(int))\n    )  # Number of voxels (must be integer)\n    target += \":\"\n    target += \",\".join(map(str, specs[3:].astype(float)))  # Voxel size (can be float)\n    if len(specs) == 9:\n        target += \":\"\n        target += \",\".join(map(str, specs[6:].astype(float)))  # Origin (can be float)\n\n    return target\n</code></pre>"},{"location":"reference/navis/transforms/cmtk/#navis.transforms.cmtk.requires_cmtk","title":"<code>navis.transforms.cmtk.requires_cmtk</code>","text":"<p>Check if CMTK is available.</p> Source code in <code>navis/transforms/cmtk.py</code> <pre><code>def requires_cmtk(func):\n    \"\"\"Check if CMTK is available.\"\"\"\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        if not _cmtkbin:\n            raise ValueError(\n                \"Cannot find CMTK. Please install from \"\n                \"http://www.nitrc.org/projects/cmtk and \"\n                \"make sure that it is your path!\"\n            )\n        return func(*args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"reference/navis/transforms/cmtk/#navis.transforms.cmtk.xform_cmtk","title":"<code>navis.transforms.cmtk.xform_cmtk</code>","text":"<p>Xform 3d coordinates.</p> PARAMETER DESCRIPTION <code>points</code> <pre><code>            Points to transform. DataFrame must have x/y/z columns.\n</code></pre> <p> TYPE: <code>           (N, 3) array | pandas.DataFrame</code> </p> <code>transforms</code> <pre><code>            Either filepath to CMTK transform or `CMTKtransform`.\n            Multiple regs must be given as list and will be applied\n            sequentially in the order provided.\n</code></pre> <p> TYPE: <code>       filepath(s) | CMTKtransform(s)</code> </p> <code>inverse</code> <pre><code>            Whether to invert transforms. If single boolean will\n            apply to all transforms. Can also provide `inverse` as\n            list of booleans.\n</code></pre> <p> TYPE: <code>          bool | list thereof</code> DEFAULT: <code>False</code> </p> <code>affine_fallback</code> <pre><code>            If True, points that failed to transform during warping\n            transform will be transformed using only the affine\n            transform.\n</code></pre> <p> TYPE: <code>  bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>pointsxf</code> <p>Transformed points. Will contain <code>np.nan</code> for points that did not transform.</p> <p> TYPE: <code>(N, 3) numpy.ndarray</code> </p> Source code in <code>navis/transforms/cmtk.py</code> <pre><code>@requires_cmtk\ndef xform_cmtk(\n    points: np.ndarray,\n    transforms,\n    inverse: bool = False,\n    affine_fallback: bool = False,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"Xform 3d coordinates.\n\n    Parameters\n    ----------\n    points :            (N, 3) array | pandas.DataFrame\n                        Points to transform. DataFrame must have x/y/z columns.\n    transforms :        filepath(s) | CMTKtransform(s)\n                        Either filepath to CMTK transform or `CMTKtransform`.\n                        Multiple regs must be given as list and will be applied\n                        sequentially in the order provided.\n    inverse :           bool | list thereof\n                        Whether to invert transforms. If single boolean will\n                        apply to all transforms. Can also provide `inverse` as\n                        list of booleans.\n    affine_fallback :   bool\n                        If True, points that failed to transform during warping\n                        transform will be transformed using only the affine\n                        transform.\n\n    Returns\n    -------\n    pointsxf :          (N, 3) numpy.ndarray\n                        Transformed points. Will contain `np.nan` for points\n                        that did not transform.\n\n    \"\"\"\n    transforms = list(utils.make_iterable(transforms))\n\n    if isinstance(inverse, bool):\n        inverse = [inverse] * len(transforms)\n\n    directions = [\"forward\" if not i else \"inverse\" for i in inverse]\n\n    for i, r in enumerate(transforms):\n        if not isinstance(r, CMTKtransform):\n            if not isinstance(r, (str, pathlib.Path)):\n                raise TypeError(\"`reg` must be filepath or CMTKtransform\")\n            transforms[i] = CMTKtransform(r, directions=directions[i])\n\n    # Combine all transforms into a sequence of transforms\n    seq = TransformSequence(*transforms)\n\n    # Transform points\n    xf = seq.xform(points)\n\n    # If requested, try again with affine only for points that failed to xform\n    if affine_fallback:\n        isnan = np.any(np.isnan(xf), axis=1)\n        if np.any(isnan):\n            xf[isnan] = seq.xform(points[isnan], affine_only=True)\n\n    return xf\n</code></pre>"},{"location":"reference/navis/transforms/elastix/","title":"elastix","text":""},{"location":"reference/navis/transforms/elastix/#navis.transforms.elastix.elastix_version","title":"<code>navis.transforms.elastix.elastix_version</code>","text":"<p>Get elastix version.</p> Source code in <code>navis/transforms/elastix.py</code> <pre><code>@requires_elastix\ndef elastix_version(as_string=False):\n    \"\"\"Get elastix version.\"\"\"\n    p = subprocess.run([_elastixbin / \"elastix\", \"--version\"], capture_output=True)\n    if p.stderr:\n        raise BaseException(f\"Error running elastix:\\n{p.stderr.decode()}\")\n\n    version = p.stdout.decode(\"utf-8\").rstrip()\n\n    # Extract version from \"elastix version: 5.0.1\"\n    version = version.split(\":\")[-1]\n\n    if as_string:\n        return version\n    else:\n        return tuple(int(v) for v in version.split(\".\"))\n</code></pre>"},{"location":"reference/navis/transforms/elastix/#navis.transforms.elastix.find_elastixbin","title":"<code>navis.transforms.elastix.find_elastixbin</code>","text":"<p>Find directory with elastix binaries.</p> Source code in <code>navis/transforms/elastix.py</code> <pre><code>def find_elastixbin(tool: str = \"transformix\") -&gt; str:\n    \"\"\"Find directory with elastix binaries.\"\"\"\n    for path in _search_path:\n        path = pathlib.Path(path)\n        if not path.is_dir():\n            continue\n\n        try:\n            return next(path.glob(tool)).resolve().parent\n        except StopIteration:\n            continue\n        except BaseException:\n            raise\n</code></pre>"},{"location":"reference/navis/transforms/elastix/#navis.transforms.elastix.requires_elastix","title":"<code>navis.transforms.elastix.requires_elastix</code>","text":"<p>Check if elastix is available.</p> Source code in <code>navis/transforms/elastix.py</code> <pre><code>def requires_elastix(func):\n    \"\"\"Check if elastix is available.\"\"\"\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        if not _elastixbin:\n            raise ValueError(\n                \"Could not find elastix binaries. Please download \"\n                \"the releases page at https://github.com/SuperElastix/elastix, \"\n                \"unzip at a convenient location and add that \"\n                \"location to your PATH variable. Note that you \"\n                \"will also have to set a LD_LIBRARY_PATH (Linux) \"\n                \"or DYLD_LIBRARY_PATH (OSX) variable. See the \"\n                \"elastic manual (release page) for details.\"\n            )\n        return func(*args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"reference/navis/transforms/elastix/#navis.transforms.elastix.setup_elastix","title":"<code>navis.transforms.elastix.setup_elastix</code>","text":"<p>Set up to make elastix work from inside a Python session.</p> <p>Briefly: elastix requires the <code>LD_LIBRARY_PATH</code> (Linux) or <code>LDY_LIBRARY_PATH</code> (OSX) environment variables to (also) point to the directory with the elastix <code>lib</code> directory. For reasons unknown to me, these variables do not make it into the Python session. Hence, we have to set them here explicitly.</p> <p>Above info is based on: https://github.com/jasper-tms/pytransformix</p> Source code in <code>navis/transforms/elastix.py</code> <pre><code>def setup_elastix():\n    \"\"\"Set up to make elastix work from inside a Python session.\n\n    Briefly: elastix requires the `LD_LIBRARY_PATH` (Linux) or `LDY_LIBRARY_PATH`\n    (OSX) environment variables to (also) point to the directory with the\n    elastix `lib` directory. For reasons unknown to me, these variables do not\n    make it into the Python session. Hence, we have to set them here explicitly.\n\n    Above info is based on: https://github.com/jasper-tms/pytransformix\n\n    \"\"\"\n    # Don't do anything if no elastixbin\n    if not _elastixbin:\n        return\n\n    # Check if this variable already exists\n    var = os.environ.get(\"LD_LIBRARY_PATH\", os.environ.get(\"LDY_LIBRARY_PATH\", \"\"))\n\n    # Get the actual path\n    path = (_elastixbin.parent / \"lib\").absolute()\n\n    if str(path) not in var:\n        var = f\"{path}{os.pathsep}{var}\" if var else str(path)\n\n    # Note that `LD_LIBRARY_PATH` works for both Linux and OSX\n    os.environ[\"LD_LIBRARY_PATH\"] = var\n    # As per navis/issues/112\n    os.environ[\"DYLD_LIBRARY_PATH\"] = var\n</code></pre>"},{"location":"reference/navis/transforms/factory/","title":"factory","text":""},{"location":"reference/navis/transforms/factory/#navis.transforms.factory.parse_json","title":"<code>navis.transforms.factory.parse_json</code>","text":"<p>Parse json-encoded transform (experimental).</p> PARAMETER DESCRIPTION <code>filepath</code> <pre><code>        Filepath to json file to generate transform from.\n</code></pre> <p> TYPE: <code>     str</code> </p> <code>**kwargs</code> <pre><code>        Keyword arguments passed to the respective transform class.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>transform</code> Source code in <code>navis/transforms/factory.py</code> <pre><code>def parse_json(filepath: str, **kwargs):\n    \"\"\"Parse json-encoded transform (experimental).\n\n    Parameters\n    ----------\n    filepath :      str\n                    Filepath to json file to generate transform from.\n    **kwargs\n                    Keyword arguments passed to the respective transform class.\n\n    Returns\n    -------\n    transform\n\n    \"\"\"\n    fp = pathlib.Path(filepath)\n\n    with open(fp, 'r') as f:\n        data = json.load(f)\n\n    if not isinstance(data, list):\n        data = []\n\n    transforms = []\n    for reg in data:\n        if not isinstance(reg, type(dict)):\n            raise TypeError(f'{filepath} expected data as dict or list of '\n                            f'dicts, got \"{type(reg)}\"')\n\n        if reg.get('type', None) == 'tpsreg':\n            transforms.append(TPStransform(reg['refmat'].values,\n                                           reg['tarmat'].values, **kwargs))\n        elif reg.get('type', None) == 'affine':\n            transforms.append(AffineTransform(reg['affine_matrix'], **kwargs))\n        else:\n            raise TypeError(f'{reg} has unknown \"type\"')\n\n    return TransformSequence(*transforms)\n</code></pre>"},{"location":"reference/navis/transforms/factory/#navis.transforms.factory.transform_factory","title":"<code>navis.transforms.factory.transform_factory</code>","text":"<p>Generate appropriate transform from file.</p> PARAMETER DESCRIPTION <code>filepath</code> <pre><code>        Filepath to generate transform from.\n</code></pre> <p> TYPE: <code>     str</code> </p> <code>**kwargs</code> <pre><code>        Keyword arguments passed to the respective transform class.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>transform</code> Source code in <code>navis/transforms/factory.py</code> <pre><code>def transform_factory(filepath: str, **kwargs):\n    \"\"\"Generate appropriate transform from file.\n\n    Parameters\n    ----------\n    filepath :      str\n                    Filepath to generate transform from.\n    **kwargs\n                    Keyword arguments passed to the respective transform class.\n\n    Returns\n    -------\n    transform\n\n    \"\"\"\n    fp = pathlib.Path(filepath)\n\n    # Check if file/path exists\n    if not fp.is_dir() or not fp.is_file():\n        raise ValueError(f'{fp} does not appear to exist')\n\n    if fp.endswith('.list'):\n        return CMTKtransform(fp, **kwargs)\n\n    if fp.endswith('.h5'):\n        return H5transform(fp, **kwargs)\n\n    # Custom transforms\n    if fp.endswith('.json'):\n        return parse_json(fp, **kwargs)\n\n    raise TypeError(f'Unknown transform format for {filepath}')\n</code></pre>"},{"location":"reference/navis/transforms/h5reg/","title":"h5reg","text":""},{"location":"reference/navis/transforms/h5reg/#navis.transforms.h5reg.read_points_threaded","title":"<code>navis.transforms.h5reg.read_points_threaded</code>","text":"<p>Some tinkering with using multiple processes to read voxels.</p> Source code in <code>navis/transforms/h5reg.py</code> <pre><code>def read_points_threaded(voxels, filepath, level, dir, threads=5):\n    \"\"\"Some tinkering with using multiple processes to read voxels.\"\"\"\n    splits = np.array_split(voxels, threads)\n    with concurrent.futures.ProcessPoolExecutor(max_workers=threads) as executor:\n        chunks = [(arr, filepath, level, dir) for arr in splits]\n        futures = executor.map(_read_points, chunks)\n        offset = np.vstack(list(futures))\n\n    return offset\n</code></pre>"},{"location":"reference/navis/transforms/templates/","title":"templates","text":""},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateBrain","title":"<code>navis.transforms.templates.TemplateBrain</code>","text":"<p>Generic base class for template brains.</p> <p>Minimally, a template should have a <code>name</code> and <code>label</code> property. For mirroring, it also needs a <code>boundingbox</code>.</p> <p>See flybrains for an example of how to use template brains.</p> Source code in <code>navis/transforms/templates.py</code> <pre><code>class TemplateBrain:\n    \"\"\"Generic base class for template brains.\n\n    Minimally, a template should have a `name` and `label` property. For\n    mirroring, it also needs a `boundingbox`.\n\n    See [flybrains](https://github.com/navis-org/navis-flybrains) for\n    an example of how to use template brains.\n\n    \"\"\"\n\n    def __init__(self, **properties):\n        \"\"\"Initialize class.\"\"\"\n        for k, v in properties.items():\n            setattr(self, k, v)\n\n    @property\n    def mesh(self):\n        \"\"\"Mesh represenation of this brain.\"\"\"\n        if not hasattr(self, \"_mesh\"):\n            name = getattr(self, \"regName\", getattr(self, \"name\", None))\n            raise ValueError(f\"{name} does not appear to have a mesh\")\n        return self._mesh\n</code></pre>"},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateBrain.mesh","title":"<code>mesh</code>  <code>property</code>","text":"<p>Mesh represenation of this brain.</p>"},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateBrain.__init__","title":"<code>__init__</code>","text":"<p>Initialize class.</p> Source code in <code>navis/transforms/templates.py</code> <pre><code>def __init__(self, **properties):\n    \"\"\"Initialize class.\"\"\"\n    for k, v in properties.items():\n        setattr(self, k, v)\n</code></pre>"},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateRegistry","title":"<code>navis.transforms.templates.TemplateRegistry</code>","text":"<p>Tracks template brains, available transforms and produces bridging sequences.</p> PARAMETER DESCRIPTION <code>scan_paths</code> <pre><code>        If True will scan paths on initialization.\n</code></pre> <p> TYPE: <code>   bool</code> DEFAULT: <code>True</code> </p> Source code in <code>navis/transforms/templates.py</code> <pre><code>class TemplateRegistry:\n    \"\"\"Tracks template brains, available transforms and produces bridging sequences.\n\n    Parameters\n    ----------\n    scan_paths :    bool\n                    If True will scan paths on initialization.\n\n    \"\"\"\n\n    def __init__(self, scan_paths: bool = True):\n        # Paths to scan for transforms\n        self._transpaths = _OS_TRANSPATHS.copy()\n        # Transforms\n        self._transforms = []\n        # Template brains\n        self._templates = []\n\n        if scan_paths:\n            self.scan_paths()\n\n    def __contains__(self, other) -&gt; bool:\n        \"\"\"Check if transform is in registry.\n\n        Parameters\n        ----------\n        other :     transform, filepath, tuple\n                    Either a transform (e.g. CMTKtransform), a filepath (e.g.\n                    to a .list file) or a tuple of `(source, target, transform)`\n                    where `transform` can be a transform or a filepath.\n\n        \"\"\"\n        if isinstance(other, (tuple, list)):\n            return any([t == other for t in self.transforms])\n        else:\n            return other in [t.transform for t in self.transforms]\n\n    def __len__(self) -&gt; int:\n        return len(self.transforms)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return f\"TemplateRegistry with {len(self)} transforms\"\n\n    @property\n    def transpaths(self) -&gt; list:\n        \"\"\"Paths searched for transforms.\n\n        Use `.scan_paths` to trigger a scan. Use `.register_path` to add\n        more path(s).\n        \"\"\"\n        return self._transpaths\n\n    @property\n    def templates(self) -&gt; list:\n        \"\"\"Registered template (brains).\"\"\"\n        return self._templates\n\n    @property\n    def transforms(self) -&gt; list:\n        \"\"\"Registered transforms (bridging + mirror).\"\"\"\n        return self._transforms\n\n    @property\n    def bridges(self) -&gt; list:\n        \"\"\"Registered bridging transforms.\"\"\"\n        return [t for t in self.transforms if t.type == \"bridging\"]\n\n    @property\n    def mirrors(self) -&gt; list:\n        \"\"\"Registered mirror transforms.\"\"\"\n        return [t for t in self.transforms if t.type == \"mirror\"]\n\n    def clear_caches(self):\n        \"\"\"Clear caches of all cached functions.\"\"\"\n        self.bridging_graph.cache_clear()\n        self.shortest_bridging_seq.cache_clear()\n\n    def summary(self) -&gt; pd.DataFrame:\n        \"\"\"Generate summary of available transforms.\"\"\"\n        return pd.DataFrame(self.transforms)\n\n    def register_path(self, paths: str, trigger_scan: bool = True):\n        \"\"\"Register path(s) to scan for transforms.\n\n        Parameters\n        ----------\n        paths :         str | list thereof\n                        Paths (or list thereof) to scans for transforms. This\n                        is not permanent. For permanent additions set path(s)\n                        via the `NAVIS_TRANSFORMS` environment variable.\n        trigger_scan :  bool\n                        If True, a re-scan of all paths will be triggered.\n\n        \"\"\"\n        paths = utils.make_iterable(paths)\n\n        for p in paths:\n            # Try not to duplicate paths\n            if p not in self.transpaths:\n                self._transpaths.append(p)\n\n        if trigger_scan:\n            self.scan_paths()\n\n    def register_templatebrain(self, template: \"TemplateBrain\", skip_existing=True):\n        \"\"\"Register a template brain.\n\n        This is used, for example, by navis.mirror_brain.\n\n        Parameters\n        ----------\n        template :      TemplateBrain\n                        TemplateBrain to register.\n        skip_existing : bool\n                        If True, will skip existing template brains.\n\n        \"\"\"\n        utils.eval_param(template, name=\"template\", allowed_types=(TemplateBrain,))\n\n        if template not in self._templates or not skip_existing:\n            self._templates.append(template)\n\n    def register_transform(\n        self,\n        transform: BaseTransform,\n        source: str,\n        target: str,\n        transform_type: str,\n        skip_existing: bool = True,\n        weight: int = 1,\n        weight_inv: Optional[int] = None,\n    ):\n        \"\"\"Register a transform.\n\n        Parameters\n        ----------\n        transform :         subclass of BaseTransform | TransformSequence\n                            A transform (AffineTransform, CMTKtransform, etc.)\n                            or a TransformSequence.\n        source :            str\n                            Source for forward transform.\n        target :            str\n                            Target for forward transform. Ignored for mirror\n                            transforms.\n        transform_type :    \"bridging\" | \"mirror\"\n                            Type of transform.\n        skip_existing :     bool\n                            If True will skip if transform is already in registry.\n        weight :            int\n                            Giving a transform a lower weight will make it\n                            preferable when plotting bridging sequences.\n        weight_inv :        int, optional\n                            Weight for inverse transform. If not given, will be\n                            set to same as `weight`.\n\n\n        See Also\n        --------\n        register_transformfile\n                            If you want to register a file instead of an\n                            already constructed transform.\n\n        \"\"\"\n        assert transform_type in (\"bridging\", \"mirror\")\n        assert isinstance(transform, (BaseTransform, TransformSequence))\n\n        # Translate into edge\n        edge = transform_reg(\n            source=source,\n            target=target,\n            transform=transform,\n            type=transform_type,\n            invertible=hasattr(transform, \"__neg__\"),\n            weight=weight,\n            weight_inv=weight_inv if weight_inv is not None else weight,\n        )\n\n        # Don't add if already exists\n        if not skip_existing or edge not in self:\n            self.transforms.append(edge)\n\n        # Clear cached functions\n        self.clear_caches()\n\n    def register_transformfile(self, path: str, **kwargs):\n        \"\"\"Parse and register a transform file.\n\n        File/Directory name must follow the a `{TARGET}_{SOURCE}.{ext}`\n        convention (e.g. `JRC2013_FCWB.list`).\n\n        Parameters\n        ----------\n        path :          str\n                        Path to transform.\n        **kwargs\n                        Keyword arguments are passed to the constructor of the\n                        Transform (e.g. CMTKtransform for `.list` directory).\n\n        See Also\n        --------\n        register_transform\n                        If you want to register an already constructed transform\n                        instead of a transform file that still needs to be\n                        parsed.\n\n        \"\"\"\n        assert isinstance(path, (str, pathlib.Path))\n\n        path = pathlib.Path(path).expanduser()\n\n        if not path.is_dir() and not path.is_file():\n            raise ValueError(f'File/directory \"{path}\" does not exist')\n\n        # Parse properties\n        try:\n            if \"mirror\" in path.name or \"imgflip\" in path.name:\n                transform_type = \"mirror\"\n                source = path.name.split(\"_\")[0]\n                target = None\n            else:\n                transform_type = \"bridging\"\n                target = path.name.split(\"_\")[0]\n                source = path.name.split(\"_\")[1].split(\".\")[0]\n\n            # Initialize the transform\n            transform = factory.factory_methods[path.suffix](path, **kwargs)\n\n            self.register_transform(\n                transform=transform,\n                source=source,\n                target=target,\n                transform_type=transform_type,\n            )\n        except BaseException as e:\n            logger.error(f\"Error registering {path} as transform: {str(e)}\")\n\n    def scan_paths(self, extra_paths: List[str] = None):\n        \"\"\"Scan registered paths for transforms and add to registry.\n\n        Will skip transforms that already exist in this registry.\n\n        Parameters\n        ----------\n        extra_paths :   list of str\n                        Any Extra paths to search.\n\n        \"\"\"\n        search_paths = self.transpaths\n\n        if isinstance(extra_paths, str):\n            extra_paths = [i for i in extra_paths.split(\";\") if len(i) &gt; 0]\n            search_paths = np.append(search_paths, extra_paths)\n\n        for path in search_paths:\n            path = pathlib.Path(path).expanduser()\n            # Skip if path does not exist\n            if not path.is_dir():\n                continue\n\n            # Go over the file extensions we can work with (.h5, .list, .json)\n            # These file extensions are registered in the\n            # `navis.transforms.factory` module\n            for ext in factory.factory_methods:\n                for hit in path.rglob(f\"*{ext}\"):\n                    if hit.is_dir() or hit.is_file():\n                        # Register this file\n                        self.register_transformfile(hit)\n\n        # Clear cached functions\n        self.clear_caches()\n\n    @functools.lru_cache()\n    def bridging_graph(\n        self, reciprocal: Union[Literal[False], int, float] = True\n    ) -&gt; nx.DiGraph:\n        \"\"\"Generate networkx Graph describing the bridging paths.\n\n        Parameters\n        ----------\n        reciprocal :    bool | float\n                        If True or float, will add forward and inverse edges for\n                        transforms that are invertible. If float, the inverse\n                        edges' weights will be scaled by that factor. Can\n                        be used to generally prefer forward transforms over inverse ones.\n\n        Returns\n        -------\n        networkx.MultiDiGraph\n\n        \"\"\"\n        # Drop mirror transforms\n        bridge = [t for t in self.transforms if t.type == \"bridging\"]\n        bridge_inv = [t for t in bridge if t.invertible]\n\n        # Generate graph\n        # Note we are using MultiDi graph here because we might\n        # have multiple edges between nodes. For example, there\n        # is a JFRC2013DS_JFRC2013 and a JFRC2013_JFRC2013DS\n        # bridging registration. If we include the inverse, there\n        # will be two edges connecting JFRC2013DS and JFRC2013 in\n        # both directions\n        G = nx.MultiDiGraph()\n        edges = [\n            (\n                t.source,\n                t.target,\n                {\n                    \"transform\": t.transform,\n                    \"type\": type(t.transform).__name__,\n                    \"weight\": t.weight,\n                },\n            )\n            for t in bridge\n        ]\n\n        if reciprocal:\n            if isinstance(reciprocal, numbers.Number):\n                rv_edges = [\n                    (\n                        t.target,\n                        t.source,\n                        {\n                            \"transform\": -t.transform,  # note inverse transform!\n                            \"type\": str(type(t.transform)).split(\".\")[-1],\n                            \"weight\": t.weight_inv * reciprocal,\n                        },\n                    )\n                    for t in bridge_inv\n                ]\n            else:\n                rv_edges = [\n                    (\n                        t.target,\n                        t.source,\n                        {\n                            \"transform\": -t.transform,  # note inverse transform!\n                            \"type\": str(type(t.transform)).split(\".\")[-1],\n                            \"weight\": t.weight_inv,\n                        },\n                    )\n                    for t in bridge_inv\n                ]\n            edges += rv_edges\n\n        G.add_edges_from(edges)\n\n        return G\n\n    def find_bridging_path(\n        self,\n        source: str,\n        target: str,\n        via: Optional[str] = None,\n        avoid: Optional[str] = None,\n        reciprocal=True,\n    ) -&gt; tuple:\n        \"\"\"Find bridging path from source to target.\n\n        Parameters\n        ----------\n        source :        str\n                        Source from which to transform to `target`.\n        target :        str\n                        Target to which to transform to.\n        via :           str | list thereof, optional\n                        Force specific intermediate template(s).\n        avoid :         str | list thereof, optional\n                        Avoid going through specific intermediate template(s).\n        reciprocal :    bool | float\n                        If True or float, will add forward and inverse edges for\n                        transforms that are invertible. If float, the inverse\n                        edges' weights will be scaled by that factor.\n\n        Returns\n        -------\n        path :          list\n                        Path from source to target: [source, ..., target]\n        transforms :    list\n                        Transforms as [[path_to_transform, inverse], ...]\n\n        \"\"\"\n        # Generate (or get cached) bridging graph\n        G = self.bridging_graph(reciprocal=reciprocal)\n\n        if len(G) == 0:\n            raise ValueError(\"No bridging registrations available\")\n\n        # Do not remove the conversion to list - fuzzy matching does act up\n        # otherwise\n        nodes = list(G.nodes)\n        if source not in nodes:\n            best_match = fw.process.extractOne(\n                source, nodes, scorer=fw.fuzz.token_sort_ratio\n            )\n            raise ValueError(\n                f'Source \"{source}\" has no known bridging '\n                f'registrations. Did you mean \"{best_match[0]}\" '\n                \"instead?\"\n            )\n        if target not in G.nodes:\n            best_match = fw.process.extractOne(\n                target, nodes, scorer=fw.fuzz.token_sort_ratio\n            )\n            raise ValueError(\n                f'Target \"{target}\" has no known bridging '\n                f'registrations. Did you mean \"{best_match[0]}\" '\n                \"instead?\"\n            )\n\n        if via:\n            via = list(utils.make_iterable(via))  # do not remove the list() here\n            for v in via:\n                if v not in G.nodes:\n                    best_match = fw.process.extractOne(\n                        v, nodes, scorer=fw.fuzz.token_sort_ratio\n                    )\n                    raise ValueError(\n                        f'Via \"{v}\" has no known bridging '\n                        f'registrations. Did you mean \"{best_match[0]}\" '\n                        \"instead?\"\n                    )\n\n        if avoid:\n            avoid = list(utils.make_iterable(avoid))\n\n        # This will raise a error message if no path is found\n        if not via and not avoid:\n            try:\n                path = nx.shortest_path(G, source, target, weight=\"weight\")\n            except nx.NetworkXNoPath:\n                raise nx.NetworkXNoPath(\n                    f\"No bridging path connecting {source} and {target} found.\"\n                )\n        else:\n            # Go through all possible paths and find one that...\n            found_any = False  # track if we found any path\n            found_good = False  # track if we found a path matching the criteria\n            for path in nx.all_simple_paths(G, source, target):\n                found_any = True\n                # ... has all `via`s...\n                if via and all([v in path for v in via]):\n                    # ... and none of the `avoid`\n                    if avoid:\n                        if not any([v in path for v in avoid]):\n                            found_good = True\n                            break\n                    else:\n                        found_good = True\n                        break\n                # If we only have `avoid` but no `via`\n                elif avoid and not any([v in path for v in avoid]):\n                    found_good = True\n                    break\n\n            if not found_any:\n                raise nx.NetworkXNoPath(\n                    f\"No bridging path connecting {source} and {target} found.\"\n                )\n            elif not found_good:\n                if via and avoid:\n                    raise nx.NetworkXNoPath(\n                        f\"No bridging path connecting {source}\"\n                        f'and {target} via \"{via}\" and '\n                        f'avoiding \"{avoid}\" found'\n                    )\n                elif via:\n                    raise nx.NetworkXNoPath(\n                        f\"No bridging path connecting {source}\"\n                        f'and {target} via \"{via}\" found.'\n                    )\n                else:\n                    raise nx.NetworkXNoPath(\n                        f\"No bridging path connecting {source}\"\n                        f'and {target} avoiding \"{avoid}\" found.'\n                    )\n\n        # `path` holds the sequence of nodes we are traversing but not which\n        # transforms (i.e. edges) to use\n        transforms = []\n        for n1, n2 in zip(path[:-1], path[1:]):\n            this_edges = []\n            i = 0\n            # First collect all edges between those two nodes\n            # - this is annoyingly complicated with MultiDiGraphs\n            while True:\n                try:\n                    e = G.edges[(n1, n2, i)]\n                except KeyError:\n                    break\n                this_edges.append([e[\"transform\"], e[\"weight\"]])\n                i += 1\n\n            # Now find the edge with the highest weight\n            # (inverse transforms might have a lower weight)\n            this_edges = sorted(this_edges, key=lambda x: x[-1])\n            transforms.append(this_edges[-1][0])\n\n        return path, transforms\n\n    def find_all_bridging_paths(\n        self,\n        source: str,\n        target: str,\n        via: Optional[str] = None,\n        avoid: Optional[str] = None,\n        reciprocal: bool = True,\n        cutoff: int = None,\n    ) -&gt; tuple:\n        \"\"\"Find all bridging paths from source to target.\n\n        Parameters\n        ----------\n        source :        str\n                        Source from which to transform to `target`.\n        target :        str\n                        Target to which to transform to.\n        via :           str | list thereof, optional\n                        Force specific intermediate template(s).\n        avoid :         str | list thereof, optional\n                        Avoid specific intermediate template(s).\n        reciprocal :    bool | float\n                        If True or float, will add forward and inverse edges for\n                        transforms that are invertible. If float, the inverse\n                        edges' weights will be scaled by that factor.\n        cutoff :        int, optional\n                        Depth to stop the search. Only paths of length\n                        &lt;= cutoff are returned.\n\n        Returns\n        -------\n\n        path :          list\n                        Path from source to target: [source, ..., target]\n        transforms :    list\n                        Transforms as [[path_to_transform, inverse], ...]\n\n        \"\"\"\n        # Generate (or get cached) bridging graph\n        G = self.bridging_graph(reciprocal=reciprocal)\n\n        if len(G) == 0:\n            raise ValueError(\"No bridging registrations available\")\n\n        # Do not remove the conversion to list - fuzzy matching does act up\n        # otherwise\n        nodes = list(G.nodes)\n        if source not in nodes:\n            best_match = fw.process.extractOne(\n                source, nodes, scorer=fw.fuzz.token_sort_ratio\n            )\n            raise ValueError(\n                f'Source \"{source}\" has no known bridging '\n                f'registrations. Did you mean \"{best_match[0]}\" '\n                \"instead?\"\n            )\n        if target not in G.nodes:\n            best_match = fw.process.extractOne(\n                target, nodes, scorer=fw.fuzz.token_sort_ratio\n            )\n            raise ValueError(\n                f'Target \"{target}\" has no known bridging '\n                f'registrations. Did you mean \"{best_match[0]}\" '\n                \"instead?\"\n            )\n\n        if via and via not in G.nodes:\n            best_match = fw.process.extractOne(\n                via, nodes, scorer=fw.fuzz.token_sort_ratio\n            )\n            raise ValueError(\n                f'Via \"{via}\" has no known bridging '\n                f'registrations. Did you mean \"{best_match[0]}\" '\n                \"instead?\"\n            )\n\n        # This will raise a error message if no path is found\n        for path in nx.all_simple_paths(G, source, target, cutoff=cutoff):\n            # Skip paths that don't contain `via`\n            if isinstance(via, str) and (via not in path):\n                continue\n            elif isinstance(via, (list, tuple, np.ndarray)) and not all(\n                [v in path for v in via]\n            ):\n                continue\n\n            # Skip paths that contain `avoid`\n            if isinstance(avoid, str) and (avoid in path):\n                continue\n            elif isinstance(avoid, (list, tuple, np.ndarray)) and any(\n                [v in path for v in avoid]\n            ):\n                continue\n\n            # `path` holds the sequence of nodes we are traversing but not which\n            # transforms (i.e. edges) to use\n            transforms = []\n            for n1, n2 in zip(path[:-1], path[1:]):\n                this_edges = []\n                i = 0\n                # First collect all edges between those two nodes\n                # - this is annoyingly complicated with MultiDiGraphs\n                while True:\n                    try:\n                        e = G.edges[(n1, n2, i)]\n                    except KeyError:\n                        break\n                    this_edges.append([e[\"transform\"], e[\"weight\"]])\n                    i += 1\n\n                # Now find the edge with the highest weight\n                # (inverse transforms might have a lower weight)\n                this_edges = sorted(this_edges, key=lambda x: x[-1])\n                transforms.append(this_edges[-1][0])\n\n            yield path, transforms\n\n    @functools.lru_cache()\n    def shortest_bridging_seq(\n        self,\n        source: str,\n        target: str,\n        via: Optional[str] = None,\n        inverse_weight: float = 0.5,\n    ) -&gt; tuple:\n        \"\"\"Find shortest bridging sequence to get from source to target.\n\n        Parameters\n        ----------\n        source :            str\n                            Source from which to transform to `target`.\n        target :            str\n                            Target to which to transform to.\n        via :               str | list of str\n                            Waystations to traverse on the way from source to\n                            target.\n        inverse_weight :    float\n                            Weight for inverse transforms. If &lt; 1 will prefer\n                            forward transforms.\n\n        Returns\n        -------\n        sequence :          (N, ) array\n                            Sequence of registrations that will be traversed.\n        transform_seq :     TransformSequence\n                            Class that collates the required transforms to get\n                            from source to target.\n\n        \"\"\"\n        # Generate sequence of nodes we need to find a path for\n        # Minimally it's just from source to target\n        nodes = np.array([source, target])\n\n        if via:\n            nodes = np.insert(nodes, 1, via)\n\n        seq = [nodes[0]]\n        transforms = []\n        for n1, n2 in zip(nodes[:-1], nodes[1:]):\n            path, tr = self.find_bridging_path(n1, n2, reciprocal=inverse_weight)\n            seq = np.append(seq, path[1:])\n            transforms = np.append(transforms, tr)\n\n        if any(np.unique(seq, return_counts=True)[1] &gt; 1):\n            logger.warning(f\"Bridging sequence contains loop: {'-&gt;'.join(seq)}\")\n\n        # Generate the transform sequence\n        transform_seq = TransformSequence(*transforms)\n\n        return seq, transform_seq\n\n    def find_mirror_reg(self, template: str, non_found: str = \"raise\") -&gt; tuple:\n        \"\"\"Search for a mirror transformation for given template.\n\n        Typically a mirror transformation specifies a non-rigid transformation\n        to correct asymmetries in an image.\n\n        Parameters\n        ----------\n        template :  str\n                    Name of the template to find a mirror transformation for.\n        non_found : \"raise\" | \"ignore\"\n                    What to do if no mirror transformation is found. If \"ignore\"\n                    and no mirror transformation found, will silently return\n                    `None`.\n\n        Returns\n        -------\n        tuple\n                    Named tuple containing a mirror transformation. Will only\n                    ever return one - even if multiple are available.\n\n        \"\"\"\n        for tr in self.mirrors:\n            if tr.source == template:\n                return tr\n\n        if non_found == \"raise\":\n            raise ValueError(f\"No mirror transformation found for {template}\")\n        return None\n\n    def find_closest_mirror_reg(self, template: str, non_found: str = \"raise\") -&gt; str:\n        \"\"\"Search for the closest mirror transformation for given template.\n\n        Typically a mirror transformation specifies a non-rigid transformation\n        to correct asymmetries in an image.\n\n        Parameters\n        ----------\n        template :  str\n                    Name of the template to find a mirror transformation for.\n        non_found : \"raise\" | \"ignore\"\n                    What to do if there is no path to a mirror transformation.\n                    If \"ignore\" and no path is found, will silently return\n                    `None`.\n\n        Returns\n        -------\n        str\n                    Name of the closest template with a mirror transform.\n\n        \"\"\"\n        # Templates with mirror registrations\n        temps_w_mirrors = [t.source for t in self.mirrors]\n\n        # Add symmetrical template brains\n        temps_w_mirrors += [\n            t.label for t in self.templates if getattr(t, \"symmetrical\", False) == True\n        ]\n\n        if not temps_w_mirrors:\n            raise ValueError(\"No mirror transformations registered\")\n\n        # If this template has a mirror registration:\n        if template in temps_w_mirrors:\n            return template\n\n        # Get bridging graph\n        G = self.bridging_graph()\n\n        if template not in G.nodes:\n            raise ValueError(\n                f'\"{template}\" does not appear to be a registered template'\n            )\n\n        # Get path lengths from template to all other nodes\n        pl = nx.single_source_dijkstra_path_length(G, template)\n\n        # Subset to targets that have a mirror reg\n        pl = {k: v for k, v in pl.items() if k in temps_w_mirrors}\n\n        # Find the closest mirror\n        cl = sorted(pl.keys(), key=lambda x: pl[x])\n\n        # If any, return the closests\n        if cl:\n            return cl[0]\n\n        if non_found == \"raise\":\n            raise ValueError(\n                f'No path to a mirror transformation found for \"{template}\"'\n            )\n\n        return None\n\n    def find_template(self, name: str, non_found: str = \"raise\") -&gt; \"TemplateBrain\":\n        \"\"\"Search for a given template (brain).\n\n        Parameters\n        ----------\n        name :      str\n                    Name of the template to find a mirror transformation for.\n                    Searches against `name` and `label` (short name) properties\n                    of registered templates.\n        non_found : \"raise\" | \"ignore\"\n                    What to do if no mirror transformation is found. If \"ignore\"\n                    and no mirror transformation found, will silently return\n                    `None`.\n\n        Returns\n        -------\n        TemplateBrain\n\n        \"\"\"\n        for tmp in self.templates:\n            if getattr(tmp, \"label\", None) == name:\n                return tmp\n            if getattr(tmp, \"name\", None) == name:\n                return tmp\n\n        if non_found == \"raise\":\n            raise ValueError(f'No template brain registered that matches \"{name}\"')\n        return None\n\n    def plot_bridging_graph(self, **kwargs):\n        \"\"\"Draw bridging graph using networkX.\n\n        Parameters\n        ----------\n        **kwargs\n                    Keyword arguments are passed to `networkx.draw_networkx`.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        # Get graph\n        G = self.bridging_graph(reciprocal=False)\n\n        # Draw nodes and edges\n        node_labels = {n: n for n in G.nodes}\n        pos = nx.kamada_kawai_layout(G)\n\n        # Draw all nodes\n        nx.draw_networkx_nodes(\n            G, pos=pos, node_color=\"lightgrey\", node_shape=\"o\", node_size=300\n        )\n        nx.draw_networkx_labels(\n            G, pos=pos, labels=node_labels, font_color=\"k\", font_size=10\n        )\n\n        # Draw edges by type of transform\n        edge_types = set([e[2][\"type\"] for e in G.edges(data=True)])\n\n        lines = []\n        labels = []\n        for t, c in zip(edge_types, sns.color_palette(\"muted\", len(edge_types))):\n            subset = [e for e in G.edges(data=True) if e[2][\"type\"] == t]\n            nx.draw_networkx_edges(\n                G, pos=pos, edgelist=subset, edge_color=mcl.to_hex(c), width=1.5\n            )\n            lines.append(Line2D([0], [0], color=c, linewidth=2, linestyle=\"-\"))\n            labels.append(t)\n\n        plt.legend(lines, labels)\n</code></pre>"},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateRegistry.bridges","title":"<code>bridges: list</code>  <code>property</code>","text":"<p>Registered bridging transforms.</p>"},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateRegistry.mirrors","title":"<code>mirrors: list</code>  <code>property</code>","text":"<p>Registered mirror transforms.</p>"},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateRegistry.templates","title":"<code>templates: list</code>  <code>property</code>","text":"<p>Registered template (brains).</p>"},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateRegistry.transforms","title":"<code>transforms: list</code>  <code>property</code>","text":"<p>Registered transforms (bridging + mirror).</p>"},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateRegistry.transpaths","title":"<code>transpaths: list</code>  <code>property</code>","text":"<p>Paths searched for transforms.</p> <p>Use <code>.scan_paths</code> to trigger a scan. Use <code>.register_path</code> to add more path(s).</p>"},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateRegistry.bridging_graph","title":"<code>bridging_graph</code>  <code>cached</code>","text":"<p>Generate networkx Graph describing the bridging paths.</p> PARAMETER DESCRIPTION <code>reciprocal</code> <pre><code>        If True or float, will add forward and inverse edges for\n        transforms that are invertible. If float, the inverse\n        edges' weights will be scaled by that factor. Can\n        be used to generally prefer forward transforms over inverse ones.\n</code></pre> <p> TYPE: <code>   bool | float</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>networkx.MultiDiGraph</code> Source code in <code>navis/transforms/templates.py</code> <pre><code>@functools.lru_cache()\ndef bridging_graph(\n    self, reciprocal: Union[Literal[False], int, float] = True\n) -&gt; nx.DiGraph:\n    \"\"\"Generate networkx Graph describing the bridging paths.\n\n    Parameters\n    ----------\n    reciprocal :    bool | float\n                    If True or float, will add forward and inverse edges for\n                    transforms that are invertible. If float, the inverse\n                    edges' weights will be scaled by that factor. Can\n                    be used to generally prefer forward transforms over inverse ones.\n\n    Returns\n    -------\n    networkx.MultiDiGraph\n\n    \"\"\"\n    # Drop mirror transforms\n    bridge = [t for t in self.transforms if t.type == \"bridging\"]\n    bridge_inv = [t for t in bridge if t.invertible]\n\n    # Generate graph\n    # Note we are using MultiDi graph here because we might\n    # have multiple edges between nodes. For example, there\n    # is a JFRC2013DS_JFRC2013 and a JFRC2013_JFRC2013DS\n    # bridging registration. If we include the inverse, there\n    # will be two edges connecting JFRC2013DS and JFRC2013 in\n    # both directions\n    G = nx.MultiDiGraph()\n    edges = [\n        (\n            t.source,\n            t.target,\n            {\n                \"transform\": t.transform,\n                \"type\": type(t.transform).__name__,\n                \"weight\": t.weight,\n            },\n        )\n        for t in bridge\n    ]\n\n    if reciprocal:\n        if isinstance(reciprocal, numbers.Number):\n            rv_edges = [\n                (\n                    t.target,\n                    t.source,\n                    {\n                        \"transform\": -t.transform,  # note inverse transform!\n                        \"type\": str(type(t.transform)).split(\".\")[-1],\n                        \"weight\": t.weight_inv * reciprocal,\n                    },\n                )\n                for t in bridge_inv\n            ]\n        else:\n            rv_edges = [\n                (\n                    t.target,\n                    t.source,\n                    {\n                        \"transform\": -t.transform,  # note inverse transform!\n                        \"type\": str(type(t.transform)).split(\".\")[-1],\n                        \"weight\": t.weight_inv,\n                    },\n                )\n                for t in bridge_inv\n            ]\n        edges += rv_edges\n\n    G.add_edges_from(edges)\n\n    return G\n</code></pre>"},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateRegistry.clear_caches","title":"<code>clear_caches</code>","text":"<p>Clear caches of all cached functions.</p> Source code in <code>navis/transforms/templates.py</code> <pre><code>def clear_caches(self):\n    \"\"\"Clear caches of all cached functions.\"\"\"\n    self.bridging_graph.cache_clear()\n    self.shortest_bridging_seq.cache_clear()\n</code></pre>"},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateRegistry.find_all_bridging_paths","title":"<code>find_all_bridging_paths</code>","text":"<p>Find all bridging paths from source to target.</p> PARAMETER DESCRIPTION <code>source</code> <pre><code>        Source from which to transform to `target`.\n</code></pre> <p> TYPE: <code>       str</code> </p> <code>target</code> <pre><code>        Target to which to transform to.\n</code></pre> <p> TYPE: <code>       str</code> </p> <code>via</code> <pre><code>        Force specific intermediate template(s).\n</code></pre> <p> TYPE: <code>          str | list thereof</code> DEFAULT: <code>None</code> </p> <code>avoid</code> <pre><code>        Avoid specific intermediate template(s).\n</code></pre> <p> TYPE: <code>        str | list thereof</code> DEFAULT: <code>None</code> </p> <code>reciprocal</code> <pre><code>        If True or float, will add forward and inverse edges for\n        transforms that are invertible. If float, the inverse\n        edges' weights will be scaled by that factor.\n</code></pre> <p> TYPE: <code>   bool | float</code> DEFAULT: <code>True</code> </p> <code>cutoff</code> <pre><code>        Depth to stop the search. Only paths of length\n        &lt;= cutoff are returned.\n</code></pre> <p> TYPE: <code>       int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>path</code> <p>Path from source to target: [source, ..., target]</p> <p> TYPE: <code>list</code> </p> <code>transforms</code> <p>Transforms as [[path_to_transform, inverse], ...]</p> <p> TYPE: <code>list</code> </p> Source code in <code>navis/transforms/templates.py</code> <pre><code>def find_all_bridging_paths(\n    self,\n    source: str,\n    target: str,\n    via: Optional[str] = None,\n    avoid: Optional[str] = None,\n    reciprocal: bool = True,\n    cutoff: int = None,\n) -&gt; tuple:\n    \"\"\"Find all bridging paths from source to target.\n\n    Parameters\n    ----------\n    source :        str\n                    Source from which to transform to `target`.\n    target :        str\n                    Target to which to transform to.\n    via :           str | list thereof, optional\n                    Force specific intermediate template(s).\n    avoid :         str | list thereof, optional\n                    Avoid specific intermediate template(s).\n    reciprocal :    bool | float\n                    If True or float, will add forward and inverse edges for\n                    transforms that are invertible. If float, the inverse\n                    edges' weights will be scaled by that factor.\n    cutoff :        int, optional\n                    Depth to stop the search. Only paths of length\n                    &lt;= cutoff are returned.\n\n    Returns\n    -------\n\n    path :          list\n                    Path from source to target: [source, ..., target]\n    transforms :    list\n                    Transforms as [[path_to_transform, inverse], ...]\n\n    \"\"\"\n    # Generate (or get cached) bridging graph\n    G = self.bridging_graph(reciprocal=reciprocal)\n\n    if len(G) == 0:\n        raise ValueError(\"No bridging registrations available\")\n\n    # Do not remove the conversion to list - fuzzy matching does act up\n    # otherwise\n    nodes = list(G.nodes)\n    if source not in nodes:\n        best_match = fw.process.extractOne(\n            source, nodes, scorer=fw.fuzz.token_sort_ratio\n        )\n        raise ValueError(\n            f'Source \"{source}\" has no known bridging '\n            f'registrations. Did you mean \"{best_match[0]}\" '\n            \"instead?\"\n        )\n    if target not in G.nodes:\n        best_match = fw.process.extractOne(\n            target, nodes, scorer=fw.fuzz.token_sort_ratio\n        )\n        raise ValueError(\n            f'Target \"{target}\" has no known bridging '\n            f'registrations. Did you mean \"{best_match[0]}\" '\n            \"instead?\"\n        )\n\n    if via and via not in G.nodes:\n        best_match = fw.process.extractOne(\n            via, nodes, scorer=fw.fuzz.token_sort_ratio\n        )\n        raise ValueError(\n            f'Via \"{via}\" has no known bridging '\n            f'registrations. Did you mean \"{best_match[0]}\" '\n            \"instead?\"\n        )\n\n    # This will raise a error message if no path is found\n    for path in nx.all_simple_paths(G, source, target, cutoff=cutoff):\n        # Skip paths that don't contain `via`\n        if isinstance(via, str) and (via not in path):\n            continue\n        elif isinstance(via, (list, tuple, np.ndarray)) and not all(\n            [v in path for v in via]\n        ):\n            continue\n\n        # Skip paths that contain `avoid`\n        if isinstance(avoid, str) and (avoid in path):\n            continue\n        elif isinstance(avoid, (list, tuple, np.ndarray)) and any(\n            [v in path for v in avoid]\n        ):\n            continue\n\n        # `path` holds the sequence of nodes we are traversing but not which\n        # transforms (i.e. edges) to use\n        transforms = []\n        for n1, n2 in zip(path[:-1], path[1:]):\n            this_edges = []\n            i = 0\n            # First collect all edges between those two nodes\n            # - this is annoyingly complicated with MultiDiGraphs\n            while True:\n                try:\n                    e = G.edges[(n1, n2, i)]\n                except KeyError:\n                    break\n                this_edges.append([e[\"transform\"], e[\"weight\"]])\n                i += 1\n\n            # Now find the edge with the highest weight\n            # (inverse transforms might have a lower weight)\n            this_edges = sorted(this_edges, key=lambda x: x[-1])\n            transforms.append(this_edges[-1][0])\n\n        yield path, transforms\n</code></pre>"},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateRegistry.find_bridging_path","title":"<code>find_bridging_path</code>","text":"<p>Find bridging path from source to target.</p> PARAMETER DESCRIPTION <code>source</code> <pre><code>        Source from which to transform to `target`.\n</code></pre> <p> TYPE: <code>       str</code> </p> <code>target</code> <pre><code>        Target to which to transform to.\n</code></pre> <p> TYPE: <code>       str</code> </p> <code>via</code> <pre><code>        Force specific intermediate template(s).\n</code></pre> <p> TYPE: <code>          str | list thereof</code> DEFAULT: <code>None</code> </p> <code>avoid</code> <pre><code>        Avoid going through specific intermediate template(s).\n</code></pre> <p> TYPE: <code>        str | list thereof</code> DEFAULT: <code>None</code> </p> <code>reciprocal</code> <pre><code>        If True or float, will add forward and inverse edges for\n        transforms that are invertible. If float, the inverse\n        edges' weights will be scaled by that factor.\n</code></pre> <p> TYPE: <code>   bool | float</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>path</code> <p>Path from source to target: [source, ..., target]</p> <p> TYPE: <code>list</code> </p> <code>transforms</code> <p>Transforms as [[path_to_transform, inverse], ...]</p> <p> TYPE: <code>list</code> </p> Source code in <code>navis/transforms/templates.py</code> <pre><code>def find_bridging_path(\n    self,\n    source: str,\n    target: str,\n    via: Optional[str] = None,\n    avoid: Optional[str] = None,\n    reciprocal=True,\n) -&gt; tuple:\n    \"\"\"Find bridging path from source to target.\n\n    Parameters\n    ----------\n    source :        str\n                    Source from which to transform to `target`.\n    target :        str\n                    Target to which to transform to.\n    via :           str | list thereof, optional\n                    Force specific intermediate template(s).\n    avoid :         str | list thereof, optional\n                    Avoid going through specific intermediate template(s).\n    reciprocal :    bool | float\n                    If True or float, will add forward and inverse edges for\n                    transforms that are invertible. If float, the inverse\n                    edges' weights will be scaled by that factor.\n\n    Returns\n    -------\n    path :          list\n                    Path from source to target: [source, ..., target]\n    transforms :    list\n                    Transforms as [[path_to_transform, inverse], ...]\n\n    \"\"\"\n    # Generate (or get cached) bridging graph\n    G = self.bridging_graph(reciprocal=reciprocal)\n\n    if len(G) == 0:\n        raise ValueError(\"No bridging registrations available\")\n\n    # Do not remove the conversion to list - fuzzy matching does act up\n    # otherwise\n    nodes = list(G.nodes)\n    if source not in nodes:\n        best_match = fw.process.extractOne(\n            source, nodes, scorer=fw.fuzz.token_sort_ratio\n        )\n        raise ValueError(\n            f'Source \"{source}\" has no known bridging '\n            f'registrations. Did you mean \"{best_match[0]}\" '\n            \"instead?\"\n        )\n    if target not in G.nodes:\n        best_match = fw.process.extractOne(\n            target, nodes, scorer=fw.fuzz.token_sort_ratio\n        )\n        raise ValueError(\n            f'Target \"{target}\" has no known bridging '\n            f'registrations. Did you mean \"{best_match[0]}\" '\n            \"instead?\"\n        )\n\n    if via:\n        via = list(utils.make_iterable(via))  # do not remove the list() here\n        for v in via:\n            if v not in G.nodes:\n                best_match = fw.process.extractOne(\n                    v, nodes, scorer=fw.fuzz.token_sort_ratio\n                )\n                raise ValueError(\n                    f'Via \"{v}\" has no known bridging '\n                    f'registrations. Did you mean \"{best_match[0]}\" '\n                    \"instead?\"\n                )\n\n    if avoid:\n        avoid = list(utils.make_iterable(avoid))\n\n    # This will raise a error message if no path is found\n    if not via and not avoid:\n        try:\n            path = nx.shortest_path(G, source, target, weight=\"weight\")\n        except nx.NetworkXNoPath:\n            raise nx.NetworkXNoPath(\n                f\"No bridging path connecting {source} and {target} found.\"\n            )\n    else:\n        # Go through all possible paths and find one that...\n        found_any = False  # track if we found any path\n        found_good = False  # track if we found a path matching the criteria\n        for path in nx.all_simple_paths(G, source, target):\n            found_any = True\n            # ... has all `via`s...\n            if via and all([v in path for v in via]):\n                # ... and none of the `avoid`\n                if avoid:\n                    if not any([v in path for v in avoid]):\n                        found_good = True\n                        break\n                else:\n                    found_good = True\n                    break\n            # If we only have `avoid` but no `via`\n            elif avoid and not any([v in path for v in avoid]):\n                found_good = True\n                break\n\n        if not found_any:\n            raise nx.NetworkXNoPath(\n                f\"No bridging path connecting {source} and {target} found.\"\n            )\n        elif not found_good:\n            if via and avoid:\n                raise nx.NetworkXNoPath(\n                    f\"No bridging path connecting {source}\"\n                    f'and {target} via \"{via}\" and '\n                    f'avoiding \"{avoid}\" found'\n                )\n            elif via:\n                raise nx.NetworkXNoPath(\n                    f\"No bridging path connecting {source}\"\n                    f'and {target} via \"{via}\" found.'\n                )\n            else:\n                raise nx.NetworkXNoPath(\n                    f\"No bridging path connecting {source}\"\n                    f'and {target} avoiding \"{avoid}\" found.'\n                )\n\n    # `path` holds the sequence of nodes we are traversing but not which\n    # transforms (i.e. edges) to use\n    transforms = []\n    for n1, n2 in zip(path[:-1], path[1:]):\n        this_edges = []\n        i = 0\n        # First collect all edges between those two nodes\n        # - this is annoyingly complicated with MultiDiGraphs\n        while True:\n            try:\n                e = G.edges[(n1, n2, i)]\n            except KeyError:\n                break\n            this_edges.append([e[\"transform\"], e[\"weight\"]])\n            i += 1\n\n        # Now find the edge with the highest weight\n        # (inverse transforms might have a lower weight)\n        this_edges = sorted(this_edges, key=lambda x: x[-1])\n        transforms.append(this_edges[-1][0])\n\n    return path, transforms\n</code></pre>"},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateRegistry.find_closest_mirror_reg","title":"<code>find_closest_mirror_reg</code>","text":"<p>Search for the closest mirror transformation for given template.</p> <p>Typically a mirror transformation specifies a non-rigid transformation to correct asymmetries in an image.</p> PARAMETER DESCRIPTION <code>template</code> <pre><code>    Name of the template to find a mirror transformation for.\n</code></pre> <p> TYPE: <code> str</code> </p> <code>non_found</code> <pre><code>    What to do if there is no path to a mirror transformation.\n    If \"ignore\" and no path is found, will silently return\n    `None`.\n</code></pre> <p> TYPE: <code>'raise' | ignore</code> DEFAULT: <code>'raise'</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Name of the closest template with a mirror transform.</p> Source code in <code>navis/transforms/templates.py</code> <pre><code>def find_closest_mirror_reg(self, template: str, non_found: str = \"raise\") -&gt; str:\n    \"\"\"Search for the closest mirror transformation for given template.\n\n    Typically a mirror transformation specifies a non-rigid transformation\n    to correct asymmetries in an image.\n\n    Parameters\n    ----------\n    template :  str\n                Name of the template to find a mirror transformation for.\n    non_found : \"raise\" | \"ignore\"\n                What to do if there is no path to a mirror transformation.\n                If \"ignore\" and no path is found, will silently return\n                `None`.\n\n    Returns\n    -------\n    str\n                Name of the closest template with a mirror transform.\n\n    \"\"\"\n    # Templates with mirror registrations\n    temps_w_mirrors = [t.source for t in self.mirrors]\n\n    # Add symmetrical template brains\n    temps_w_mirrors += [\n        t.label for t in self.templates if getattr(t, \"symmetrical\", False) == True\n    ]\n\n    if not temps_w_mirrors:\n        raise ValueError(\"No mirror transformations registered\")\n\n    # If this template has a mirror registration:\n    if template in temps_w_mirrors:\n        return template\n\n    # Get bridging graph\n    G = self.bridging_graph()\n\n    if template not in G.nodes:\n        raise ValueError(\n            f'\"{template}\" does not appear to be a registered template'\n        )\n\n    # Get path lengths from template to all other nodes\n    pl = nx.single_source_dijkstra_path_length(G, template)\n\n    # Subset to targets that have a mirror reg\n    pl = {k: v for k, v in pl.items() if k in temps_w_mirrors}\n\n    # Find the closest mirror\n    cl = sorted(pl.keys(), key=lambda x: pl[x])\n\n    # If any, return the closests\n    if cl:\n        return cl[0]\n\n    if non_found == \"raise\":\n        raise ValueError(\n            f'No path to a mirror transformation found for \"{template}\"'\n        )\n\n    return None\n</code></pre>"},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateRegistry.find_mirror_reg","title":"<code>find_mirror_reg</code>","text":"<p>Search for a mirror transformation for given template.</p> <p>Typically a mirror transformation specifies a non-rigid transformation to correct asymmetries in an image.</p> PARAMETER DESCRIPTION <code>template</code> <pre><code>    Name of the template to find a mirror transformation for.\n</code></pre> <p> TYPE: <code> str</code> </p> <code>non_found</code> <pre><code>    What to do if no mirror transformation is found. If \"ignore\"\n    and no mirror transformation found, will silently return\n    `None`.\n</code></pre> <p> TYPE: <code>'raise' | ignore</code> DEFAULT: <code>'raise'</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>Named tuple containing a mirror transformation. Will only ever return one - even if multiple are available.</p> Source code in <code>navis/transforms/templates.py</code> <pre><code>def find_mirror_reg(self, template: str, non_found: str = \"raise\") -&gt; tuple:\n    \"\"\"Search for a mirror transformation for given template.\n\n    Typically a mirror transformation specifies a non-rigid transformation\n    to correct asymmetries in an image.\n\n    Parameters\n    ----------\n    template :  str\n                Name of the template to find a mirror transformation for.\n    non_found : \"raise\" | \"ignore\"\n                What to do if no mirror transformation is found. If \"ignore\"\n                and no mirror transformation found, will silently return\n                `None`.\n\n    Returns\n    -------\n    tuple\n                Named tuple containing a mirror transformation. Will only\n                ever return one - even if multiple are available.\n\n    \"\"\"\n    for tr in self.mirrors:\n        if tr.source == template:\n            return tr\n\n    if non_found == \"raise\":\n        raise ValueError(f\"No mirror transformation found for {template}\")\n    return None\n</code></pre>"},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateRegistry.find_template","title":"<code>find_template</code>","text":"<p>Search for a given template (brain).</p> PARAMETER DESCRIPTION <code>name</code> <pre><code>    Name of the template to find a mirror transformation for.\n    Searches against `name` and `label` (short name) properties\n    of registered templates.\n</code></pre> <p> TYPE: <code>     str</code> </p> <code>non_found</code> <pre><code>    What to do if no mirror transformation is found. If \"ignore\"\n    and no mirror transformation found, will silently return\n    `None`.\n</code></pre> <p> TYPE: <code>'raise' | ignore</code> DEFAULT: <code>'raise'</code> </p> RETURNS DESCRIPTION <code>TemplateBrain</code> Source code in <code>navis/transforms/templates.py</code> <pre><code>def find_template(self, name: str, non_found: str = \"raise\") -&gt; \"TemplateBrain\":\n    \"\"\"Search for a given template (brain).\n\n    Parameters\n    ----------\n    name :      str\n                Name of the template to find a mirror transformation for.\n                Searches against `name` and `label` (short name) properties\n                of registered templates.\n    non_found : \"raise\" | \"ignore\"\n                What to do if no mirror transformation is found. If \"ignore\"\n                and no mirror transformation found, will silently return\n                `None`.\n\n    Returns\n    -------\n    TemplateBrain\n\n    \"\"\"\n    for tmp in self.templates:\n        if getattr(tmp, \"label\", None) == name:\n            return tmp\n        if getattr(tmp, \"name\", None) == name:\n            return tmp\n\n    if non_found == \"raise\":\n        raise ValueError(f'No template brain registered that matches \"{name}\"')\n    return None\n</code></pre>"},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateRegistry.plot_bridging_graph","title":"<code>plot_bridging_graph</code>","text":"<p>Draw bridging graph using networkX.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <pre><code>    Keyword arguments are passed to `networkx.draw_networkx`.\n</code></pre> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>None</code> Source code in <code>navis/transforms/templates.py</code> <pre><code>def plot_bridging_graph(self, **kwargs):\n    \"\"\"Draw bridging graph using networkX.\n\n    Parameters\n    ----------\n    **kwargs\n                Keyword arguments are passed to `networkx.draw_networkx`.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    # Get graph\n    G = self.bridging_graph(reciprocal=False)\n\n    # Draw nodes and edges\n    node_labels = {n: n for n in G.nodes}\n    pos = nx.kamada_kawai_layout(G)\n\n    # Draw all nodes\n    nx.draw_networkx_nodes(\n        G, pos=pos, node_color=\"lightgrey\", node_shape=\"o\", node_size=300\n    )\n    nx.draw_networkx_labels(\n        G, pos=pos, labels=node_labels, font_color=\"k\", font_size=10\n    )\n\n    # Draw edges by type of transform\n    edge_types = set([e[2][\"type\"] for e in G.edges(data=True)])\n\n    lines = []\n    labels = []\n    for t, c in zip(edge_types, sns.color_palette(\"muted\", len(edge_types))):\n        subset = [e for e in G.edges(data=True) if e[2][\"type\"] == t]\n        nx.draw_networkx_edges(\n            G, pos=pos, edgelist=subset, edge_color=mcl.to_hex(c), width=1.5\n        )\n        lines.append(Line2D([0], [0], color=c, linewidth=2, linestyle=\"-\"))\n        labels.append(t)\n\n    plt.legend(lines, labels)\n</code></pre>"},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateRegistry.register_path","title":"<code>register_path</code>","text":"<p>Register path(s) to scan for transforms.</p> PARAMETER DESCRIPTION <code>paths</code> <pre><code>        Paths (or list thereof) to scans for transforms. This\n        is not permanent. For permanent additions set path(s)\n        via the `NAVIS_TRANSFORMS` environment variable.\n</code></pre> <p> TYPE: <code>        str | list thereof</code> </p> <code>trigger_scan</code> <pre><code>        If True, a re-scan of all paths will be triggered.\n</code></pre> <p> TYPE: <code> bool</code> DEFAULT: <code>True</code> </p> Source code in <code>navis/transforms/templates.py</code> <pre><code>def register_path(self, paths: str, trigger_scan: bool = True):\n    \"\"\"Register path(s) to scan for transforms.\n\n    Parameters\n    ----------\n    paths :         str | list thereof\n                    Paths (or list thereof) to scans for transforms. This\n                    is not permanent. For permanent additions set path(s)\n                    via the `NAVIS_TRANSFORMS` environment variable.\n    trigger_scan :  bool\n                    If True, a re-scan of all paths will be triggered.\n\n    \"\"\"\n    paths = utils.make_iterable(paths)\n\n    for p in paths:\n        # Try not to duplicate paths\n        if p not in self.transpaths:\n            self._transpaths.append(p)\n\n    if trigger_scan:\n        self.scan_paths()\n</code></pre>"},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateRegistry.register_templatebrain","title":"<code>register_templatebrain</code>","text":"<p>Register a template brain.</p> <p>This is used, for example, by navis.mirror_brain.</p> PARAMETER DESCRIPTION <code>template</code> <pre><code>        TemplateBrain to register.\n</code></pre> <p> TYPE: <code>     TemplateBrain</code> </p> <code>skip_existing</code> <pre><code>        If True, will skip existing template brains.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>navis/transforms/templates.py</code> <pre><code>def register_templatebrain(self, template: \"TemplateBrain\", skip_existing=True):\n    \"\"\"Register a template brain.\n\n    This is used, for example, by navis.mirror_brain.\n\n    Parameters\n    ----------\n    template :      TemplateBrain\n                    TemplateBrain to register.\n    skip_existing : bool\n                    If True, will skip existing template brains.\n\n    \"\"\"\n    utils.eval_param(template, name=\"template\", allowed_types=(TemplateBrain,))\n\n    if template not in self._templates or not skip_existing:\n        self._templates.append(template)\n</code></pre>"},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateRegistry.register_transform","title":"<code>register_transform</code>","text":"<p>Register a transform.</p> PARAMETER DESCRIPTION <code>transform</code> <pre><code>            A transform (AffineTransform, CMTKtransform, etc.)\n            or a TransformSequence.\n</code></pre> <p> TYPE: <code>        subclass of BaseTransform | TransformSequence</code> </p> <code>source</code> <pre><code>            Source for forward transform.\n</code></pre> <p> TYPE: <code>           str</code> </p> <code>target</code> <pre><code>            Target for forward transform. Ignored for mirror\n            transforms.\n</code></pre> <p> TYPE: <code>           str</code> </p> <code>transform_type</code> <pre><code>            Type of transform.\n</code></pre> <p> TYPE: <code>   \"bridging\" | \"mirror\"</code> </p> <code>skip_existing</code> <pre><code>            If True will skip if transform is already in registry.\n</code></pre> <p> TYPE: <code>    bool</code> DEFAULT: <code>True</code> </p> <code>weight</code> <pre><code>            Giving a transform a lower weight will make it\n            preferable when plotting bridging sequences.\n</code></pre> <p> TYPE: <code>           int</code> DEFAULT: <code>1</code> </p> <code>weight_inv</code> <pre><code>            Weight for inverse transform. If not given, will be\n            set to same as `weight`.\n</code></pre> <p> TYPE: <code>       int</code> DEFAULT: <code>None</code> </p> See Also <p>register_transformfile                     If you want to register a file instead of an                     already constructed transform.</p> Source code in <code>navis/transforms/templates.py</code> <pre><code>def register_transform(\n    self,\n    transform: BaseTransform,\n    source: str,\n    target: str,\n    transform_type: str,\n    skip_existing: bool = True,\n    weight: int = 1,\n    weight_inv: Optional[int] = None,\n):\n    \"\"\"Register a transform.\n\n    Parameters\n    ----------\n    transform :         subclass of BaseTransform | TransformSequence\n                        A transform (AffineTransform, CMTKtransform, etc.)\n                        or a TransformSequence.\n    source :            str\n                        Source for forward transform.\n    target :            str\n                        Target for forward transform. Ignored for mirror\n                        transforms.\n    transform_type :    \"bridging\" | \"mirror\"\n                        Type of transform.\n    skip_existing :     bool\n                        If True will skip if transform is already in registry.\n    weight :            int\n                        Giving a transform a lower weight will make it\n                        preferable when plotting bridging sequences.\n    weight_inv :        int, optional\n                        Weight for inverse transform. If not given, will be\n                        set to same as `weight`.\n\n\n    See Also\n    --------\n    register_transformfile\n                        If you want to register a file instead of an\n                        already constructed transform.\n\n    \"\"\"\n    assert transform_type in (\"bridging\", \"mirror\")\n    assert isinstance(transform, (BaseTransform, TransformSequence))\n\n    # Translate into edge\n    edge = transform_reg(\n        source=source,\n        target=target,\n        transform=transform,\n        type=transform_type,\n        invertible=hasattr(transform, \"__neg__\"),\n        weight=weight,\n        weight_inv=weight_inv if weight_inv is not None else weight,\n    )\n\n    # Don't add if already exists\n    if not skip_existing or edge not in self:\n        self.transforms.append(edge)\n\n    # Clear cached functions\n    self.clear_caches()\n</code></pre>"},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateRegistry.register_transformfile","title":"<code>register_transformfile</code>","text":"<p>Parse and register a transform file.</p> <p>File/Directory name must follow the a <code>{TARGET}_{SOURCE}.{ext}</code> convention (e.g. <code>JRC2013_FCWB.list</code>).</p> PARAMETER DESCRIPTION <code>path</code> <pre><code>        Path to transform.\n</code></pre> <p> TYPE: <code>         str</code> </p> <code>**kwargs</code> <pre><code>        Keyword arguments are passed to the constructor of the\n        Transform (e.g. CMTKtransform for `.list` directory).\n</code></pre> <p> DEFAULT: <code>{}</code> </p> See Also <p>register_transform                 If you want to register an already constructed transform                 instead of a transform file that still needs to be                 parsed.</p> Source code in <code>navis/transforms/templates.py</code> <pre><code>def register_transformfile(self, path: str, **kwargs):\n    \"\"\"Parse and register a transform file.\n\n    File/Directory name must follow the a `{TARGET}_{SOURCE}.{ext}`\n    convention (e.g. `JRC2013_FCWB.list`).\n\n    Parameters\n    ----------\n    path :          str\n                    Path to transform.\n    **kwargs\n                    Keyword arguments are passed to the constructor of the\n                    Transform (e.g. CMTKtransform for `.list` directory).\n\n    See Also\n    --------\n    register_transform\n                    If you want to register an already constructed transform\n                    instead of a transform file that still needs to be\n                    parsed.\n\n    \"\"\"\n    assert isinstance(path, (str, pathlib.Path))\n\n    path = pathlib.Path(path).expanduser()\n\n    if not path.is_dir() and not path.is_file():\n        raise ValueError(f'File/directory \"{path}\" does not exist')\n\n    # Parse properties\n    try:\n        if \"mirror\" in path.name or \"imgflip\" in path.name:\n            transform_type = \"mirror\"\n            source = path.name.split(\"_\")[0]\n            target = None\n        else:\n            transform_type = \"bridging\"\n            target = path.name.split(\"_\")[0]\n            source = path.name.split(\"_\")[1].split(\".\")[0]\n\n        # Initialize the transform\n        transform = factory.factory_methods[path.suffix](path, **kwargs)\n\n        self.register_transform(\n            transform=transform,\n            source=source,\n            target=target,\n            transform_type=transform_type,\n        )\n    except BaseException as e:\n        logger.error(f\"Error registering {path} as transform: {str(e)}\")\n</code></pre>"},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateRegistry.scan_paths","title":"<code>scan_paths</code>","text":"<p>Scan registered paths for transforms and add to registry.</p> <p>Will skip transforms that already exist in this registry.</p> PARAMETER DESCRIPTION <code>extra_paths</code> <pre><code>        Any Extra paths to search.\n</code></pre> <p> TYPE: <code>  list of str</code> DEFAULT: <code>None</code> </p> Source code in <code>navis/transforms/templates.py</code> <pre><code>def scan_paths(self, extra_paths: List[str] = None):\n    \"\"\"Scan registered paths for transforms and add to registry.\n\n    Will skip transforms that already exist in this registry.\n\n    Parameters\n    ----------\n    extra_paths :   list of str\n                    Any Extra paths to search.\n\n    \"\"\"\n    search_paths = self.transpaths\n\n    if isinstance(extra_paths, str):\n        extra_paths = [i for i in extra_paths.split(\";\") if len(i) &gt; 0]\n        search_paths = np.append(search_paths, extra_paths)\n\n    for path in search_paths:\n        path = pathlib.Path(path).expanduser()\n        # Skip if path does not exist\n        if not path.is_dir():\n            continue\n\n        # Go over the file extensions we can work with (.h5, .list, .json)\n        # These file extensions are registered in the\n        # `navis.transforms.factory` module\n        for ext in factory.factory_methods:\n            for hit in path.rglob(f\"*{ext}\"):\n                if hit.is_dir() or hit.is_file():\n                    # Register this file\n                    self.register_transformfile(hit)\n\n    # Clear cached functions\n    self.clear_caches()\n</code></pre>"},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateRegistry.shortest_bridging_seq","title":"<code>shortest_bridging_seq</code>  <code>cached</code>","text":"<p>Find shortest bridging sequence to get from source to target.</p> PARAMETER DESCRIPTION <code>source</code> <pre><code>            Source from which to transform to `target`.\n</code></pre> <p> TYPE: <code>           str</code> </p> <code>target</code> <pre><code>            Target to which to transform to.\n</code></pre> <p> TYPE: <code>           str</code> </p> <code>via</code> <pre><code>            Waystations to traverse on the way from source to\n            target.\n</code></pre> <p> TYPE: <code>              str | list of str</code> DEFAULT: <code>None</code> </p> <code>inverse_weight</code> <pre><code>            Weight for inverse transforms. If &lt; 1 will prefer\n            forward transforms.\n</code></pre> <p> TYPE: <code>   float</code> DEFAULT: <code>0.5</code> </p> RETURNS DESCRIPTION <code>sequence</code> <p>Sequence of registrations that will be traversed.</p> <p> TYPE: <code>(N, ) array</code> </p> <code>transform_seq</code> <p>Class that collates the required transforms to get from source to target.</p> <p> TYPE: <code>TransformSequence</code> </p> Source code in <code>navis/transforms/templates.py</code> <pre><code>@functools.lru_cache()\ndef shortest_bridging_seq(\n    self,\n    source: str,\n    target: str,\n    via: Optional[str] = None,\n    inverse_weight: float = 0.5,\n) -&gt; tuple:\n    \"\"\"Find shortest bridging sequence to get from source to target.\n\n    Parameters\n    ----------\n    source :            str\n                        Source from which to transform to `target`.\n    target :            str\n                        Target to which to transform to.\n    via :               str | list of str\n                        Waystations to traverse on the way from source to\n                        target.\n    inverse_weight :    float\n                        Weight for inverse transforms. If &lt; 1 will prefer\n                        forward transforms.\n\n    Returns\n    -------\n    sequence :          (N, ) array\n                        Sequence of registrations that will be traversed.\n    transform_seq :     TransformSequence\n                        Class that collates the required transforms to get\n                        from source to target.\n\n    \"\"\"\n    # Generate sequence of nodes we need to find a path for\n    # Minimally it's just from source to target\n    nodes = np.array([source, target])\n\n    if via:\n        nodes = np.insert(nodes, 1, via)\n\n    seq = [nodes[0]]\n    transforms = []\n    for n1, n2 in zip(nodes[:-1], nodes[1:]):\n        path, tr = self.find_bridging_path(n1, n2, reciprocal=inverse_weight)\n        seq = np.append(seq, path[1:])\n        transforms = np.append(transforms, tr)\n\n    if any(np.unique(seq, return_counts=True)[1] &gt; 1):\n        logger.warning(f\"Bridging sequence contains loop: {'-&gt;'.join(seq)}\")\n\n    # Generate the transform sequence\n    transform_seq = TransformSequence(*transforms)\n\n    return seq, transform_seq\n</code></pre>"},{"location":"reference/navis/transforms/templates/#navis.transforms.templates.TemplateRegistry.summary","title":"<code>summary</code>","text":"<p>Generate summary of available transforms.</p> Source code in <code>navis/transforms/templates.py</code> <pre><code>def summary(self) -&gt; pd.DataFrame:\n    \"\"\"Generate summary of available transforms.\"\"\"\n    return pd.DataFrame(self.transforms)\n</code></pre>"},{"location":"reference/navis/transforms/thinplate/","title":"thinplate","text":""},{"location":"reference/navis/transforms/thinplate/#navis.transforms.thinplate.distance_matrix","title":"<code>navis.transforms.thinplate.distance_matrix</code>","text":"<p>For (p1,k)-shaped X and (p2,k)-shaped Y, returns the (p1,p2) matrix where the element at [i,j] is the distance between X[i,:] and Y[j,:].</p> <p>This is a re-implementation of a morphops function using scipy to speed things up ~4 orders of magnitude.</p> Source code in <code>navis/transforms/thinplate.py</code> <pre><code>def distance_matrix(X, Y):\n    \"\"\"For (p1,k)-shaped X and (p2,k)-shaped Y, returns the (p1,p2) matrix\n    where the element at [i,j] is the distance between X[i,:] and Y[j,:].\n\n    This is a re-implementation of a morphops function using scipy to speed\n    things up ~4 orders of magnitude.\n    \"\"\"\n    return cdist(X, Y)\n</code></pre>"},{"location":"reference/navis/utils/cv/","title":"cv","text":""},{"location":"reference/navis/utils/cv/#navis.utils.cv.return_navis","title":"<code>navis.utils.cv.return_navis</code>","text":"<p>Wrap cloud-volume mesh and skeleton sources.</p> PARAMETER DESCRIPTION <code>func</code> <pre><code>        Function/method to wrap.\n</code></pre> <p> TYPE: <code>         callable</code> </p> <code>only_on_kwarg</code> <pre><code>        If True, will look for a `as_navis=True` (default=False)\n        keyword argument to determine if results should be converted\n        to navis neurons. If 'process' is set to False, the neuron\n        will not be processed by TriMesh (remove nan, duplicate vertices,etc)\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>navis/utils/cv.py</code> <pre><code>def return_navis(func, only_on_kwarg=False):\n    \"\"\"Wrap cloud-volume mesh and skeleton sources.\n\n    Parameters\n    ----------\n    func :          callable\n                    Function/method to wrap.\n    only_on_kwarg : bool\n                    If True, will look for a `as_navis=True` (default=False)\n                    keyword argument to determine if results should be converted\n                    to navis neurons. If 'process' is set to False, the neuron\n                    will not be processed by TriMesh (remove nan, duplicate vertices,etc)\n\n    \"\"\"\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        ret_navis = kwargs.pop(\"as_navis\", False)\n        process = kwargs.pop(\"process\", False)\n        res = func(*args, **kwargs)\n\n        if not only_on_kwarg or ret_navis:\n            neurons = []\n            if isinstance(res, (list, tuple)):\n                res = {getattr(n, \"id\", uuid.uuid4()): n for n in res}\n            if isinstance(res, (cv.Mesh, cv.Skeleton)):\n                res = {getattr(res, \"id\", uuid.uuid4()): res}\n\n            for k, v in res.items():\n                if isinstance(v, cv.Mesh):\n                    n = core.MeshNeuron(v, id=k, units=\"nm\", process=process)\n                    neurons.append(n)\n                elif isinstance(v, cv.Skeleton):\n                    swc_str = v.to_swc()\n                    n = io.read_swc(swc_str)\n                    n.id = k\n                    n.units = \"nm\"\n                    neurons.append(n)\n                else:\n                    logger.warning(\n                        f\"Skipped {k}: Unable to convert {type(v)} to \" \"navis Neuron.\"\n                    )\n\n            return core.NeuronList(neurons)\n        return res\n\n    return wrapper\n</code></pre>"},{"location":"reference/navis/utils/decorators/","title":"decorators","text":""},{"location":"reference/navis/utils/decorators/#navis.utils.decorators.map_neuronlist_update_docstring","title":"<code>navis.utils.decorators.map_neuronlist_update_docstring</code>","text":"<p>Add additional parameters to docstring of function.</p> Source code in <code>navis/utils/decorators.py</code> <pre><code>def map_neuronlist_update_docstring(func, allow_parallel):\n    \"\"\"Add additional parameters to docstring of function.\"\"\"\n    # Parse docstring\n    lines = func.__doc__.split(\"\\n\")\n\n    # Find a line with a parameter\n    pline = [l for l in lines if \" : \" in l][0]\n    # Get the leading whitespaces\n    wspaces = \" \" * re.search(\"( *)\", pline).end(1)\n    # Get the offset for type and description\n    offset = re.search(\"( *: *)\", pline).end(1) - len(wspaces)\n\n    # Find index of the last parameters (assuming there is a single empty\n    # line between Returns and the last parameter)\n    try:\n        lastp = [\n            i\n            for i, line in enumerate(lines[:-1])\n            if \"Returns\" in line and \"----\" in lines[i + 1]\n        ][0] - 1\n    except IndexError:\n        warnings.warn(f'Could not find \"Returns\" in docstring for function {func}')\n        return func\n\n    msg = \"\"\n    if allow_parallel:\n        msg += dedent(f\"\"\"\\\n        parallel :{\" \" * (offset - 10)}bool\n                  {\" \" * (offset - 10)}If True and input is NeuronList, use parallel\n                  {\" \" * (offset - 10)}processing. Requires `pathos`.\n        n_cores : {\" \" * (offset - 10)}int, optional\n                  {\" \" * (offset - 10)}Numbers of cores to use if `parallel=True`.\n                  {\" \" * (offset - 10)}Defaults to half the available cores.\n        \"\"\")\n\n    msg += dedent(f\"\"\"\\\n    progress :{\" \" * (offset - 10)}bool\n              {\" \" * (offset - 10)}Whether to show a progress bar. Overruled by\n              {\" \" * (offset - 10)}`navis.set_pbars`.\n    omit_failures :{\" \" * (offset - 15)}bool\n                   {\" \" * (offset - 15)}If True will omit failures instead of raising\n                   {\" \" * (offset - 15)}an exception. Ignored if input is single neuron.\n    \"\"\")\n\n    # Insert new docstring\n    lines.insert(lastp, indent(msg, wspaces))\n\n    # Update docstring\n    func.__doc__ = \"\\n\".join(lines)\n\n    return func\n</code></pre>"}]}