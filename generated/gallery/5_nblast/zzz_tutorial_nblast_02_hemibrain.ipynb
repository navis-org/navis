{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nNBLAST using light-level data\n=============================\n\nThis example demonstrates how to use NBLAST to match light-level neurons against EM skeletons.\n\n!!! important \"This example is not executed\"\n    In contrast to almost all other tutorials, this one is not executed when the documentation is built.\n    Consequently, it also does not show any code output or figures. That's because this example requires\n    downloading a large dataset (~1.8GB) and running an NBLAST against it, it is simply not feasible to\n    run this as part of the documentation build process.\n\nOne of the applications of NBLAST is to match neurons across data sets. Here we will illustrate this by taking a light-level,\nconfocal image stack and finding the same neuron in an EM connectome.\n\nSpecifically, we will use an image from Janelia's collection of split-Gal4 driver lines and match it against the neurons in the\n[hemibrain connectome](https://neuprint.janelia.org).\n\nBefore we get started make sure that:\n\n- {{ navis }} is installed and up-to-date\n- [`flybrains`](https://github.com/navis-org/navis-flybrains) is installed and you have downloaded the Saalfeld lab's and VFB bridging\n   transforms (see `flybrains.download_...` functions)\n- download and extract [hemibrain-v1.2-skeletons.tar](https://storage.googleapis.com/hemibrain/v1.2/hemibrain-v1.2-skeletons.tar.gz)\n  (kindly provided by Stuart Berg, Janelia Research Campus)\n\nNext we need to pick an image stack to use as query. You can browse the expression patterns of the Janelia split-Gal4 lines\n[here](https://splitgal4.janelia.org/cgi-bin/splitgal4.cgi). I ended up picking `LH1112` which is a very clean line containing a couple\nof WED projection neurons. Among other data, you can download these stacks as \"gendered\" (i.e. female or male) or \"unisex\" space.\nUnfortunately, all image stacks are in Janelia's `.h5j` format which I haven't figured out how to read straight into Python.\n\nTwo options:\n\n1. Load them into Fiji and save the GFP signal channel as `.nrrd` file.\n2. Go to [VirtualFlyBrain](http://www.virtualflybrain.org/), search for your line of interested LH1112 (not all lines are be available on VFB)\n   and download the \"Signal(NRRD)\" at the bottom of Term Info panel on the right hand side.\n\nI went for option 2 here and downloaded a `VFB_001013cg.nrrd`. This is the neuron we'll be searching for:\n\n![LH1112 z-stack](https://s3.amazonaws.com/janelia-flylight-imagery/Lateral+Horn+2019/LH1112/LH1112-20150313_46_A2-f-20x-brain-Split_GAL4-multichannel_mip.png)\n\nLet's get started!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import navis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we need to load the image stack and turn it into dotprops:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "query = navis.read_nrrd(\"VFB_001013cg.nrrd\", output=\"dotprops\", k=20, threshold=100)\nquery.id = \"LH1112\"  # manually set the ID to the Janelia identifier\nquery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Inspect the results\nnavis.plot3d(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!!! note\n    Depending on your image you will have to play around with the `threshold` parameter to get a decent dotprop representation.\n\nNext we need to load the hemibrain skeletons and convert them to dotprops:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Make sure to adjust the path to where you extracted the skeletons\nsk = navis.read_swc(\n    \"hemibrain-v1.2-skeletons/\", include_subdirs=True, fmt=\"{name,id:int}.swc\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These 97k skeletons include lots of small fragments - there are only ~25k proper neurons or substantial fragments thereof in the hemibrain dataset.\nSo to make our life a little easier, we will keep only the largest 30k skeletons:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sk.sort_values(\"n_nodes\")\nsk = sk[:30_000]\nsk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next up: turning those skeletons into dotprops:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that we are resampling to 1 point for every micron of cable\nBecause these skeletons are in 8nm voxels we have to use 1000/8\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "targets = navis.make_dotprops(sk, k=5, parallel=True, resample=1000 / 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!!! note\n    Making the dotprops may take a while (mostly because of the resampling). You can dedicate more\n    cores via the `n_cores` parameter. It may also make sense to save the dotprops for future\n    use e.g. by pickling them.\n\nLast but not least we need to convert the image's dotprops from their current brain space (`JRC2018U`, `U` for \"unisex\")\nto hemibrain (`JRCFIB2018Fraw`, `raw` for voxels) space.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import flybrains\n\nquery_xf = navis.xform_brain(query, source=\"JRC2018U\", target=\"JRCFIB2018Fraw\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can run the actual NBLAST:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Note that we convert from the JRCFIB2018F voxel (8x8x8nm) space to microns\nscores = navis.nblast(query_xf / 125, targets / 125, scores=\"mean\")\nscores.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!!! note\n    You can greatly speed up NBLAST by installing the optional dependency `pykdtree`:\n    ```shell\n    pip3 install pykdtree\n    ```\n\nNow we can sort those scores to find the top matches:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores.loc[\"LH1112\"].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So we did get a couple of hits here. Let's visualize the top 3:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = navis.plot3d([query_xf, targets.idx[[2030342003, 2214504597, 1069223047]]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On a final note: the scores for those matches are rather low (1 = perfect match).\n\nThe main reason for this is that our image stack contains two neurons (the left and the right version) so half of our\n`query` won't have a match in any of the individual hemibrain neurons. We could have fixed that by subsetting the query\nto the approximate hemibrain bounding box. This is also a good idea for bilateral neurons that have parts of their\narbour outside the hemibrain volume:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remove the left-hand-side neuron based on the position\nalong the x-axis (this is just one of the possible approaches)\nThis is the approximate LHS boundary of the volume\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "flybrains.JRCFIB2018Fraw.mesh.vertices[:, 0].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "query_ss = navis.subset_neuron(query_xf, query_xf.points[:, 0] <= 35_000)\nquery_ss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using `query_ss` should yield much improved scores.\n\nAnother potential pitfall is the generation of dotprops from the image itself: if you compare the image- against the\nskeleton-derived dotprops, you might notice that the latter have fewer and less dense points. That's a natural\nconsequence the image containing multiple individuals of the same cell type but we could have tried to ameliorate\nthis by some pre-processing (e.g. downsampling or thinning the image).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}