{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nSkeletons from light-level data\n===============================\n\nThis tutorial will show you how to extract skeletons from confocal microscopy stacks.\n\n!!! important \"This example is not executed\"\n    In contrast to almost all other tutorials, this one is not executed when the documentation is built.\n    Consequently, it also does not display any actual code output or plots - images shown are statically\n    embedded. The main reason for this is that the example requires downloading a large-ish file which\n    is a pain in the neck to get to work in the CI environment.\n\nExtracting neuron skeletons from microscopy data is a common but non-trivial task. There are about\nas many ways to do this as there are people doing it - from fully manual to fully automated tracing.\n\nIn this tutorial, we will show you a fully automated way using a number of easy-to-install Python\npackages. If this isn't for you, check out the [Alternatives](#alternatives) section at the end of this tutorial.\n\n## Requirements:\n\nPlease make sure you have the following packages installed:\n\n- [`pynrrd`](https://github.com/mhe/pynrrd) to load image stacks\n  ```shell\n  pip install pynrrd -U\n  ```\n- [`connected-components-3d`](https://github.com/seung-lab/connected-components-3d) (cc3d) to label connected components\n  ``` shell\n  pip install connected-components-3d -U\n  ```\n- [`kimimaro`](https://github.com/seung-lab/kimimaro) to extract the skeletons\n  ```shell\n  pip install kimimaro -U\n  ```\n\n## Preparing the data\n\nThe pipeline we're using here was designed for pre-segmented data, so there is little in the way\nof dealing with noisy data. Fortunately, the image stack we will use is exceptionally clean which\nmakes the skeletonization process very straightforward.\n\nIn practice, you may have to do some pre-processing to clean up your data before running the skeletonization.\nIf your run-of-the-mill thresholding, denoising, etc. doesn't cut it, you can also try more advanced\nsegmentation techniques.\n\nThere are various fairly easy-to-use tools available for this, e.g. [Ilastik](https://www.ilastik.org) (see the\n[pixel classification](https://www.ilastik.org/documentation/pixelclassification/pixelclassification) and\n[voxel segmentation](https://www.ilastik.org/documentation/voxelsegmentation/voxelsegmentation) tutorials) or\n[DeepImageJ](https://deepimagej.github.io/).\n\n### Download Image Stack\n\nAs example data, we will use a confocal stack from the [Janelia Split-Gal4 collection](https://splitgal4.janelia.org/cgi-bin/splitgal4.cgi).\nWe picked the [SS00731](https://flweb.janelia.org/cgi-bin/view_splitgal4_imagery.cgi?line=SS00731)\nline because it's already fairly clean as is and there are high-resolution stacks\nwith stochastic multi-color labeling of individual neurons available for download.\n\nScroll all the way to the bottom of the page and in the dropdown for the left-most image,\nselect \"Download H5J stack: Unaligned\".\n\n![download](../../../_static/lm_tut/download.png)\n\n### Convert to NRRD\n\nNext, we need to open this file in [Fiji/ImageJ](https://imagej.net/software/fiji/) to convert it to\na format we can work with in Python:\n\n1. Fire up Fiji/ImageJ\n2. Drag & drop the `SS00731-20140620_20_C5-f-63x-ventral-Split_GAL4-unaligned_stack.h5j` file into Fiji\n3. Go to \"Image\" -> \"Colors\" -> \"Split Channels\" to split the image into the channels\n4. Discard all but the red \"C1\" channel with our neurons\n5. Go to \"Image\" -> \"Type\" -> \"8-bit\" to convert the image to 8-bit (optional but recommended)\n6. Save via \"File\" -> \"Save As\" -> \"NRRD\" and save the file as `neuron.nrrd`\n\n![Z stack](../../../_static/lm_tut/C1.gif)\n\n## Extracting the Skeleton\n\nNow that we have that file in a format we can load it into Python, we can get started:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import kimimaro\nimport nrrd\nimport navis\nimport cc3d\nimport numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First load the image stack:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# `im` is numpy array, `header` is a dictionary\nim, header = nrrd.read(\n    \"neuron.nrrd\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we need to find some sensible threshold to binarize the image. This is not strictly\nnecessary (see the further note down) but at least for starters this more intuitive.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Threshold the image\nmask = (im >= 20).astype(np.uint8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can inspect the mask to see if the thresholding worked as expected:\n```python\nimport matplotlib.pyplot as plt\nplt.imshow(mask.max(axis=2))\n```\n\nWith the `octarine` backend, you can also visualize the volume in 3D:\n```python\n# spacing can be found in the `header` dictionary\nimport octarine as oc\nv = oc.Viewer()\nv.add_volume(mask, spacing=(.19, .19, .38))\n```\n\n![mask](../../../_static/lm_tut/mask.png)\n\nA couple notes on the thresholding:\n\n- feel free to test the thresholding in e.g. ImageJ/Fiji\n- remove as much background as possible without disconnecting neurites\n- perfection is the enemy of progress: we can denoise/reconnect during postprocessing\n\nNext, we we need to label the connected components in the image:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract the labels\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "labels, N = cc3d.connected_components(mask, return_N=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the labels:\n```python\nimport cmap\nimport octarine as oc\nv = oc.Viewer()\nv.add_volume(labels, spacing=(.19, .19, .38), color=cmap.Colormap('prism'))\n```\n\n![labels](../../../_static/lm_tut/labels.png)\n\n!!! experiment\n    `cc3d.connected_component` also works with non-thresholded image - see the `delta` parameter.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Collect some statistics\nstats = cc3d.statistics(labels)\n\nprint(\"Total no. of labeled componenents:\", N)\nprint(\"Per-label voxel counts:\", np.sort(stats[\"voxel_counts\"])[::-1])\nprint(\"Label IDs:\", np.argsort(stats[\"voxel_counts\"])[::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nTotal no. of labeled componenents: 37836\nPer-label voxel counts: [491996140    527374    207632 ...         1         1         1]\nLabel IDs: [    0  6423  6091 ... 22350 22351 18918]\n```\n\nNote how label `0` has suspiciously many voxels? That's because this is the background label.\nWe need to make sure to exlude it from the skeletonization process:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "to_skeletonize = np.arange(1, N)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can run the actual skeletonization!\n\n!!! note \"Skeletonization paramters\"\n    There are a number of parameters that are worth explaining\n    first because you might want to tweak them for your data:\n\n    - `scale` & `const`: control how detailed your skeleton will be: lower = more detailed but more noise\n    - `anisotropy`: controls the voxel size - see the `header` dictionary for the voxel size of our image\n    - `dust_threshold`: controls how small connected components are skipped\n    - `object_ids`:  a list of labels to process (remember that we skipped the background label)\n    - `max_path`: if this is set, the algorithm will only process N paths in each skeleton - you can use\n      this to finish early (e.g. for testing)\n\n    See the [`kimimaro` repository](https://github.com/seung-lab/kimimaro) for a detailed explanation\n    of the parameters!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "skels = kimimaro.skeletonize(\n    labels,\n    teasar_params={\n        \"scale\": 1.5,\n        \"const\": 1,  # physical units (1 micron in our case)\n        \"pdrf_scale\": 100000,\n        \"pdrf_exponent\": 4,\n        \"soma_acceptance_threshold\": 3.5,  # physical units\n        \"soma_detection_threshold\": 1,  # physical units\n        \"soma_invalidation_const\": 0.5,  # physical units\n        \"soma_invalidation_scale\": 2,\n        \"max_paths\": None,  # default None\n    },\n    object_ids=list(to_skeletonize), # process only the specified labels\n    dust_threshold=500,  # skip connected components with fewer than this many voxels\n    anisotropy=(0.19, .19, 0.38),  # voxel size in physical units\n    progress=True,  # show progress bar\n    parallel=6,  # <= 0 all cpu, 1 single process, 2+ multiprocess\n    parallel_chunk_size=1,  # how many skeletons to process before updating progress bar\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`skels` is a dictionary of `{label: cloudvolume.Skeleton}`. Let's convert these to {{ navis }} neurons:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Convert skeletons to NAVis neurons\nnl = navis.NeuronList([navis.read_swc(s.to_swc(), id=i) for i, s in skels.items()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the voxel sizes in `stats`, we can make an educated guess that label `6423` is one of our neurons.\nLet's visualize it in 3D:\n\n```python\nimport octarine as oc\nv = oc.Viewer()\nv.add_neurons(nl.idx[6423], color='r', linewidth=2, radius=False))\nv.add_volume(im, spacing=(.19, .19, .38), opacity=.5)\n```\n\n![stack animation](../../../_static/lm_tut/stack.gif)\n\nThis looks pretty good off the bat! Now obviously we will have the other large neuron (label `6091`)\nplus bunch of smaller skeletons in our NeuronList. Let's have a look at those as well:\n\n![all skeletons](../../../_static/lm_tut/all_skeletons.png)\n\nZooming in on `6091` you will see that it wasn't fully skeletonized: some of the branches are missing\nand others are disconnected. That's either because our threshold for the mask was too high (this neuron\nhad a weaker signal than the other) and/or we dropped too many fragments during the skeletonization process\n(see the `dust_threshold` parameter).\n\n![zoom in](../../../_static/lm_tut/zoom_in.png)\n\n## Alternatives\n\nIf the pipeline described in this tutorial does not work for you, there are a number of alternatives:\n\n1. [Simple Neurite Tracer](https://imagej.net/plugins/snt/index) is a popular ImageJ plugin for semi-automated tracing\n2. Folks at the Allen Institute for Brain Science have published a [protocol for reconstructing neurons](https://portal.brain-map.org/explore/toolkit/morpho-reconstruction/vaa3d-mozak)\n3. [NeuTube](https://neutracing.com/tutorial/) is an open-source software for reconstructing neurongs from fluorescence microscopy images\n\n## Acknowledgements\n\nThe packages we used here were written by the excellent Will Silversmith from the Seung lab in Princeton.\nThe image stack we processed is from the Janelia Split-Gal4 collection and was published as part of the\n[Cheong, Eichler, Stuerner, _et al._ (2024)](https://elifesciences.org/reviewed-preprints/96084v1) paper.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}