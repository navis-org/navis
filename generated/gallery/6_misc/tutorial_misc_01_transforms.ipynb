{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nTransformations\n===============\n\nThis tutorial will show you how to transform and mirror neurons.\n\n## Introduction\n\nAs of version `0.5.0`, {{ navis }} includes functions that let you transform and mirror spatial data (e.g. neurons).\nThis new functionality splits into high- and low-level functions. In this tutorial, we will start by exploring\nthe higher-level functions that most users will use and then take a sneak peak at the low-level functions.\n\nAt the moment, navis supports the following transform types:\n\n - [CMTK](https://www.nitrc.org/projects/cmtk/) warp transforms\n - [Hdf5](https://github.com/saalfeldlab/template-building/wiki/Hdf5-Deformation-fields) deformation fields\n - [Elastix](https://elastix.lumc.nl/) transforms\n - landmark-based thin-plate spline transforms\n - affine transforms\n\n\n### flybrains\n\nSince {{ navis }} brings the utility but does not ship with any transforms, we have to either generate those ourselves or\nget them elsewhere. Here, we will showcase the [flybrains](https://github.com/navis-org/navis-flybrains) library that provides\na number of different transforms directly to {{ navis }}. Setting up and registering your own custom transforms will be\ndiscussed further down.\n\nFirst, you need to get [flybrains](https://github.com/navis-org/navis-flybrains). Please follow the instructions to install\nand download the bridging registrations before you continue.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import flybrains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importing `flybrains` automatically registers the transforms with {{ navis }}. This in turn allows {{ navis }} to plot a\nsequence of bridging transformations to map between any connected template spaces.\n\n![Flybrain Bridging Graph](https://github.com/navis-org/navis-flybrains/blob/main/_static/bridging_graph.png?raw=true)\n\nIn addition to those bridging transforms, `flybrains` also contains mirror registrations (we will cover those later), meta data\nand meshes for the template brains:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This is the Janelia \"hemibrain\" template brain\nprint(flybrains.JRCFIB2018F)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import navis\nimport matplotlib.pyplot as plt\n\n# This is the hemibrain neuropil surface mesh\nfig, ax = navis.plot2d(flybrains.JRCFIB2018F, view=(\"x\", \"-z\"))\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can check the registered transforms like so:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "navis.transforms.registry.summary()\n\n# !!! note\n#     The documentation is built in an environment with a minimal number of transforms registered. If you have installed\n#     and imported `flybrains`, you should see a lot more than what is shown above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using ``xform_brain``\n\nFor high-level transforming, you will want to use [`navis.xform_brain`][]. This function takes a `source` and `target` argument\nand tries to find a bridging sequence that gets you to where you want. Let's try it out:\n\n!!! info\n    Incidentally, the example neurons that {{ navis }} ships with are from the Janelia hemibrain project and are therefore in\n    `JRCFIB2018raw` space (\"raw\" means uncalibrated voxel space which is 8x8x8nm for this dataset). We will be using those\n    but there is nothing stopping you from using the {{ navis }} interface with neuPrint (the tutorials on\n    [interfaces](../#interfaces)) to fetch your favourite hemibrain neurons and transform those.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load the example hemibrain neurons (JRCFIB2018raw space)\nnl = navis.example_neurons()\nnl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = navis.plot2d([nl, flybrains.JRCFIB2018Fraw], view=(\"x\", \"-z\"))\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's say we want these neurons in `JRC2018F` template space. Before we do the actual transform it's useful to quickly check above\nbridging graph to see what we expect to happen:\n\n??? info \"What is JRC2018F?\"\n    `JRC2018F` is a standard brain made from averaging over multiple fly brains. See\n    [Bogovic et al., 2020](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0236495) for details.\n\nHave a look at the bridging graph above: first, we know that we are starting in `JRCFIB2018Fraw` space. From there, it's two\nsimple affine transforms to go from voxels to nanometers and from nanometers to micrometers. Once we are in `JRCFIB2018Fum` space, we can use a\nHdf5 transform generated by the Saalfeld lab to map to `JRC2018F`. Note that the arrows in the bridging graph indicate the transforms'\nforward directions but they can all be inversed to traverse the graph.\n\nLet's give this a shot:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "xf = navis.xform_brain(nl, source=\"JRCFIB2018Fraw\", target=\"JRC2018F\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Painless, wasn't it? Let's see if it worked:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the transformed neurons and the JRC2018F template brain\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = navis.plot2d([xf, flybrains.JRC2018F], color=\"r\", view=(\"x\", \"-y\"))\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That worked like a charm! I highly recommend you read through the documentation for [`navis.xform_brain`][] and check out the parameters\nyou can use to fine-tune it.\n\n\n## Using ``mirror_brain``\n\nAnother useful type of transform is mirroring using [`navis.mirror_brain`][] to e.g. mirror neurons from the left to the right side of\na given brain. The way this works is this:\n\n1. Reflect coordinates about the midpoint of the mirror axis (affine transformation)\n2. Apply a warping transformation to compensate for e.g. left/right asymmetries\n\nFor the first step, we need to know the length of the mirror axis. This is why - similar to having registered transforms - we need to have\nmeta data about the template space (i.e. the bounding box) available to {{ navis }}.\n\nThe second step is optional. For example, `JRC2018F` and `JRC2018U` are templates generate from averaging multiple fly brains and are\ntherefore already mirror symmetrical, meaning we don't need the additional warping transform. `flybrains` does include some mirror transforms\nthough: e.g. for `FCWB`, `VNCIS1` or `JFRC2`!\n\nSince our neurons are already in `JRC2018F` space, let's try mirroring them:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mirrored = navis.mirror_brain(xf, template=\"JRC2018F\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = navis.plot2d(\n    [xf, mirrored, flybrains.JRC2018F], color=[\"r\"] * 5 + [\"g\"] * 5, view=(\"x\", \"-y\")\n)\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perfect! As noted above, this only works if the `template` is registered with {{ navis }} and if it contains info about its bounding box.\nIf you only have the bounding box at hand but no template brain, check out the lower level function [`navis.transforms.mirror`][].\n\n\n## Low-level functions\n\n### Adding your own transforms\nLet's assume you want to add your own transforms. There are four different transform types:\n\n- [`navis.transforms.affine.AffineTransform`][]\n- [`navis.transforms.cmtk.CMTKtransform`][]\n- [`navis.transforms.h5reg.H5transform`][]\n- [`navis.transforms.thinplate.TPStransform`][]\n\nTo show you how to use them, we will create a new thin plate spline transform using [`TPStransform`][navis.transforms.thinplate.TPStransform].\nIf you look at the bridging graph again, you might note the `\"FAFB14\"` template brain: it stands for `\"Full Adult Fly Brain\"` (the `14` is a\nversion number for the alignment). We will use landmarks to generate a mapping between this 14th and the previous 13th iteration.\n\nFirst we will grab the landmarks from the Saalfeld's lab [elm](https://github.com/saalfeldlab/elm) repository:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\n# These landmarks map betweet FAFB (v14 and v13) and a light level template\n# We will use only the v13 and v14 landmarks\nlandmarks_v14 = pd.read_csv(\n    \"https://github.com/saalfeldlab/elm/raw/master/lm-em-landmarks_v14.csv\", header=None\n)\nlandmarks_v13 = pd.read_csv(\n    \"https://github.com/saalfeldlab/elm/raw/master/lm-em-landmarks_v13.csv\", header=None\n)\n\n# Name the columns\nlandmarks_v14.columns = landmarks_v13.columns = [\n    \"label\",\n    \"use\",\n    \"lm_x\",\n    \"lm_y\",\n    \"lm_z\",\n    \"fafb_x\",\n    \"fafb_y\",\n    \"fafb_z\",\n]\n\nlandmarks_v13.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can use those landmarks to generate a thin plate spine transform:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from navis.transforms.thinplate import TPStransform\n\ntr = TPStransform(\n    landmarks_source=landmarks_v14[[\"fafb_x\", \"fafb_y\", \"fafb_z\"]].values,\n    landmarks_target=landmarks_v13[[\"fafb_x\", \"fafb_y\", \"fafb_z\"]].values,\n)\n# note: navis.transforms.MovingLeastSquaresTransform has similar properties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The transform has a method that we can use to transform points but first we need some data in `FAFB14` space:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Transform our neurons into FAFB 14 space\nxf_fafb14 = navis.xform_brain(nl, source=\"JRCFIB2018Fraw\", target=\"FAFB14\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's see if we can use the v14:octicons-arrow-right-24:v13 transform:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Transform the nodes of the first two neurons\npts_v14 = xf_fafb14[:2].nodes[[\"x\", \"y\", \"z\"]].values\npts_v13 = tr.xform(pts_v14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quick check how the v14 and v13 coordinates compare:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Original in black, transformed in red\nfig, ax = navis.plot2d(pts_v14, scatter_kws=dict(c=\"k\"), view=(\"x\", \"-y\"))\n_ = navis.plot2d(pts_v13, scatter_kws=dict(c=\"r\"), ax=ax, view=(\"x\", \"-y\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So that did... something. To be honest, I'm not sure what to expect for the FAFB 14:octicons-arrow-right-24:13 transform but let's assume this is correct and move on.\n\nNext, we will register this new transform with {{ navis }} so that we can use it with higher level functions:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Register the transform\nnavis.transforms.registry.register_transform(\n    tr, source=\"FAFB14\", target=\"FAFB13\", transform_type=\"bridging\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that's done we can use `FAFB13` with [`navis.xform_brain`][]:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Transform our neurons into FAFB 14 space\nxf_fafb13 = navis.xform_brain(xf_fafb14, source=\"FAFB14\", target=\"FAFB13\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = navis.plot2d(xf_fafb14, c='k', view=(\"x\", \"-y\"))\n_ = navis.plot2d(xf_fafb13, c='r', ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Registering Template Brains\n\nFor completeness, lets also have a quick look at registering additional template brains.\n\nTemplate brains are represented in navis as [`navis.transforms.templates.TemplateBrain`][] and there is currently no canonical way of\nconstructing them: you can associate as much or as little data with them as you like. However, for them to be useful they should have\na `name`, a `label` and a `boundingbox` property.\n\nMinimally, you could do something like this:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Construct template brain from base class\nmy_brain = navis.transforms.templates.TemplateBrain(\n    name=\"My template brain\",\n    label=\"my_brain\",\n    boundingbox=[[0, 100], [0, 100], [0, 100]],\n)\n\n# Register with navis\nnavis.transforms.registry.register_templatebrain(my_brain)\n\n# Now you can use it with mirror_brain:\nimport numpy as np\n\npts = np.array([[10, 10, 10]])\npts_mirrored = navis.mirror_brain(pts, template=\"my_brain\")\n\n# Plot the points\nfig, ax = plt.subplots()\nax.scatter(pts[:, 0], pts[:, 1], c=\"k\", alpha=1, s=50, label=\"Original\")\nax.scatter(\n    pts_mirrored[:, 0], pts_mirrored[:, 1], c=\"r\", alpha=1, s=50, label=\"Mirrored\"\n)\nax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While this is a working solution, it's not very pretty: for example, `my_brain` does have the default docstring and no fancy string\nrepresentation (e.g. for `print(my_brain)`). I highly recommend you take a look at how [flybrains](https://github.com/navis-org/navis-flybrains)\nconstructs and packages the templates.\n\n## Acknowledgments\n\nMuch of the transform module is modelled after functions written by Greg Jefferis for the [natverse](http://natverse.org). Likewise,\n[flybrains](https://github.com/navis-org/navis-flybrains) is a port of data collected by Greg Jefferis for `nat.flybrains` and `nat.jrcbrains`.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}