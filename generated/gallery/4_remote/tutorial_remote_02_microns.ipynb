{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nThe MICrONS Datasets\n====================\n\nIn this tutorial we will explore the MICrONS datasets.\n\nThe [Allen Institute for Brain Science](https://alleninstitute.org/) in collaboration with Princeton University,\nand Baylor College of Medicine released two large connecotmics dataset:\n\n1. A \"Cortical mm<sup>3</sup>\" of mouse visual cortex. This one is broken into two portions: \"65\" and \"35\"\n2. A smaller \"Layer 2/3\" dataset of mouse visual cortex.\n\nAll of these can be browsed via the [MICrONS Explorer](https://www.microns-explorer.org/) using neuroglancer.\nThese data are public and thanks to the excellent [`cloud-volume`](https://github.com/seung-lab/cloud-volume)\nand [`caveclient`](https://github.com/CAVEconnectome/CAVEclient) libraries, developed by\nWilliam Silversmith, Forrest Collman, Sven Dorkenwald, Casey Schneider-Mizell and others, we can easily fetch\nneurons and their connectivity.\n\nFor easier interaction, {{ navis }} ships with a small interface to these datasets. To use it, we will have to\nmake sure `caveclient` (and with it `cloud-volume`) is installed:\n\n```shell\npip install caveclient cloud-volume -U\n```\n\nThe first time you run below code, you might have to get and set a client secret. Simply follow the instructions\nin the terminal and when in doubt, check out the section about authentication in the\n[`caveclient` docs](https://caveconnectome.github.io/CAVEclient/tutorials/authentication/).\n\nLet's get started:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import navis\nimport navis.interfaces.microns as mi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You will find that most functions in the interface accept a `datastack` parameter. At the time of writing, the available stacks are:\n\n - `cortex65` (also called \"minnie65\") is the anterior portion of the cortical mm<sup>3</sup> dataset\n - `cortex35` (also called \"minnie35\") is the (smaller) posterior portion of the cortical mm<sup>3</sup> dataset\n - `layer 2/3` (also called \"pinky\") is the earlier, smaller cortical dataset\n\nIf not specified, the default is `cortex65`. Both `cortex65` and `cortex35` always map to the most recent version of that dataset.\nYou can use [`get_datastacks`](navis.interfaces.microns.get_datastacks) to see all available datastacks:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mi.get_datastacks()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's start with some basic queries using the `caveclient` directly:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Initialize the client for the 65 part of cortical mm^3 (i.e. \"Minnie\")\nclient = mi.get_cave_client(datastack=\"cortex65\")\n\n# Fetch available annotation tables\nclient.materialize.get_tables()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These are the available public tables which we can use to fetch meta data. Let's check out `baylor_log_reg_cell_type_coarse_v1`.\nNote that there is also a `baylor_gnn_cell_type_fine_model_v2` table which contains more detailed cell types.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Get cell type table\nct = client.materialize.query_table(\"baylor_log_reg_cell_type_coarse_v1\")\nct.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ct.cell_type.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!!! important\n    Not all neurons in the dataset have been proofread. In theory, you can check if a neuron has been proofread using the\n    corresponding annotation table:\n    ```python\n    table = client.materialize.query_table('proofreading_status_public_release')#\n    fully_proofread = table[\n          table.status_dendrite.isin(['extented', 'clean']) &\n          table.status_axon.isin(['extented', 'clean'])\n      ].pt_root_id.values\n    ```\n    However, it appears that the proofreading status table may be outdated at the moment.\n\nLet's fetch one of the excitatory neurons:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n = mi.fetch_neurons(\n    ct[ct.cell_type == \"excitatory\"].pt_root_id.values[0], with_synapses=False\n)[0]\nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!!! note \"Neuron IDs\"\n    The neuron IDs in MICrONS are called \"root IDs\" because they represent collections of supervoxels - or rather\n    hierarchical layers of chunks of which the lowest layer are supervoxel IDs.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MICrONS neurons can be fairly large, i.e. have lots of faces. You can try using using a higher `lod` (\"level of detail\", higher = coarser)\nbut not all datastacks actually support multi-resolution meshes. If they don't (like this one) the `lod` parameter is silently ignored.\n\nFor visualization in this documentation we will simplify the neuron a little. For this, you need either\n`open3d` (`pip3 install open3d`), `pymeshlab` (`pip3 install pymeshlab`) or Blender 3D on your computer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Reduce face counts to 1/3 of the original\nn_ds = navis.simplify_mesh(n, F=1 / 3)\n\n# Inspect (note the lower face/vertex counts)\nn_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the downsample neuron (again: the downsampling is mostly for the sake of this documentation)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "navis.plot3d(\n    n_ds,\n    radius=False,\n    color=\"r\",\n    legend=False,  # hide the legend (more space for the plot)\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nice! Now let's run a bit of analysis.\n\n## Sholl Analysis\n\nSholl analysis is a simple way to quantify the complexity of a neuron's arbor. It counts the number of intersections\na neuron's arbor makes with concentric spheres around a center (typically the soma). The number of intersections is\nthen plotted against the radius of the spheres.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\n# The neuron mesh will automatically be skeletonized for this analysis\n# Note: were defining radii from 0 to 160 microns in 5 micron steps\nsha = navis.sholl_analysis(n, center=\"soma\", radii=np.arange(0, 160_000, 5_000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the results\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\nsha.intersections.plot(c=\"r\")\n\nax.set_xlabel(\"radius [nm]\")\nax.set_ylabel(\"# of intersections\")\nax.patch.set_color((0, 0, 0, 0))  # Make background transparent\nfig.patch.set_color((0, 0, 0, 0))\n\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "See [`navis.sholl_analysis`][] for ways to fine tune the analysis. Last but not least a quick visualization with the neuron:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import Normalize\nfrom matplotlib.cm import ScalarMappable\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n# Plot one of the excitatory neurons\nfig, ax = navis.plot2d(n, view=(\"x\", \"y\"), figsize=(10, 10), c=\"k\", method=\"2d\")\n\ncmap = plt.get_cmap(\"viridis\")\n\n# Plot Sholl circles and color by number of intersections\ncenter = n.soma_pos\n# Drop the outer Sholl circles where there are no intersections\nnorm = Normalize(vmin=0, vmax=(sha.intersections.max() + 1))\nfor r in sha.index.values:\n    ints = sha.loc[r, \"intersections\"]\n    ints_norm = norm(ints)\n    color = cmap(ints_norm)\n\n    c = plt.Circle(center[:2], r, ec=color, fc=\"none\")\n    ax.add_patch(c)\n\n# Add colorbar\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n_ = plt.colorbar(\n    ScalarMappable(norm=norm, cmap=cmap), cax=cax, label=\"# of intersections\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Render Videos\n\nBeautiful data like the MICrONS datasets lend themselves to visualizations. For making high quality videos (and renderings)\nI recommend you check out the tutorial on navis' [Blender interface](../../../gallery/3_interfaces/tutorial_interfaces_02_blender).\nHere's a little taster:\n\n <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/wl3sFG7WQJc\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}